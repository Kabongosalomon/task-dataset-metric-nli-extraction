<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Long</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale datasets have successively proven their fundamental importance in several research fields, especially for early progress in some emerging topics. In this paper, we focus on the problem of visual speech recognition, also known as lipreading, which has received increasing interest in recent years. We present a naturally-distributed large-scale benchmark for lip-reading in the wild, named LRW-1000, which contains 1, 000 classes with 718, 018 samples from more than 2, 000 individual speakers. Each class corresponds to the syllables of a Mandarin word composed of one or several Chinese characters. To the best of our knowledge, it is currently the largest word-level lipreading dataset and also the only public large-scale Mandarin lip-reading dataset. This dataset aims at covering a "natural" variability over different speech modes and imaging conditions to incorporate challenges encountered in practical applications. It has shown a large variation in this benchmark in several aspects, including the number of samples in each class, video resolution, lighting conditions, and speakers' attributes such as pose, age, gender, and make-up. Besides providing a detailed description of the dataset and its collection pipeline, we evaluate several typical popular lip-reading methods and perform a thorough analysis of the results from several aspects. The results demonstrate the consistency and challenges of our dataset, which may open up some new promising directions for future work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual speech recognition, also known as lip-reading, is a task of recognizing the speech content in a video only based on visual information. It has been demonstrated that incorporating visual information in audio-based speech recognition systems can bring obvious performance improvements, especially in cases where multiple speakers are present or the acoustic signal is noisy <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>The common procedure of lip-reading involves two steps: analyzing motion information in the given image sequence and transforming this information into words or sentences. This procedure links lip-reading to two closely related fields: audio-based speech recognition and action recognition, both of which relies on a similar analyzation to an input sequence to obtain predicted results. However, currently there exists a large performance gap between lip-reading and these two closely related tasks. One main reason is that there were few * Authors make equal contributions to this work. large-scale lip-reading datasets in the past, which was likely a major obstacle to the progress in lip-reading.</p><p>Fortunately, with the development of deep learning technologies, some researchers have begun to collect large-scale data for lip-reading in recent years using deep learning tools. Existing public datasets can be divided into two categories: word-level dataset and sentence-level dataset. We focus on word-level lip-reading in this paper. One outstanding benchmark is the LRW <ref type="bibr" target="#b5">[6]</ref> dataset proposed in 2016, which has 500 classes and displays substantial diversity in speech conditions. In addition, all the videos in this dataset are of fixed size and length, which provides much convenience to the community. The best performance on this dataset in terms of Top-1 classification accuracy has reached as high as 83% in merely two years. Some other popular word-level lip-reading datasets include OuluVS <ref type="bibr" target="#b21">[22]</ref> and OuluVS2 <ref type="bibr" target="#b0">[1]</ref>, which were released in 2009 and 2015 respectively. There are 10 classes in both datasets and the state-of-the-art model has achieved an accuracy of more than 90% on both datasets. These exciting and encouraging results mark a significant and praiseworthy improvement in lip-reading. However, lipreading in natural or "in-the-wild" settings remains challenging due to the large variations in the practical real-world environment. Meanwhile, these appealing results also call for more challenging datasets to trigger new progresses and inspire novel ideas for lip-reading.</p><p>To this end, we collect a naturally-distributed large-scale dataset for lip-reading in the wild. The contributions in this paper are summarized as follows.</p><p>Firstly, we present a challenging 1, 000-class lip-reading dataset. Each class corresponds to the syllables of a Mandarin word which is composed of one or several Chinese characters. The labels are also provided in the format of English letters and so anyone who knows English could understand and use the data. In total, there are 718, 018 samples from more than 2000 speakers, with over 1 million Chinese character instances covering 286 Chinese syllables. To the best of our knowledge, this database is currently the largest word-level lip-reading dataset and also the only one large-scale Mandarin lip-reading dataset.</p><p>Secondly, our benchmark aims to provide naturallydistributed data to the community, highlighted by the follow- ing properties: (a) it contains large variations in speech conditions, including lighting conditions, resolution of videos, and speaker's attribute variations in pose, speech rate, age, gender, make-up and so on, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>; (b) some classes are allowed to contain more samples than some others, which is consistent with the actual case that some words indeed occur more frequently than others; (c) samples of the same word are not limited to a previously specified length range to allow different speech rates. These three properties make this dataset very consistent with practical settings.</p><p>Thirdly, we provide a comprehensive comparison of the current popular lip-reading methods and perform a detailed analysis of their performance in several different settings to analyze the effect of different factors on lip-reading, including the performance with respect to image scales, word's length, speaker's pose and the model capacity on naturallydistributed data. The results demonstrate the consistency and the challenges of our benchmark, which may lead to some new inspirations to the related research communities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we provide an overview of current wordlevel lip-reading datasets, followed by a survey of state-ofthe-art methods targeting at lip-reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Word-level Lip-reading Datasets</head><p>Some well-known word-level lip-reading datasets are summarized in <ref type="table" target="#tab_0">Table I</ref>. All these datasets have contributed greatly to the progress of lip-reading. In this part, we will give a brief review of these well-known datasets shown in the table.</p><p>AVICAR <ref type="bibr" target="#b12">[13]</ref> and AVLetters <ref type="bibr" target="#b15">[16]</ref> were proposed in 2004 and 2002 respectively and were widely used at an early period. The words in these two datasets are composed by 10 digits and 26 letters from 100 speakers and 10 speakers respectively. These two datasets has provided an important and initial impetus for early progress in automatic lipreading.</p><p>OuluVS <ref type="bibr" target="#b21">[22]</ref>, released in 2009, consists of 10 phrases spoken by 20 subjects with 817 sequences in total. This dataset provides cropped mouth region sequences, which brings much convenience to related researchers. However, the average number of samples in each class is merely 81.7, which is not enough to cover the various conditions in practical applications.</p><p>OuluVS2 <ref type="bibr" target="#b0">[1]</ref>, released in 2015, extends the number of subjects in OuluVS to 53. The speakers are recorded from five fixed different views: frontal, profile, 30 • , 45 • and 60 • . One major difference compared with AVLetters and OuluVS is that OuluVS2 contains several different viewpoints, which makes it more difficult than the above three datasets and is therefore widely used in previous lip-reading studies. However, the viewpoints are all fixed in this dataset, and also, there are few variations beyond the view conditions. LRW <ref type="bibr" target="#b5">[6]</ref>, an appealing large-scale lip-reading dataset released in 2016, contains 500 classes with more than a thousand speakers. The videos are no longer posed videos recorded in controlled lab environments as above, but are extracted from TV shows and thus cover a large variation of speech conditions. This remains a challenging dataset until now and has been widely used by most existing lip-reading methods. However, one pre-defining setting of this dataset is that all the words are ensured to have a roughly equal duration and each class is specified to contain roughly the same number of samples. This setting leads to a gap between the data and practical applications because word frequencies and speech rates are actually not uniform in the real world. We believe that if a model learned from data which has a natural diversity over these two points can still achieve good performance, it should also perform well when applied to practical applications.</p><p>Although there have been many English lip-reading datasets as listed above, there are very few Mandarin lipreading datasets available up to now. With the rapid development of scientific technologies, automatic lip-reading of any language would definitely catch more and more researchers' attention over time. Therefore, we hope LRW-1000 could fill a part of the gap for automatic lip-reading of Mandarin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lip reading Methods</head><p>Automated lip-reading has been studied in the computer vision fields for decades. Most early methods focus on designing appropriate hand-engineering features to obtain good representations. Some well-known features include the Discrete Cosine Transform (DCT), active appearance model (AAM), motion history image (MHI), Local Binary Pattern (LBP) and optical flow , to name a few. With the rapid development of deep learning technologies, more and more work began to perform end-to-end recognition with the help of deep neural networks (DNN). According to the types of the front-end network, modern lip-reading methods can be roughly divided into the following three categories.</p><p>(1) Fully 2D CNN based: Two-dimensional convolution has been proved successful in extracting representative features in image-based recognition tasks. With this inspiration, some early lip-reading work <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b8">[9]</ref> try to obtain a discriminative representation of each frame individually with some pretrained 2D CNN models, such as theVGGNet <ref type="bibr" target="#b3">[4]</ref> and residual networks <ref type="bibr" target="#b11">[12]</ref>. One representative work is the multi-tower structure proposed by Chung and Zisserman in <ref type="bibr" target="#b8">[9]</ref>, where each tower takes a single frame or a Tchannel image as input with each channel corresponding to a single frame in grayscale. The activations from all the towers are concatenated to produce the final representation of the whole sequence. This multi-tower structure has been proved effective by appealing results on the current challenging dataset LRW.</p><p>(2) Fully 3D CNN based: One direct reason for the wide use of 3D convolutional layers in lip-reading has much to do with the success of 3D CNN in action recognition <ref type="bibr" target="#b20">[21]</ref>. One popular method whose front-end network is completely based on 3D convolution is the LipNet model <ref type="bibr" target="#b1">[2]</ref>. It contains three 3D convolutional layers which transform the raw input video into spatial-temporal features and feed them to the following gated recurrent units (GRUs) to generate the final transcription. The effectiveness has been proved by its remarkable performance on the public dataset which has surpassed professional human lip-readers by a large margin.</p><p>(3) Mixture of 2D and 3D convolution: The regular 2D spatial convolutional layers have been proved to be effective in extracting discriminative features in the spatial domain, while spatio-temporal convolutional layers are believed to be able to better capture the temporal dynamics in a sequence. For this reason, some researchers have begun to combine the advantages of the two types to generate even stronger features. In <ref type="bibr" target="#b19">[20]</ref>, Stafylakis and Tzimiropoulos proposed to combine a spatio-temporal convolutional layer with a 2D residual network to produce the final representation of the sequence. It has achieved the state-of-the-art performance on LRW with an accuracy of 83%.</p><p>In this paper, we evaluate each of the above state-of-theart approaches on our proposed benchmark and present a detailed analysis of the results which may provide some inspirations for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATA CONSTRUCTION</head><p>In this section, we describe the pipeline for collecting and processing the LRW-1000 benchmark, as shown in <ref type="figure" target="#fig_1">Fig.  2</ref>. We first present the choice of television programs from which the dataset was created and then provide details of the data preprocessing procedures, which interleave automatic process with manual annotation and extra filtering efforts to make the data consistent for research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Program Selection and Data Collection</head><p>In our benchmark, all collected programs are either broadcast news or conversational programs with a focus on news and current events. To encourage the diversity of speakers and speech content, we select programs from both regional and national TV stations, covering a wide range of male and female TV presenters, guests, reporters and interviewees who speak Mandarin or dialectal Chinese. The final program list is composed by 26 broadcast sources with 51 programs and yields more than 500 hours of raw videos over the two-month data collection period. This large range endows the data with a nearly full coverage of commonly used words and a natural diversity in several aspects as in practical applications.</p><p>The broadcast collection described above is retrieved daily through an IPTV streaming service in China, hosted by Northeastern University. It produces 25 fps recordings in H.264 encoding, with 1.5 to 7.5Mbps video bitrates and 128 to 160Kbps audio bitrates. The video resolution is 1920 × 1080 for high-definition channels and 1024 × 576 for standard-definition channels. This makes our data cover a wide range of scales. Since the source videos were recorded through cable TV and re-encoded in real-time, they may contain temporal discontinuities which appear as frozen frames or artifacts. We clip each video up to the first occurrence of such abnormality and feed the obtained segment to subsequent procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Shot Boundary Detection</head><p>We firstly employ a shot boundary detector by comparing the color histograms of adjacent frames. Within each detected shot, we choose three evenly spaced frames and perform face detection with a multi-view face detector in  the SeetaFaceEngine2 toolkit <ref type="bibr" target="#b18">[19]</ref>. If none of them contains a face larger than 20 × 20 pixels, we dismiss the shot as not containing any potential speakers. What is worth noting is that although we deliberately set a low minimum size of the candidate faces to closely mimic the in-the-wild setting, statistics still show that there are very few samples with lip resolution below 20 × 20, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Annotations, Face Detection, and Face Tracking</head><p>Most Chinese TV programs have no verbatim subtitles, so we create rough transcripts of the videos with the commercial iFLYREC speech recognition service, time-aligned at the sentence level. This process automatically detects voiced segments in the audio track and diarizes it by different speakers. We then isolate sentences which are within shots retained in the previous stage, and manually annotate each video clip with the active speaker's position, gender, exact endpoints of the speech, and also the speech content. Finally, to further refine the manually-checked text annotations, a more robust ASR tool by iFLYTEK is used to produce very faithful transcripts of the utterance which are compared again with the manually checked transcripts. After several rounds of interleaved mannual and automatic check, the final annotation is believed to be accurate enough for the final use.</p><p>To associate each utterance with the corresponding speaker's face, we use the landmark detector in SeetaFaceEngine2 on the first frame and check by comparing the coordinates of each detected face with the manual annotation. Then, a kernelized correlation filter (KCF) tracker is utilized to the selected face in the given duration to obtain the whole speaking sequence. During the tracking process, we perform automatic validation of the tracking quality every 15 frames with the CNN-based face detector in SeetaFaceEngine2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Audio-to-Video Synchronization</head><p>After the above process, we check for the synchronization issues and find that similar to <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b4">[5]</ref>, the audio and video streams in a few collected videos may be out of sync, with the largest offset being less than one second. To tackle this problem, we introduce the SyncNet model in <ref type="bibr" target="#b6">[7]</ref>, which extracts visual features from 5 frames of cropped faces using a 3D VGG-M network and computes their distance to the MFCC-based audio features. The model searches for offsets within ±15 frames, attempting to minimize the distance between the two features so that the two modalities are synchronized. We run the model over all the extracted utterances from each video and average the distances across these samples. If the determined offset is greater than ±7 frames in any clip or the samples do not reach a consent, we will perform shifting of the video stream manually to obtain the final synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Facial Landmark Detection and Mouth Region Extraction</head><p>At this stage, we have obtained face tracks of individuals speaking, as well as synchronized audio with corresponding transcripts. The next step is to extract the mouth regions. We first detect facial landmarks with the SeetaFaceEngine2 toolkit. Using these landmarks, the detected faces are first rotated so that the eyes are barely on a horizontal line. Then, a square mouth-centered RoI is extracted for each frame. To account for the yaw variations, the size of the RoI is set to the horizontal distance between the two mouth corners extended by an empirically determined factor of 0.12, or twice the distance between the nose tip and the center of the mouth (d M N ), whichever is larger. However, this crop sometimes extends beyond the desired region for extremely small faces, so we restrict the size of the region to be no more than 3.2d M N .</p><p>In other words, the size of a RoI bounding box is determined by</p><formula xml:id="formula_0">w = min{3.2d M N , max{2d M N , 1.12x r − 0.88x l }},</formula><p>where x l and x r are the x coordinates of the left and right mouth corners. Finally, to smooth the resulting boxes, we apply a first-order Savitzky-Golay filter with window length 3 to the estimated face rotations, the coordinates of the x and y centers, and the size of the ROIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Validating the Extracted RoIs</head><p>On some extremely challenging videos where the yaw and pitch angles are large, the landmark predictor fails and the extracted RoIs are inaccurate or even wrong. We train a binary CNN classifier to remove these non-lip images from the dataset. We begin the training process by using the initial unfiltered crops as positive samples and generate negative samples by shifting the crop region randomly in the original frame. After convergence, we filter the dataset using the trained model and fine-tune on the resulting subset. The trained model has a high recall (e.g. it easily picks up glasses at the top corner, and sometimes fails on profile views and low-resolution images, which are scarcer in the dataset), so we ask a human annotator to revise the inference results and remove false alarms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASET STATISTICS</head><p>LRW-1000 is challenging due to its large variations in scale, resolution, background clutter, and speaker's attributes including pose, age, gender, make-up and so on. They are all important factors to consider when building a robust and practical lip-reading system. To facilitate the study of lipreading, we provide cropped lip images and so users don't have to struggle with the trivial and cumbersome details of preprocessing. To quantify the properties of the datasets, we perform a comprehensive analysis based on the statistics of several aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Source Videos</head><p>We select 51 television programs with 840 videos in total, where each raw video has a variable duration of 20 minutes to 2 hours. All the programs fall within the class of news and current affairs. Because different programs always have different broadcasters, we split all the videos of a single program into only one subset of the train, test, and validation set to ensure that there are no or few overlapped speakers among train, test and validation set. In summary, there are 840 videos with about 508 hours' duration in total. The principle of splitting train/test/validation follows two points: (a) there are no or few overlapped speakers in these three sets; (b) the total duration of these three sets follows a ratio of about 8 : 1 : 1, which means the number of samples in these three sets follows a similar ratio round 8 : 1 : 1.</p><p>Considering the above two points, we finally select 634 videos of 44 programs with more than 415 hours for training, 84 videos of 4 programs with 43.4 hours for test, and 122 videos of 3 programs with 48.95 hours for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Word Samples</head><p>In this subsection, we present the statistics about the word samples in this benchmark.</p><p>The final extracted samples in our benchmark have a duration of about 57 hours with 718, 018 video clips in total, which are selected from the above 840 raw videos. On the average, each class has about 718 samples which makes it adequate to learn deep models. The minimum and maximum length of the data are about 0.01 seconds and 2.25 seconds respectively, with an average of about 0.3 seconds for each sample. This is reasonable as some words are indeed relatively short in our practical speaking process, especially the wake-up words in many speech-assistant systems. On the other hand, the abundance of such instances in real-world settings also suggests that they should not be overlooked in our research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Lip Region Resolution</head><p>Considering the diversity of scales of input videos in practical applications, we do not delete those sequences with small or large sizes. Instead, we collect these words according to their intrinsic natural distribution and found that there are indeed only few low or large sizes. Most of them contribute to some moderate value, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The two peaks in the figure are resulted by the two different types of the source videos: standard definition (SD) and high definition (HD). The existence of this case brings our benchmark much closer to practical applications which have a large resolution coverage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Speakers</head><p>There are more than 2, 000 speakers in the 840 videos used to construct our benchmark. The speakers are mostly interviewers, broadcasters, program guests, and so on. The large number and diversity in their identity equip the data with a broad coverage of age, pose, gender, accent, and personal speaking habits. These factors make the data very challenging for most existing lip-reading methods. We would evaluate the state-of-the-art word-level lip-reading models on our benchmark and the results should be very meaningful for designing practical lip-reading models. Among the multiple characteristics of speakers, we select pose as a statistical object because it is believed to be especially critical for the lip-reading task compared with other characteristics. We present the distribution of data in the pitch, yaw, and roll rotations respectively in <ref type="figure" target="#fig_3">Fig. 4</ref>. We can see that although we do not perform deliberate filtering, the data is still mainly comprised of frontal views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we present the evaluation results of popular lip-reading methods and give a detailed analysis to illustrate the characteristics and challenges of the proposed benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline Methods</head><p>We cast the word-level lip-reading task on our benchmark as a multi-class recognition problem and evaluate three popular methods on this dataset. Specifically, we evaluate three types of models with different types of front-end network: a fully 2D CNN based front-end, a fully 3D CNN based frontend and a front-end mixing 2D and 3D convolutional layers. Based on these three types of models, we hope to provide a relatively complete analysis and comparison of the currently popular methods.</p><p>The first network architecture in our experiments is the LSTM-5 network based on the multi-tower structure proposed in <ref type="bibr" target="#b8">[9]</ref>, which is completely composed of 2D convolutional layers. This structure has achieved an appealing performance on the public word-level dataset LRW <ref type="bibr" target="#b5">[6]</ref>. The second network is based on LipNet <ref type="bibr" target="#b1">[2]</ref> which contains only three spatio-temporal convolutional layers as the front-end. The third network is the model proposed in <ref type="bibr" target="#b19">[20]</ref> which contains a 3D convolutional layer cascaded with a residual network as the front-end network. During the experiments, the original LipNet consistently failed to converge. We believe that this is because this dataset is too complex to be learned by only three spatio-temporal layers. Therefore, we propose to transform the 2D DenseNet into a 3D counterpart and apply it as the fully 3D convolutional front-end. We named this model as D3D (DenseNet in 3D version), whose structure is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. These three models are abbreviated as "LSTM-5", "3D+2D" and "D3D" respectively in the experiments.</p><p>To perform a fair comparison, all the three models are combined with a back-end network of the same structure which contains a two-layer bi-directional RNN to perform the final recognition. The recurrent units used in our experiments are bidirectional Gated Recurrent Units. In the remainder of this section, we compare these three models side by side and also find some interesting observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Data Preprocessing:</head><p>In our experiments, all the images are converted to grayscale and normalized with respect to the overall mean and variance. When fed into the models, the frames in each sequence are cropped in the same random position for training and centrally cropped for validation and test. All the images are resized to a fixed size of 122 × 122 and then cropped to a size of 112 × 112. As an effective data augmentation step, we also randomly flip all the frames in the same sequence horizontally. To accelerate the training process, we divide the training process into two stages. In the first stage, we choose shorter sequences with a length below 30, allowing a larger batch size for training. Then we add the remaining sequences to the training set when the models exhibit a tendency of convergence. We also randomly repeat some samples in the the training process to further accelerate the convergence.</p><p>2) Parameters Settings: Our implementation is based on PyTorch and the models are trained on servers with four NVIDIA Titan X GPUs, with 12GB memory of each one. We use the Adam optimizer with an initial learning rate of 0.001, with β = (0.9, 0.99). All the networks are pretrained on LRW. During the training process, we apply dropout with probability 0.5 to the last layer of each model to prevent the model from being trapped in some local optima for the LRW dataset.</p><p>3) Evaluation Protocols: We provide two evaluation metrics in our experiments. The recognition accuracy over all 1, 000 classes is naturally considered as the base metric, since this is a classification task. Meanwhile, motivated by the large diversity of the data shown in many aspects, such as the number of samples in each class, we also provide the Kappa coefficient as a second evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Recognition Results</head><p>To evaluate the effects of different factors on lip-reading, we split the data into different difficulty levels according to the input scales (resolution), speaker's pose (degree), and the sample length (number of frames), as shown in <ref type="table" target="#tab_0">Table II</ref>. We now present a thorough comparison of the models on all three levels to obtain a complete and comprehensive analysis of the results. (1) General Performance: We show the results on LRW and LRW-1000 in <ref type="table" target="#tab_0">Table III and Table IV</ref> respectively. We can see that there is a similar trend of these three models in both LRW and LRW-1000. The method combining 3D convolution together with 2D convolution performs best on both datasets. The LSTM-5 architecture, which relies only on the 2D convolutional layers performs worse compared to the other two models. This is reasonable because 3D convolution has an advantage for capturing short-term motion information, which has been proved important in lip-reading. However, the network with a fully 3D front-end cannot surpass the model combining 2D and 3D convolutional layers. This result proves the necessity of 2D convolutional layers for extracting fine-grained features in the spatial domain, which is quite useful for discriminating words with similar lip movements. In addition, the performance gap between these three models on LRW-1000 is not too wide and the Top-1 accuracy ranges from 25.76% to 38.19% among the 1, 000 classes, which confirms both the challenges and the consistency of our data.  (2) Performance vs. Word Length: There is a small amount of samples with a relatively short duration in our benchmark, which can be used to roughly evaluate the performance of lip-reading models in extreme cases. The length is measured by the number of frames in our experiments. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref> and <ref type="table">Table V</ref>, all models perform similarly when the word has a relatively short duration. As the length of the word gradually increases, the performance of all three models becomes better and more stable, likely because the context included in a sample increases simultaneously with the word's length. One other possible reason is that the number of samples within classes of longer durations is larger than those of shorter durations.</p><p>(3) Performance vs. Input Scales: We evaluate the models on the three levels which are divided by resolution, as shown  in <ref type="table" target="#tab_0">Table II</ref>. Data with a resolution smaller than 50 × 50 falls in the hard level. Similarly, data with a resolution smaller than 100 × 100 and 150 × 150 fall in the medium level and the easy level, respectively. We can see that the performance of the models do tend to increase as we make a transition from the hard level to the medium level and from the medium level to the easy level. As shown in <ref type="table" target="#tab_0">Table II</ref> and <ref type="figure" target="#fig_6">Fig. 7</ref>, the results show that higher input resolution does indeed help improve the lip-reading performance, but the performance would stabilize when the input scale is above some value. On the other hand, the performance gap between these three settings are not wide and the accuracy is still close to 30% for the 1, 000 classes even in the hard-level, where all the test sequences have a resolution below 50 × 50. This result again demonstrates the consistency of our data which covers a large variation over input scales. (4) Performance vs. speaker pose: In this section, we evaluate the models under different poses measured by the yaw rotation. As shown in <ref type="figure" target="#fig_7">Fig. 8</ref> and <ref type="table" target="#tab_0">Table VII</ref>, the performance of all three models drops greatly as the yaw angle increases. This may pose a serious challenge to most current lip-reading models in real-world scenarios. When speakers are viewed from a large angle, there is too much occlusion in the lip region, making it hard to learn the patterns from the data. This significant drop of performance when the camera viewpoints shift from frontal to profile may point out a challenging direction worthy of deeper study.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we have proposed a large-scale naturallydistributed word-level benchmark, named LRW-1000, for lipreading in the wild. We have evaluated representative lipreading methods on our dataset to compare the effects of different factors on lip-reading. With this new dataset, we wish to present the community with some challenges of the lip-reading task -scale, pose and word duration variations. These factors are ubiquitous in many real-world applications and very challenging for current lip-reading models. We look forward to new exciting research results inspired by the benchmark and the corresponding results provided in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Examples of speakers in our dataset, which show a large variation of speech conditions, including lighting conditions, resolution, speaker's age, pose, gender, and make-up etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Pipeline to generate samples in our dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Scale distribution of the data, measured by the pixel-level width of the lip region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Pose distribution of the data in our benchmark, measured by angle degrees. Note that roll has been removed after preprocessing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>The proposed D3D network (DenseNet in 3D version).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Recognition accuracy across word lengths (number of frames).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Recognition accuracy across input scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Recognition accuracy across poses (measured by yaw rotation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I A</head><label>I</label><figDesc>SUMMARY OF EXISTING WELL-KNOWN WORD-LEVEL LIP-READING DATASETS</figDesc><table><row><cell>Datasets</cell><cell cols="2"># of Classes # of Speakers</cell><cell>Resolution</cell><cell>Pose</cell><cell>Envir.</cell><cell>Color/Gray</cell><cell>Year</cell><cell>Best Acc.</cell></row><row><cell>AVICAR [13]</cell><cell>10</cell><cell>100</cell><cell>-</cell><cell>Controlled</cell><cell>In-car</cell><cell>Gray</cell><cell cols="2">2004 37.9% [11]</cell></row><row><cell>AVLetters [16]</cell><cell>26</cell><cell>10</cell><cell>Fixed 80 × 60</cell><cell>Controlled</cell><cell>Lab</cell><cell>Gray</cell><cell cols="2">2002 65.13% [3]</cell></row><row><cell>OuluVS [22]</cell><cell>10</cell><cell>20</cell><cell>Fixed 80 × 60</cell><cell>Controlled</cell><cell>Lab</cell><cell>Color</cell><cell>2009</cell><cell>91.4% [6]</cell></row><row><cell>OuluVS2 [1]</cell><cell>10</cell><cell>53</cell><cell cols="2">Fixed (6 different sizes) Controlled</cell><cell>Lab</cell><cell>Color</cell><cell cols="2">2015 95.6% [18]</cell></row><row><cell>LRW [6]</cell><cell>500</cell><cell>&gt; 1000</cell><cell>Fixed 256 × 256</cell><cell>Natural</cell><cell>TV</cell><cell>Color</cell><cell cols="2">2016 83.0% [20]</cell></row><row><cell>LRW-1000</cell><cell>1000</cell><cell>&gt; 2000</cell><cell>Naturally distributed</cell><cell>Natural</cell><cell>TV</cell><cell>Color</cell><cell>2018</cell><cell>38.19%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PARTITION</head><label>II</label><figDesc>OF DIFFERENT DIFFICULTY LEVELS ON LRW-1000</figDesc><table><row><cell>Criterion</cell><cell>Easy</cell><cell cols="2">Medium Hard</cell></row><row><cell>Input Scale</cell><cell>≤150</cell><cell>≤100</cell><cell>≤50</cell></row><row><cell>Pose</cell><cell>≥20</cell><cell>≥40</cell><cell>≥60</cell></row><row><cell>Sample Length</cell><cell>≤30</cell><cell>≤15</cell><cell>≤5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RECOGNITION</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell>RESULTS ON LRW</cell></row><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>LSTM-5</cell><cell>66.0%</cell></row><row><cell>D3D</cell><cell>78.0%</cell></row><row><cell>3D+2D</cell><cell>83.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">RECOGNITION RESULTS ON LRW-1000</cell></row><row><cell>Method</cell><cell>Top-1</cell><cell>Top-5</cell><cell>Top-10</cell><cell>Kappa (Top-1)</cell></row><row><cell>LSTM-5</cell><cell>25.76%</cell><cell>48.74%</cell><cell>59.73%</cell><cell>0.24</cell></row><row><cell>D3D</cell><cell>34.76%</cell><cell>59.80%</cell><cell>69.81%</cell><cell>0.33</cell></row><row><cell>3D+2D</cell><cell>38.19%</cell><cell>63.50%</cell><cell>73.30%</cell><cell>0.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI PERFORMANCE</head><label>VI</label><figDesc>W.R.T INPUT SCALES ON LRW-1000</figDesc><table><row><cell>Method</cell><cell>Easy</cell><cell>Medium</cell><cell>Hard</cell><cell>All</cell></row><row><cell>LSTM-5</cell><cell>26.41%</cell><cell>24.38%</cell><cell>21.02%</cell><cell>25.76%</cell></row><row><cell>D3D</cell><cell>35.31%</cell><cell>32.98%</cell><cell>27.75%</cell><cell>34.76%</cell></row><row><cell>3D+2D</cell><cell>39.08%</cell><cell>36.07%</cell><cell>31.18%</cell><cell>38.19%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII PERFORMANCE</head><label>VII</label><figDesc>W.R.T POSE ON LRW-1000</figDesc><table><row><cell>Method</cell><cell>Easy</cell><cell>Medium</cell><cell>Hard</cell><cell>All</cell></row><row><cell>LSTM-5</cell><cell>17.03%</cell><cell>14.51%</cell><cell>11.6%</cell><cell>25.76%</cell></row><row><cell>D3D</cell><cell>23.31%</cell><cell>19.95%</cell><cell>15.78%</cell><cell>34.76%</cell></row><row><cell>3D+2D</cell><cell>24.89%</cell><cell>20.76%</cell><cell>15.9%</cell><cell>38.19%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was supported in part by the National Key R&amp;D Program of China (grant 2017YFA0700800), Natural Science Foundation of China (grants 61702486, 61876171). Shiguang Shan and Xilin Chen are corresponding co-authors of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ouluvs2: A multi-view audiovisual database for non-rigid mouth motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Anina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition,IEEE International Conference and Workshops on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Freitas</surname></persName>
		</author>
		<idno>abs/1611.01599:1-12</idno>
		<title level="m">Lipnet: Sentence-level lipreading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mkpls: Manifold kernel partial least squares for lipreading and speaker identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bakry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="684" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-view Lip-reading, ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lip reading in profile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to lip read words by watching videos. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audio-visual speech modeling for continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classification and feature extraction by simplexization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Info. For. Sec</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Avicar: Audiovisual speech corpus in a car environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Goudeseune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suketu</forename><surname>Kamdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Borys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2489" to="2492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lip reading using a dynamic feature of lip images and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ariki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer and Information Science</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extraction of visual features for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bangham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="213" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extraction of visual features for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bangham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="213" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lipreading using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiro</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hiroshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuya</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1149" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-toend multi-view lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seetaface</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seetafaceengine2</surname></persName>
		</author>
		<ptr target="https://github.com/seetaface" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Combining residual networks with lstms for lipreading. Interspeech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lipreading with local spatiotemporal descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1254" to="1265" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
