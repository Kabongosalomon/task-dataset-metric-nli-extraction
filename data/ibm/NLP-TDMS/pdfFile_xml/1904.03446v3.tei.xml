<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-12">12 Aug 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Wei</forename><surname>Gan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft STC Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
							<email>sheng.zhao@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft STC Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-12">12 Aug 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: grapheme-to-phoneme conversion</term>
					<term>knowledge distillation</term>
					<term>transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grapheme-to-phoneme (G2P) conversion is an important task in automatic speech recognition and text-to-speech systems. Recently, G2P conversion is viewed as a sequence to sequence task and modeled by RNN or CNN based encoderdecoder framework. However, previous works do not consider the practical issues when deploying G2P model in the production system, such as how to leverage additional unlabeled data to boost the accuracy, as well as reduce model size for online deployment. In this work, we propose token-level ensemble distillation for G2P conversion, which can (1) boost the accuracy by distilling the knowledge from additional unlabeled data, and (2) reduce the model size but maintain the high accuracy, both of which are very practical and helpful in the online production system. We use token-level knowledge distillation, which results in better accuracy than the sequence-level counterpart. What is more, we adopt the Transformer instead of RNN or CNN based models to further boost the accuracy of G2P conversion. Experiments on the publicly available CMU-Dict dataset and an internal English dataset demonstrate the effectiveness of our proposed method. Particularly, our method achieves 19.88% WER on CMUDict dataset, outperforming the previous works by more than 4.22% WER, and setting the new state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Grapheme-to-phoneme (G2P) conversion aims to generate a sequence of pronunciation symbols (phonemes) given a sequence of letters (graphemes), which is an important component in automatic speech recognition and text-to-speech systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> to provide accurate pronunciations for the words not covered by the lexicon. G2P conversion can be viewed as a sequence to sequence task and modeled by the encoder-decoder framework. <ref type="bibr" target="#b2">[3]</ref> adopt LSTM for G2P conversion and achieve improvements than the previous joint n-gram model <ref type="bibr" target="#b3">[4]</ref>. <ref type="bibr" target="#b4">[5]</ref> use convolutional sequence to sequence model and non-sequential decoding, and attain the previous best results on the public CMUDict dataset.</p><p>While previous works introduced the neural sequence to sequence models into G2P conversion and indeed achieved improvements over conventional methods, they did not take into account several practical issues of G2P conversion in the production system. First, considering training data is always costly through human labeling, how to further leverage the unlimited amount of unlabeled data is critical to improve the performance This work was done while the first author was an intern at Microsoft. of G2P conversion. Second, large or ensemble models are too costly to serve when deploying in the online systems. How to reduce the model size but maintain high accuracy is essential.</p><p>Inspired by the knowledge distillation in computer vision <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and natural language processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, in this work, we propose the token-level ensemble distillation for G2P conversion, to address the practical problems mentioned above. First, we use knowledge distillation to leverage the large amount of unlabeled words. Specifically, we train a teacher model to generate the phoneme sequence as well as its probability distribution given unlabeled grapheme sequence, and regard the unlabeled grapheme sequence and the generated phoneme sequence as pseudo labeled data, and add them into the original training data. Second, we train a variety of models (CNN, RNN and Transformer) for ensemble to get higher accuracy, and transfer the knowledge of the ensemble models to a light-weight model that is suitable for online deployment, again by knowledge distillation. Besides, we adopt Transformer <ref type="bibr" target="#b10">[11]</ref> instead of RNN or CNN as the basic encoder-decoder model structure, since it demonstrates advantages in a variety of sequence to sequence tasks, such as neural machine translation <ref type="bibr" target="#b10">[11]</ref>, text summarization <ref type="bibr" target="#b11">[12]</ref>, automatic speech recognition <ref type="bibr" target="#b12">[13]</ref>.</p><p>We conduct experiments on CMUDict 0.7b and our internal dataset, and also leverage additional unlabeled words crawled from the web. Our proposed method significantly boosts the accuracy of G2P conversion by 4.22% WER compared with the previous works. Specifically, Transformer model achieves higher accuracy than RNN and CNN based models, and tokenlevel distillation outperforms sequence-level distillation.</p><p>Our contributions are listed as follows: (1) We propose token-level ensemble distillation for grapheme-to-phoneme conversion. <ref type="bibr" target="#b1">(2)</ref> We are the first to use unlabeled words to boost the accuracy of grapheme-to-phoneme conversion, and also the first to introduce Transformer into this task and achieve better performance. (3) Our method achieves the state-of-the-art accuracy on CMUdict dataset, outperforming the previous best result by 4.22% WER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In this section, we briefly review the background of graphemeto-phoneme conversion, Transformer model, as well as knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Grapheme-to-Phoneme conversion</head><p>The G2P conversion is the process that generating the phoneme sequence (pronunciation) according to the grapheme sequence (word). G2P conversion is necessary and important as lexicon cannot cover all words, due to many words are long-tailed and a lot of new words and compound words appear. The spelling and pronunciation are not exactly corresponding for some languages, e.g. English. What is more, the alignments between graphemes and phonemes are complex. A grapheme may correspond to no phoneme, a single phoneme or many phonemes, as shown in <ref type="table" target="#tab_0">Table 1</ref>, which makes G2P a hard task.  <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> for G2P conversion. Recently, sequence to sequence models have achieved great success in machine translation task <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, and are soon applied on G2P conversion. <ref type="bibr" target="#b2">[3]</ref> demonstrated that sequence to sequence models outperform joint sequence n-gram models. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">21]</ref> combined joint n-gram models with Bi-LSTM models and achieved good performance in G2P conversion. <ref type="bibr" target="#b4">[5]</ref> adopted convolutional sequence to sequence model and proposed the non-sequential decoding <ref type="bibr" target="#b22">[22]</ref> for G2P conversion, which achieved the previous state-of-theart result on the public CMUDict 0.7b dataset.</p><p>While these sequence to sequence models achieve good performance on G2P conversion, there is still a gap when deploying online. In this work, we propose token-level ensemble distillation based on Transformer model, which can not only boost the accuracy of the G2P conversion with unlabeled words, but also reduce the model size for online deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformer</head><p>Transformer <ref type="bibr" target="#b10">[11]</ref> has achieved the state-of-the-art performance in many NLP tasks <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>. The encoder and decoder in Transformer has N identical layers, and each layer in encoder consists of two different sub-layers: multi-head self-attention and feed-forward network, while the decoder has an additional multi-head attention sub-layer. Multi-head attention is to perform the attention function h times in parallel, allowing the model to jointly attend to information from different representation subspaces at different positions. Residual connection is employed between each sub-layer. Transformer can better model the interactions between any two tokens in the sequence and the computation of each token in the encoder and decoder can be parallel during training, which shows advantages over the RNN based models. To the best of our knowledge, this is the first work to apply Transformer in G2P conversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Knowledge Distillation</head><p>Knowledge distillation was first introduced by <ref type="bibr" target="#b27">[27]</ref> for model compression, where a light student model can approximate the accuracy of a heavy and cumbersome teacher model. <ref type="bibr" target="#b5">[6]</ref> first applied knowledge distillation on neural networks, and then a lot of works expand the usage of knowledge distillation to a variety of tasks, such as image classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref> and natural language processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. In this work, we leverage knowledge distillation to distill the knowledge from additional unlabeled word, as well as from the ensemble models, both of which are beneficial for the online production system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Token-Level Ensemble Distillation</head><p>In this section, we propose the token-level ensemble knowledge distillation to boost the accuracy of G2P conversion, as well as reduce the model size for online deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Token-Level Knowledge Distillation</head><p>Denote D = {(x, y) ∈ X × Y} as the training corpus which consists of the paired grapheme and phoneme sequence. A G2P model based on sequence to sequence learning aims to minimize the negative log-likelihood loss on corpus D:</p><formula xml:id="formula_0">LNLL(θ) = − (x,y)∈D log P (y|x; θ),<label>(1)</label></formula><p>where the likelihood P (y | x; θ) can be factored by the chainrule and formulated as the cross-entropy between the one-hot label and per-token probability:</p><formula xml:id="formula_1">log P (y|x; θ) = Ty t=1 |V| k=1 1{yt = k} log P (yt = k|y&lt;t, x; θ),<label>(2)</label></formula><p>where Ty is the length of the target sequence, |V| is the vocabulary size of the phonemes, yt is the t-th target token in the phoneme sequence, and 1{·} is the indicator function indicating the id of the phoneme in vocabulary.</p><p>In token-level knowledge distillation, the one-hot label becomes the probability distribution output of the teacher model:</p><formula xml:id="formula_2">LKD(θ) = − (x,y)∈D Ty t=1 |V| k=1 Q(yt = k|y&lt;t, x; θT ) × log P (yt = k|y&lt;t, x; θ),<label>(3)</label></formula><p>where Q(yt = k|y&lt;t, x; θT ) is the probability distribution output of the teacher model θT .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ensemble Distillation with Diverse Models</head><p>Model ensemble can incorporate the advantages of individual models, and reduce the effect of overfitting in a spirit of the bagging method <ref type="bibr" target="#b30">[30]</ref>. However, the online production system cannot support large ensemble models for G2P conversion. Knowledge distillation is an effective way to distill the knowledge from strong ensemble models into single model. The ensemble distillation can be formulated as follows:</p><formula xml:id="formula_3">LKD(θ) = − (x,y)∈D Ty t=1 |V| k=1Q (yt = k|y&lt;t, x) × log P (yt = k|y&lt;t, x; θ),<label>(4)</label></formula><formula xml:id="formula_4">Q(yt = k|y&lt;t, x) = M m=1 Q(yt = k|y&lt;t, x; θ m T ) M ,<label>(5)</label></formula><p>whereQ is the probability distribution combined by M models (θ 1 T to θ m T ), which is simply the average of the probability distribution of M models at each step of the target sequence.</p><p>The performance of the individual models and the diversity between them are essential for ensemble. On the one hand, we train deeper models to achieve higher accuracy. On the other hand, we choose Transformer <ref type="bibr" target="#b10">[11]</ref>, Bi-LSTM <ref type="bibr" target="#b17">[18]</ref>, and convolutional sequence to sequence <ref type="bibr" target="#b31">[31]</ref> models to increase the diversity of ensemble models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Knowledge Distillation with Unlabeled Source Words</head><p>In G2P conversion, it is easy to obtain abundant unlabeled source words (graphemes) from lexicon corpus of news or wikipedia. Knowledge distillation gives a way of using unlabeled source data. The teacher model can generate the target phoneme sequence given the unlabeled source grapheme sequence, and the generated phoneme sequence can be used as the label for student model. What is more, more unlabeled data can help distill the knowledge of the teacher model to the student model. In this work, we also use token-level knowledge distillation for unlabeled source words. Denote D ′ = {x ∈ X } as the corpus of unlabeled source words. The knowledge distillation loss with unlabeled source words is as follows:</p><formula xml:id="formula_5">L ′ KD (θ) = − x∈D ′ T y ′ t=1 |V| k=1Q (y ′ t = k|y ′ &lt;t , x) × log P (y ′ t = k|y ′ &lt;t , x; θ),<label>(6)</label></formula><formula xml:id="formula_6">y ′ ∼Q(y|x)<label>(7)</label></formula><p>where y ′ is generated by the ensemble model (Equation <ref type="formula" target="#formula_6">7)</ref>, Q(y ′ t = k|y ′ &lt;t , x) is the probability distribution output of the ensemble model and is calculated by <ref type="bibr">Equation 5</ref>.</p><p>The total loss of our method is the weighted combination of the original negative log-likelihood loss and the knowledge distillation loss <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> on the labeled data, as well as the knowledge distillation loss on the unlabeled data:</p><formula xml:id="formula_7">LT OT AL(θ) = (1−λ)LNLL(θ)+λLKD(θ)+L ′ KD (θ),<label>(8)</label></formula><p>where each loss term is formulated in Equation 1, 4 and 6, λ is the weight to trade off between the two loss terms on labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>In this section, we conduct experiments to verify the effectiveness of the proposed method. We first introduce the datasets used, and then describe the implementation details. At last, we report the results of our method and conduct some comparisons and analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Datasets</head><p>We use two datasets to evaluate our proposed method: the first one is the publicly available CMUDict 0.7b and the other one is our internal dataset. For the public CMUDict 0.7b dataset, we use the same training/validation/test split (108952 training words, 5447 validation words and 12855 test words) as in <ref type="bibr" target="#b21">[21]</ref>, which is released in the CNTK toolkit <ref type="bibr" target="#b0">1</ref> . The sizes of the grapheme and phoneme vocabulary are 27 and 39 respectively. To be consistent with the previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">21]</ref>, stress markings are removed and the multiple pronunciations are retained. Our internal dataset contains 184243 training words, 10837 validation words, 21678 test words, which includes uppercase and lowercase letters and stress markings. We keep the stress markings in training and ignore the stress during test. The sizes of the grapheme and phoneme vocabulary in our internal dataset are 54 and 73 respectively. We train our models on the training set and select the best hyperparameters according to the validation set. We crawl nearly 2,000,000 unlabeled source words from the lexicon corpus of Google news 2 . As the crawled data contains words of other languages, unknown tokens and spelling errors, we first filter the data by removing the words with unknown tokens and then choose the top 300,000 unlabeled words according to their similarity to the training data 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Model Configurations</head><p>Ensemble Model We train the sequence to sequence based G2P models with different model structures for ensemble, including Transformer <ref type="bibr" target="#b10">[11]</ref>, Bi-LSTM <ref type="bibr" target="#b17">[18]</ref> and CNN based sequence to sequence model <ref type="bibr" target="#b31">[31]</ref>. We use 4 Transformer models, 3 CNN models and 3 Bi-LSTM models with different hyperparameters for ensemble, which give the best performance on the validation set. The 4 Transformer models share the same hidden size (256) but vary in the number of the encoder-decoder layers <ref type="bibr">(6-6, 6-4, 8-6, 8-4)</ref>. For the 3 CNN models, they share the same hidden size (256) but vary in the number of encoder-decoder layers (10-10, 10-10, 8-8) and convolutional kernel widths (3, 2, 2) respectively. For the 3 Bi-LSTM models, they share the same number of encoder-decoder layers (1-1), but with different hidden sizes (256, 384 and 512).</p><p>Student Model We choose Transformer as the student model and use the default configurations (256 hidden size and 6-6 layers of encoder-decoder) unless otherwise stated. We also vary the number of layers for the encoder and decoder to analyze and compare the accuracy and memory/time cost, which is essential for online deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Training and Evaluation</head><p>We implement experiments with the fairseq-py 4 library in Py-Torch. We use Adam optimizer for all models and follow the learning rate schedule in <ref type="bibr" target="#b10">[11]</ref>. The dropout is 0.3 for Bi-LSTM and CNN models, while the residual dropout, attention dropout and ReLU dropout for Transformer models is 0.2, 0.4, 0.4 respectively. We set the λ in Equation 8 to 0.9 according to the validation performance. We train each model on 8 NVIDIA M40 GPUs. Each GPU contains roughly 4000 tokens in one mini-batch. We use beam search during inference and set beam size to 10. We use WER (word error rate) and PER (phoneme error rate) to measure the accuracy of G2P conversion. Edit distance is used in PER calculation. In WER calculation, considering the multiple pronunciations, word error is counted only when the output differs from all the references, following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b32">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Analyses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Achieving State-Of-The-Art Accuracy</head><p>We first compare our method with previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">21]</ref> on CMUDict 0.7b dataset, as shown in <ref type="table" target="#tab_1">Table 2</ref>. Sequitur G2P <ref type="bibr" target="#b3">[4]</ref> is a well established G2P conversion tool using joint sequence modelling and is widely used as a baseline for comparison. <ref type="bibr" target="#b21">[21]</ref> used the ensemble of Bi-LSTM and joint n-gram model. The convolutional sequence to sequence model with non-sequential greedy decoding (NSGD) <ref type="bibr" target="#b4">[5]</ref> is the previous state-of-the-art on CMUDict 0.7b dataset <ref type="bibr" target="#b4">5</ref> . It can be seen that our method on 6-layer encoder and 6-layer decoder Transformer achieves the new state-of-the-art result of 19.88% WER, outperforming NSGD by 4.22% WER. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Reducing Model Size by 6x</head><p>Our method can also greatly reduce the model size for online deployment. We compare the WER, the number of parameters, and the inference speed between the baseline and our method, as shown in <ref type="table" target="#tab_2">Table 3</ref>. The baseline method just uses transformer model (6-6 layers of encoder-decoder) without leveraging the ensemble knowledge distillation and unlabeled source words.</p><p>To compare the inference speed, we use the time consumed by generating the outputs of the test set (12855 words) on a single M40 GPU with 12000 max tokens in one mini-batch. It can be seen from <ref type="table" target="#tab_2">Table 3</ref> that our method can still reach high accuracy with 1-1 layer of encoder-decoder, which can significantly reduce the model size by nearly 6 times and the time cost by nearly 4 times compared with the baseline model, but still achieving higher accuracy in terms of WER. The reduction in model size and inference time cost demonstrate the effectiveness of our method for online deployment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Analyses of Our Method</head><p>We first study the effect of distilling from unlabeled source words, as shown in <ref type="table" target="#tab_3">Table 4</ref>. It can be seen that unlabeled source words can boost the accuracy by nearly 1% WER, demonstrating the effectiveness by introducing abundant unlabeled data into knowledge distillation. We also compare token-level distillation with sequence-level distillation, where the student <ref type="bibr" target="#b4">5</ref> They use a training/validation/test split different from <ref type="bibr" target="#b21">[21]</ref> and ours. Therefore, we reproduce their work with on our training/ validation/test split, based on their public codebase (https://github. com/ctr4si/NSGD G2P), and get similar result as theirs. models are directly trained on the top-1 beam search results of the teacher network. As shown in <ref type="table" target="#tab_4">Table 5</ref>, the result demonstrate the advantage of token-level distillation. Furthermore, we study the effect of ensemble teacher model in knowledge distillation. As shown in <ref type="table" target="#tab_5">Table 6</ref>, the ensemble teacher model can boost the accuracy by more than 1% WER, compared with the single teacher model (a Transformer model with 6-layer encoder and 6-layer decoder), which demonstrates the strong ensemble teacher model is essential to guarantee the performance of student model in knowledge distillation. At last, we compare Transformer with RNN <ref type="bibr" target="#b21">[21]</ref> and CNN <ref type="bibr" target="#b4">[5]</ref> based models, without using knowledge distillation and unlabeled data, as shown in <ref type="table" target="#tab_6">Table 7</ref>. We can see that Transformer model outperforms the RNN and CNN based models used in previous works, demonstrating the advantage of Transformer model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Results on Our Internal Dataset</head><p>We compare our method with the previous state-of-the-art CNN with NSGD <ref type="bibr" target="#b4">[5]</ref> (which is reproduced by ourself) on our internal dataset, as shown in <ref type="table" target="#tab_7">Table 8</ref>. Our method outperforms CNN with NSGD by 3.52% WER, which demonstrates the effectiveness of our method for G2P conversion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have proposed the token-level ensemble distillation with unlabeled source words for G2P conversion. Experiments on the publicly available CMUDict 0.7b dataset and our internal dataset demonstrate the effectiveness of our method on both improving the accuracy of G2P conversion and reducing the model size for online deployment. For future work, we will leverage more unlabeled data and pre-training <ref type="bibr" target="#b33">[33]</ref> to improve the performance, and extend our work to other languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An example of the alignments between graphemes and phonemes.</figDesc><table><row><cell cols="2">graphemes B</cell><cell>U</cell><cell>B</cell><cell>B</cell><cell>L</cell><cell>E</cell></row><row><cell>phonemes</cell><cell cols="6">B AH null B AH:L null</cell></row><row><cell cols="7">Joint sequence n-gram models have been widely used</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison between our method and the previous works on CMUDict 0.7b dataset.</figDesc><table><row><cell>Method</cell><cell>PER</cell><cell>WER</cell></row><row><cell>Sequitur G2P [4]</cell><cell>6.12%</cell><cell>25.71%</cell></row><row><cell cols="2">Bi-LSTM + n-gram [21] 5.76%</cell><cell>24.88%</cell></row><row><cell>CNN with NSGD [5]</cell><cell>5.58%</cell><cell>24.10%</cell></row><row><cell>Our method</cell><cell cols="2">4.60% 19.88%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of WER, number of parameters and inference time between the baseline and our method.</figDesc><table><row><cell>Method</cell><cell>Layers</cell><cell>WER</cell><cell>Parameters</cell><cell>Time</cell></row><row><cell>Baseline</cell><cell>6-6</cell><cell cols="3">21.07% 11.09 millions 17.8s</cell></row><row><cell>Our method</cell><cell>1-1</cell><cell>20.25%</cell><cell>1.85 millions</cell><cell>4.4s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of our method with and without unlabeled source words.</figDesc><table><row><cell>Method</cell><cell>PER</cell><cell>WER</cell></row><row><cell cols="3">Without unlabeled data 4.78% 20.71%</cell></row><row><cell>With unlabeled data</cell><cell cols="2">4.60% 19.88%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison between token-level and sequence-level distillation.</figDesc><table><row><cell>Method</cell><cell>PER</cell><cell>WER</cell></row><row><cell cols="3">Sequence-level 4.71% 20.32%</cell></row><row><cell>Token-level</cell><cell cols="2">4.60% 19.88%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different teacher models for knowledge distillation.</figDesc><table><row><cell>Method</cell><cell>PER</cell><cell>WER</cell></row><row><cell>Single teacher model</cell><cell cols="2">4.93% 21.05%</cell></row><row><cell cols="3">Ensemble teacher model 4.60% 19.88%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison of Transformer, LSTM and CNN.</figDesc><table><row><cell>Method</cell><cell>PER</cell><cell>WER</cell></row><row><cell cols="3">Bi-LSTM + n-gram [21] 5.76% 24.88%</cell></row><row><cell>CNN with NSGD [5]</cell><cell cols="2">5.58% 24.10%</cell></row><row><cell>Transformer</cell><cell cols="2">4.96% 21.07%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Results on our internal dataset.</figDesc><table><row><cell>Method</cell><cell>PER</cell><cell>WER</cell></row><row><cell cols="2">CNN with NSGD [5] 3.79%</cell><cell>22.39%</cell></row><row><cell>Our method</cell><cell cols="2">3.04% 18.87%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Microsoft/CNTK/tree/master/Examples/Sequen ceToSequence/CMUDict/Data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/mmihaltz/word2vec-GoogleNews-vectors<ref type="bibr" target="#b2">3</ref> We use the distance between the 1/2/3-gram distribution of training words and unlabeled words, where the 1/2/3-gram means 1/2/3 consecutive characters.<ref type="bibr" target="#b3">4</ref> https://github.com/pytorch/fairseq</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Almost unsupervised text to speech and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fastspeech: Fast, robust and controllable text to speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09263</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence neural net models for grapheme-to-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00196</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint-sequence models for grapheme-tophoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="434" to="451" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence model with nonsequential greedy decoding for grapheme to phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kimt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2486" to="2490" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04770</idno>
		<title level="m">Born again neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07947</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ensemble distillation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sankaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01802</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilingual neural machine translation with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sentence-wise smooth regularization for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="210" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional and joint models for grapheme-tophoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Encoding linear models as weighted finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Achieving human parity on automatic chinese to english news translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05567</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Jointly learning to align and convert graphemes to phonemes with neural attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="76" to="82" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multitask sequence-tosequence models for grapheme-to-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Milde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2536" to="2540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Nonautoregressive neural machine translation with enhanced decoder input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09664</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attending to mathematical language with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wangperawong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02825</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The evolved transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11117</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Large scale distributed neural network training through online distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ormandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03235</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Knowledge distillation in generations: More tolerant teachers educate better students</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.05551</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ensemble methods in machine learning,&quot; in International workshop on multiple classifier systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grapheme-to-phoneme conversion using long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4225" to="4229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mass: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02450</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
