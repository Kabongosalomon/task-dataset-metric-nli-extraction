<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KBGAN: Adversarial Learning for Knowledge Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara Santa Barbara</addrLine>
									<postCode>93106</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KBGAN: Adversarial Learning for Knowledge Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce KBGAN, an adversarial learning framework to improve the performances of a wide range of existing knowledge graph embedding models. Because knowledge graphs typically only contain positive facts, sampling useful negative training examples is a nontrivial task. Replacing the head or tail entity of a fact with a uniformly randomly selected entity is a conventional method for generating negative facts, but the majority of the generated negative facts can be easily discriminated from positive facts, and will contribute little towards the training. Inspired by generative adversarial networks (GANs), we use one knowledge graph embedding model as a negative sample generator to assist the training of our desired model, which acts as the discriminator in GANs. This framework is independent of the concrete form of generator and discriminator, and therefore can utilize a wide variety of knowledge graph embedding models as its building blocks. In experiments, we adversarially train two translation-based models, TRANSE and TRANSD, each with assistance from one of the two probability-based models, DISTMULT and COMPLEX. We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR. Experimental results show that adversarial training substantially improves the performances of target embedding models under various settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graph <ref type="bibr" target="#b4">(Dong et al., 2014</ref>) is a powerful graph structure that can provide direct access of knowledge to users via various applications such as structured search, question answering, and intelligent virtual assistant. A common representation of knowledge graph beliefs is in the form of a discrete relational triple such as Locate-dIn <ref type="bibr">(NewOrleans,Louisiana)</ref>.</p><p>A main challenge for using discrete representation of knowledge graph is the lack of capability of accessing the similarities among different entities and relations. Knowledge graph embedding (KGE) techniques (e.g., <ref type="bibr">RESCAL</ref>  <ref type="bibr" target="#b15">(Nickel et al., 2011)</ref>, TRANSE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, DIST-MULT , and COMPLEX <ref type="bibr" target="#b19">(Trouillon et al., 2016)</ref>) have been proposed in recent years to deal with the issue. The main idea is to represent the entities and relations in a vector space, and one can use machine learning technique to learn the continuous representation of the knowledge graph in the latent space.</p><p>However, even steady progress has been made in developing novel algorithms for knowledge graph embedding, there is still a common challenge in this line of research. For space efficiency, common knowledge graphs such as Freebase <ref type="bibr" target="#b1">(Bollacker et al., 2008)</ref>, <ref type="bibr">Yago (Suchanek et al., 2007)</ref>, and <ref type="bibr">NELL (Mitchell et al., 2015)</ref> by default only stores beliefs, rather than disbeliefs. Therefore, when training the embedding models, there is only the natural presence of the positive examples. To use negative examples, a common method is to remove the correct tail entity, and randomly sample from a uniform distribution <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>. Unfortunately, this approach is not ideal, because the sampled entity could be completely unrelated to the head and the target relation, and thus the quality of randomly generated negative examples is often poor (e.g, Locate-dIn(NewOrleans,BarackObama)). Other approach might leverage external ontological constraints such as entity types <ref type="bibr" target="#b9">(Krompaß et al., 2015)</ref> to generate negative examples, but such resource does not always exist or accessible.</p><p>In this work, we provide a generic solution to improve the training of a wide range of knowl-   <ref type="bibr" target="#b5">(Goodfellow et al., 2014)</ref>, we propose a novel adversarial learning framework, namely, KBGAN, for generating better negative examples to train knowledge graph embedding models. More specifically, we consider probabilitybased, log-loss embedding models as the generator to supply better quality negative examples, and use distance-based, margin-loss embedding models as the discriminator to generate the final knowledge graph embeddings. Since the generator has a discrete generation step, we cannot directly use the gradient-based approach to backpropagate the errors. We then consider a onestep reinforcement learning setting, and use a variance-reduction REINFORCE method to achieve this goal. Empirically, we perform experiments on three common KGE datasets (FB15K-237, WN18 and WN18RR), and verify the adversarial learning approach with a set of KGE models. Our experiments show that across various settings, this adversarial learning mechanism can significantly improve the performance of some of the most commonly used translation based KGE methods. Our contributions are three-fold:</p><formula xml:id="formula_0">+ k)|R| MANIFOLDE (hyperplane) |(h + r head ) T (t + r tail ) − Dr| k|E| + (2k + 1)|R| RESCAL h T Wrt k|E| + k 2 |R| HOLE r T (h t) ( is circular correlation) k|E| + k|R| CONVE f (vec(f ([h;r] * ω))W)t k|E| + k|R| + kcmn</formula><p>• We are the first to consider adversarial learning to generate useful negative training examples to improve knowledge graph embedding.</p><p>• This adversarial learning framework applies to a wide range of KGE models, without the need of external ontologies constraints.</p><p>• Our method shows consistent performance gains on three commonly used KGE datasets.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Graph Embeddings</head><p>A large number of knowledge graph embedding models, which represent entities and relations in a knowledge graph with vectors or matrices, have been proposed in recent years. RESCAL <ref type="bibr" target="#b15">(Nickel et al., 2011)</ref> is one of the earliest studies on matrix factorization based knowledge graph embedding models, using a bilinear form as score function. TRANSE <ref type="bibr" target="#b2">(Bordes et al., 2013</ref>) is the first model to introduce translation-based embedding.</p><p>Later variants, such as TRANSH <ref type="bibr" target="#b21">(Wang et al., 2014)</ref>, TRANSR <ref type="bibr" target="#b10">(Lin et al., 2015)</ref> and TRANSD <ref type="bibr" target="#b6">(Ji et al., 2015)</ref>, extend TRANSE by projecting the embedding vectors of entities into various spaces. DISTMULT  simplifies RESCAL by only using a diagonal matrix, and COMPLEX <ref type="bibr" target="#b19">(Trouillon et al., 2016)</ref> extends DISTMULT into the complex number field. <ref type="bibr" target="#b13">(Nickel et al., 2015)</ref> is a comprehensive survey on these models. Some of the more recent models achieve strong performances. MANIFOLDE <ref type="bibr" target="#b23">(Xiao et al., 2016)</ref> embeds a triple as a manifold rather than a point. HOLE <ref type="bibr" target="#b14">(Nickel et al., 2016)</ref> employs circular correlation to combine the two entities in a triple. CONVE <ref type="bibr" target="#b3">(Dettmers et al., 2017)</ref> uses a convolutional neural network as the score function. However, most of these studies use uniform sampling to generate negative training examples <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>. Because our framework is independent of the concrete form of models, all these models can be potentially incorporated into our framework, regardless of the complexity. As a proof of principle, our work focuses on simpler models. Table 1 summarizes the score functions and dimensions of all models mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative Adversarial Networks and its Variants</head><p>Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) was originally proposed for generating samples in a continuous space such as images. A GAN consists of two parts, the generator and the discriminator. The generator accepts a noise input and outputs an image. The discriminator is a classifier which classifies images as "true" (from the ground truth set) or "fake" (generated by the generator). When training a GAN, the generator and the discriminator play a minimax game, in which the generator tries to generate "real" images to deceive the discriminator, and the discriminator tries to tell them apart from ground truth images. GANs are also capable of generating samples satisfying certain requirements, such as conditional GAN <ref type="bibr" target="#b11">(Mirza and Osindero, 2014)</ref>. It is not possible to use GANs in its original form for generating discrete samples like natural language sentences or knowledge graph triples, because the discrete sampling step prevents gradients from propagating back to the generator. SE-QGAN  is one of the first successful solutions to this problem by using reinforcement learning-It trains the generator using policy gradient and other tricks. <ref type="bibr">IRGAN</ref>   is a recent work which combines two categories of information retrieval models into a discrete GAN framework. Likewise, our framework relies on policy gradient to train the generator which provides discrete negative triples.</p><p>The discriminator in a GAN is not necessarily a classifier. Wasserstein GAN or WGAN <ref type="bibr" target="#b0">(Arjovsky et al., 2017</ref>) uses a regressor with clipped parameters as its discriminator, based on solid analysis about the mathematical nature of GANs. GOGAN (Juefei-Xu et al., 2017) further replaces the loss function in WGAN with marginal loss. Although originating from very different fields, the form of loss function in our framework turns out to be more closely related to the one in GOGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approaches</head><p>In this section, we first define two types of training objectives in knowledge graph embedding models to show how KBGAN can be applied. Then, we demonstrate a long overlooked problem about negative sampling which motivates us to propose KBGAN to address the problem. Finally, we dive into the mathematical, and algorithmic details of <ref type="bibr">KBGAN.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Types of Training Objectives</head><p>For a given knowledge graph, let E be the set of entities, R be the set of relations, and T be the set of ground truth triples. In general, a knowledge graph embedding (KGE) model can be formulated as a score function f (h, r, t), h, t ∈ E, r ∈ R which assigns a score to every possible triple in the knowledge graph. The estimated likelihood of a triple to be true depends only on its score given by the score function.</p><p>Different models formulate their score function based on different designs, and therefore interpret scores differently, which further lead to various training objectives. Two common forms of training objectives are particularly of our interest: Marginal loss function is commonly used by a large group of models called translation-based models, whose score function models distance between points or vectors, such as TRANSE, TRANSH, TRANSR, TRANSD and so on. In these models, smaller distance indicates a higher likelihood of truth, but only qualitatively. The marginal loss function takes the following form:</p><formula xml:id="formula_1">L m = (h,r,t)∈T [f (h, r, t) − f (h , r, t ) + γ] + (1)</formula><p>where γ is the margin, [·] + = max(0, ·) is the hinge function, and (h , r, t ) is a negative triple. The negative triple is generated by replacing the head entity or the tail entity of a positive triple with a random entity in the knowledge graph, or formally (h , r, t ) ∈ {(h , r, t)|h ∈ E} ∪ {(h, r, t )|t ∈ E}. Log-softmax loss function is commonly used by models whose score function has probabilistic interpretation. Some notable examples are RESCAL, DISTMULT, COMPLEX. Applying the softmax function on scores of a given set of triples gives the probability of a triple to be the best one among them:</p><formula xml:id="formula_2">p(h, r, t) = exp f (h,r,t) (h ,r,t ) exp f (h ,r,t ) .</formula><p>The loss function is the negative log-likelihood of this probabilistic model:</p><formula xml:id="formula_3">L l = (h,r,t)∈T − log exp f (h, r, t) exp f (h , r, t ) (h , r, t ) ∈ {(h, r, t)} ∪ N eg(h, r, t) (2)</formula><p>where N eg(h, r, t) ⊂ {(h , r, t)|h ∈ E} ∪ {(h, r, t )|t ∈ E} is a set of sampled corrupted triples. Other forms of loss functions exist, for example CONVE uses a triple-wise logistic function to model how likely the triple is true, but by far the two described above are the most common. Also, softmax function gives an probabilistic distribution over a set of triples, which is necessary for a generator to sample from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weakness of Uniform Negative Sampling</head><p>Most previous KGE models use uniform negative sampling for generating negative triples, that is, replacing the head or tail entity of a positive triple with any of the entities in E, all with equal probability. Most of the negative triples generated in this way contribute little to learning an effective embedding, because they are too obviously false.</p><p>To demonstrate this issue, let us consider the following example. Suppose we have a ground truth triple LocatedIn(NewOrleans,Louisiana), and corrupt it by replacing its tail entity. First, we remove the tail entity, leaving Lo-catedIn(NewOrleans,?). Because the relation Lo-catedIn constraints types of its entities, "?" must be a geographical region. If we fill "?" with a random entity e ∈ E, the probability of e having a wrong type is very high, resulting in ridiculous triples like Lo-catedIn(NewOrleans,BarackObama) or Locate-dIn(NewOrleans,StarTrek). Such triples are considered "too easy", because they can be eliminated solely by types. In contrast, Locate-dIn(NewOrleans,Florida) is a very useful negative triple, because it satisfies type constraints, but it cannot be proved wrong without detailed knowl-edge of American geography. If a KGE model is fed with mostly "too easy" negative examples, it would probably only learn to represent types, not the underlying semantics.</p><p>The problem is less severe to models using logsoftmax loss function, because they typically samples tens or hundreds of negative triples for one positive triple in each iteration, and it is likely to have a few useful negatives among them. For instance, <ref type="bibr" target="#b19">(Trouillon et al., 2016)</ref> found that a 100:1 negative-to-positive ratio results in the best performance for COMPLEX. However, for marginal loss function, whose negative-to-positive ratio is always 1:1, the low quality of uniformly sampled negatives can seriously damage their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generative Adversarial Training for Knowledge Graph Embedding Models</head><p>Inspired by GANs, we propose an adversarial training framework named KBGAN which uses a KGE model with softmax probabilities to provide high-quality negative samples for the training of a KGE model whose training objective is marginal loss function. This framework is independent of the score functions of these two models, and therefore possesses some extent of universality. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the overall structure of <ref type="bibr">KBGAN.</ref> In parallel to terminologies used in GAN literature, we will simply call these two models generator and discriminator respectively in the rest of this paper. We use softmax probabilistic models as the generator because they can adequately model the "sampling from a probability distribu- tion" process of discrete GANs, and we aim at improving discriminators based on marginal loss because they can benefit more from high-quality negative samples. Note that a major difference between GAN and our work is that, the ultimate goal of our framework is to produce a good discriminator, whereas GANS are aimed at training a good generator. In addition, the discriminator here is not a classifier as it would be in most <ref type="bibr">GANs.</ref> Intuitively, the discriminator should assign a relatively small distance to a high-quality negative sample. In order to encourage the generator to generate useful negative samples, the objective of the generator is to minimize the distance given by discriminator for its generated triples. And just like the ordinary training process, the objective of the discriminator is to minimize the marginal loss between the positive triple and the generated negative triple. In an adversarial training setting, the generator and the discriminator are alternatively trained towards their respective objectives.</p><p>Suppose that the generator produces a probability distribution on negative triples p G (h , r, t |h, r, t) given a positive triple (h, r, t), and generates negative triples (h , r, t ) by sampling from this distribution. Let f D (h, r, t) be the score function of the discriminator. The objective of the discriminator can be formulated as minimizing the following marginal loss function:</p><formula xml:id="formula_4">L D = (h,r,t)∈T [f D (h, r, t) − f D (h , r, t ) + γ] + (h , r, t ) ∼ p G (h , r, t |h, r, t) (3)</formula><p>The only difference between this loss function and Equation 1 is that it uses negative samples from the generator.</p><p>The objective of the generator can be formulated as maximizing the following expectation of negative distances:</p><formula xml:id="formula_5">R G = (h,r,t)∈T E[−f D (h , r, t )]</formula><p>(h , r, t ) ∼ p G (h , r, t |h, r, t) (4) R G involves a discrete sampling step, so we cannot find its gradient with simple differentiation. We use a simple special case of Policy Gradient Theorem 1 <ref type="bibr" target="#b17">(Sutton et al., 2000)</ref> to obtain the gradient of R G with respect to parameters of the generator:   where the second approximate equality means we approximate the expectation with sampling in practice. Now we can calculate the gradient of R G and optimize it with gradient-based algorithms.</p><formula xml:id="formula_6">∇ G R G = (h,r,t)∈T E (h ,r,t )∼p G (h ,r,t |h,r,t) [−f D (h , r, t )∇ G log p G (h , r, t |h, r, t)] (h,r,t)∈T 1 N (h i ,r,t i )∼p G (h ,r,t |h,r,t),i=1...N [−f D (h , r, t )∇ G log p G (h ,</formula><p>Policy Gradient Theorem arises from reinforcement learning (RL), so we would like to draw an analogy between our model and an RL model. The generator can be viewed as an agent which interacts with the environment by performing actions and improves itself by maximizing the reward returned from the environment in response of its actions. Correspondingly, the discriminator can be viewed as the environment. Using RL terminologies, (h, r, t) is the state (which determines what actions the actor can take), p G (h , r, t |h, r, t) is the policy (how the actor choose actions), (h , r, t ) is the action, and −f D (h , r, t ) is the reward. The method of optimizing R G described above is called REINFORCE (Williams, 1992) algorithm in RL. Our model is a simple special case of RL, called one-step RL. In a typical RL setting, each action performed by the agent will change its state, and the agent will perform a series of actions (called an epoch) until it reaches certain states or the number of actions reaches a certain limit. However, in the analogy above, actions does not affect the state, and after each action we restart with another unrelated state, so each epoch consists of only one action.</p><p>To reduce the variance of REINFORCE algorithm, it is common to subtract a baseline from the reward, which is an arbitrary number that only depends on the state, with-out affecting the expectation of gradients. 2 In our case, we replace −f D (h , r, t ) with −f D (h , r, t ) − b(h, r, t) in the equation above to introduce the baseline. To avoid introducing new parameters, we simply let b be a constant, the average reward of the whole training set:</p><formula xml:id="formula_7">b = (h,r,t)∈T E (h ,r,t )∼p G (h ,r,t |h,r,t) [−f D (h , r, t )].</formula><p>In practice, b is approximated by the mean of rewards of recently generated negative triples.</p><p>Let the generator's score function to be f G (h, r, t), given a set of candidate negative triples N eg(h, r, t) ⊂ {(h , r, t)|h ∈ E} ∪ {(h, r, t )|t ∈ E}, the probability distribution p G is modeled as:</p><formula xml:id="formula_8">p G (h , r, t |h, r, t) = exp f G (h , r, t ) exp f G (h * , r, t * ) (h * , r, t * ) ∈ N eg(h, r, t) (6)</formula><p>Ideally, N eg(h, r, t) should contain all possible negatives. However, knowledge graphs are usually highly incomplete, so the "hardest" negative triples are very likely to be false negatives (true facts). To address this issue, we instead generate N eg(h, r, t) by uniformly sampling of N s entities (a small number compared to the number of all possible negatives) from E to replace h or t. Because in real-world knowledge graphs, true negatives are usually far more than false negatives, such set would be unlikely to contain any false negative, and the negative selected by the generator would likely be a true negative. Using a small N eg(h, r, t) can also significantly reduce computational complexity.</p><p>Besides, we adopt the "bern" sampling technique <ref type="bibr" target="#b21">(Wang et al., 2014)</ref> which replaces the "1" side in "1-to-N" and "N-to-1" relations with higher probability to further reduce false negatives.</p><p>Algorithm 1 summarizes the whole adversarial training process. Both the generator and the dis-criminator require pre-training, which is the same as conventionally training a single KBE model with uniform negative sampling. Formally speaking, one can pre-train the generator by minimizing the loss function defined in Equation <ref type="formula">(1)</ref>, and pre-train the discriminator by minimizing the loss function defined in Equation <ref type="formula">(2)</ref>. Line 14 in the algorithm assumes that we are using the vanilla gradient descent as the optimization method, but obviously one can substitute it with any gradientbased optimization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate our proposed framework, we test its performance for the link prediction task with different generators and discriminators. For the generator, we choose two classical probability-based KGE model, DISTMULT and COMPLEX, and for the discriminator, we also choose two classical translation-based KGE model, TRANSE and TRANSD, resulting in four possible combinations of generator and discriminator in total. See <ref type="table" target="#tab_1">Table  1</ref> for a brief summary of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>We use three common knowledge base completion datasets for our experiment: FB15k-237, WN18 and WN18RR. FB15k-237 is a subset of FB15k introduced by <ref type="bibr" target="#b18">(Toutanova and Chen, 2015)</ref>, which removed redundant relations in FB15k and greatly reduced the number of relations. Likewise, WN18RR is a subset of WN18 introduced by <ref type="bibr" target="#b3">(Dettmers et al., 2017)</ref> which removes reversing relations and dramatically increases the difficulty of reasoning. Both FB15k and WN18 are first introduced by <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> and have been commonly used in knowledge graph researches. Statistics of datasets we used are shown in <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Protocols</head><p>Following previous works like  and <ref type="bibr" target="#b19">(Trouillon et al., 2016)</ref>, for each run, we report two common metrics, mean reciprocal ranking (MRR) and hits at 10 (H@10). We only report scores under the filtered setting <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, which removes all triples appeared in training, validating, and testing sets from candidate triples before obtaining the rank of the ground truth triple. 4.1.3 Implementation Details 3 In the pre-training stage, we train every model to convergence for 1000 epochs, and divide every epoch into 100 mini-batches. To avoid overfitting, we adopt early stopping by evaluating MRR on the validation set every 50 epochs. We tried γ = 0.5, 1, 2, 3, 4, 5 and L 1 , L 2 distances for TRANSE and TRANSD, and λ = 0.01, 0.1, 1, 10 for DISTMULT and COMPLEX, and determined the best hyperparameters listed on table 2, based on their performances on the validation set after pre-training. Due to limited computation resources, we deliberately limit the dimensions of embeddings to k = 50, similar to the one used in earlier works, to save time. We also apply certain constraints or regularizations to these models, which are mostly the same as those described in their original publications, and also listed on table 2.</p><p>In the adversarial training stage, we keep all the hyperparamters determined in the pre-training stage unchanged. The number of candidate negative triples, N s , is set to 20 in all cases, which is proven to be optimal among the candidate set of {5, 10, 20, 30, 50}. We train for 5000 epochs, with 100 mini-batches for each epoch. We also use early stopping in adversarial training by evaluating MRR on the validation set every 100 epochs.</p><p>We use the self-adaptive optimization method Adam <ref type="bibr" target="#b8">(Kingma and Ba, 2015)</ref> for all trainings, and always use the recommended default setting α = 0.001, β 1 = 0.9, β 2 = 0.999, = 10 −8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Results of our experiments as well as baselines are shown in <ref type="table" target="#tab_6">Table 4</ref>. All settings of adversarial training bring a pronounced improvement to the model, which indicates that our method is consistently effective in various cases. TRANSE performs slightly worse than TRANSD on FB15k-237 and WN18, but better on WN18RR. Using DIST-MULT or COMPLEX as the generator does not affect performance greatly. TRANSE and TRANSD enhanced by KBGAN can significantly beat their corresponding baseline implementations, and outperform stronger baselines in some cases. As a prototypical and proofof-principle experiment, we have never expected state-of-the-art results. Being simple models pro-   <ref type="bibr" target="#b10">(Lin et al., 2015)</ref> with its default parameters. Results marked with ‡ are copied from <ref type="bibr" target="#b3">(Dettmers et al., 2017)</ref>. All other baseline results are copied from their original papers. posed several years ago, TRANSE and TRANSD has their limitations in expressiveness that are unlikely to be fully compensated by better training technique. In future researches, people may try employing more advanced models into KBGAN, and we believe it has the potential to become stateof-the-art.</p><p>To illustrate our training progress, we plot performances of the discriminator on validation set over epochs, which are displayed in <ref type="figure" target="#fig_2">Figure 2</ref>. As all these graphs show, our performances are always in increasing trends, converging to its max-imum as training proceeds, which indicates that KBGAN is a robust GAN that can converge to good results in various settings, although GANs are wellknown for difficulty in convergence. Fluctuations in these graphs may seem more prominent than other KGE models, but is considered normal for an adversially trained model. Note that in some cases the curve still tends to rise after 5000 epochs. We do not have sufficient computation resource to train for more epochs, but we believe that they will also eventually converge.  <ref type="table">Table 5</ref>: Examples of negative samples in WN18 dataset. The first column is the positive fact, and the term in bold is the one to be replaced by an entity in the next two columns. The second column consists of random entities drawn from the whole dataset. The third column contains negative samples generated by the generator in the last 5 epochs of training. Entities in italic are considered to have semantic relation to the positive one</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case study</head><p>To demonstrate that our approach does generate better negative samples, we list some examples of them in <ref type="table">Table 5</ref>, using the KBGAN (TRANSE + DISTMULT) model and the WN18 dataset. All hyperparameters are the same as those described in Section 4.1.3.</p><p>Compared to uniform random negatives which are almost always totally unrelated, the generator generates more semantically related negative samples, which is different from type relatedness we used as example in Section 3.2, but also helps training. In the first example, two of the five terms are physically related to the process of distilling liquids. In the second example, three of the five entities are geographical objects. In the third example, two of the five entities express the concept of "gather".</p><p>Because we deliberately limited the strength of generated negatives by using a small N s as described in Section 3.3, the semantic relation is pretty weak, and there are still many unrelated entities. However, empirical results (when selecting the optimal N s ) shows that such situation is more beneficial for training the discriminator than generating even stronger negatives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of the KBGAN framework. The generator (G) calculates a probability distribution over a set of candidate negative triples, then sample one triples from the distribution as the output. The discriminator (D) receives the generated negative triple as well as the ground truth triple (in the hexagonal box), and calculates their scores. G minimizes the score of the generated negative triple by policy gradient, and D minimizes the marginal loss between positive and negative triples by gradient descent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :9</head><label>1</label><figDesc>The KBGAN algorithm Data: training set of positive fact triples T = {(h, r, t)} Input: Pre-trained generator G with parameters θG and score function fG(h, r, t), and pre-trained discriminator D with parameters θD and score function fD(h, r, t) Output: Adversarially trained discriminator 1 b ←− 0; // baseline for policy gradient 2 repeat 3 Sample a mini-batch of data T batch from T ;4 GG ←− 0, GD ←− 0; // gradients of parameters of G and D 5 rsum ←− 0; // for calculating the baseline 6 for (h, r, t) ∈ T batch do 7 Uniformly randomly sample Ns negative triples N eg(h, r, t) = {(h i , r, t i )}i=1...N s ; 8 Obtain their probability of being generated: pi = exp f G (h i ,r,t i ) Ns j=1 exp f G (h j ,r,t j ) ; Sample one negative triple (h s , r, t s ) from N eg(h, r, t) according to {pi}i=1...N s . Assume its probability to be ps; 10 GD ←− GD + ∇ θ D [fD(h, r, t) − fD(h s , r, t s ) + γ]+; // accumulate gradients for D 11 r ←− −fD(h s , r, t s ), rsum ←− rsum + r; // r is the reward 12 GG ←− GG + (r − b)∇ θ G log ps; // accumulate gradients for G 13 end 14 θG ←− θG + ηGGG, θD ←− θD − ηDGD; // update parameters 15 b ← rsum/|T batch |; // update baseline 16 until convergence;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Learning curves of KBGAN. All metrics improve steadily as training proceeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Model Score function f (h, r, t)Number of parameters TRANSE ||h + r − t|| 1/2 k|E| + k|R| TRANSD ||(I + rphp T )h + r − (I + rptp T )t|| 1/2 2k|E| + 2k|R| DISTMULT &lt; h, r, t &gt; (= k i=1 hiriti) k|E| + k|R| COMPLEX &lt; h, r,t &gt; (h, r, t ∈ C k ) 2k|E| + 2k|R|TRANSH ||(I − rprp T )h + r − (I + rprp T )t|| 1/2 k|E| + 2k|R| TRANSR ||Wrh + r − Wrt|| 1/2 k|E| + (k 2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Some selected knowledge graph embedding models. The four models above the double line are considered in this paper. Except for COMPLEX, all boldface lower case letters represent vectors in R k , and boldface upper case letters represent matrices in R k×k . I is the identity matrix.</figDesc><table><row><cell>edge graph embedding models. Inspired by the</cell></row><row><cell>recent advances of generative adversarial deep</cell></row><row><cell>models</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>distance, k = 50, γ = 3 ||e|| 2 ≤ 1, ||r|| 2 ≤ 1 TRANSD L 1 distance, k = 50, γ = 3 ||e|| 2 ≤ 1, ||r|| 2 ≤ 1, ||e p || 2 ≤ 1, ||r p || 2 ≤ 1 DISTMULT k = 50, λ = 1/0.1 L2 regularization: L reg = L + λ||Θ|| 2 2 COMPLEX 2k = 50, λ = 1/0.1 L2 regularization: L reg = L + λ||Θ|| 2 2</figDesc><table><row><cell>Model</cell><cell>Hyperparameters</cell><cell>Constraints or Regularizations</cell></row><row><cell>TRANSE</cell><cell>L 1</cell><cell></cell></row><row><cell></cell><cell></cell><cell>r, t |h, r, t)] (5)</cell></row><row><cell></cell><cell></cell><cell>1 A proof can be found in the supplementary material</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Hyperparameter settings of the 4 models we used. For DISTMULT and COMPLEX, λ = 1 is used for FB15k-237 and λ = 0.1 is used for WN18 and WN18RR. All other hyperparameters are shared among all datasets. L is the global loss defined in Equation(2). Θ represents all parameters in the model.</figDesc><table><row><cell>Dataset</cell><cell>#r</cell><cell>#ent.</cell><cell>#train</cell><cell>#val</cell><cell>#test</cell></row><row><cell cols="6">FB15k-237 237 14,541 272,115 17,535 20,466</cell></row><row><cell>WN18</cell><cell>18</cell><cell cols="3">40,943 141,442 5,000</cell><cell>5,000</cell></row><row><cell>WN18RR</cell><cell>11</cell><cell cols="2">40,943 86,835</cell><cell>3,034</cell><cell>3,134</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Statistics of datasets we used in the experiments. "r": relations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Experimental results. Results of KBGAN are results of its discriminator (on the left of the "+" sign). Underlined results are the best ones among our implementations. Results marked with † are produced by running Fast-TransX</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A proof of such fact can also be found in the supplementary material</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The KBGAN source code is available at https:// github.com/cai-lw/KBGAN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We propose a novel adversarial learning method for improving a wide range of knowledge graph embedding models-We designed a generatordiscriminator framework with dual KGE components. Unlike random uniform sampling, the generator model generates higher quality negative examples, which allow the discriminator model to learn better. To enable backpropagation of error, we introduced a one-step REINFORCE method to seamlessly integrate the two modules. Experimentally, we tested the proposed ideas with four commonly used KGE models on three datasets, and the results showed that the adversarial learning framework brought consistent improvements to various KGE models under different settings.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conferrence on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01476</idno>
		<title level="m">Convolutional 2d knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04865</idno>
		<title level="m">Vishnu Naresh Boddeti, and Marios Savvides. 2017. Gang of gans: Generative adversarial networks with maximum margin ranking</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Type-constrained representation learning in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Krompaß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.01784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neverending learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Bhavana Dalvi Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00759</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso Poggio</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Irgan: A minimax game for unifying generative and discriminative information retrieval models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 40th International ACM SIGIR Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From one point to a manifold: Knowledge graph embedding for precise link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
