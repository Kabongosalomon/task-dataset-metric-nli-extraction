<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning and Planning in Complex Action Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadamin</forename><surname>Barekatain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Schmitt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
						</author>
						<title level="a" type="main">Learning and Planning in Complex Action Spaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many important real-world problems have action spaces that are high-dimensional, continuous or both, making full enumeration of all possible actions infeasible. Instead, only small subsets of actions can be sampled for the purpose of policy evaluation and improvement. In this paper, we propose a general framework to reason in a principled way about policy evaluation and improvement over such sampled action subsets. This sample-based policy iteration framework can in principle be applied to any reinforcement learning algorithm based upon policy iteration. Concretely, we propose Sampled MuZero, an extension of the MuZero algorithm that is able to learn in domains with arbitrarily complex action spaces by planning over sampled actions. We demonstrate this approach on the classical board game of Go and on two continuous control benchmark domains: DeepMind Control Suite and Real-World RL Suite.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Real-world environments abound with complexity in their action space. Physical reality is continuous both in space and time; hence many important problems, most notably physical control tasks, have continuous multi-dimensional action spaces. The joints of a robotic hand can assume arbitrary angles; the acceleration of a self-driving car should vary smoothly to minimise discomfort for passengers. Discrete problems also often have high-dimensional action spaces, leading to an exponential number of possible actions. Many other domains have richly structured actions spaces such as sentences, queries, images, or serialised objects. Consequently, a truly general reinforcement learning (RL) algorithm must be able to deal with such complex action spaces in order to be successfully applied to those real-world problems.</p><p>Recent advances in deep learning and RL have indeed led to remarkable progress in model-free RL algorithms for continuous action spaces <ref type="bibr" target="#b22">(Lillicrap et al., 2015;</ref><ref type="bibr" target="#b33">Schulman et al., 2017;</ref><ref type="bibr" target="#b2">Barth-Maron et al., 2018;</ref><ref type="bibr" target="#b0">Abdolmaleki et al., 2018;</ref><ref type="bibr" target="#b18">Hoffman et al., 2020)</ref> and other complex action spaces <ref type="bibr" target="#b8">(Dulac-Arnold et al., 2016)</ref>. Simultaneously, planning based methods have enjoyed huge successes in domains with discrete action spaces, surpassing human performance in the classical games of chess and Go <ref type="bibr" target="#b36">(Silver et al., 2018)</ref> or poker <ref type="bibr" target="#b4">(Brown &amp; Sandholm, 2018;</ref><ref type="bibr" target="#b26">Moravčík et al., 2017)</ref>. The prospect of combining these two areas of research holds great promise for real-world applications.</p><p>The model-based MuZero  RL algorithm took a step towards applicability in real-world problems by learning a model of the environment and thus unlocking the use of the powerful methods of planning in domains where the dynamics of the environment are unknown or impossible to simulate efficiently. However, MuZero was only applied to domains with relatively small action spaces; small enough to be in fact enumerated in full by the tree-based search at its core.</p><p>Sample-based methods provide a powerful approach to dealing with large complex actions spaces. Rather than enumerating all possible actions, the idea is to sample a small subset of actions and compute the optimal policy or value function with respect to those samples. This simple strategy is so general that it can be applied to large, continuous, or structured action spaces. Specifically, action sampling can be used both to propose improvements to the policy at each of the sampled actions, and subsequently to evaluate the proposed improvements. However, to correctly improve or evaluate the policy across the entire action space, and not just the samples, one must understand how the sampling procedure interacts with both policy improvement and policy evaluation.</p><p>In this work, we propose a framework to reason in a principled way about policy improvement and evaluation computed over small subsets of sampled actions. We show how this local information can be used to train a global policy, act and even perform explicit steps of policy evaluation for the purpose of planning and local policy iteration. This sample-based framework can in principle be applied arXiv:2104.06303v1 <ref type="bibr">[cs.</ref>LG] 13 Apr 2021 to any reinforcement learning algorithm based upon policy iteration. Concretely, we propose Sampled MuZero, an algorithmically simple extension of the MuZero 1 algorithm that facilitates its application to domains with complex action spaces.</p><p>To demonstrate the generality of this approach, we apply our algorithm to two continuous control benchmark domains, the DeepMind Control Suite  and Real-World RL Suite <ref type="bibr" target="#b9">(Dulac-Arnold et al., 2020)</ref>. We also demonstrate that our algorithm can be applied to large discrete action spaces, by sampling the actions in the game of Go, and show that high performance can be maintained even when sub-sampling a small fraction of possible moves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Previous research in reinforcement learning for complex or continuous action spaces has often focused on model-free algorithms.</p><p>Deep Deterministic Policy Gradient (DDPG) exploits the fact that in action spaces that are entirely continuous (no discrete action dimensions), the action-value function Q(s, a) can be assumed to be differentiable with respect to the action a in order to efficiently compute policy gradients <ref type="bibr" target="#b34">(Silver et al., 2014;</ref><ref type="bibr" target="#b22">Lillicrap et al., 2015)</ref>. Distributed Distributional Deterministic Policy Gradients (D4PG) extends DDPG by using a distributional value function and a distributed training setup <ref type="bibr" target="#b2">(Barth-Maron et al., 2018)</ref>. Trust Region Policy Optimisation (TRPO) uses a hard KL constraint to ensure that the updated policy remains close to the previous policy during the policy improvement step <ref type="bibr" target="#b32">(Schulman et al., 2015)</ref>, to avoid catastrophic collapse. Proximal Policy Optimisation (PPO) has the same goal as TRPO, but instead uses the KL-divergence as a penalty in the loss function or clipping in the value function <ref type="bibr" target="#b33">(Schulman et al., 2017)</ref>. This results in a simpler algorithm with empirically better performance. In the regime of data-efficient off-policy algorithms, recent advances have derived actor-critic algorithms that optimise a (relative-)entropy regularised RL objective such as SAC <ref type="bibr" target="#b12">(Haarnoja et al., 2018)</ref>, MPO , <ref type="bibr">AWR (Peng et al., 2019)</ref>. Among these, MPO uses a sample based policy improvement step that can be related to our algorithm (see section 4.4). Distributional MPO (DMPO) extends MPO to use a distributional Q-function <ref type="bibr" target="#b18">(Hoffman et al., 2020)</ref>.</p><p>Model-based control for high dimensional action spaces has recently seen a resurgence of interest (see e.g. <ref type="bibr" target="#b13">Hafner et al., 2018;</ref><ref type="bibr" target="#b21">Koul et al., 2020)</ref>). While most of these algorithms consider direct policy optimisation against a learned model some have considered 1 The discussion in this paper applies equally to AlphaZero and MuZero; in the text we will only refer to MuZero for simplicity. combinations of rollout based search/planning with policy learning. <ref type="bibr" target="#b27">(Piché et al., 2018)</ref> use planning via sequential importance sampling of action sequences sampled from a SAC policy. <ref type="bibr" target="#b3">(Bhardwaj et al., 2020)</ref> use a learned simulator to construct K-step returns for learning a soft Q-function. Closest to our work,  consider a sample based policy update similar to ours -but using a policy improvement operator based on the KL regularised objective rather than the MCTS based policy improvement that we consider here.</p><p>Sparse sampling algorithms <ref type="bibr" target="#b19">(Kearns et al., 1999)</ref> are an effective approach to planning in large state spaces. The main idea is to sample K possible state transitions from each state, drawn from a generative model of the underlying MDP. Collectively, these samples provide a search tree over a subset of the MDP; planning over the sampled tree provides a near-optimal approximation, for large K, to the optimal policy for the full MDP, independent of the size of the state space. Indeed, sampling is known to address the curse of dimensionality in some cases <ref type="bibr" target="#b29">(Rust, 1997)</ref>. However, sparse sampling typically enumerates all possible actions from each state, and does not address issues relating to large action spaces. In contrast, our method samples actions rather than state transitions. In principle, it would be straightforward to combine both ideas; however, we focus in this paper upon the novel aspect relating to large action spaces and utilise deterministic transition models.</p><p>There have been several previous attempts at generalising AlphaZero and MuZero to continuous action spaces. These attempts have shown that such an extension is possible in principle, but have so far been restricted to very low dimensional cases and not yet demonstrated effectiveness in high-dimensional tasks. A0C <ref type="bibr" target="#b25">(Moerland et al., 2018)</ref> describes an extension of AlphaZero to continuous action spaces using a continuous policy representation and REIN-FORCE <ref type="bibr" target="#b44">(Williams, 1992)</ref> to estimate the gradients for the reverse KL divergence between the neural network policy estimate and the target MCTS policy, demonstrating some learning on the 1D Pendulum task.  describe a similar extension of MuZero to continuous actions and show promising results outperforming soft actor-critic (SAC) <ref type="bibr" target="#b12">(Haarnoja et al., 2018)</ref> on environments with 1 and 2 dimensional action spaces. The factorised policy representation described by <ref type="bibr" target="#b40">(Tang &amp; Agrawal, 2020)</ref> shows good results in a variety of domains; by representing each action dimension with a separate categorical distribution it efficiently avoids the exponential explosion in the number of actions faced by a simple discretisation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>We consider a standard reinforcement learning setup in which an agent acts in an environment by sequentially choosing actions over a sequence of time-steps in order to maximise a cumulative reward. We model the problem as a Markov decision process (MDP) which comprises a state space S, an action space A, an initial state distribution, a stationary transition dynamics distribution and a reward function r : S × A → R.</p><p>The agent's behaviour is controlled by a policy π : S → P(A) which maps states to a probability distribution over the action space. The return from a state is defined as the sum of discounted future rewards G t = i γ i r(s t+i , a t+i ) where γ is a discount factor in [0, 1]. The goal of the agent is to learn a policy which maximises the expected return from the start distribution.</p><p>In order to do so, a common strategy called policy evaluation consists in learning a value function that estimates the expected return of following policy π from a state s t or a state action pair (s t , a t ). The value function can then be used in a process called policy improvement, to find and learn better policies by for instance increasing the probabilities of actions with higher values. The process of repeatedly doing policy evaluation followed by policy improvement is at the heart of many reinforcement learning algorithms and is called policy iteration.</p><p>Naturally, a lot of research focuses on improving the methods for policy evaluation and policy improvement. One direction for scaling the efficiency of both is to evaluate, from the current state, several possible actions, or even several possible future trajectories by using a model, instead of just extracting information from the trajectory that was executed. Those evaluations can then be used to build a locally better policy over those actions. Planning algorithms such as Monte Carlo Tree Search (MCTS) <ref type="bibr" target="#b7">(Coulom, 2006)</ref> take this even further and make several local policy iteration steps by repeatedly performing a policy improvement step followed by an explicit local step of policy evaluation of the improved policy in the aim of generating an even better policy locally.</p><p>From this perspective, the MuZero algorithm can be understood as the combination of two processes of policy evaluation and policy improvement. The inner process, concretely MuZero's MCTS search, provides the policy improvement for the outer process which in turn learns the quantities: the model, reward function, value function and the policy, necessary for the inner process. Specifically, in the outer process, MuZero learns a deep neural network parameterising a model, a reward function, a state-value function and a policy. Policy improvement is accomplished by regressing the parametric policy towards the improved policy built by MuZero's MCTS search. The improved policy is also used for acting. The value function is learned using the usual tools of policy evaluation such as temporal-difference learning <ref type="bibr" target="#b39">(Sutton, 1988)</ref>. These two objectives coupled with the learning of the reward function drive the learning of the model. In the inner process, MuZero's MCTS search takes several analytical policy iteration steps: values in the search tree are estimated by explicitly averaging n-step returns bootstrapped from the value function (policy evaluation) while visits are directed towards high policy and high value actions (policy improvement). This results in an improved policy and an estimate of the value of this improved policy that can be used for the outer process.</p><p>This raises a few questions, especially in the case where only a small subset of the action space can be evaluated to build the locally improved policy.</p><p>• how to select the actions or trajectories to be evaluated • how to build a locally improved policy over those actions • how to use the locally improved policy to learn about the global policy • how to use it to act • how to perform an explicit local step of policy evaluation of the improved policy for planning • how all these steps interact with each other In the following, we will assume that the actions to be evaluated are sampled from some proposal distribution β and that we have at our disposal some process to build a locally improved policy. We will mainly focus on the last four questions and propose a general framework to reason in a principled way about policy evaluation and improvement over such sampled action subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Sample-based Policy Iteration</head><p>Let π : S → P(A) be a policy and Iπ : S → P(A) be an improved policy of π: ∀s ∈ S, v Iπ (s) ≥ v π (s). If we had complete access to Iπ, we would directly use it for policy improvement by projecting it back onto the space of realisable policies. However, when the action space A is too large, it might only be feasible to compute an improved policy over a small subset of actions.</p><p>It is not immediately clear how to use this locally improved policy to perform principled policy improvement, or policy evaluation of the improved policy, because this locally improved policy only gives us information regarding the sampled actions.</p><p>We propose a framework which relies on writing both operations as an expectation with respect to the fully improved policy Iπ and use the samples we have to estimate these expectations. This allows us to use the conceptually correct target Iπ to define the objectives and clearly surface the approximations that are introduced afterwards. Specifically, we will restrict ourselves to the general class of policy improvement operators that we call action-independent as defined below in 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Operator view of Policy Improvement</head><p>We use the concepts introduced by <ref type="bibr" target="#b10">(Ghosh et al., 2020)</ref> and decompose policy improvement into the successive application of two operators: (a) a policy improvement operator I which maps any policy to a policy achieving strictly larger return; and (b) a projection operator P, which finds the best approximation of this improved policy in the space of realisable policies. With those notations, the process of policy improvement can be written as P • I. <ref type="bibr" target="#b10">(Ghosh et al., 2020)</ref> showed that the policy gradient algorithm can be thought of having the following policy improvement operator: Iπ(s, a) ∝ π(s, a)Q(s, a) where Q(s, a) is the action-value function. They also showed that PPO's <ref type="bibr" target="#b33">(Schulman et al., 2017)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Action-Independent Policy Improvement Operator</head><p>We define a policy improvement operator as actionindependent if it can be written as:</p><formula xml:id="formula_0">Iπ(a|s) = f (s, a, Z(s))</formula><p>where Z(s) is a unique state dependent normalising factor defined by ∀a ∈ A, f (s, a, Z(s)) ≥ 0 and a∈A f (s, a, Z(s)) = 1. 2 All of the policy improvement operators described above are action-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPO Example:</head><p>MPO's policy improvement operator can be written Iπ(a|s) = f (s, a, Z(s)) = π(s, a) exp(Q(s, a)/τ )/Z(s) and Z(s) = a π(s, a) exp(Q(s, a)/τ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sample-Based Action-Independent Policy Improvement Operator</head><p>Let {a i } be K actions sampled from a proposal distribution β andβ(a|s) = 1 K i δ a,ai the corresponding empirical 2 In the continuous case, sums would be replaced by integrals. distribution 3 which is non-zero only on the sampled actions {a i }.</p><p>We define the sample-based action-independent 4 policy improvement operator aŝ</p><formula xml:id="formula_1">I β π(a|s) = (β/β)(a|s)f (s, a,Ẑ β (s))</formula><p>whereẐ β (s) is a unique state dependent normalising factor defined by ∀a ∈ A, (β/β)(a|s)f (s, a,Ẑ β (s)) ≥ 0 and a∈A (β/β)(a|s)f (s, a,Ẑ β (s)) = 1. We have used the shorthand notation (β/β)(a|s) to meanβ(a|s)/β(a|s).</p><p>MPO Example: MPO's sample-based action-independent policy improvement operator using β = π would therefore beÎ β π(a|s) =β(a|s) exp(Q(s, a)/τ )/Ẑ β (s) witĥ Z β (s) = aβ (a|s) exp(Q(s, a)/τ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Computing an expectation with respect to Iπ</head><p>We focus in this section on evaluating for a given state s the expectation E a∼Iπ [X|s] of a random variable X given actions {a i } sampled from a distribution β and the samplebased improved policyÎ β π.</p><p>Theorem. For a given random variable X, we have</p><formula xml:id="formula_2">E a∼Iπ [X|s] = lim K→∞ a∈AÎ β π(a|s)X(s, a)</formula><p>Furthermore, a∈AÎ β π(a|s)X(s, a) is approximately normally distributed around E a∼Iπ [X|s] as K → ∞:</p><formula xml:id="formula_3">a∈AÎ β π(a|s)X(s, a) ∼ N (E a∼Iπ [X|s], σ 2 K ) where σ 2 = V ar a∼β [ f (s,a,Z(s)) β X(s, a)|s].</formula><p>Proof See Appendix E Corollary. The sample-based policy improvement operator converges in distribution to the true policy improvement operator: lim</p><formula xml:id="formula_4">K→∞Î β π = Iπ</formula><p>and is approximately normally distributed around the true policy improvement operator as K → ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. See Appendix E</head><p>We illustrate this result in <ref type="figure">Figure 1</ref>. <ref type="figure">Figure 1</ref>. Sample-based Policy Improvement. On the left, the current policy π(a|s). Next, K actions {ai} are sampled from a proposal distribution β andβ(a|s) is the corresponding empirical distribution. A sample-based improved policyÎ β π(a|s) = (β/β)(a|s)f (s, a,Ẑ β (s)) is then built. As the number of samples K increasesÎ β π(a|s) converges to the improved policy Iπ(a|s) = f (s, a, Z(s)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Sample-based Policy Evaluation and Improvement</head><p>The previous expression computing an estimate of E a∼Iπ [X|s] using the quantityÎ β π and the sampled actions {a i } can be used for policy improvement and policy evaluation of the improved policy.</p><p>Policy improvement can be performed by for instance instantiating X = − log π θ , minimising the cross-entropy between π θ and the improved policy Iπ:</p><formula xml:id="formula_5">CE = E a∼Iπ [− log π θ ].</formula><p>Additionally, samples from Iπ can be obtained by resampling an action a fromÎ β π. This procedure also known as Sampling Importance Resampling (SIR) <ref type="bibr" target="#b28">(Rubin, 1987)</ref> gives us a way to act with the improved policy and reuse the usual tools such as temporal-difference learning to do policy evaluation of the improved policy.</p><p>Finally, for instance for the purpose of planning, an explicit step of policy evaluation of the improved policy can be computed by estimating 1-step or n-step returns. Using for example X = r + γV lets us backpropagate the value V by one step in a search tree: V (s) = E a∼Iπ [r + γV |s].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Sampled MuZero</head><p>Building on the sample-based policy iteration framework established in the previous section, we now instantiate those ideas in the context of a complete system. Concretely, we apply our sampling procedure to the MuZero algorithm, to produce a new algorithm that we term Sampled MuZero. This algorithm may be applied to any domain where MuZero can be applied; but furthermore can also be used, in principle, to learn and plan in domains with arbitrarily complex action spaces.</p><p>As introduced in the background section, MuZero may be understood as combining an inner process of policy iteration, within its Monte-Carlo tree search, and an outer process, in its overall interactions with the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Inner Policy Evaluation and Improvement</head><p>Specifically, within its search tree, MuZero estimates values by explicitly averaging n-step returns samples (policy evaluation) and selects the next node to evaluate and expand by recursively maximising (policy improvement) over the probabilistic upper confidence tree (PUCT) bound  arg</p><formula xml:id="formula_6">max a Q(s, a) + c(s) · π(s, a) b N (s, b) 1 + N (s, a)</formula><p>where c(s) is an exploration factor controlling the influence of the policy π(s, a) relative to the values Q(s, a) as nodes are visited more often.</p><p>Naive Modification. A first approach to extending MuZero's MCTS search is to search over the sampled actions {a i } and keep the PUCT formula unchanged, directly using the probabilities π coming from the policy network in the PUCT formula just like in MuZero. The search's visit count distribution f can then be used to construct the sampled-basedÎ β π =β/βf to correct for the effect of sampling at policy network training time and acting time, but also dynamically as the tree is built for value backpropagation (inner policy evaluation). Theoretically this procedure is not complicated, but in practice it might lead to unstable results because of the f /β term, especially if f is represented by normalised visit counts which have limited numerical precision.</p><p>Proposed Modification. Instead, we propose to search with probabilitiesπ β (s, a) proportional to (β/βπ)(s, a), in place of π(s, a) in the PUCT formula and directly use the resulting visit count distributions just like in MuZero. We use the following Theorem to justify this proposed modification.</p><p>Theorem. Let Iπ be the visit count distribution 5 of MuZero's search using prior π when considering the whole 5 for a given number of simulations action space A and let Iπ β be the visit count distribution obtained by searching using priorπ β . Then, Iπ β is approximately equal to the sample-based policy improvement associated to Iπ. In other words, Iπ β ≈Î β π.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. See Appendix F</head><p>We can therefore directly use the results of the previous section 4.4 and in particular,</p><formula xml:id="formula_7">E a∼Iπ [X|s] ≈ a∈A Iπ β (s, a)X(s, a)</formula><p>This lets us conclude that the only modification beyond sampling that needs to be made to MuZero is to useπ β instead of π in the PUCT formula. The rest of the MuZero algorithm, from estimating the values in the search tree by averaging n-step returns, to acting and training the policy network using the visit count distribution, can proceed unchanged.</p><p>Remark. Note that, if β = π 1/τ , the probabilitiesπ β used in the PUCT formula can be written:π β =β/βπ = βπ 1−1/τ . If τ = 1,π β is equal to the empirical sampling/prior distributionβ. This means that the search is guided by a potentially quasi uniform priorβ but only evaluates relatively high probability actions. If τ &gt; 1, the search evaluates more diverse samples but is guided by more peaked probabilitiesβπ 1−1/τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Outer Policy Improvement</head><p>Once the inner iterations of policy improvement and policy evaluation within Monte-Carlo tree search have been completed, the net result is a set of visit counts N (s, a) at the root state s of the search tree, corresponding to each sampled action a. These visit counts may be normalised to provide the sample-based improved policy Iπ β (a|s) = N (s, a)/ b N (s, b). Following the argument in the previous section, these visit counts already take account of the fact that the root actions were sampled according to β.</p><p>Hence all that remains is to project the sample-based improved policy back onto the space of representable policies, using an appropriate projection operator P. Following MuZero, we choose a standard projection operator for probability distributions that selects parameters θ minimising the KL divergence KL(Iπ β ||π θ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Outer Policy Evaluation</head><p>To select actions, the agent samples its behaviour from its sample-based improved policy, Iπ β (a|s) = N (s, a)/ b N (s, b). As above, we note that this already corrects for the sampling procedure in the construction of the visit counts, and hence may be used directly as a policy.</p><p>The outer policy evaluation step then follows directly from MuZero, i.e. a value function is trained from n-step returns, using trajectories of behaviour generated by the samplebased improved policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Search Tree Node Expansion</head><p>In MuZero, each time a leaf node is expanded, all the N = |A| actions of the action space are returned alongside the probabilities π the policy network assigns to each of those actions.</p><p>Proposed Modification. In Sampled MuZero, we instead sample K N actions from a distribution β and return each action a along with its corresponding probabilities π(s, a) and β(s, a).</p><p>We note that, if the number of simulations of the search is much bigger than K, techniques such as progressive widening <ref type="bibr" target="#b6">(Chaslot et al., 2008)</ref> could in principle be used to dynamically sample more actions for nodes on highly visited search paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Sampling distribution β</head><p>In principle any sampling distribution β with a wide support can be used, including the uniform distribution. However, as only a limited number of samples can be drawn, it is preferable to sample moves that are likely according to our current estimate for the best policy, i.e. the policy network. 6</p><p>Proposed Modification. We use β = π, potentially modulated by a temperature parameter. To encourage exploration and to make sure that even low prior moves have an opportunity to be reassessed from time to time, MuZero combines the prior π produced by the policy network with Dirichlet noise at the root of the search tree. We obtain the same behaviour in Sampled MuZero by also including noise in β and π, ensuring that low prior moves can be sampled and searched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We evaluated the performance of Sampled MuZero on a variety of reinforcement learning environments. We focus upon standard benchmark environments in which clear baselines are available for comparison. We use those benchmarks to explore two important properties of real-world applications. First, whether Sampled MuZero is sufficiently general to operate across discrete and continuous environments of very different types. Second, whether the algorithm is robust to sampling -that is, whether we can come close to the performance of algorithms that have access to the entire action set (and are therefore not scalable to large action spaces), when only sampling a small fraction of the action space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Go</head><p>Go has long been a challenge problem for AI, with only the AlphaGo <ref type="bibr" target="#b31">Schrittwieser et al., 2020)</ref> family of algorithms finally surpassing human professional players. It is a domain that requires deep and precise planning and as such is an ideal domain to put the planning capabilities of Sampled MuZero to the test.</p><p>Using MuZero as a baseline, we trained multiple instances of Sampled MuZero with varying number of action samples K ∈ {15, 25, 50, 100} (see <ref type="figure" target="#fig_1">Figure 2</ref>). The size of the action space in 19x19 Go is 362 (all board points plus pass), so all the tested values of K only cover a small part of the action space. As expected, the performance improves as K increases, with K = 50 samples already closely approaching the performance of the baseline that is allowed to search over all possible actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Atari</head><p>We also performed the same experiment as in <ref type="figure" target="#fig_1">Figure 2</ref> for the Arcade game of Ms. Pacman, from the classic Atari RL benchmark. The action space in Atari is of size 18. Searching with K = 2 samples is not sufficient for efficient learning, but already with K = 3 samples performance rapidly approaches the baseline that is allowed to search all possible actions without sampling <ref type="figure" target="#fig_2">(Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">DeepMind Control Suite</head><p>The DeepMind Control Suite  provides a set of continuous control tasks based on MuJoCo <ref type="bibr" target="#b43">(Todorov et al., 2012)</ref> and has been widely used as a benchmark to assess performance of continuous control algorithms. For the experiments in this paper we use the task classification and data budgets introduced in Acme (Hoffman et al., 2020), evaluating Sampled MuZero on the easy, medium and hard tasks. We additionally evaluated Sampled MuZero on the manipulator tasks which are known to be interesting and difficult.</p><p>In its most common setup, the control suite domains provide 1 dimensional state inputs (as opposed to 2 dimensional image inputs in board games and Atari as used by MuZero). We therefore used a variation of the MuZero model architecture in which all convolutions are replaced by fully-connected layers (see Appendix A for further details). For the policy prediction, we chose the factored policy representation introduced by <ref type="bibr" target="#b40">(Tang &amp; Agrawal, 2020)</ref>, representing each dimension by a categorical distribution. There are however no difficulties in working directly with continuous actions and we show results with a policy prediction parameterised with a Gaussian distribution on the hard and manipulator tasks in the Appendix <ref type="figure">(Figure A.1)</ref>.</p><p>Sampled MuZero showed good performance across the task set ( <ref type="figure" target="#fig_6">Figure 7</ref> for full results), with especially good results for tasks in the most difficult hard and manipulator categories ( <ref type="figure" target="#fig_3">Figure 4</ref>) such as humanoid.run or the manipulator tasks in general.</p><p>The control suite domains can also be configured to provide raw pixel inputs instead of 1 dimensional state inputs. We ran Sampled MuZero on the same tasks with the same data budget (25M frames) and the same hyperparameters. As demonstrated in <ref type="figure" target="#fig_4">Figure 5</ref>, Sampled MuZero can be applied to efficiently learn from raw pixel inputs as well. It is particularly remarkable that Sampled MuZero can learn to control the 21 dimensional humanoid from raw pixel inputs only. In addition, we compared Sampled MuZero to the Dreamer agent <ref type="bibr" target="#b14">(Hafner et al., 2019)</ref> in Appendix A.2, <ref type="table" target="#tab_1">Table 2</ref>. Sampled MuZero equalled or surpassed the Dreamer agent's performance in all tasks, without any action repeat (Dreamer uses an action repeat of 2), observation reconstruction, or any hyperparameter re-tuning.  <ref type="bibr" target="#b18">(Hoffman et al., 2020) and</ref><ref type="bibr">D4PG (Barth-Maron et al., 2018)</ref>. The x-axis shows millions of environment frames, the y-axis mean episode return. Hard tasks as proposed by <ref type="bibr" target="#b18">(Hoffman et al., 2020)</ref>. Plot titles include the task name and the dimensionality of the action space.    based CMU humanoid tasks, controlling a humanoid body with 56 action dimensions. Sampled MuZero outperforms previously reported results for both forage, go-to-target and run-walls <ref type="bibr" target="#b24">(Merel et al., 2019)</ref> as well as run-gaps <ref type="bibr" target="#b37">(Song et al., 2020)</ref> while using more than an order of magnitude fewer environment interactions.</p><p>To investigate the scalability to more complex action spaces, we also applied Sampled MuZero to the dm control  based Locomotion environment. In this set of high-dimensional tasks, the agent must control a humanoid body with 56 action dimensions to accomplish a variety of goals ( <ref type="figure" target="#fig_5">Figure 6</ref>). In all tasks Sampled MuZero not only outperformed previously reported results, but it did so using more than an order of magnitude fewer interactions with the environment.</p><p>Finally, we investigated the impact on performance of the number of samples in the Appendix <ref type="figure" target="#fig_9">(Figure 10</ref>). We show that Sampled MuZero can learn high dimensional action tasks with as little as K = 5 samples. Furthermore, we evaluated the stability of Sampled MuZero, both from state inputs and raw pixel inputs, in <ref type="figure" target="#fig_10">Figure 11</ref> and <ref type="figure" target="#fig_1">Figure 12</ref>. We show that Sampled MuZero's performance is overall very reproducible across tasks and number of samples. We also verified the practical importance of usingπ β instead of just π in Sampled MuZero's PUCT formula in <ref type="figure" target="#fig_2">Figure 13</ref>. We find that, as suggested by the theory, it is much more robust to useπ β .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Real-World RL Challenge Benchmark</head><p>The real-world Reinforcement Learning (RWRL) Challenge set of benchmark tasks (Dulac-Arnold et al., 2020) is a set of continuous control tasks that aims to capture the aspects of real-world tasks that commonly cause RL algorithms to fail. We used this benchmark to test the robustness of our proposed algorithm to complications such as delays, partial observability or stochasticity. We used the same neural network architecture as for the DeepMind Control Suite with the addition of an LSTM <ref type="bibr" target="#b17">(Hochreiter &amp; Schmidhuber, 1997)</ref> to deal with partial observability. <ref type="table">Table 1</ref>, Sampled MuZero significantly outperformed baseline algorithms in all three challenge difficulties. We provide full learning curve results in the Appendix <ref type="figure" target="#fig_7">(Figure 8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper we introduced a unified framework for learning and planning in discrete, continuous and structured complex action spaces. Our approach is based upon a simple principle of sampling actions. By careful book-keeping we have shown how one may take account of the sampling process during policy improvement and policy evaluation. In principle, the same sample-based strategy could be applied to a variety of other reinforcement algorithms in which the policy is updated by, or approximated by, an action-independent improvement step. Concretely, we have focused upon applying our framework to the model-based planning algorithm of MuZero, resulting in our new algorithm Sampled MuZero. Our empirical results show that the idea is both general, succeeding across a wide variety of discrete and continuous benchmark environments, and robust, scaling gracefully down to small numbers of samples. These results suggest that the ideas introduced in this paper may also be effective in larger scale applications where it is not feasible to enumerate the action space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DeepMind Control Suite and Real-World RL Experiments</head><p>For the continuous control experiments where the input is 1 dimensional (as opposed to 2 dimensional image inputs in board games and Atari as used by MuZero), we used a variation of the MuZero model architecture in which all convolutions are replaced by fully connected layers.</p><p>The representation function processed the input via an input block composed of a linear layer, followed by a Layer Normalisation and a tanh activation. The resulting embedding was then processed by a ResNet v2 style pre-activation residual tower <ref type="bibr" target="#b15">(He et al., 2016)</ref> coupled with Layer Normalisation <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> and Rectified Linear Unit (ReLU) activations. We used 10 blocks, each block containing 2 layers with a hidden size of 512.</p><p>For the Real-World RL experiments, we additionally inserted an LSTM module <ref type="bibr" target="#b17">(Hochreiter &amp; Schmidhuber, 1997)</ref> in the representation function between the input block and the residual tower to deal with partial observability. We trained the LSTM using truncated backpropagation through time for 8 steps, initialised from LSTM states stored during acting, each step having the last 4 observations concatenated together, for an effective unroll step of 32 steps.</p><p>The dynamics function processed the action via an action block composed of a linear layer, followed by a Layer Normalisation and a ReLU activation. The action embedding was then added to the dynamics function's input embedding and then processed by a residual tower using the same architecture as the residual tower for the representation function.</p><p>The reward and value predictions used the categorical representation introduced in MuZero . We used 51 bins for both the value and the reward predictions with the value being able to represent values between [−150.0, 150.0] and the reward being able to represent values between [−1.0, 1.0]. We used n-step bootstrapping with n = 5 and a discount of 0.99 consistent with Acme (Hoffman et al., 2020).</p><p>We used the factored policy representation introduced by <ref type="bibr" target="#b40">(Tang &amp; Agrawal, 2020)</ref> representing each dimension by a categorical distribution over B = 7 bins for the policy prediction.</p><p>To implement the network, we used the modules provided by the Haiku neural network library <ref type="bibr" target="#b16">(Hennigan et al., 2020)</ref>.</p><p>We used the Adam optimiser <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2015)</ref> with decoupled weight decay <ref type="bibr" target="#b23">(Loshchilov &amp; Hutter, 2017)</ref> for training. We used a weight decay scale of 2 · 10 −5 , a batch size of 1024 an initial learning rate of 10 −4 , decayed to 0 over 1 million training batches using a cosine schedule: lr = lr init 1 2 1 + cos π step max steps where lr init = 10 −4 and max steps = 10 6 .</p><p>For replay, we keep a buffer of the most recent 2000 sequences, splitting episodes into subsequences of length up to 500. Samples are drawn from the replay buffer according to prioritised replay <ref type="bibr" target="#b30">(Schaul et al., 2016)</ref> using the same priority and hyperparameters as in MuZero.</p><p>We trained Sampled MuZero using K = 20 samples and a search budget of 50 simulations per move. At the root of the search tree only, we evaluated all sampled actions before the start of the search and used those to initialise the Q(s, a) quantities in the PUCT formula (Appendix D). We evaluated Sampled MuZero's network checkpoints throughout training playing 100 games with a search budget of 50 simulations per move and picked the move with the highest number of visits to act, consistent with previous work.</p><p>We used Acme <ref type="bibr" target="#b18">(Hoffman et al., 2020)</ref> to produce the results for DMPO <ref type="bibr" target="#b18">(Hoffman et al., 2020) and</ref><ref type="bibr">D4PG (Barth-Maron et al., 2018)</ref>. Compared to Acme, we used bigger networks <ref type="bibr">(Policy Network layers = (512, 512, 256, 128), Critic Network Layers = (1024</ref><ref type="bibr">, 1024</ref>) and a bigger batch size of 1024 for better comparison. Each task was run with three seeds.</p><p>We provide full learning curve results on the DeepMind Control Suite <ref type="figure" target="#fig_6">(Figure 7)</ref> and Real-World RL <ref type="figure" target="#fig_7">(Figure 8</ref>) tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Gaussian policy parameterisation</head><p>Even though a categorical policy representation was used to compute the main results, Sampled MuZero can also be applied working directly with continuous actions. <ref type="figure" target="#fig_8">Figure 9</ref> shows results on the hard and manipulator tasks when the policy prediction is parameterised by a Gaussian distribution.</p><p>The performance is similar across almost all tasks but we found that Gaussian distributions are harder to optimise than their categorical counterpart and that using entropy regularisation was useful to produce better results (we used a coefficient of 5e-3). It is possible that these results could be improved with better regularisation schemes such as constraining the deviation of the mean and standard deviation as in the MPO  algorithm. In contrast, we did not need to add any regularisation to train the categorical distribution. . The x-axis shows millions of environment frames, the y-axis mean episode return. Tasks are grouped into easy, medium and hard as proposed by <ref type="bibr" target="#b18">(Hoffman et al., 2020)</ref>. Plot titles include the task name and the dimensionality of the action space.    <ref type="bibr" target="#b14">(Hafner et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Sampled MuZero from Pixels</head><p>In addition to Sampled MuZero's results on the hard and manipulator tasks when learning from raw pixel inputs, we compared Sampled MuZero to the Dreamer agent <ref type="bibr" target="#b14">(Hafner et al., 2019)</ref> in <ref type="table" target="#tab_1">Table 2</ref>. We used the 20 tasks and the 5 million environment steps experimental setup defined by <ref type="bibr" target="#b14">(Hafner et al., 2019)</ref>. Sampled MuZero equalled or surpassed the Dreamer agent's performance in all 20 tasks, without any action repeat (Dreamer uses an action repeat of 2), observation reconstruction, or any hyperparameter re-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Ablation on the number of samples</head><p>We trained multiple instances of Sampled MuZero with varying number of action samples K ∈ {3, 5, 10, 20, 40} on the humanoid.run task for which the action is 21 dimensional. We ran six seeds for each instance. Surprisingly K = 3 is already sufficient to learn a good policy and performance does not seem to be improved by sampling more than K = 10 samples (see <ref type="figure" target="#fig_9">Figure 10)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Reproducibility</head><p>In order to evaluate the reproducibility of Sampled MuZero from state inputs and raw pixel inputs, we show the indi- vidual performance of 3 seeds on the hard and manipulator tasks in <ref type="figure" target="#fig_10">Figure 11</ref>. Overall, the variation in performance across seeds is minimal.</p><p>In addition, we show the individual performance of 6 seeds when sampling K = 3, 5, 10, 20, 40 actions on the humanoid.run task. We observe that even when the number of samples is small, performance stays very reproducible across runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Ablation on usingπ β vs π</head><p>We evaluated the practical importance of usingπ β =β/βπ instead of just π in Sampled MuZero's PUCT formula and ran experiments on the humanoid.run task.</p><p>We expect that as the number of samples increases, the difference will go away as lim K→∞πβ = lim K→∞β /βπ = π.</p><p>We therefore looked at the difference in performance when drawing K = 5 or K = 20 samples.</p><p>Furthermore, evaluating the Q values of all sampled actions at the root of the search tree before the start of the search puts more emphasis on the values and less on the prior in the PUCT formula. We therefore also show the difference in performance with and without Q evaluations (no Q in the <ref type="figure">figure)</ref>.</p><p>The experiments in <ref type="figure" target="#fig_2">Figure 13</ref> confirm that it is much better to useπ β when the number of samples is small and not evaluating the Q values. The performance drop of using π is attenuated by evaluating the Q values at the root of the search tree, but it is still better to useπ β even in that case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Go Experiments</head><p>For the Go experiments, we mostly used the same neural network architecture, optimisation and hyperparameters used by MuZero  with the following differences. Instead of using the outcome of the game to train the value network, we used n-step bootstrapping with   n = 25 where the value used to bootstrap was the averaged predictions of a target network applied to 4 consecutive states at indices n + i for i ∈ [0, 3]. We averaged multiple consecutive target network value predictions due to the alternation of perspective for value prediction in two-player games; using the average of multiple estimates ensures that learning is based on the estimates for both sides. We observed that this reduced value overfitting and allowed us to train MuZero while generating less data. In addition, we used a search budget of 400 simulations per move instead of 800 in order to use less computation.</p><p>We evaluated the network checkpoints of MuZero and Sampled MuZero throughout training playing 100 matches with a search budget of 800 simulations per move. We anchored the Elo scale to a final MuZero baseline performance of 2000 Elo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Atari Experiments</head><p>For the Atari experiments, we used the same architecture, optimisation and hyperparameters used by MuZero (Schrittwieser et al., 2020).</p><p>We evaluated the network checkpoints of MuZero and Sampled MuZero throughout training playing 100 games with a search budget of 50 simulations per move.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Search</head><p>The full PUCT formula used in Sampled MuZero is: where we used lim K→∞β = β to go from line 1 to 2.</p><p>We therefore have Theorem. For a given random variable X, we have where we used the law of large numbers to go from line 2 to 3, replacing the expectation with the limit of a sum, and the lemma to go from line 3 to 4.</p><p>Using the central limit theorem from line 2, we can also show that as K → ∞, Making the approximation of swapping inẐ β for Z based on the lemma, we obtain that as K → ∞: Proof. We obtain the corollary by using X(s, a) = 1(a) in conjunction with Iπ(a|s) = E a∼Iπ [1(a)|s] and I β π(a|s) = b∈AÎ β π(s, b)1(a)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. The MuZero Policy Improvement Operator</head><p>Recent work <ref type="bibr" target="#b11">(Grill et al., 2020)</ref> showed that MuZero's visit count distribution was tracking the solutionπ of a regularised policy optimisation problem:</p><formula xml:id="formula_8">π = arg max Π Q T Π − λ N KL(π, Π)</formula><p>where KL is the Kullback-Leibler divergence and λ N is a constant dependent on c and the total number N of simulations.</p><p>π can be computed analytically:</p><p>π(a|s) = λ N π(s, a) Z(s) − Q(s, a)</p><p>where Z(s) is a normalising factor such that ∀a ∈ A,π(a|s) ≥ 0 and a∈Aπ (a|s) = 1.</p><p>In other words, using the terminology introduced in Section 4, MuZero's policy improvement can be approximately written:</p><p>Iπ <ref type="formula">(</ref> and is therefore action-independent.</p><p>Let's consider the visit count distribution Iπ β obtained by searching using priorπ β =β/βπ.</p><p>Using <ref type="bibr" target="#b11">(Grill et al., 2020)</ref>, we can write: whereẐ β (s) is such that ∀a ∈ A, Iπ β (s, a) ≥ 0 and a∈A Iπ β (s, a) = 1. This shows that Iπ β is the action-independent sample-based policy improvement operator associated to Iπ.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>policy improvement operator is Iπ(s, a) ∝ exp(Q(s, a)/τ ) where τ is a temperature parameter. Similarly, MPO's (Abdolmaleki et al., 2018) policy improvement operator can be written Iπ(s, a) ∝ π(s, a) exp(Q(s, a)/τ ) and AWR (Peng et al., 2019) uses a similar form of improved policy, replacing the action-value function by the advantage function Iπ(s, a) ∝ π(s, a) exp(A(s, a)/τ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Results in the classical board game of Go. Performance of Sampled MuZero (1 seed per experiment) with different number K ∈ {15, 25, 50, 100} of samples throughout training compared to a MuZero baseline that always considers all available actions (action space of size 362). Elo scale anchored to final baseline performance of 2000 Elo. As the number of sampled actions increases, performance rapidly approaches the baseline that always considers all actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Results in Ms. Pacman. Performance of Sampled MuZero (1 seed per experiment) with different number of samples throughout training compared to a MuZero baseline that always considers all available actions. As the number of sampled actions increases, performance rapidly approaches the baseline that always considers all actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Results in DM Control Suite Hard and Manipulator tasks. Performance of Sampled MuZero (3 seeds per experiment) throughout training compared to DMPO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Results in DM Control Suite Hard and Manipulator tasks of Sampled MuZero learning from raw pixel inputs. Performance of Sampled MuZero (3 seeds per experiment) learning from raw pixel inputs throughout training compared to Sampled MuZero learning from state inputs. The x-axis shows millions of environment frames, the y-axis mean episode return.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Results for 56D CMU Humanoid Locomotion tasks. Performance of Sampled MuZero (1 seed per experiment) throughout training in dm control</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Results in DM Control Suite tasks. Performance of Sampled MuZero (3 seeds per experiment) throughout training compared to DMPO (Hoffman et al., 2020) and D4PG (Barth-Maron et al., 2018)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Sampled MuZero results for the Real-Word RL benchmark. Performance of Sampled MuZero (3 seeds per experiment) throughout training on the easy, medium and hard variations of difficulty. The x-axis shows millions of environment frames, the y-axis mean episode return. Tasks are grouped into easy, medium and hard. Plot titles include the task name.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Comparison between a Categorical and Gaussian parameterisation of the policy prediction for Sampled MuZero. Performance of Sampled MuZero (3 seeds per experiment) throughout training on the DM Control Hard and Manipulator tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Performance of Sampled MuZero with different number of samples on the humanoid.run task. Performance of Sampled MuZero (6 seeds per experiment) throughout training on the DM Control Humanoid Run task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Reproducibility of Sampled MuZero from state and raw pixel inputs on the hard and manipulator tasks. Performance of Sampled MuZero (3 seeds per experiment) throughout training on the DM Control Humanoid Run task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Reproducibility of Sampled MuZero on the humanoid.run task with 3, 5, 10, 20 and 40 action samples. Performance of Sampled MuZero (6 seeds per experiment) throughout training on the DM Control Humanoid Run task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 .</head><label>13</label><figDesc>Performance of Sampled MuZero usingπ β vs π on the humanoid.run task. Performance of Sampled MuZero (3 seeds per experiment) throughout training on the DM Control Humanoid Run task evaluated with K = 5 or K = 20 samples and with or without (no Q) evaluating the Q values of all sampled actions at the root of the search tree. It is much more robust to usê π β over π in Sampled MuZero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>arg max a Q(s, a) + c(s) · (β β π)(s, a) b N (s, b) 1 + N (s, a) where c(s) = c 1 + log 1 + c 2 + b N (s, b) c 2with c 1 = 1.25 and c 2 = 19652 in the experiments for this paper. Note that at visit counts N (s) = b N (s, b) c 2 , the log in the exploration term is approximately 0 and the formula can be written:arg max a Q(s, a) + c 1 · (β β π)(s, a) b N (s, b) 1 + N (s, a)E. Sample-based Policy Improvement and Evaluation ProofsLemma.Ẑ β and Z are linked by: a,Ẑ β (s))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>the uniqueness of Z that lim K→∞Ẑβ = Z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>E</head><label></label><figDesc>a∼Iπ [X|s] = lim K→∞ a∈AÎ β π(a|s)X(s, a) Furthermore, a∈AÎ β π(a|s)X(s, a) is approximately normally distributed around E a∼Iπ [X|s] as K → ∞: a∈AÎ β π(a|s)X(s, a) ∼ N (E a∼Iπ [X|s], σ 2 K ) where σ 2 = V ar a∼β [ f (s,a,Z(s)) β X(s, a)|s]. Proof. We have E a∼Iπ [X(s, a)|s] = E a∼β [(Iπ/β)(a|s)X(s, a)|s]= E a∼β [f (s, a, Z(s))/β(a|s)X(s, a)|s] β)(a|s)f (s, a,Ẑ β (s))X(s, a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>a∈A (β/β)(a|s)f (s, a, Z(s))X(s, a) → N (E a∼Iπ [X|s], σ 2 K ) in distribution with σ 2 = V ar a∼β [ f (s,a,Z(s)) β X(s, a)|s].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>The sample-based policy improvement operator converges in distribution to the true policy improvement operator: limK→∞Î β π = IπFurthermore, the sample-based policy improvement operator is approximately normally distributed around the true policy improvement operator as K → ∞:I β π(a|s) ∼ N (Iπ(a|s), σ 2 K )where σ 2 = V ar a∼β [ f (s,a,Z(s)) β 1(a)|s].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>a|s) ≈ f (s, a, Z(s)) where f (s, a, Z(s)) = λ N π(a|s) Z(s) − Q(s, a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Iπ β (s, a) ≈ λ Nπ β (a|s) Z β (s) − Q(s, a) = λ N (β/βπ)(a|s) Z β (s) − Q(s, a)= (β/β)(a|s)f (s, a,Ẑ β (s))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance of Sampled MuZero compared to the Dreamer agent. Sampled MuZero equals or outperforms the Dreamer agent in all tasks. Dreamer results from</figDesc><table><row><cell>Tasks</cell><cell cols="2">Dreamer SMuZero</cell></row><row><cell>acrobot.swingup</cell><cell>365.26</cell><cell>417.52</cell></row><row><cell>cartpole.balance</cell><cell>979.56</cell><cell>984.86</cell></row><row><cell>cartpole.balance sparse</cell><cell>941.84</cell><cell>998.14</cell></row><row><cell>cartpole.swingup</cell><cell>833.66</cell><cell>868.87</cell></row><row><cell>cartpole.swingup sparse</cell><cell>812.22</cell><cell>846.91</cell></row><row><cell>cheetah.run</cell><cell>894.56</cell><cell>914.39</cell></row><row><cell>ball in cup.catch</cell><cell>962.48</cell><cell>977.38</cell></row><row><cell>finger.spin</cell><cell>498.88</cell><cell>986.38</cell></row><row><cell>finger.turn easy</cell><cell>825.86</cell><cell>972.53</cell></row><row><cell>finger.turn hard</cell><cell>891.38</cell><cell>963.07</cell></row><row><cell>hopper.hop</cell><cell>368.97</cell><cell>528.24</cell></row><row><cell>hopper.stand</cell><cell>923.72</cell><cell>926.50</cell></row><row><cell>pendulum.swingup</cell><cell>833.00</cell><cell>837.76</cell></row><row><cell>quadruped.run</cell><cell>888.39</cell><cell>923.54</cell></row><row><cell>quadruped.walk</cell><cell>931.61</cell><cell>933.77</cell></row><row><cell>reacher.easy</cell><cell>935.08</cell><cell>982.26</cell></row><row><cell>reacher.hard</cell><cell>817.05</cell><cell>971.53</cell></row><row><cell>walker.run</cell><cell>824.67</cell><cell>931.06</cell></row><row><cell>walker.stand</cell><cell>977.99</cell><cell>987.79</cell></row><row><cell>walker.walk</cell><cell>961.67</cell><cell>975.46</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">δa,a i represents the Kronecker delta function. In the continuous case, it would be replaced by the Dirac delta function δ(a − ai).4  We will omit the action-independent qualifier in the rest of the text when it is clear from the context.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Note that MuZero with a limited number of simulations will only visit the high prior moves</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Jost Tobias Springenberg for providing very detailed feedback and constructive suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bohez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Relative entropy regularized policy iteration</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distributed Distributional Deterministic Policy Gradients</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Informa-Learning and Planning in Complex Action Spaces tion theoretic model predictive q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning for Dynamics and Control</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="840" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Superhuman AI for heads-up no-limit poker: Libratus beats top professionals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sandholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">6374</biblScope>
			<biblScope unit="page" from="418" to="424" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagined value gradients: Model-based policy optimization with transferable latent dynamics models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neunert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="566" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive strategies for monte-carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chaslot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uiterwijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bouzy</surname></persName>
		</author>
		<idno type="DOI">10.1142/S1793005708001094</idno>
	</analytic>
	<monogr>
		<title level="j">New Mathematics and Natural Computation</title>
		<imprint>
			<biblScope unit="volume">04</biblScope>
			<biblScope unit="page" from="343" to="357" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient selectivity and backup operators in monte-carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on computers and games</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning in large discrete action spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sunehag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coppin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An empirical investigation of the challenges of real-world reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paduraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An operator view of policy gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Monte-carlo tree search as regularized policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Soft actorcritic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning latent dynamics for planning from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davidson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04551</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dream to control: Learning behaviors by latent imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01603</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1603.05027</idno>
		<ptr target="http://arxiv.org/abs/1603.05027" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haiku</surname></persName>
		</author>
		<ptr target="http://github.com/deepmind/dm-haiku" />
		<title level="m">Sonnet for JAX</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Behbahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baumli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00979</idno>
		<ptr target="https://arxiv.org/abs/2006.00979" />
		<title level="m">A research framework for distributed reinforcement learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A sparse sampling algorithm for near-optimal planning in large markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 16th International Joint Conference on Artificial Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1324" to="1331" />
		</imprint>
	</monogr>
	<note>IJ-CAI&apos;99</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dream and search to control: Latent space planning for continuous control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<ptr target="http://arxiv.org/abs/1711.05101" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical visuomotor control of humanoids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJfYvo09Y7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Moerland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Broekens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Jonker</surname></persName>
		</author>
		<title level="m">AlphaZero in continuous action space</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Expert-level artificial intelligence in heads-up no-limit poker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moravčík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lisỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Waugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">; X B</forename><surname>Deepstack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00177</idno>
	</analytic>
	<monogr>
		<title level="m">Advantage-weighted regression: Simple and scalable off-policy reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="page" from="508" to="513" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning and Planning in Complex Action Spaces Peng</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probabilistic planning with sequential monte carlo methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The calculation of posterior distributions by data augmentation: Comment: A noniterative sampling/importance resampling alternative to the data augmentation algorithm for creating a few imputations when fractions of missing information are modest: The sir algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<idno>01621459</idno>
		<ptr target="http://www.jstor.org/stable/2289460" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">398</biblScope>
			<biblScope unit="page" from="543" to="546" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using randomization to break the curse of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rust</surname></persName>
		</author>
		<idno>00129682, 14680262</idno>
		<ptr target="http://www.jstor.org/stable/2171751" />
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="487" to="516" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="issue">7839</biblScope>
			<biblScope unit="page" from="604" to="609" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v32/silver14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>Xing, E. P. and Jebara, T.</editor>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="22" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6419</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On-policy maximum a posteriori policy optimization for discrete and continuous control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>V-Mpo</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SylOlp4FvH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Local search for policy iteration in continuous control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05545</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<ptr target="http://incompleteideas.net/papers/sutton-88-with-erratum.pdf" />
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Discretizing continuous action space for on-policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Deepmind control suite</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bohez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12983</idno>
		<title level="m">dm control: Software and tasks for continuous control</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2012.6386109</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Continuous control for searching and planning with a learned model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Duvaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A self-tuning actor-critic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
