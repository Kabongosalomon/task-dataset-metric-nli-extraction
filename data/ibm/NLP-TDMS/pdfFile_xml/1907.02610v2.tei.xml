<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Robustness through Local Linearization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongli</forename><surname>Qin</surname></persName>
							<email>chongliqin@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Gowal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Dj)</roleName><forename type="first">Google</forename><surname>Krishnamurthy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dvijotham</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><forename type="middle">Kohli</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<affiliation>
								<orgName>DeepMind </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> DeepMind </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> DeepMind </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> DeepMind </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> DeepMind </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Robustness through Local Linearization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adversarial training is an effective methodology for training deep neural networks that are robust against adversarial, norm-bounded perturbations. However, the computational cost of adversarial training grows prohibitively as the size of the model and number of input dimensions increase. Further, training against less expensive and therefore weaker adversaries produces models that are robust against weak attacks but break down under attacks that are stronger. This is often attributed to the phenomenon of gradient obfuscation; such models have a highly non-linear loss surface in the vicinity of training examples, making it hard for gradient-based attacks to succeed even though adversarial examples still exist. In this work, we introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data, thereby penalizing gradient obfuscation while encouraging robustness. We show via extensive experiments on CIFAR-10 and ImageNet, that models trained with our regularizer avoid gradient obfuscation and can be trained significantly faster than adversarial training. Using this regularizer, we exceed current state of the art and achieve 47% adversarial accuracy for ImageNet with ∞ adversarial perturbations of radius 4/255 under an untargeted, strong, white-box attack. Additionally, we match state of the art results for CIFAR-10 at 8/255.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a seminal paper, Szegedy et al. <ref type="bibr" target="#b24">[25]</ref> demonstrated that neural networks are vulnerable to visually imperceptible but carefully chosen adversarial perturbations which cause neural networks to output incorrect predictions. After this revealing study, a flurry of research has been conducted with the focus of making networks robust against such adversarial perturbations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>. Concurrently, researchers devised stronger attacks that expose previously unknown vulnerabilities of neural networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Of the many approaches proposed <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>, adversarial training <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> is empirically the best performing algorithm to train networks robust to adversarial perturbations. However, the cost of adversarial training becomes prohibitive with growing model complexity and input dimensionality. This is primarily due to the cost of computing adversarial perturbations, which is incurred at each step of adversarial training. In particular, for each new mini-batch one must perform multiple iterations of 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. a gradient-based optimizer on the network's inputs to find said perturbations. <ref type="bibr" target="#b0">1</ref> As each step of this optimizer requires a new backwards pass, the total cost of adversarial training scales as roughly the number of such steps. Unfortunately, effective adversarial training of ImageNet often requires large number of steps to avoid problems of gradient obfuscation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>, making it much more expensive than conventional training, almost prohibitively so. One approach which can alleviate the cost of adversarial training is training against weaker adversaries that are cheaper to compute. For example, by taking fewer gradient steps to compute adversarial examples during training. However, this can produce models which are robust against weak attacks, but break down under strong attacks -often due to gradient obfuscation. In particular, one form of gradient obfuscation occurs when the network learns to fool a gradient based attack by making the loss surface highly convoluted and non-linear (see <ref type="figure" target="#fig_0">Fig 1)</ref>, this has also been observed by Papernot et al <ref type="bibr" target="#b19">[20]</ref>. In turn the nonlinearity prevents gradient based optimization methods from finding an adversarial perturbation within a small number of iterations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>. In contrast, if the loss surface was linear in the vicinity of the training examples, which is to say well-predicted by local gradient information, gradient obfuscation cannot occur. In this paper, we take up this idea and introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data. We call this regularizer the local linearity regularizer (LLR). Empirically, we find that networks trained with LLR exhibit far less gradient obfuscation, and are almost equally robust against strong attacks as they ares against weak attacks.</p><p>The main contributions of our paper are summarized below:</p><p>• We show that training with LLR is significantly faster than adversarial training, allowing us to train a robust ImageNet model with a 5× speed up when training on 128 TPUv3 cores <ref type="bibr" target="#b8">[9]</ref>. • We show that LLR trained models exhibit higher robustness relative to adversarially trained models when evaluated under strong attacks. Adversarially trained models can exhibit a decrease in accuracy of 6% when increasing the attack strength at test time for CIFAR-10, whereas LLR shows only a decrease of 2%. • We achieve new state of the art results for adversarial accuracy against untargeted white-box attack for ImageNet (with = 4/255 2 ): 47%. Furthermore, we match state of the art results for CIFAR 10 (with = 8/255): 52.81% 3 . • We perform a large scale evaluation of existing methods for adversarially robust training under consistent, strong, white-box attacks. For this we recreate several baseline models from the literature, training them both for CIFAR-10 and ImageNet (where possible). <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>We denote our classification function by f (x; θ) : x → R C , mapping input features x to the output logits for classes in set C, i.e. p i (y|x; θ) = exp (f i (x; θ)) / j exp (f j (x; θ)), with θ being the model parameters and y being the label. Adversarial robustness for f is defined as follows: a network is robust to adversarial perturbations of magnitude at input x if and only if argmax</p><formula xml:id="formula_0">i∈C f i (x; θ) = argmax i∈C f i (x + δ; θ) ∀δ ∈ B p ( ) = {δ : δ p ≤ }.<label>(1)</label></formula><p>In this paper, we focus on p = ∞ and we use B( ) to denote B ∞ ( ) for brevity. Given the dataset is drawn from distribution D, the standard method to train a classifier f is empirical risk minimization (ERM), which is defined by: min θ E (x,y)∼D [ (x; y, θ)]. Here, (x; y, θ) is the standard cross-entropy loss function defined by (x; y, θ) = −y T log (p(x; θ)) , (2) where p i (x; θ) is defined as above, and y is a 1-hot vector representing the class label. While ERM is effective at training neural networks that perform well on holdout test data, the accuracy on the test set goes to zero under adversarial evaluation. This is a result of a distribution shift in the data induced by the attack. To rectify this, adversarial training <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14]</ref> seeks to perturb the data distribution by performing adversarial attacks during training. More concretely, adversarial training minimizes the loss function</p><formula xml:id="formula_1">E (x,y)∼D max δ∈B( ) (x + δ; y, θ) ,<label>(3)</label></formula><p>where the inner maximization, max δ∈B( ) (x + δ; y, θ), is typically performed via a fixed number of steps of a gradient-based optimization method. One such method is Projected-Gradient-Descent (PGD) which performs the following gradient step:</p><formula xml:id="formula_2">δ ← Proj (δ − η∇ δ (x + δ; y, θ)) ,<label>(4)</label></formula><p>where Proj(x) = argmin ξ∈B( ) x − ξ . Another popular gradient-based method is to use the sign of the gradient <ref type="bibr" target="#b7">[8]</ref>. The cost of solving Eq (3) is dominated by the cost of solving the inner maximization problem. Thus, the inner maximization should be performed efficiently to reduce the overall cost of training. A naive approach is to reduce the number of gradient steps performed by the optimization procedure. Generally, the attack is weaker when we do fewer steps. If the attack is too weak, the trained networks often display gradient obfuscation as highlighted in <ref type="figure" target="#fig_0">Fig 1.</ref> Since the invention of adversarial training, a corpus of work has researched alternative ways of making networks robust. One such approach is the TRADES method <ref type="bibr" target="#b29">[30]</ref> which is a form of regularization that specifically maximizes the trade-off between robustness and accuracy -as many studies have observed these two quantities to be at odds with each other <ref type="bibr" target="#b25">[26]</ref>. Others, such as work by Ding et al <ref type="bibr" target="#b6">[7]</ref> adaptively increase the perturbation radius by find the minimal length perturbation which changes the output label. Some have proposed architectural changes which promote adversarial robustness, such as the "denoise" model <ref type="bibr" target="#b27">[28]</ref> for ImageNet.</p><p>The work presented here is a regularization technique which enforces the loss function to be well approximated by its linear Taylor expansion in a sufficiently small neighbourhood. There has been work before which uses gradient information as a form of regularization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19]</ref>. The work presented in this paper is closely related to the paper by Moosavi et al <ref type="bibr" target="#b18">[19]</ref>, which highlights that adversarial training reduces the curvature of (x; y, θ) with respect to x. Leveraging an empirical observation (the highest curvature is along the direction ∇ x (x; y, θ)), they further propose an algorithm to mimic the effects of adversarial training on the loss surface. The algorithm results in comparable performance to adversarial training with a significantly lower cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivating the Local Linearity Regularizer</head><p>As described above, the cost of adversarial training is dominated by solving the inner maximization problem max δ∈B( ) (x + δ). Throughout we abbreviate (x; y, θ) with (x). We can reduce this cost simply by reducing the number of PGD (as defined in Eq (4)) steps taken to solve max δ∈B( ) (x + δ). To motivate the local linearity regularizer (LLR), we start with an empirical analysis of how the behavior of adversarial training changes as we increase the number of PGD steps used during training. We find that the loss surface becomes increasingly linear as we increase the number of PGD steps, captured by the local linearity measure defined below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local Linearity Measure</head><p>Suppose that we are given an adversarial perturbation δ ∈ B( ). The corresponding adversarial loss is given by (x + δ). If our loss surface is smooth and approximately linear, then (x + δ) is well approximated by its first-order Taylor expansion (x) + δ T ∇ x (x). In other words, the absolute difference between these two values,</p><formula xml:id="formula_3">g(δ; x) = (x + δ) − (x) − δ T ∇ x (x) ,<label>(5)</label></formula><p>is an indicator of how linear the surface is. Consequently, we consider the quantity</p><formula xml:id="formula_4">γ( , x) = max δ∈B( ) (x + δ) − (x) − δ T ∇ x (x) ,<label>(6)</label></formula><p>to be a measure of how linear the surface is within a neighbourhood B( ). We call this quantity the local linearity measure.   <ref type="figure">Figure 2</ref>: Plots showing that γ( , x) is large (on the order of 10 1 ) when we train with just one or two steps of PGD for inner maximization, (2a). In contrast, γ( , x) becomes increasingly smaller (on the order of 10 −1 ) as we increase the number of PGD steps to 4 and above, (2b). The x-axis is the number of training iterations and the y-axis is γ( , x), here = 8/255 for CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Empirical Observations on Adversarial Training</head><p>We measure γ( , x) for networks trained with adversarial training on CIFAR-10, where the inner maximization max δ∈B( ) (x + δ) is performed with 1, 2, 4, 8 and 16 steps of PGD. γ( , x) is measured throughout training on the training set <ref type="bibr" target="#b4">5</ref> . The architecture used is a wide residual network <ref type="bibr" target="#b28">[29]</ref> 28 in depth and 10 in width (Wide-ResNet-28-10). The results are shown in <ref type="figure">Fig 2a and 2b. Fig 2a</ref> shows that when we train with one and two steps of PGD for the inner maximization, the local loss surface is extremely non-linear. An example visualization of such a loss surface is given in <ref type="figure" target="#fig_6">Fig 5a.</ref> However, when we train with four or more steps of PGD for the inner maximization, the surface is relatively well approximated by (x) + δ T ∇ x (x) as shown in <ref type="figure">Fig 2b.</ref> An example of the loss surface is shown in <ref type="figure" target="#fig_6">Fig 5b.</ref> For the adversarial accuracy of the networks, see <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Local Linearity Regularizer (LLR)</head><p>From the section above, we make the empirical observation that the local linearity measure γ( , x) decreases as we train with stronger attacks <ref type="bibr" target="#b5">6</ref> . In this section, we give some theoretical justifications of why local linearity γ( , x) correlates with adversarial robustness, and derive a regularizer from the local linearity measure that can be used for training of robust models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Local Linearity Upper Bounds Adversarial Loss</head><p>The following proposition establishes that the adversarial loss (x + δ) is upper bounded by the local linearity measure, plus the change in loss as predicted by the gradient (which is given by |δ T ∇ x (x)|). </p><formula xml:id="formula_5">| (x + δ) − (x)| ≤ |δ T ∇ x (x)| + γ( , x).<label>(7)</label></formula><p>See Appendix B for the proof.</p><p>From Eq <ref type="bibr" target="#b6">(7)</ref> it is clear that the adversarial loss tends to (x), i.e., (x + δ) → (x), as both |δ ∇ x (x)| → 0 and γ( ; x) → 0 for all δ ∈ B( ). And assuming (x + δ) ≥ (δ) one also has the</p><formula xml:id="formula_6">upper bound (x + δ) ≤ (x) + |δ T ∇ x (x)| + γ( , x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Local Linearity Regularization (LLR)</head><p>Following the analysis above, we propose the following objective for adversarially robust training</p><formula xml:id="formula_7">L(D) = E D (x) + λγ( , x) + µ|δ T LLR ∇ x (x)| LLR ,<label>(8)</label></formula><p>where λ and µ are hyper-parameters to be optimized, and δ LLR = argmax δ∈B( ) g(δ; x) (recall the definition of g(δ; x) from Eq <ref type="formula" target="#formula_3">(5)</ref>). We highlighted γ( , x) as this is the local linearity term which measures how well approximated the loss is with respect to its linear Taylor approximation. Concretely, we are trying to find the point</p><formula xml:id="formula_8">δ LLR in B( ) where the linear approx- imation (x) + δ T ∇ x (x) is maximally violated. To train we penalize both its linear violation γ( , x) = (x + δ LLR ) − (x) − δ T LLR ∇ x (x) and the gradient magnitude term δ T LLR ∇ x (x)</formula><p>, as required by the above proposition. We note that, analogous to adversarial training, LLR requires an inner optimization to find δ LLR -performed via gradient descent. However, as we will show in the experiments, much fewer optimization steps are required for the overall scheme to be effective. Pseudo-code for training with this regularizer is given in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Local Linearity γ( ; x) is a sufficient regularizer by itself</head><p>Interestingly, under certain reasonable approximations and standard choices of loss functions, we can bound |δ ∇ x (x)| in terms of γ( ; x). See Appendix C for details. Consequently, the bound in Eq <ref type="bibr" target="#b6">(7)</ref> implies that minimizing γ( ; x) (along with the nominal loss (x)) is sufficient to minimize the adversarial loss (x + δ). This prediction is confirmed by our experiments. However, our experiments also show that including |δ ∇ x (x)| in the objective along with (x) and γ( ; x) works better in practice on certain datasets, especially ImageNet. See Appendix F.3 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>We perform experiments using LLR on both CIFAR-10 <ref type="bibr" target="#b12">[13]</ref> and ImageNet <ref type="bibr" target="#b4">[5]</ref> datasets. We show that LLR gets state of the art adversarial accuracy on CIFAR-10 (at = 8/255) and ImageNet (at = 4/255) evaluated under a strong adversarial attack. Moreover, we show that as the attack strength increases, the degradation in adversarial accuracy is more graceful for networks trained using LLR than for those trained with standard adversarial training. Further, we demonstrate that training using LLR is 5× faster for ImageNet. Finally, we show that, by linearizing the loss surface, models are less prone to gradient obfuscation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10:</head><p>The perturbation radius we examine is = 8/255 and the model architectures we use are Wide-ResNet-28-8, Wide-ResNet-40-8 <ref type="bibr" target="#b28">[29]</ref>. Since the validity of our regularizer requires (x) to be smooth, the activation function we use is softplus function log(1 + exp(x)), which is a smooth version of ReLU. The baselines we compare our results against are adversarial training (ADV) <ref type="bibr" target="#b15">[16]</ref>, TRADES <ref type="bibr" target="#b29">[30]</ref> and CURE <ref type="bibr" target="#b18">[19]</ref>. We recreate these baselines from the literature using the same network architecture and activation function. The evaluation is done on the full test set of 10K images.</p><p>ImageNet: The perturbation radii considered are = 4/255 and = 16/255. The architecture used for this is from <ref type="bibr" target="#b9">[10]</ref> which is ResNet-152. We use softplus as activation function. For = 4/255, the baselines we compare our results against is our recreated versions of ADV <ref type="bibr" target="#b15">[16]</ref> and denoising model (DENOISE) <ref type="bibr" target="#b27">[28]</ref>. 7 For = 16/255, we compare LLR to ADV <ref type="bibr" target="#b15">[16]</ref> and DENOISE <ref type="bibr" target="#b27">[28]</ref> networks which have been published from the the literature. Due to computational constraints, we limit ourselves to evaluating all models on the first 1K images of the test set.</p><p>To make sure that we have a close estimate of the true robustness, we evaluate all the models on a wide range of attacks these are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Setup</head><p>To accurately gauge the true robustness of our network, we tailor our attack to give the lowest possible adversarial accuracy. The two parts which we tune to get the optimal attack is the loss function for the attack and its corresponding optimization procedure. The loss functions used are described below, for the optimization procedure please refer to Appendix F.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions:</head><p>The three loss functions we consider are summarized in <ref type="table" target="#tab_0">Table 1</ref>. We use the difference between logits for the loss function rather than the cross-entropy loss as we have empirically found the former to yield lower adversarial accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attack Name</head><p>Loss  For CIFAR-10, the main adversarial accuracy results are given in <ref type="table" target="#tab_1">Table 2</ref>. We compare LLR training to ADV <ref type="bibr" target="#b15">[16]</ref>, CURE <ref type="bibr" target="#b18">[19]</ref> and TRADES <ref type="bibr" target="#b29">[30]</ref>, both with our re-implementation and the published models <ref type="bibr" target="#b7">8</ref> . Note that our re-implementation using softplus activations perform at or above the published results for ADV, CURE and TRADES. This is largely due to the learning rate schedule used, which is the similar to the one used by TRADES <ref type="bibr" target="#b29">[30]</ref>.  For ImageNet, we compare against adversarial training (ADV) <ref type="bibr" target="#b15">[16]</ref> and the denoising model (DE-NOISE) <ref type="bibr" target="#b27">[28]</ref>. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. For a perturbation radius of 4/255, LLR gets 47% adversarial accuracy under the Untargeted attack which is notably higher than the adversarial accuracy obtained via adversarial training which is 39.70%. Moreover, LLR is trained with just two-steps of PGD rather than 30 steps for adversarial training. The amount of computation needed for each method is further discussed in Sec 5.2.1. <ref type="table" target="#tab_3">Table 3</ref> are the results for = 16/255. We note a significant drop in nominal accuracy when we train with LLR to perturbation radius 16/255. When testing for perturbation radius of 16/255 we also show that the adversarial accuracy under Untargeted is very poor (below 8%) for all methods. We speculate that this perturbation radius is too large for the robustness problem. Since adversarial perturbations should be, by definition, imperceptible to the human eye, upon inspection of the images generated using an adversarial attack (see <ref type="figure" target="#fig_9">Fig 8)</ref> -this assumption no longer holds true. The images generated appear to consist of super-imposed object parts of other classes onto the target image. This leads us to believe that a more fine-grained analysis of what should constitute "robustness for ImageNet" is an important topic for debate.</p><formula xml:id="formula_9">Function Metric Random-Targeted max δ∈B( ) f r (x + δ) − f t (x + δ) Attack Success Rate Untargeted max δ∈B( ) f s (x + δ) − f t (x + δ) Adversarial Accuracy Multi-Targeted max δ∈B( ) max i∈C f i (x + δ) − f t (x + δ) Adversarial Accuracy</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Runtime Speed</head><p>For ImageNet, we trained on 128 TPUv3 cores <ref type="bibr" target="#b8">[9]</ref>, the total training wall time for the LLR network (4/255) is 7 hours for 110 epochs. Similarly, for the adversarially trained (ADV) networks the total wall time is 36 hours for 110 epochs. This is a 5× speed up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Accuracy Degradation: Strong vs Weak Evaluation</head><p>The resulting model trained using LLR degrades gracefully in terms of adversarial accuracy when we increase the strength of attack, as shown in <ref type="figure">Fig 3.</ref> In particular, <ref type="figure">Fig 3a shows</ref> that, for CIFAR-10, when the attack changes from Untargeted to Multi-Targeted, the LLR's accuracy remains similar with only a 2.18% drop in accuracy. Contrary to adversarial training (ADV), where we see a 5.64% drop in accuracy. We also see similar trends in accuracy in <ref type="table" target="#tab_1">Table 2</ref>. This could indicate that some level of obfuscation may be happening under standard adversarial training.</p><p>As we empirically observe that LLR evaluates similarly under weak and strong attacks, we hypothesize that this is because LLR explicitly linearizes the loss surface. An extreme case would be when the surface is completely linear -in this instance the optimal adversarial perturbation would be found with just one PGD step. Thus evaluation using a weak attack is often good enough to get an accurate gauge of how it will perform under a stronger attack.</p><p>For ImageNet, see <ref type="figure">Fig 3b,</ref> the adversarial accuracy trained using LLR remains significantly higher (7.5%) than the adversarially trained network going from a weak to a stronger attack.  We use either the standard adversarial training objective (ADV-1, ADV-2) or the LLR objective (LLR-1, LLR-2) and taking one or two steps of PGD to maximize each objective. To train LLR-1/2, we only optimize the local linearity γ( , x), i.e. µ in Eq. (8) is set to zero. We see that for adversarial training, as shown in <ref type="figure">Figs 4a, 4c</ref>, the loss surface becomes highly non-linear and jagged -in other words obfuscated. Additionally in this setting, the adversarial accuracy under our strongest attack is 0% for both, see <ref type="table">Table 6</ref>. In contrast, the loss surface is smooth when we train using LLR as shown in <ref type="figure">Figs 4b, 4d</ref>. Further, <ref type="table">Table 6</ref> shows that we obtain an adversarial accuracy of 44.50% with the LLR-2 network under our strongest evaluation. We also evaluate the values of γ( , x) for the CIFAR-10 test set after these networks are trained. This is shown in <ref type="figure" target="#fig_12">Fig 7.</ref> The values of γ( , x) are comparable when we train with LLR using two steps of PGD to adversarial training with 20 steps of PGD. By comparison, adversarial training with two steps of PGD results in much larger values of γ( , x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Resistance to Gradient Obfuscation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We show that, by promoting linearity, deep classification networks are less susceptible to gradient obfuscation, thus allowing us to do fewer gradient descent steps for the inner optimization. Our novel linearity regularizer promotes locally linear behavior as justified from a theoretical perspective. The resulting models achieve state of the art adversarial robustness on the CIFAR-10 and Imagenet datasets, and can be trained 5× faster than regular adversarial training.    <ref type="table">Table 4</ref>: <ref type="table">Table showing the</ref>  Proof. Firstly we note that | (x + δ) − (x)| can be rewritten as the following:</p><formula xml:id="formula_10">| (x + δ) − (x)| = δ T ∇ x (x) + (x + δ) − (x) − δ T ∇ x (x)</formula><p>. Thus we can form the following bound:</p><formula xml:id="formula_11">| (x + δ) − (x)| ≤ δ T ∇ x (x) + g(δ; x), where g(δ; x) = (x + δ) − (x) − δ T ∇ x (x)</formula><p>. We note that since γ( , x) = max δ∈B( ) g(δ; x), therefore for all δ ∈ B( )</p><formula xml:id="formula_12">| (x + δ) − (x)| ≤ δ T ∇ x (x) + γ( , x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Local Linearity γ( , x) is a sufficient regularizer by itself C.1 A local quadratic model of the loss</head><p>The starting point for proving our bounds will be the following local quadratic approximation of the loss:</p><formula xml:id="formula_13">(x + δ) = (x) + δ ∇ x (x) + 1 2 δ G(x)δ + ε(δ),<label>(9)</label></formula><p>Here, G(x) is the Generalized Gauss-Newton matrix (GGN) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18]</ref>, and ε(δ) denotes the error of the approximation.</p><p>The GGN is a Hessian-alternative which appears frequently in approximate 2nd-order optimization algorithms for neural networks. It is defined for losses of the form (x) = ν(y, f (x)), where ν(y, z) is convex in z.</p><p>(Valid examples for ν(y, z) include the standard softmax cross-entropy error and squared error.) It's given by</p><formula xml:id="formula_14">G(x) = J H ν J,</formula><p>where J is the Jacobian of f , and H ν is the Hessian of ν(y, z) with respect to z.</p><p>One interpretation of the GGN is that it's the Hessian of a modified lossˆ (x) ≡ ν(y,f (x)), wherê f is the local linear approximation of f (given byf (x + δ) = Jδ + f (x)). For certain standard loss functions (including the ones we consider) it also corresponds to the Fisher Information Matrix associated with the network's predictive distribution <ref type="bibr" target="#b17">[18]</ref>.</p><p>In the context of optimization, the local quadratic approximation induced by the GGN tends to work better than the actual 2nd-order Taylor series [e.g. 17], perhaps because it gives a better approximation to (x + δ) over non-trivial distances <ref type="bibr" target="#b17">[18]</ref>. (It must necessarily be a worse approximation for very small values of δ, since the 2nd-order Taylor series is clearly optimal in that respect.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Bounds for common loss functions</head><p>Our basic strategy in proving the following results is to rearrange Eq (9) to establish the following bound on the curvature in terms of g(δ; x) which is defined in Eq (5) in the main text:</p><formula xml:id="formula_15">1 2 δ G(x)δ = (x + δ) − ( (x) + δ ∇ x (x)) − ε(δ) ≤ | (x + δ) − ( (x) + δ ∇ x (x))| + |ε(δ)| = g(δ; x) + |ε(δ)|.<label>(10)</label></formula><p>We then show that for both the squared error and softmax cross-entropy loss functions, one can bound |δ ∇ x (x)| in terms of the curvature g(δ; x) and by extension is bounded by the local linearity measure: γ( ; x) = max δ∈B( ) g(δ; x). Note that such a bound won't exist for general loss functions.</p><p>Proposition C.1. Suppose that ν(y, z) = 1 2 y − z 2 is the squared error and z = f (x; θ) is the output of the neural network. Then for any perturbation vector δ ∈ B( ) we have</p><formula xml:id="formula_16">|δ ∇ x (x)| ≤ 2 2 (x)(γ( ; x) + |ε(δ)|),</formula><p>where ε(δ) is the error of the local quadratic approximation defined in Equation 9.</p><p>Proposition C.2. Suppose that ν(y, z) = log(y p(z)) is the softmax cross-entropy error, where y is a 1-hot target vector, and p(z) is the vector of probabilities computed via the softmax function. Then for any perturbation vector δ ∈ B( ) we have</p><formula xml:id="formula_17">|δ ∇ x (x)| ≤ 2 y p(z) (γ( ; x) + |ε(δ)|),</formula><p>where ε(δ) is the error of the local quadratic approximation defined in Equation 9.</p><p>Remark. We note p y is just the probability of the target label under the model. And so 1/p y won't be very big, provided that the model is properly classifying the data with some reasonable degree of certainty. (Indeed, for highly certain predictions it will be close to 1.) Thus the upper bound given in Proposition C.2 should shrink at a reasonable rate as the regularizer γ( ; x) does, provided that error term ε(δ) is negligable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proofs</head><p>D.1 Proof of Proposition C.1</p><p>Proof. For convenience we will write (x) = 1 2 r(x) 2 , where we have defined r(x) = y − f (x).</p><p>We observe that for the squared error loss, ∇ x (x) = −J r and G(x) = J J (because H ν = I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thus by Equation 10 we have</head><formula xml:id="formula_18">Jδ 2 = δ J Jδ = δ G(x)δ ≤ 2(g(δ; x) + |ε(δ)|) ≤ 2(γ( ; x) + |ε(δ)|).</formula><p>Using these facts, and applying the Cauchy-Schwarz inequality, we get</p><formula xml:id="formula_19">|δ ∇ x (x)| 2 = | − δ J r| 2 = |(Jδ) r| 2 ≤ Jδ 2 r 2 ≤ 8(γ( ; x) + |ε(δ)|) (x).</formula><p>Taking the square root of both sides yields the claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Proof of Proposition C.2</head><p>Proof. We begin by defining r(x) = y − p, and observing that for the softmax cross-entropy loss,</p><formula xml:id="formula_20">∇ x (x) = −J r, and H(x) = J H ν (z)J where H ν (z) = diag(p) − pp .</formula><p>Because the entries of p are non-negative and sum to 1 we can factor this as</p><formula xml:id="formula_21">H ν = CC , where C = diag(q) − pq ,</formula><p>and where q is defined as the entry-wise square root of the vector p. To see that this is correct, note that</p><formula xml:id="formula_22">CC = (diag(q) − pq )(diag(q) − pq ) = diag(q) 2 − diag(q)qp − pq diag(q) + pq qp = diag(p) − pp − pp + pp = H ν ,</formula><p>where we have used the properties of q and p, such as q q = 1, diag(q)q = p, etc.</p><p>Using this factorization we can rewrite the curvature term as</p><formula xml:id="formula_23">δ G(x)δ = δ J H ν (z)Jδ = ∆z H ν (z)∆z = ∆z CC ∆z = C ∆z 2 ,</formula><p>where we have defined ∆z = Jδ (intuitively, this is "the change in z due to δ"). Thus by Equation <ref type="bibr" target="#b9">10</ref> we have C ∆z 2 ≤ 2(g(δ; x) + |ε(δ)|) ≤ 2(γ( ; x) + |ε(δ)|). Let v = 1 q y y, which is well defined because q is entry-wise positive (since p must be), and y is a one-hot vector. Using said properties of y and q we have that</p><formula xml:id="formula_24">Cv = (diag(q) − pq ) 1 q y y = 1 q y q y − 1 q y p(q y) = y − p = r,</formula><p>where denotes the entry-wise product.</p><p>It thus follows that δ ∇ x (x) = −δ J r = z r = ∆z (Cv) = (C ∆z) v.</p><p>Using the above facts, and applying the Cauchy-Schwarz inequality, we arrive at</p><formula xml:id="formula_25">|δ ∇ x (x)| 2 = |(C ∆z) v| 2 ≤ C ∆z 2 v 2 ≤ 2(γ( ; x) + |ε(δ)|) 1 (q y) 2 y 2 = 2 p y (γ( ; x) + |ε(δ)|),</formula><p>where we have used the facts that (q y) 2 = p y and y = 1. Taking the square root of both sides yields the claim. </p><formula xml:id="formula_26">Get mini-batch B = {(x i1 , y i1 ), · · · , (x i b , y i b )}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Calculte loss wrt to minibatch L B = 1 b b j=1 (x ij ; y ij ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Initialize initial perturbation δ uniformly in the interval [− , ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for all j ∈ {0, 1, . . . , M } do 7: </p><formula xml:id="formula_27">Calculate g = 1 b b t=1 ∇ δ g(δ; x it , y it ) at δ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Experiments and Results: Supplementary F.1 Evaluation Setup</head><p>Optimization: Rather than using the sign of the gradient (FGSM) <ref type="bibr" target="#b7">[8]</ref>, we do the update steps using Adam <ref type="bibr" target="#b11">[12]</ref> as the optimizer. More concretely, the update on the adversarial perturbation is δ ← Proj (δ − ηAdam(∇ δ l(x + δ; y))). We have consistently found that using Adam gives a stronger attack compared to the sign of the gradient. For Multi-Targeted (see <ref type="table" target="#tab_0">Table 1</ref>), the step size is set to be η = 0.1 and we run for 200 steps. For Untargeted and Random-Targeted, we use a step size schedule setting η = 0.1 up until 100 steps then 0.01 up until 150 steps and 0.001 for the last 50 steps. We find these to give us the best adversarial accuracy evaluation, the decrease in step size is especially helpful in cases where the gradient is obfuscated. Furthermore, we use 20 different random initialization (we term this a random restart) of the perturbation, δ 0 , for going through the optimization procedure. We consider an attack successful if any of these 20 random restarts is successful. For CIFAR-10 we also show results for FGSM with 20 steps (FGSM-20) with a step size /10 as this is a commonly used attack for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Training and Hyperparameters</head><p>The hyperparameters for λ and µ are chosen by doing a hyperparameter sweep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10:</head><p>For all of the baselines we recreated and the LLR network we used the same schedule which is inspired by TRADES <ref type="bibr" target="#b29">[30]</ref>. For Wide-ResNet-28-8, we use initial learning rate 0.1 and we decrease after 100 and 105 epochs. We train till 110 epochs. For Wide-ResNet-40-8 we use initial learning rate 0.1 and we decrease after 100 and 105 epochs with a factor of 0.1. We train to 110 epochs. The optimizer we used momentum 0.9. For LLR the λ = 4 and µ = 3, the weight placed on the nominal loss (x) is also 2. We use l 2 -regularization of 2e-4. The training is done on a batch size of 256. We also slowly increase the size of the perturbation radius over 15 epochs starting from 0.0 until it gets to 8/255. For Wide-ResNet-28-8, Wide-ResNet-40-8 we train with 10 and 15 steps of PGD respectively using Adam with step size of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet (4/255):</head><p>To train the LLR network the initial learning rate is 0.1, the decay schedule is similar to <ref type="bibr" target="#b27">[28]</ref>, we decay by 0.1 after 35, 70 and 95 epochs. We train for 100 epochs. The LLR hyperparameters are λ = 3 and µ = 6, the weights placed on the nominal loss is 3. We use l 2regularization of 1e-4. The training is done on batch size of 512. We slowly increase the perturbation radius over 20 epochs from 0 to 4/255. We train with 2 steps of PGD using Adam and step size 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet (16/255):</head><p>To train the LLR network the initial learning rate is 0.1, we decay by 0.1 after 17 and 35 epochs and 50 epochs -we train to 55 epochs. The LLR hyperparameters are λ = 3 and µ = 9, the weights placed on the nominal loss is 3. We use l 2 -regularization of 1e-4. The training is done on batch size of 512. We slowly increase the perturbation radius over 90 epochs from 0 to 16/255. We train with 10 steps of PGD using Adam with step size of 0.1.</p><p>Batch Normalization During training we use the local batch statistics at the nominal point. Suppose µ, σ denotes the local batch statistics at every layer of the network for point x. Let us also denote (x; y, µ, σ) to be the loss function corresponding to when we use batch statistics µ and σ. Then the loss we calculate at train time is the following  <ref type="table">Table 5</ref>: By removing δ T ∇ x l(x) B from LLR shown in Eq. (8), the adversarial accuracy evaluated using multi-targeted reduces by 1.75% for CIFAR-10 while the adversarial reduces by 5.70%.</p><formula xml:id="formula_28">(x; y, µ, σ) + µ δ T LLR ∇ x (x; y, µ, σ) + λ max δ∈B( )</formula><p>We investigate the effects of adding the term δ T ∇ x l(x) into LLR shown in Eq. <ref type="bibr" target="#b7">(8)</ref>. The results are shown in <ref type="table">Table 5</ref>. We can see that adding the term δ T ∇ x (x) only yields minor improvements to the adversarial accuracy (49.38% vs 51.13%) for CIFAR-10, while we get a boost of almost 6% adversarial accuracy for ImageNet (41.30% vs 47.00%).  <ref type="table">Table 6</ref>: This shows that LLR trained with even just two steps of PGD can get an adversarial accuracy of 44.50% under the strongest evaluation, while both adversarially trained networks (ADV-1, ADV-2) gets 0.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Resistance to Gradient Obfuscation</head><p>In <ref type="figure">Fig 6 we</ref> show the adversarial perturbations for networks ADV-2 and LLR-2. We see that, in contrast to LLR-2, the adversarial perturbation for ADV-2 looks similar to random noise. When the adversarial perturbation resembles random noise, this is often a sign that the network is gradient obfuscated.</p><p>Furthermore, we show that the adversarial accuracy for LLR-2 is 44.50% as opposed to ADV-2 which is 0%. Surprisingly, even training with just 1 step of PGD for LLR (LLR-1) we obtain non-zero adversarial accuracy.</p><p>Label: deer -Prediction: deer -Adversarial: bird   In <ref type="figure" target="#fig_12">Fig 7,</ref> we show the values of γ( , x) we obtain when we train with LLR or adversarial training (ADV). To find γ( , x) = max δ∈B( ) g(δ, x) we maximize g(δ, x) by running 50 steps of PGD with step size 0.1. Here, we see that values of γ( , x) for adversarial training with 20 steps of PGD is similar to LLR-2. In contrast, adversarial training (ADV-2) with just two steps of PGD gives much higher values of γ( , x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Adversarially Perturbed Images for 16/255</head><p>The perturbation radius 16/255 has become the norm <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref> to use to gauge how robust a network is on ImageNet. However, to be robust we need to make sure that the perturbation is sufficiently small such that it does not significantly affect our visual perception. We hypothesize that this perturbation radius is outside of this regime. The image which is attacked is the 1st of the validation set for ImageNet, the true label is "bobsled". Each row displays the images in the following order: original image, adversarially perturbed image and the adversarial perturbation (scaled). The attack attempts to imprint faint images onto the white background. Additionally, the curb (where we often expect a bobsled to be next to) on the right of the original image has been completely removed by the attack.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of gradient obfuscated surface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 4 . 1 .</head><label>41</label><figDesc>Consider a loss function (x) that is once-differentiable, and a local neighbourhood defined by B( ). Then for all δ ∈ B( )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 Figure 4 :</head><label>24</label><figDesc>Comparing the loss surface, (x), after we train using just 1 or 2 steps of PGD for the inner maximization of either the adversarial objective (ADV) max δ∈B( ) (x + δ) or the linearity objective (LLR) γ( , x) = max δ∈B( ) (x + δ) − (x) − δ T ∇ (x) . Results are shown for image 126 in test set of CIFAR-10, the nominal label is deer. ADV-i refers to adversarial training with i PGD steps, similarly with LLR-i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>A plot of (x) around the image 126 of CIFAR-10 test set which shows that training with just 1 step of PGD for adversarial training gets highly non-linear loss surface -(5a), while training with 8 steps of PGD the surface becomes more smooth -(5b). (5a, 5b) are (x) projection onto 2D plane, where one direction is the adversarial perturbation while the other is random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 Proposition 4 . 1 .</head><label>141</label><figDesc>corresponding nominal accuracy and adversarial accuracy for networks trained shown in Fig 2. The Multi-Targeted is described in Sec. 5.1. B Local Linearity Upper Bounds Robustness: Proof of Proposition 4.Consider a loss function (x) that is once-differentiable, and a local neighbourhood defined by B( ). Then for all δ ∈ B( ) | (x + δ) − (x)| ≤ |δ∇ x (x)| + γ( , x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>E Local Linearity Regularizer -Algorithm Algorithm 1</head><label>1</label><figDesc>Local Linearization of Network Require: Training data X = {(x 1 , y 1 ), · · · , (x N , y N )}. Learning rate lr and batch size for training b and number of iterations N . Number of iterations for inner optimization M and step size s and network architecture parameterized by θ. 1: Initialize variables θ. 2: for all i ∈ {0, 1, . . . , N } do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>8 :</head><label>8</label><figDesc>Update δ ← Proj(δ − s × Optimizer(g)) objective L = L B + 1/b b j=1 λg(δ; x ij , y ij ) + µ δ T ∇ x l(x) 11: θ ← θ − lr × Optimizer(∇ θ L) 12: end forNote g(δ; x, y) = (x + δ; y) − (x; y) − δ T ∇ x (x; y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>(a) ADV- 2 2 Figure 6 :</head><label>226</label><figDesc>Label: deer -Prediction: deer -Adversarial: bird (b) LLR-We show adversarial examples arising from training with either 2-step PGD adversarial training or 2-step PGD LLR. For both (a) and (b), the first image is the original, the second is the adversarially perturbed image and the third image to is the scaled adversarial perturbation found using 50 steps of PGD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>This is a histogram plot of the values of γ(8/255, x) on the test set after training is done either with two steps of PGD for the linearity objective (orange); two-steps of PGD for the adversarial objective (blue) or 20 steps of PGD for the adversarial objective (green). The statistics of γ( , x) after training with 2-steps of PGD with the linearity objective aligns well with training using 20 steps of the adversarial objective. In contrast, training with 2-steps of PGD of the adversarial objective gets very different looking histogram, where we obtain much higher values of γ( , x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Fig 8 shows that we can find examples which not only wipe out objects (the curbs) in the image, but can actually add faint images onto the white background. This significantly affects our visual perception of the image. Label: bobsled -Prediction: tench -Adversarial: washbasin Label: bobsled -Prediction: tench -Adversarial: washer Label: bobsled -Prediction: tench -Adversarial: snowmobile Images created with adversarial attack on a LLR model for perturbation radius 16/255.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>This shows the loss functions corresponding to the attacks we use for evaluation and also the metric we measure on the test set for each of these attacks. Notation-wise, s = argmax i =t fi(x + δ) is the highest logit excluding the logits corresponding to the correct class t, note s can change through the optimization procedure. For the Random-Targeted attack, r is a randomly chosen target label that is not t and does not change throughout the optimization. C stands for the set of class labels. For the Multi-Targeted attack we maximize fi(x + δ) − fT (x + δ) for all i ∈ C, and consider the attack successful if any of the individual attacks on each each target class i are successful. The metric used on the Random-Targeted attack is the attack success rate: the percentage of attacks where the target label r is indeed the output label (this metric is especially important for ImageNet at = 16/255). For the other attacks we use the adversarial accuracy as the metric which is the accuracy on the test set after the attack.</figDesc><table><row><cell cols="2">5.2 Results for Robustness</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">CIFAR-10: Wide-ResNet-28-8 (8/255)</cell><cell></cell></row><row><cell>Methods</cell><cell>Nominal</cell><cell>FGSM-20</cell><cell>Untargeted</cell><cell>Multi-Targeted</cell></row><row><cell>Attack Strength</cell><cell></cell><cell>Weak</cell><cell>Strong</cell><cell>Very Strong</cell></row><row><cell>ADV[16]</cell><cell>87.25%</cell><cell>48.89%</cell><cell>45.92%</cell><cell>44.54%</cell></row><row><cell>CURE[19]</cell><cell>80.76%</cell><cell>39.76%</cell><cell>38.87%</cell><cell>37.57%</cell></row><row><cell>ADV(S)</cell><cell>85.11%</cell><cell>56.76%</cell><cell>53.96%</cell><cell>48.79%</cell></row><row><cell>CURE(S)</cell><cell>84.31%</cell><cell>48.56%</cell><cell>47.28%</cell><cell>45.43%</cell></row><row><cell>TRADES(S)</cell><cell>87.40%</cell><cell>51.63</cell><cell>50.46%</cell><cell>49.48%</cell></row><row><cell>LLR (S)</cell><cell>86.83%</cell><cell>54.24%</cell><cell>52.99%</cell><cell>51.13%</cell></row><row><cell></cell><cell></cell><cell cols="2">CIFAR-10: Wide-ResNet-40-8 (8/255)</cell><cell></cell></row><row><cell>ADV(R)</cell><cell>85.58%</cell><cell>56.32%</cell><cell>52.34%</cell><cell>46.89%</cell></row><row><cell>TRADES(R)</cell><cell>86.25%</cell><cell>53.38%</cell><cell>51.76%</cell><cell>50.84%</cell></row><row><cell>ADV(S)</cell><cell>85.27%</cell><cell>57.94%</cell><cell>55.26%</cell><cell>49.79%</cell></row><row><cell>CURE(S)</cell><cell>84.45%</cell><cell>49.41%</cell><cell>47.69%</cell><cell>45.51%</cell></row><row><cell>TRADES(S)</cell><cell>88.11%</cell><cell>53.03%</cell><cell>51.65%</cell><cell>50.53%</cell></row><row><cell>LLR (S)</cell><cell>86.28%</cell><cell>56.44%</cell><cell>54.95%</cell><cell>52.81%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Model accuracy results for CIFAR-10. Our LLR regularizer performs the best under the strongest attack (highlighted column). (S) denotes softplus activation; (R) denotes ReLU activation; and models with (S, R) are our implementations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Interestingly, for adversarial training (ADV), using the Multi-Targeted attack for evaluation gives significantly lower adversarial accuracy compared to Untargeted. The accuracy obtained are 49.79% and 55.26% respectively. Evaluation using Multi-Targeted attack consistently gave the lowest adversarial accuracy throughout. Under this attack, the methods which stand out amongst the rest are LLR and TRADES. Using LLR we get state of the art results with 52.81% adversarial accuracy.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ImageNet: ResNet-152 (4/255)</cell><cell></cell></row><row><cell>Methods</cell><cell>PGD steps</cell><cell>Nominal</cell><cell>Untargeted</cell><cell>Random-Targeted</cell></row><row><cell></cell><cell></cell><cell>Accuracy</cell><cell></cell><cell>Success Rate</cell></row><row><cell>ADV</cell><cell>30</cell><cell>69.20%</cell><cell>39.70%</cell><cell>0.50%</cell></row><row><cell>DENOISE</cell><cell>30</cell><cell>69.70%</cell><cell>38.90%</cell><cell>0.40%</cell></row><row><cell>LLR</cell><cell>2</cell><cell>72.70%</cell><cell>47.00%</cell><cell>0.40%</cell></row><row><cell></cell><cell></cell><cell cols="2">ImageNet: ResNet-152 (16/255)</cell><cell></cell></row><row><cell>ADV [28]</cell><cell>30</cell><cell>64.10%</cell><cell>6.30%</cell><cell>40.00%</cell></row><row><cell cols="2">DENOISE [28] 30</cell><cell>66.80%</cell><cell>7.50%</cell><cell>38.00%</cell></row><row><cell>LLR</cell><cell>10</cell><cell>51.20%</cell><cell>6.10%</cell><cell>43.80%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>LLR gets 47% adversarial accuracy for 4/255 -7.30% higher than DENOISE and ADV. For 16/255,</figDesc><table /><note>LLR gets similar robustness results, but it comes at a significant cost to the nominal accuracy. Note Multi- Targeted attacks for ImageNet requires looping over 1000 labels, this evaluation can take up to several days even on 50 GPUs thus is omitted from this table. The column of the strongest attack is highlighted.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While computing the globally optimal adversarial example is NP-hard<ref type="bibr" target="#b10">[11]</ref>, gradient descent with several random restarts was empirically shown to be quite effective at computing adversarial perturbations of sufficient quality.<ref type="bibr" target="#b1">2</ref> This means that every pixel is perturbed independently by up to 4 units up or down on a scale where pixels take values ranging between 0 and 255.<ref type="bibr" target="#b2">3</ref> We note that TRADES<ref type="bibr" target="#b29">[30]</ref> gets 55% against a much weaker attack; under our strongest attack, it gets 52.5%.<ref type="bibr" target="#b3">4</ref> Baselines created are adversarial training, TRADES and CURE<ref type="bibr" target="#b18">[19]</ref>. To the contrary of CIFAR-10, we are currently unable to achieve consistent and competitive results on ImageNet at = 4/255 using TRADES.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">To measure γ( , x) we find max δ∈B( ) g(δ; x) with 50 steps of PGD using Adam as the optimizer and 0.1 as the step size.<ref type="bibr" target="#b5">6</ref> Here, we imply an increase in the number of PGD steps for the inner maximization max δ∈B( ) (x + δ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We attempted to use TRADES on ImageNet but did not manage to get competitive results. Thus they are omitted from the baselines.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Note the network published for TRADES [30] uses a Wide-ResNet-34-10 so this is not shown in the table but under the same rigorous evaluation we show that TRADES get 84.91% nominal accuracy, 53.41% under Untargeted and 52.58% under Multi-Targeted. We've also ran DeepFool (not in the table as the attack is weaker) giving ADV(S): 64.29%, CURE(S): 58.73%, TRADES(S): 63.4%, LLR(S): 65.87%.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01442</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Max-margin adversarial (mma) training: Direct input space margin maximization through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Gavin Weiguang Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kry Yik Chau</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruitong</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02637</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/tpu/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reluplex: An efficient smt solver for verifying deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="97" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02613</idno>
		<title level="m">Characterizing adversarial subspaces using local intrinsic dimensionality</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning via hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="735" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1193</idno>
		<title level="m">New insights and perspectives on the natural gradient method</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09716</idno>
		<title level="m">Robustness via curvature regularization, and vice versa</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia conference on computer and communications security</title>
		<meeting>the 2017 ACM on Asia conference on computer and communications security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast curvature matrix-vector products for second-order gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1723" to="1738" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tangent prop-a formalism for specifying selected invariances in an adaptive network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Victorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="895" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><forename type="middle">Kushman</forename><surname>Pixeldefend</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10766</idno>
		<title level="m">Leveraging generative models to understand and defend against adversarial examples</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<title level="m">Robustness may be at odds with accuracy. stat</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adversarial risk and the dangers of evaluating against weak attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05666</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03411</idno>
		<title level="m">Laurens van der Maaten, Alan Yuille, and Kaiming He. Feature denoising for improving adversarial robustness</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08573</idno>
		<title level="m">Theoretically principled trade-off between robustness and accuracy</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
