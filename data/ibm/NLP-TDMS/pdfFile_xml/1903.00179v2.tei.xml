<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pyramid Feature Attention Network for Saliency detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangqian</forename><surname>Wu</surname></persName>
							<email>xqwu@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pyramid Feature Attention Network for Saliency detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Saliency detection is one of the basic challenges in computer vision. How to extract effective features is a critical point for saliency detection. Recent methods mainly adopt integrating multi-scale convolutional features indiscriminately. However, not all features are useful for saliency detection and some even cause interferences. To solve this problem, we propose Pyramid Feature Attention network to focus on effective high-level context features and low-level spatial structural features. First, we design Context-aware Pyramid Feature Extraction (CPFE) module for multi-scale high-level feature maps to capture rich context features. Second, we adopt channel-wise attention (CA) after CPFE feature maps and spatial attention (SA) after low-level feature maps, then fuse outputs of CA &amp; SA together. Finally, we propose an edge preservation loss to guide network to learn more detailed information in boundary localization. Extensive evaluations on five benchmark datasets demonstrate that the proposed method outperforms the state-ofthe-art approaches under different evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Saliency detection aims to locate the important parts of natural images which attract our attention. As the preprocessing of computer vision applications, e.g. object detection <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b35">35]</ref>, visual tracking <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">14]</ref>, image retrieval <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b13">13]</ref> and semantic segmentation <ref type="bibr" target="#b9">[9]</ref>, saliency detection attracts many researchers. Currently, the most effective saliency detection methods are based on the fully convolutional network (FCN). FCN stacks multiple convolution and pooling layers to gradually increase the receptive field and generate the high-level semantic information, which plays a crucial role in saliency detection. However, the pooling layers reduce the size of the feature maps and deteriorate the boundaries of the salient objects.</p><p>To deal with this problem, some works introduce hand-craft features to preserve the boundaries of salient objects <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b28">28]</ref>. <ref type="bibr" target="#b18">[18]</ref> extracts the hand-craft features to compute the salient values of super-pixels. <ref type="bibr" target="#b28">[28]</ref> partitions the image into regions by hand-craft features. When generating saliency maps, the hand-craft features and the CNN highlevel features are complementary but extracted separately in these methods. However, it is difficult to effectively fuse the complementary features extracted separately. Furthermore, hand-craft features extraction is a time-consuming procedure.</p><p>Besides hand-craft features, some works discover that the features from different layers of the network are also complementary and integrate the multi-scale features for saliency detection <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b29">29]</ref>. More specifically, the features at deep layers typically contain global context-aware information, which are suitable to locate the salient regions correctly. The features at shallow layers contain the spatial structural details, which are suitable to locate boundaries. These methods fused different scale features without considering their different contribution for saliency, it is not optimal for saliency detection. To overcome these problems, attention model <ref type="bibr" target="#b45">[45]</ref> and gate function <ref type="bibr" target="#b42">[42]</ref> are introduced to the saliency detection networks. However, the methods ignore the different characteristics of the high-level and low-level features, which may affect the extraction of effective features.</p><p>In this paper, we propose a novel salient object detection method named Pyramid Feature Attention (PFA) network. In consideration of the different characteristics of different level features ( <ref type="figure" target="#fig_0">Fig.1 (c,e)</ref>), the saliency maps from low-level features contain many noises, while the saliency maps from high-level features only get an approximate area. Therefore, for high-level features, inspired by SIFT <ref type="bibr" target="#b23">[23]</ref> feature extraction algorithm, we design a context-aware pyramid feature extraction(CPFE) module to get multi-scale multi-receptive-field high-level features, and then we use channel-wise attention(CA) to select appropriate scale and receptive-field for generating saliency regions. In training process, CA assigns large weights to the channels which play important role for saliency detection( <ref type="figure" target="#fig_0">Fig.1 (f)</ref>). In order to refine the boundaries of saliency regions, we fuse low-level features with edge information. But not all edge information is effective for refining saliency maps, we expect to focus on the boundaries between salient objects and background. So we use spatial attention to better focus on the effective low-level features, and obtain clear saliency boundaries( <ref type="figure" target="#fig_0">Fig.1 (d)</ref>). After the processing of different attention mechanisms, the high-level features and low-level features are complementary-aware and suitable to generate saliency map. In addition, different from previous saliency detection approaches, we propose an edge preservation loss to guide network to learn more detailed information in boundary localization. With the above considerations, the proposed method PFA network can produce good saliency maps.</p><p>In short, our contributions are summarized as follows: 1. We propose a Pyramid Feature Attention (PFA) network for image saliency detection. For high-level feature, we adopt a context-aware pyramid feature extraction module and a channel-wise attention module to capture rich context information. For low-level feature, we adopt spatial attention module to filter out some background details.</p><p>2. We design a novel edge preservation loss to guide network to learn more detailed information in boundary localization.</p><p>3. The proposed model achieves the state-of-the-art on several challenging datasets. The experiments prove the effectiveness and superiority of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Salient Object Detection</head><p>In the past decade, there are a number of approaches for saliency detection. Early approaches <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b17">17]</ref> estimate the salient value based on hand-crafted features. Those approaches detect salient objects with humanlike intuitive feelings and heuristic priors, such as color contrast <ref type="bibr" target="#b5">[5]</ref>, boundary background <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39]</ref> and center prior <ref type="bibr" target="#b17">[17]</ref>. These direct techniques are known to be friendly to keep fine im-age structure. Nevertheless, the hand-craft features and priors can hardly capture high-level and global semantic knowledge about the objects.</p><p>In recent years, many efforts about various network architectures have been made in saliency detection. Some experiments <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b29">29]</ref> show that high-level features in deep layers encode the semantic information for getting an abstract description of objects, while low-level features in shallow layers keep spatial details for reconstructing the object boundaries ( <ref type="figure" target="#fig_0">Fig.1 (c,e)</ref>). Accordingly, some works bring multi-level features into saliency detection. Hou et al. <ref type="bibr" target="#b15">[15]</ref> propose a saliency method by introducing short connections to the skip-layer structures within the HED architecture. Wang et al. <ref type="bibr" target="#b31">[31]</ref> propose a saliency detection method based on recurrent fully convolutional networks (RFCNs). Luo et al. <ref type="bibr" target="#b24">[24]</ref> combine the local and global information through a multi-resolution grid structure. Zhang et al. <ref type="bibr" target="#b43">[43]</ref> aggregate multi-level features by concatenating feature maps from both high levels and low levels directly. Zhang et al. <ref type="bibr" target="#b42">[42]</ref> propose a bi-directional message passing module, where messages can transmit mutually controlled by gate function. However, some features may cause interferences in saliency detection. How to get various features and select effective ones becomes an important problem in saliency detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention Mechanisms</head><p>Attention mechanisms have been successfully applied in various tasks such as machine translation <ref type="bibr" target="#b11">[11]</ref>, object recognition <ref type="bibr" target="#b25">[25]</ref>, image captioning <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b36">36]</ref>, visual question answering <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b41">41]</ref> and pose estimation <ref type="bibr" target="#b6">[6]</ref>. Chu et al. <ref type="bibr" target="#b6">[6]</ref> propose a network with multi-context attention mechanism into an end-to-end framework for human pose estimation. Chen et al. <ref type="bibr" target="#b3">[3]</ref> propose a SCA-CNN network that incorporates spatial and channel-wise attention in CNN for image captioning. Zhang et al. <ref type="bibr" target="#b45">[45]</ref> propose a progressive attention guided network which generates attentive features by channel-wise and spatial attention mechanisms sequentially for saliency detection.</p><p>Due to attention mechanisms have great ability to select features, it is a perfect fit for saliency detection. While integrating the convolutional features, most existing methods treat multi-level features without distinction. Some methods adopted certain valid strategies, such as gate function <ref type="bibr" target="#b42">[42]</ref> and progressive attention <ref type="bibr" target="#b45">[45]</ref>, but those methods select features in a certain direction and ignore the differences between high-level and low-level features. Different from them, for high-level feature, we adopt context-aware pyramid feature extraction(CPFE) module and channel-wise attention module to capture rich context information. In CPFE module, we adopt multi-scale atrous convolutions on the side of three high-level blocks of VGG net, then channel-wise attention mechanism assigns large weights to channels which show high response to salient objects. For low-level feature, there exists some background regions which distract the generation of saliency map. Spatial attention mechanism filters out some background details according to high-level features and focus more on the foreground regions, which helps to generate effective features for saliency prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pyramid Feature Attention Network</head><p>In this paper, we propose a novel saliency detection method, which contains a context-aware pyramid feature extraction module and a channel-wise attention module to capture context-aware multi-scale multi-receptive-field high-level features, a spatial attention module for low-level feature maps to refine salient object details and an effective edge preservation loss to guide network to learn more detailed information in boundary localization. The overall architecture is illustrated in <ref type="figure" target="#fig_1">Fig.2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Context-aware pyramid feature extraction</head><p>Visual context is quite important for saliency detection. Existing CNN models learn features of objects by stacking multiple convolutional and pooling layers. However, the salient objects have large variations in scale, shape and position. Previous methods usually directly use the bottom-to-up convolution and pooling layers, that may not be effectively to handle these complicated variations. Inspired by the feature extraction of SIFT <ref type="bibr" target="#b23">[23]</ref>, we try to design a novel module to extract the features of scale, shape and location invariances. The scale-invariant feature transform (SIFT) is a feature detection algorithm in computer vision to detect and describe local features in im-ages. The algorithm proposed the Laplassian of Gaussian representation <ref type="bibr" target="#b23">[23]</ref> which fused scale space representations and pyramid multi-resolution representations. The scale space representations which are processed by several different Gaussian kernel functions with same resolution; and the pyramid multi-resolution representations which are processed by down samples with different resolutions. Similar with Gaussian function in SIFT, we use atrous convolution <ref type="bibr" target="#b4">[4]</ref> to get features with same scale but different receptive fields. Similar with pyramid multi-resolution representations in SIFT, we take conv3-3, conv4-3 and conv5-3 of VGG-16 <ref type="bibr" target="#b27">[27]</ref> to extract multi-scale features.</p><p>Specifically, the context-aware pyramid feature extraction module is shown in <ref type="figure" target="#fig_3">Fig.3</ref>. We take conv 3-3 , conv 4-3 and conv 5-3 in VGG-16 as the basic high-level features. To make the final extracted high-level features contain the features of scale and shape invariances, we adopt atrous convolution with different dilation rates, which are set to 3, 5 and 7 to capture multi-receptive-field context information. Then we combine the feature maps from different atrous convolutional layers and a 1×1 dimension reduction feature by cross-channel concatenation. After this, we get three different scale features with context-aware information, we upsample the two smaller ones to the largest one. Finally, we combine them by cross-channel concatenation as the output of the context-aware pyramid feature extraction module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention mechanism</head><p>We exploit context-aware pyramid feature extraction to get multi-scale multi-receptive-field high-level features. Different features have different semantic values to generate saliency maps. But most existing methods integrate multi-  scale features without distinction, which lead to information redundancy. More importantly, inaccurate information at some levels would lead to a performance degradation or even wrong prediction. It is significant to filter these features and fucus more on valuable features. In this subsection we will talk about the attention mechanisms in PFA network. According to the characteristics of different level features, we adopt channel-wise attention for high-level features and spacial attention for low-level features to select effective features. In addition, we don't use spacial attention for high-level features, because high-level features contain high abstract semantics <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b45">45]</ref>, there is no need to filter spacial information. While, we don't use channel-wise attention for low-level feature, because there are almost no semantic distinctions among different channels of low-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Channel-wise attention</head><p>Different channels of features in CNNs generate response to different semantics <ref type="bibr" target="#b16">[16]</ref>. From <ref type="figure" target="#fig_0">Fig.1</ref>, the saliency map from high-level features is just a rough result, some essential regions may be weakened. We add channel-wise attention (CA) <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b3">3]</ref> module after context-aware pyramid feature extraction to weighted multi-scale multi-receptivefield high-level features. The CA will assign larger weight to channels which show high response to salient objects . We unfold high-level features</p><formula xml:id="formula_0">f h ∈ R W ×H×C as f h = [ f h 1 , f h 2 ,..., f h C ], where f h i ∈ R W ×H</formula><p>is the i-th slice of f h and C is the total channel number. First, we apply average pooling to each f h i to obtain a channel-wise feature vector v h ∈ R C . After that, two consecutive fully connected(FC) layer to fully capture channel-wise dependencies(see <ref type="figure" target="#fig_4">Fig.4</ref> ). As <ref type="bibr" target="#b16">[16]</ref>, to limit model complexity and aid generalisation, we encode the channel-wise feature vector by forming a bottleneck with two FC layers around the non-linearity. Then, through using sigmoid operation, we take the normalization processing measures to the encoded channel-wise feature vector mapped to [0,1].</p><formula xml:id="formula_1">CA = F (v h , W ) = σ 1 (f c 2 (δ(f c 1 (v h , W 1 )), W 2 )) (1)</formula><p>Where W refers to parameters in channel-wise attention block, σ 1 refers to sigmoid operation, f c refers to FC layers, δ refers to the ReLU function. The final output f h of the block is obtained by weighting the context-aware pyramid features with CA.</p><formula xml:id="formula_2">f h = CA · f h<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Spacial attention</head><p>Natural images usually contains a wealth of details of foreground and complex background. From <ref type="figure" target="#fig_0">Fig.1</ref>, the saliency map from low-level features contains a lot of details which easily brings bad results. In saliency detection, we want to obtain detailed boundaries between salient objects and background without other texture which can distract human attention. Therefore, instead of considering all spatial positions equally, we adopt spatial attention to focus more on the foreground regions, which helps to generate effective features for saliency prediction. We represent low-level features as f l ∈ R W ×H×C . The set of spatial locations is denoted by R = {(x, y)|x = 1, ..., W ; y = 1, ..., H}, where j =(x,y) is the spatial coordinate of low-level features. For increasing receptive field and getting global information but not increasing parameters, similar to <ref type="bibr" target="#b26">[26]</ref>, we apply two convolution layers ,one's kernel is 1×k and the other's is k×1, for high-level feature to capture spacial concerns(see <ref type="figure" target="#fig_4">Fig.4</ref> ). Then, using sigmoid operation, we take the normalization processing measures to the encoded spacial feature map mapped to [0,1].</p><formula xml:id="formula_3">C 1 = conv 2 (conv 1 ( f h , W 1 1 ), W 2 1 ))<label>(3)</label></formula><formula xml:id="formula_4">C 2 = conv 1 (conv 2 ( f h , W 1 2 ), W 2 2 ))<label>(4)</label></formula><formula xml:id="formula_5">SA = F ( f h , W ) = σ 2 (C 1 + C 2 )<label>(5)</label></formula><p>Where W refers to parameters in spacial attention block, σ 2 refers to sigmoid operation, conv 1 and conv 2 refers to 1×k×C and k×1×1 convlution layer respectively and we set k=9 in experiment. The final output f l of the block is obtained by weighting f l with SA.</p><formula xml:id="formula_6">f l = SA · f l<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss function</head><p>In machine learning and mathematical optimization, loss functions represent the price paid for inaccuracy of predictions in classification problems. In saliency object detection, we always use the cross-entropy loss between the final saliency map and the ground truth. The loss function is defined as:</p><formula xml:id="formula_7">L S = − size(Y ) i=0 (α s Y i log(P i ) +(1 − α s )(1 − Y i )log(1 − P i )) (7)</formula><p>where Y means the ground truth and P means the saliency map of network output, α s means a balance parameter of positive and negative samples and we set α s = 0.528 which calculated from groundtruth of the training set. However, the loss function just provides general guidance to generate saliency map. We use a simpler strategy to emphasize generation of the salient object boundaries details. First, we use Laplace Operator <ref type="bibr" target="#b12">[12]</ref> to get boundaries of ground truth and saliency map of network output, and then we use the cross-entropy loss to supervise the generation of salient object boundaries.</p><formula xml:id="formula_8">∆f = ∂ 2 f ∂x 2 + ∂ 2 f ∂y 2<label>(8)</label></formula><p>∆ f = abs(tanh(conv(f, K laplace ))) (9)</p><formula xml:id="formula_9">L B = − size(Y ) i=0 (∆Y i log(∆P i ) +(1 − ∆Y i )log(1 − ∆P i ))<label>(10)</label></formula><p>The Laplace operator is a second order differential operator in the n-dimensional Euclidean space, defined as the divergence of the gradient (∆f ). Because the second derivative can be used to detect edges, we use the Laplace operator to get salient object boundaries. The Laplace operator in two dimensions is given by Eq.8, where x and y are the standard Cartesian coordinates of the xy-plane. In fact, since the Laplacian uses the gradient of images, it calls internally the convolution operation to perform its computation. Then we use absolute operation followed by tanh activatioin Eq.9 map the value to [0,1]. Finally we use the cross-entropy loss to supervise the generation of salient object boundaries Eq.10. The total loss function is their weighted sum:</p><formula xml:id="formula_10">L = αL S + (1 − α)L B<label>(11)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Criteria</head><p>The performance evaluation is utilized on five standard benchmark datasets: DUTS-test <ref type="bibr" target="#b30">[30]</ref>, ECSSD <ref type="bibr" target="#b37">[37]</ref>, HKU-IS <ref type="bibr" target="#b19">[19]</ref>, PASCAL-S <ref type="bibr" target="#b21">[21]</ref> and DUT-OMRON <ref type="bibr" target="#b40">[40]</ref>. DUTS <ref type="bibr" target="#b30">[30]</ref> is a large scale dataset, which contains 10553 images for training and 5019 images for testing. ECSSD <ref type="bibr" target="#b37">[37]</ref> contains 1,000 images with many semantically meaningful and complex structures in their ground truth segmentation. HKU-IS <ref type="bibr" target="#b19">[19]</ref> contains 4447 challenging images with multiple disconnected salient objects, overlapping the image boundary or low color contrast. PASCAL-S <ref type="bibr" target="#b21">[21]</ref> contains 850 images, different salient objects are labeled with different saliencies. DUT-OMRON <ref type="bibr" target="#b40">[40]</ref> has 5,168 high quality images. Images of this dataset have one or more salient objects and relatively complex background.</p><p>Same as other state-of-the-art salient object detection methods, three popular criteria are used for performance evaluation, i.e. precision and recall curve (denoted PR curve), F-measure, weighted F-measure (denoted wF β ), and mean absolute error (M AE).</p><p>The precision and recall are computed by comparing the binary map under different thresholds between predicted saliency map and ground truth, the thresholds are from 0 to 255. wF β is a overall evaluation standard computed by the weighted combination of precision and recall:</p><formula xml:id="formula_11">F β = (1 + β 2 ) × P recision × Recall β 2 × P recision + Recall<label>(12)</label></formula><p>Image GT Amulet DCL DSS NLDF BDMPM PAGRN RFCN SRM UCF Ours Where β 2 = 0.3 as used in other approaches. Mean absolute error (M AE) is computed by:</p><formula xml:id="formula_12">M AE = 1 W × H W x=1 H y=1 |P (x, y) − Y (x, y)|<label>(13)</label></formula><p>where Y is the ground truth(GT ), and P is the saliency map of network output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use VGG-16 pre-trained on Imagenet <ref type="bibr" target="#b7">[7]</ref> as basic model. The DUTS-train dataset is used to train our model, which contains 10553 images. As suggested in <ref type="bibr" target="#b22">[22]</ref>, we don't use the validation set and train the model until training loss converges. To make the model robust, we adopt some data augmentation techniques: random rotating, random cropping, random brightness, saturation and contrast changing, and random horizontal flipping.</p><p>When training, we set α = 1.0 at beginning to generate rough saliency map. In this period, our model is trained using SGD <ref type="bibr" target="#b2">[2]</ref> with an initial learning rate 1e-2, the image size is 256×256 , the batch size is 22. Then we adjust different α to refine the boundaries of saliency map,and find α = 0.7 is the optimal setting in experiment Tab.2. In this period, the image size, batch size is same as the previous period, but the initial learning rate is 1e-3. The code will be found at https://github.com/CaitinZhao/cvpr2019_ Pyramid-Feature-Attention-Network-for-Saliency-dete</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-arts</head><p>The performance of the proposed method is compared with eleven state-of-the-art salient object detection approaches on five test datasets, including BDMPM <ref type="bibr" target="#b42">[42]</ref>, GRL <ref type="bibr" target="#b33">[33]</ref>, PAGRN <ref type="bibr" target="#b45">[45]</ref>, Amulet <ref type="bibr" target="#b43">[43]</ref>, SRM <ref type="bibr" target="#b32">[32]</ref>, UCF <ref type="bibr" target="#b44">[44]</ref>, DCL <ref type="bibr" target="#b20">[20]</ref>, DHS <ref type="bibr" target="#b22">[22]</ref>, ELD <ref type="bibr" target="#b18">[18]</ref>, NLDF <ref type="bibr" target="#b24">[24]</ref> and RFCN <ref type="bibr" target="#b31">[31]</ref>. For fair comparisons, we use the implementations with recommended parameters and the saliency maps provided by the authors.  <ref type="figure" target="#fig_5">Fig.5</ref>, our method gets the best detection results which are much close to the ground truth in various challenging scenarios. To be specific, (1) the proposed method not only highlights the correct salient object regions clearly, but also well suppresses the saliencies  <ref type="bibr" target="#b42">[42]</ref> 0.8508 0.0484 0.9249 0.0478 0.9200 0.0392 0.8806 0.0788 0.7740 0.0635 GRL <ref type="bibr" target="#b33">[33]</ref> 0.8341 0.0509 0.9230 0.0446 0.9130 0.0377 0.8811 0.0799 0.7788 0.0632 PAGRN <ref type="bibr" target="#b45">[45]</ref> 0.8546 0.0549 0.9237 0.0643 0.9170 0.0479 0.8690 0.0940 0.7709 0.0709 Amulet <ref type="bibr" target="#b43">[43]</ref> 0.7773 0.0841 0.9138 0.0604 0.8968 0.0511 0.8619 0.0980 0.7428 0.0976 SRM <ref type="bibr" target="#b32">[32]</ref> 0.8269 0.0583 0.9158 0.0564 0.9054 0.0461 0.8677 0.0859 0.7690 0.0694 UCF <ref type="bibr" target="#b44">[44]</ref> 0.7723 0.1112 0.9018 0.0704 0.8872 0.0623 0.8492 0.1099 0.7296 0.1203 DCL <ref type="bibr" target="#b20">[20]</ref> 0.7857 0.0812 0.8959 0.0798 0.8899 0.0639 0.8457 0.1115 0.7567 0.0863 DHS <ref type="bibr" target="#b22">[22]</ref> 0.8114 0.0654 0.9046 0.0622 0.8901 0.0532 0.8456 0.0960 --DSS <ref type="bibr" target="#b15">[15]</ref> 0.8135 0.0646 0.8959 0.0647 0.9011 0.0476 0.8506 0.0998 0.7603 0.0751 ELD <ref type="bibr" target="#b18">[18]</ref> 0.7372 0.0924 0.8674 0.0811 0.8409 0.0734 0.7882 0.1228 0.7195 0.0909 NLDF <ref type="bibr" target="#b24">[24]</ref> 0.8125 0.0648 0.9032 0.0654 0.9015 0.0481 0.8518 0.1004 0.7532 0.0796 RFCN <ref type="bibr" target="#b31">[31]</ref> 0.7826 0.0893 0.8969 0.0972 0.8869 0.0806 0.8554 0.1159 0.7381 0.0945 <ref type="figure">Figure 6</ref>. Quantitative comparisons of the proposed approach and eleven state-of-the-art CNN based salient object detection approaches on five datasets. The first and second rows are the PR curves and F-measure curves of different methods respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Visual Comparison</head><p>of background regions, so as to produce the detection results with higher contrast between salient objects and background than other approaches. (2) With the help of the edge preservation loss, the proposed method is able to generate the salient maps with clear boundaries and consistent saliencies.</p><p>(3) The saliency maps are much better than other works when salient objects are similar to background ( <ref type="figure" target="#fig_5">Fig.5</ref> the 2,5,7 rows) and the salient objects have special semantic information( <ref type="figure" target="#fig_0">Fig.5 the 1,3</ref>  <ref type="figure">Fig.6</ref>, the PR curve and F-measure curve of our method are significantly higher than other methods, which means our method is more robust than other approaches even on challenging datasets. To be specific, our method gets larger improvement compared with the best existing approach on DUT-OMRON dataset. DUT-OMRON dataset is a difficult and challenging saliency detection dataset, in which there are many complex natural scenes images and the color of   <ref type="table">Table 2</ref>. The effectiveness of edge preservation loss. The score of wF β and M AE in our method when α is given different values. The best result is shown in red. The test dataset is DUTS-test. salient objects is similar to the background. The proposed method can effectively find correct salient objects with powerful feature extraction capability and apt attention mechanisms, which make the network focus on salient objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The Effectiveness of edge preservation loss</head><p>In Sec.3.3 we propose an effective edge preservation loss to guide network to learn more detailed information in boundary localization. <ref type="figure" target="#fig_7">Fig.7</ref> shows the saliency maps generated from our method and boundary maps calculated by Eq.9 with edge preservation loss or not. These results illustrate that the edge preservation loss directly enhances the generality and make our method with fine details. In addition, we found that the edge preservation loss with different α have different effects on the final results. From Tab.2, when α is 0.7 gets the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>To investigate the importance of different modules in our method, we conduct the ablation study. From Tab.3, that the proposed model contains all components (i.e. contextaware pyramid feature extraction(CPCE), channel-wise attention(CA), spacial attention(SA) and edge preservation loss(EL)) achieves the best performance, which demonstrates that all components are necessary for the proposed method to get the best salient object detection result.</p><p>We adopt the model only use high-level features as basic model, and the base M AE is 0.1003. First, we add CPFE to basic model and get decline in M AE, furthermore we add CA and get decline of 37% in MAE compared with basic model. Then we add low-level features to high-level features and prove the effectiveness of Integrating multi-scale HL CPFE CA LL SA EL M AE 0.1003 0.0815 0.0629 0.0836 0.0800 0.0528 0.0432 0.0405 features. On this basis, we add SA to low-level features and get decline of 57% in MAE compared with basic model. Finally, we add EL in the model and get the best result which get decline of 60% in MAE compared with basic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a novel salient object detection method named Pyramid Feature Attention network. In consideration of the different characteristics of different level features, for high-level features we design a context-aware pyramid feature extraction module contains different atrous convolutions at multi scales and a channel-wise attention module to capture semantic high-level features; For lowlevel features, we employ a spatial attention module to suppress the noises in background and focus on salient objects. Besides, we propose a novel edge preservation loss to guide network to learn more detailed information in boundary localization. In a word, the proposed method is expert in locating correct salient objects with powerful feature extraction capability and apt attention mechanisms, which make the network robost and effective in saliency detection. Experimental results on five datasets demonstrate that our proposed approach outperforms state-of-the-art methods under different evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An example of applying Pyramid Feature Attention network.(a) and (b) represent the input image and corresponding Ground Truth. (c) and (d) mean low-level features without or with spacial attention. (e) and (f) are high-level features without or with channel-wise attention. (g) and (h) represent the results from our method and the boundary map of (g) generated by Laplace operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of our method. CPFE means context-aware pyramid feature extraction. The high-level features are from vgg3-3, vgg4-3 and vgg5-3. The low-level features are from vgg1-2 and 2-2, which upsample to the size of vgg1-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Detailed structure of context-aware pyramid feature extraction. A context-aware feature extraction module takes a feature from a side output of net as input and it contains three 3×3 convolutional layers with different dilation rates and a 1×1 convolutional layers, the output channel of each convolutional layer is 32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Channel-wise attention (left) and spacial attention (right). Where X and X' mean weighted feature and weighting feature respectively, Y means context-aware high-level feature after CA in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visual comparisons of the proposed method and the state-of-the-art algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5</head><label>5</label><figDesc>provides a visual comparison of our method and other state-of-the-arts. From</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Visual comparison of saliency detection results with and without the edge preservation loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>0.8576 0.8602 0.8702 0.8619 M AE 0.0432 0.0427 0.0393 0.0405 0.0428</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The wF β and M AE of different salient object detection approaches on all test datasets. The best three results are shown in red, blue, and green.</figDesc><table><row><cell>Methods</cell><cell>DUTS-test wF β M AE wF β ECSSD M AE wF β HKU-IS M AE wF β PASCAL-S M AE wF β DUT-OMRON M AE</cell></row><row><cell>Ours</cell><cell>0.8702 0.0405 0.9313 0.0328 0.9264 0.0324 0.8922 0.0677 0.8557 0.0414</cell></row><row><cell>BDMPM</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Quantitative ComparisonFig.<ref type="bibr" target="#b6">6</ref> and Tab.1 provides the quantitative evaluation results of the proposed method and eleven state-of-the-art salient object detection approaches on five test datasets in terms of PR curve, F-measure curve, wF β and M AE criteria. As seen from Tab.1, our method gets the best result on five test datasets in terms of wF β and M AE, which demonstrate the efficiency of the proposed method. From</figDesc><table><row><cell>,4,6,8 rows).</cell></row><row><cell>4.3.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation Study using different components combinations. HL means use High-Level features, CPFE means use Contextaware pyramid Feature Extraction after high-level features, CA means use Channel-wise Attention after high-level features, LL means use Low-Level features, SA means use Spacial Attention after low-level features and EL means use Edge preservation Loss.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive object tracking by learning background context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Conference on</title>
		<imprint>
			<biblScope unit="page" from="23" to="30" />
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6298" to="6306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07432</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Importance filtering for image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Saliency driven total variation segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3-d object retrieval and recognition with hypergraph analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4290" to="4303" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<title level="m">Convolutional sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Elliptic partial differential equations of second order</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gilbarg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Trudinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mobile product search with bag of hash bits and boundary reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3005" to="3012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5300" to="5309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Submodular salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2043" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Large kernel matters improve semantic segmentation by global convolutional network. computer vision and pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1743" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Saliency detection via combining regionlevel and pixel-level predictions with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="809" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeply-supervised recurrent convolutional neural network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="397" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with imagelevel supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4019" to="4028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3127" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A bi-directional message passing model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
