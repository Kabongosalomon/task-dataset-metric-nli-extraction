<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Multi-pose Guided Virtual Try-on Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
							<email>wangboch@mail2</email>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zhu</surname></persName>
							<email>jzhu@m.scun.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">South China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Multi-pose Guided Virtual Try-on Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Virtual try-on system under arbitrary human poses has huge application potential, yet raises quite a lot of challenges, e.g. self-occlusions, heavy misalignment among diverse poses, and diverse clothes textures. Existing methods aim at fitting new clothes into a person can only transfer clothes on the fixed human pose, but still show unsatisfactory performances which often fail to preserve the identity, lose the texture details, and decrease the diversity of poses. In this paper, we make the first attempt towards multi-pose guided virtual try-on system, which enables transfer clothes on a person image under diverse poses. Given an input person image, a desired clothes image, and a desired pose, the proposed Multi-pose Guided Virtual Try-on Network (MG-VTON) can generate a new person image after fitting the desired clothes into the input image and manipulating human poses. Our MG-VTON is constructed in three stages: 1) a desired human parsing map of the target image is synthesized to match both the desired pose and the desired clothes shape; 2) a deep Warping Generative Adversarial Network (Warp-GAN) warps the desired clothes appearance into the synthesized human parsing map and alleviates the misalignment problem between the input human pose and desired human pose; 3) a refinement render utilizing multi-pose composition masks recovers the texture details of clothes and removes some artifacts. Extensive experiments on well-known datasets and our newly collected largest virtual try-on benchmark demonstrate that our MG-VTON significantly outperforms all state-of-the-art methods both qualitatively and quantitatively with promising multipose virtual try-on performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning to synthesize the image of person conditioned on the image of clothes and manipulate the pose simultaneously is a significant and valuable task in many applications such as virtual try-on, virtual reality, and humancomputer interaction. In this work, we propose a multi- stage method to synthesize the image of person conditioned on both clothes and pose. Given an image of a person, a desired clothes, and a desired pose, we generate the realistic image that preserves the appearance of both desired clothes and person, meanwhile reconstructing the pose, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Obviously, delicate and reasonable synthesized outfit with arbitrary pose is helpful for users in selecting clothes while shopping.</p><p>However, recent image synthesis approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref> for virtual try-on mainly focus on the fixed pose and fail to preserve the fine details, such as the clothing of lower-body and the hair of the person lose the details and style, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. In order to generate the realistic image, those methods apply a coarse-to-fine network to produce the image conditioned on clothes only. They ignore the significant features of the human parsing, which leads to synthesize blurry and unreasonable image, especially in case of conditioned on various poses. For instance, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the clothing of lower-body cannot be preserved while the clothing of upper-body is replaced. The head of the person fail to identify while conditioned different poses. Other exiting works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref> usually leverage 3D measurements to solve those issues since the 3D information have abundant details of the shape of the body that can help to generate the realistic results. However, it needs expert knowledge and huge labor cost to build the 3D models, which requires collecting the 3D annotated data and massive computation. These costs and complexity would limit the applications in the practical virtual try-on simulation.</p><p>In this paper, we study the problem of virtual try-on conditioned on 2D images and arbitrary poses, which aims to learn a mapping function from an input image of a person to another image of the same person with a new outfit and diverse pose, by manipulating the target clothes and pose. Although the image-based virtual try-on with the fixed pose has been studied widely <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>, the task of multi-pose virtual try-on is less explored. In addition, without modeling the mapping of the intricate interplay among of the appearance, the clothes, and the pose, directly using the existing virtual try-on methods to synthesized image based on different poses often result in blurry and artifacts.</p><p>Targeting on the problems mentioned above, we propose a novel Multi-pose Guided Virtual Try-on Network (MG-VTON) that can generate a new person image after fitting both desired clothes into the input image and manipulating human poses. Our MG-VTON is a multi-stage framework with generative adversarial learning. Concretely, we design a pose-clothes-guided human parsing network to estimate a plausible human parsing of the target image conditioned on the approximate shape of the body, the face mask, the hair mask, the desired clothes, and the target pose, which could guide the synthesis in an effective way with the precise region of body parts. To seamlessly fit the desired clothes on the person, we warp the desired clothes image, by exploiting a geometric matching model to estimate the transformation parameters between the mask of the input clothes image and the mask of the synthesized clothes extracted from the synthesized human parsing. In addition, we design a deep Warping Generative Adversarial Network (Warp-GAN) to synthesize the coarse result alleviating the large misalignment caused by the different poses and the diversity of clothes. Finally, we present a refinement network utilizing multi-pose composition masks to recover the texture details and alleviate the artifact caused by the large misalignment between the reference pose and the target pose.</p><p>To demonstrate our model, we collected a new dataset, named MPV, by collecting various clothes image and person images with diverse poses from the same person. In addition, we also conduct experiments on DeepFashion <ref type="bibr" target="#b37">[38]</ref> datasets for testing. Following the object evaluation protocol <ref type="bibr" target="#b29">[30]</ref>, we conduct a human subjective study on the Amazon Mechanical Turk (AMT) platform. Both quantitative and qualitative results indicate that our method achieves effective performance and high-quality images with appealing details. The main contributions are listed as follows:</p><p>• A new task of virtual try-on conditioned on multi-pose is proposed, which aims to restructure the person image by manipulating both diverse poses and clothes.</p><p>• We propose a novel Multi-pose Guided Virtual Try-on Network (MG-VTON) that generates a new person image after fitting the desired clothes onto the input person image and manipulating human poses. MG-VTON contains four modules: 1) a pose-clothes-guided human parsing network is designed to guide the image synthesis; 2) a Warp-GAN learns to synthesized realistic image by using a warping features strategy; 3) a refinement network learns to recover the texture details; 4) a mask-based geometric matching network is presented to warp clothes that enhances the visual quality of the generated image.</p><p>• A new dataset for the multi-pose guided virtual tryon task is collected, which covers person images with more poses and clothes diversity. The extensive experiments demonstrate that our approach can achieve the competitive quantitative and qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generative Adversarial Networks (GANs). GANs <ref type="bibr" target="#b6">[7]</ref> consists of a generator and a discriminator that the discriminator learns to classify between the synthesized images and the real images while the generator tries to fool the discriminator. The generator aims to generate realistic images, which are indistinguishable from the real images. And the discriminator focuses on distinguishing between the synthesized and real images. Existing works have leveraged various applications based on GANs, such as style transfer <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b33">34]</ref>, image inpainting <ref type="bibr" target="#b32">[33]</ref>, textto-image <ref type="bibr" target="#b21">[22]</ref>, and super-resolution imaging <ref type="bibr" target="#b15">[16]</ref>. Inspired by those impressive results of GANs, we also apply the adversarial loss to exploit a virtual try-on method with GANs.</p><p>Person image synthesis. Skeleton-aided <ref type="bibr" target="#b31">[32]</ref> proposed a skeleton-guided person image generation method, which conditioned on a person image and the target skeletons. PG2 <ref type="bibr" target="#b16">[17]</ref> applied a coarse-to-fine framework that consists of a coarse stage and a refined stage. Besides, they proposed a novel model <ref type="bibr" target="#b17">[18]</ref> to further improve the quality of result by using a decomposition strategy. The deformableGANs <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b0">[1]</ref> made attempt to alleviate the misalignment problem between different poses by using affine transformation on the coarse rectangle region and warped the parts on pixellevel, respectively. V-UNET <ref type="bibr" target="#b4">[5]</ref> introduced a variational U-Net <ref type="bibr" target="#b23">[24]</ref> to synthesize the person image by restructuring the shape with stickman label. <ref type="bibr" target="#b20">[21]</ref> applied CycleGAN <ref type="bibr" target="#b35">[36]</ref> directly to manipulate pose. However, all those works fail to preserve the texture details consistency corresponding with the pose. The reason behind that is they ignore to consider  <ref type="figure">Figure 2</ref>. The overview of the proposed MG-VTON. Stage I: We first decompose the reference image into three binary masks. Then, we concatenate them with the target clothes and target pose as an input of the conditional parsing network to predict human parsing map. Stage II: Next, we warp clothes, remove the clothing from the reference image, and concatenate them with the target pose and synthesized parsing to synthesize the coarse result by using Warp-GAN. Stage III: We finally refine the coarse result with a refinement render, conditioning on the warped clothes, target pose, and the coarse result. the interplay between the human parsing map and the pose in the person image synthesis. The human parsing map can guide the generator to synthesize image in the precise region level that ensures the coherence of body structure.</p><p>Virtual try-on. VITON <ref type="bibr" target="#b7">[8]</ref> and CP-VTON <ref type="bibr" target="#b28">[29]</ref> all presented an image-based virtual try-on network, which can transfer a desired clothes on the person by using a warping strategy. VITON computed the transformation mapping by the shape context TPS warps <ref type="bibr" target="#b1">[2]</ref> directly. CP-VTON introduced a learning method to estimate the transformation parameters. FashionGAN <ref type="bibr" target="#b36">[37]</ref> learned to generate new clothes on the input image of the person conditioned on a sentence describing the different outfit. However, the above all methods synthesized the image of person only on the fixed pose, which limits the applications in the practical virtual try-on simulation. ClothNet <ref type="bibr" target="#b14">[15]</ref> presented an imagebased generative model to produce new clothes conditioned on color. CAGAN <ref type="bibr" target="#b9">[10]</ref> proposed a conditional analogy network to synthesize person image conditioned on the paired of clothes, which limits the practical virtual try-on scenarios. In order to generate the realistic-look person image in different clothes, ClothCap <ref type="bibr" target="#b19">[20]</ref> utilized the 3D scanner to capture the clothes, the shape of the body automatically. <ref type="bibr" target="#b25">[26]</ref> presented a virtual fitting system that requires the 3D body shape, which is laborious for collecting the annotation. In this paper, we introduce a novel and effective method for learning to synthesize image with the new outfit on the person through adversarial learning, which can manipulate the pose simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MG-VTON</head><p>We propose a novel Multi-pose Guided Virtual Try-on Network (MG-VTON) that learns to synthesize the new person image for virtual try-on by manipulating both clothes and pose. Given an input person image, a desired clothes, and a desired pose, the proposed MG-VTON aims to produce a new image of the person wearing the desired clothes and manipulating the pose. Inspired by the coarse-to-fine idea <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>, we adopt an outline-coarse-fine strategy that divides this task into three subtasks, including the conditional parsing learning, the Warp-GAN, and the refinement render. The <ref type="figure">Figure 2</ref> illustrates the overview of MG-VTON.</p><p>We first apply the pose estimator <ref type="bibr" target="#b3">[4]</ref> to estimate the pose. Then, we encode the pose as 18 heatmaps, which is filled with ones in a circle with radius 4 pixels and zeros elsewhere. A human parser <ref type="bibr" target="#b5">[6]</ref> is used to predict the human segmentation maps, consisting of 20 labels, from which we extract the binary mask of the face, the hair, and the shape of the body. Following VITON <ref type="bibr" target="#b7">[8]</ref>, we downsample the shape of the body to a lower resolution <ref type="bibr">(16 × 12)</ref> and directly resize it to the original resolution (256×192), which alleviates the artifacts caused by the variety of the body shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditional Parsing Learning</head><p>To preserve the structural coherence of the person while manipulating both clothes and the pose, we design a poseclothes-guided human parsing network, conditioned on the image of clothes, the pose heatmap, the approximated shape of the body, the mask of the face, and the mask of hair. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the baseline methods failed to preserve some parts of the person (e.g., the color of the trousers and the style of hair were replaced.), due to feeding the image of the person and clothes into the model directly. In this work, we leverage the human parsing maps to address those problems, which can help generator to synthesize the highquality image on parts-level.</p><p>Formally, given an input image of person I, an input image of clothes C, and the target pose P , this stage learns to predict the human parsing map S t conditioned on clothes C and the pose P . As shown in <ref type="figure" target="#fig_1">Figure 3</ref> (a), we first extract the hair mask M h , the face mask M f , the body shape M b , and the target pose P by using a human parser <ref type="bibr" target="#b5">[6]</ref> and a pose estimator <ref type="bibr" target="#b3">[4]</ref>, respectively. We then concatenate them with the image of clothes as input  which is fed into the conditional parsing network. The inference of S t can be formulate as maximizing the posterior probability p(S t |(M h , M f , M b , C, P )). Furthermore, this stage is based on the conditional generative adversarial network (CGAN) <ref type="bibr" target="#b18">[19]</ref> which generates promising results on image manipulating. Thus, the poster probability p(S t |(M h , M f , M b , C, P )) is expressed as:</p><formula xml:id="formula_0">p(S t |(M h , M f , M b , C, P )) = G(M h , M f , M b , C, P ).</formula><p>(1) We adopt a ResNet-like network as the generator G to build the conditional parsing model. We adopt the discriminator D directly from the pix2pixHD <ref type="bibr" target="#b29">[30]</ref>. We apply the L1 loss for further improving the performance, which is advantageous for generating more smooth results <ref type="bibr" target="#b31">[32]</ref>. Inspired by the LIP <ref type="bibr" target="#b5">[6]</ref>, we apply the pixel-wise softmax loss to encourage the generator to synthesize high-quality human parsing maps. Therefore, the problem of conditional parsing learning can be formulated as: </p><p>where M denotes the concatenation of M h , M f , and M b . The loss L parsing denotes the pixel-wise softmax loss <ref type="bibr" target="#b5">[6]</ref>.</p><p>The S t denotes the ground truth human parsing. The p data represents the distributions of the real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Warp-GAN</head><p>Since the misalignment of pixels would lead to generate the blurry results <ref type="bibr" target="#b26">[27]</ref>, we introduce a deep Warping Generative Adversarial Network (Warp-GAN) warps the desired clothes appearance into the synthesized human parsing map, which alleviates the misalignment problem between the input human pose and desired human pose. Different from deformableGANs <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b0">[1]</ref>, we warp the feature map from the bottleneck layer by using both the affine and TPS (Thin-Plate Spline) <ref type="bibr" target="#b2">[3]</ref> transformation rather than process the pixel directly by using affine only. Thanks to the generalization capacity of <ref type="bibr" target="#b22">[23]</ref>, we directly use the pre-trained model of <ref type="bibr" target="#b22">[23]</ref> to estimate the transformation mapping between the reference parsing and the synthesized parsing. We then warp the w/o clothes reference image by using this transformation mapping.</p><p>As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref> (c) and (d), the proposed deep warping network consists of the Warp-GAN generator G warp and the Warp-GAN discriminator D warp . We use the geometric matching module to warp clothes image, as described in the section 3.4. Formally, we take warped clothes image C w , w/o clothes reference image I w/o clothes , the target pose P , and the synthesized human parsing S t as input of the Warp-GAN generator and synthesize the result Î = G warp (C w , I w/o clothes , P, S t ). Inspired by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>, we apply a perceptual loss to measure the distances between high-level features in the pre-trained model, which encourages generator to synthesize high-quality and realistic-look images. We formulate the perceptual loss as:</p><formula xml:id="formula_2">L perceptual (Î, I) = n i=0 α i φ i (Î) − φ i (I) 1 ,<label>(3)</label></formula><p>where φ i (I) denotes the i-th (i = 0, 1, 2, 3, 4) layer feature map in pre-trained network φ of ground truth image I. We use the pre-trained VGG19 <ref type="bibr" target="#b27">[28]</ref> as φ and weightedly sum the L1 norms of last five layer feature maps in φ to represent perceptual losses between images. The α i controls the weight of loss for each layer. In addition, following pixp2pixHD <ref type="bibr" target="#b29">[30]</ref>, due to the feature map at different scales from different layers of discriminator enhance the performance of image synthesis, we also introduce a feature loss and formulate it as:</p><formula xml:id="formula_3">L feature (Î, I) = n i=0 γ i F i (Î) − F i (I) 1 ,<label>(4)</label></formula><p>where F i (I) represent the i-th (i = 0, 1, 2) layer feature map of the trained D warp . The γ i denotes the weight of L1 loss for corresponding layer. Furthermore, we also apply the adversarial loss L adv <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref> and L1 loss L 1 <ref type="bibr" target="#b31">[32]</ref> to improve the performance. We design a weight sum losses as the loss of G warp , which encourages the G warp to synthesize realistic and natural images in different aspects, written as follows:</p><formula xml:id="formula_4">L Gwarp = λ 1 L adv + λ 2 L perceptual + λ 3 L feature + λ 4 L 1 ,<label>(5)</label></formula><p>where λ i (i = 1, 2, 3, 4) denotes the weight of corresponding loss, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Refinement render</head><p>In the coarse stage, the identification information and the shape of the person can be preserve, but the texture details are lost due to the complexity of the clothes image. Pasting the warped clothes onto the target person directly may lead to generate the artifacts. Learning the composition mask between the warped clothes image and the coarse results also generates the artifacts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref> due to the diversity of pose. To solve the above issues, we present a refinement render utilizing multi-pose composition masks to recover the texture details and remove some artifacts.</p><p>Formally, we define C w as an image of warped clothes obtained by geometric matching learning module,Î c as a coarse result generated by the Warp-GAN, P as the target pose heatmap, and G p as the generator of the refinement render. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref> (e), taking C w ,Î c , and P as input, the G p learns to predict a towards multi-pose composition mask and synthesize the rendered result: <ref type="formula">(6)</ref> where denotes the element-wise matrix multiplication. We also adopt the perceptual loss to enhance the performance that the objective function of G p can be written as:</p><formula xml:id="formula_5">I p = G p (C w ,Î, P ) C w + (1 − G p (C w ,Î, P )) Î ,</formula><formula xml:id="formula_6">L p = µ 1 L perceptual (Î p , I) + µ 2 1 − G p (C w ,Î c , P ) 1 , (7)</formula><p>where µ 1 denotes the weight of perceptual loss and µ 2 denotes the weight of the mask loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Geometric matching learning</head><p>Inspired by <ref type="bibr" target="#b22">[23]</ref>, we adopt the convolutional neural network to learn the transformation parameters, including feature extracting layers, feature matching layers, and the transformation parameters estimating layers. As shown in <ref type="figure" target="#fig_1">Figure 3</ref> (f), we take the mask of the clothes image, and the mask of body shape as input which is first passed through the feature extracting layers. Then, we predict the correlation map by using the matching layers. Finally, we apply a regression network to estimate the TPS (Thin-Plate Spline) <ref type="bibr" target="#b2">[3]</ref> transformation parameters for the clothes image directly based on the correlation map.</p><p>Formally, given an input image of clothes C and its mask C mask , following the stage of conditional parsing learning, we obtain the approximated body shape M b and the synthesized clothes maskĈ mask from the synthesized human parsing. This subtask aims to learn the transformation mapping function T with parameter θ for warping the input image of clothes C. Due to the unseen of synthesized clothes but have the synthesized clothes mask, we learn the mapping between the original clothes mask C mask and the synthesized clothes maskĈ mask obey body shape M b . Thus, the objective function of the geometric matching learning can be formulated as:</p><formula xml:id="formula_7">L geo matching (θ) = T θ (C mask ) −Ĉ mask 1 ,<label>(8)</label></formula><p>Therefore, the warped clothes C w can be formulated as C w = T θ (C), which is helpful for addressing the problem of misalignment and learning the composition mask in the above subsection 3.2 and subsection 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first make visual comparisons with other methods and then discuss the results quantitatively. We also conduct the human perceptual study and the ablation study, and further train our model on our newly collected dataset MPV test it on the Deepfashion to verify the generation capacity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Since each person image in the dataset used in VI-TON <ref type="bibr" target="#b7">[8]</ref> and CP-VTON <ref type="bibr" target="#b28">[29]</ref> only has one fixed pose, we collected the new dataset from the internet, named MPV, which contains 35,687 person images and 13,524 clothes images. Each person image in MPV has different poses. The image is in the resolution of 256 × 192. We extract the 62,780 three-tuples of the same person in the same clothes but with diverse poses. We further divide them into the train set and the test set with 52,236 and 10,544 three-tuples, respectively. Note that we shuffle the test set with different clothes and diverse pose for quality evaluation. DeepFashion <ref type="bibr" target="#b37">[38]</ref> only have the pairs of the same person in different poses but do not have the image of clothes. To verify the generalization capacity of the proposed model, we extract 10,000 pairs from DeepFashion, and randomly select clothes image from the test set of the MPV for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We apply three measures to evaluate the proposed model, including subjective and objective metrics: 1) We perform pairwise A/B tests deployed on the Amazon Mechanical Turk (AMT) platform for human perceptual study. 2) we use Structural SIMilarity (SSIM) <ref type="bibr" target="#b30">[31]</ref> to measure the similarity between the synthesized image and ground truth image. In this work, we take the target image (the same person wearing the same clothes) as the ground truth image used to compare with the synthesized image for computing SSIM.</p><p>3) We use Inception Score (IS) <ref type="bibr" target="#b24">[25]</ref> to measure the quality of the generated images, which is a common method to verify the performances for image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Setting. We train the conditional parsing network, Warp-GAN, refinement render, and geometric matching network for 200, 15, 5, 35 epochs, respectively, using ADAM optimizer <ref type="bibr" target="#b12">[13]</ref>, with the batch size of 40, learning rate of 0.0002, β 1 = 0.5, β 2 = 0.999. We use two NVIDIA Titan XP GPUs and Pytorch platform on Ubuntu 14.04.</p><p>Architecture. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, each generator of MG-VTON is a ResNet-like network, which consists of three downsample layers, three upsample layers, and nine residual blocks, each block has three convolutional layers with 3x3 filter kernels followed by the bath-norm layer and Relu activation function. Their number of filters are 64, 128, 256, 512, 512, 512, 512, 512, 512, 512, 512, 512, 256, 128, 64. For the discriminator, we apply the same architecture as pix2pixHD <ref type="bibr" target="#b29">[30]</ref>, which can handle the feature map in different scale with different layers. Each discriminator contains four downsample layers which include 4x4 kernels, InstanceNorm, and LeakyReLU activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Baselines</head><p>VITON <ref type="bibr" target="#b7">[8]</ref> and CP-VTON <ref type="bibr" target="#b28">[29]</ref> are the state-of-the-art image-based virtual try-on method which assumes the pose of the person is fixed. They all used warped clothes image to improve the visual quality, but lack of the ability to generate image under arbitrary poses. In particular, VTION directly applied shape context matching <ref type="bibr" target="#b1">[2]</ref> to compute the transformation mapping. CP-VTON borrowed the idea from <ref type="bibr" target="#b22">[23]</ref> to estimate the transformation mapping using a convolutional network. To obtain fairness, we first enriched the input of the VITON and CP-VTON by adding the target pose. Then, we retrained the VITON and CP-VTON on MPV dataset with the same splits (train set and test set) as our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Quantitative Results</head><p>We conduct experiments on two benchmarks and compare against two recent related works using two widely used metrics SSIM and IS to verify the performance of the image synthesis, summarized in <ref type="table">Table.</ref> 1, higher scores are better. The results shows that ours proposed methods significantly achieve higher scores and consistently outperform all baselines on both datasets thanks to the cooperation of our conditional parsing generator, Warp-GAN, and the refinement render. Note that the MG-VTON (w/o Render) achieves the best SSIM score and the MG-VTON (w/o Mask) achieve the best IS score, but they obtain worse visual quality results and achieve lower scores in AMT study compare with MG-VTON (ours), as illustrated in the <ref type="table" target="#tab_4">Table 2</ref> and <ref type="figure" target="#fig_5">Figure 6</ref>. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, MG-VTON (ours) synthesizes more realistic-looking results than MG-VTON (w/o Render), but the latter achieve higher SSIM score, which also can be observed in <ref type="bibr" target="#b10">[11]</ref>. Hence, we believe that the proposed MG-VTON can generate high-quality person image for multipose virtural try-on with convincing results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results</head><p>We perform visual comparisons of the proposed method with VITON <ref type="bibr" target="#b7">[8]</ref>, CP-VTON <ref type="bibr" target="#b28">[29]</ref>, MG-VTON (w/o Render), and MG-VTON (w/o Mask), illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, which shows that our model generates reasonable results with convincing details. Although the baseline methods have synthesized few details of clothes, it is far from the practice towards multi-pose virtual try-on scenario. Specifically, the identity and the clothing of the lower-body cannot be preserved by the baseline methods. Besides, the clothing of the lower-body also cannot be preserved while the clothing of upper-body is change by the baseline methods. Furthermore, the baseline methods cannot synthesize the hairstyle and face well that result in blurry and artifacts. The reasons behind are that they overlook the high-level semantics of the reference image and the relationship between the reference image and target pose in the virtual try-on task. On the contrary, we adopt clothes and pose guided network to generate the target human parsing, which is helpful to alleviate the problem that lower-body clothing and hair cannot be preserved. In addition, we also design a deep warp-ing network with an adversarial loss carefully to solve the issue that the identity cannot be preserved. Furthermore, we capture the interplay of among the poses and present a multi-pose based refined network that learns to erase the noise and artifacts.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Human Perceptual Study</head><p>We perform a human study on MPV and Deepfashion <ref type="bibr" target="#b37">[38]</ref> to evaluate the visual quality of the generated image. Similar to pix2pixHD <ref type="bibr" target="#b29">[30]</ref>, we deployed the A/B tests on the Amazon Mechanical Turk (AMT) platform. There are 1,600 images with size 256 × 192. We have shown three images for reference (reference image, clothes, pose) and two synthesized images with the option for picking. The workers are given two choices with unlimited time to pick the one image looks more realistic and natural, considering how well target clothes and pose are captured and whether the identity and the appearance of the person are preserved. Specifically, the workers are shown the reference image, target clothes, target pose, and the shuffled image pairs. We collected 8,000 comparisons from 100 unique workers. As illustrated in <ref type="table" target="#tab_4">Table 2</ref>, the image synthesized by our model obtained higher human evaluation scores and indicate the high-quality results compare to the baseline methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Ablation Study</head><p>We conduct an ablation study to analyze the important parts of our method. Observed from <ref type="table">Table.</ref> 1, MG-VTON (w/o Mask) achieves the best scores. However, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, it may inevitably generate artifacts. In <ref type="figure" target="#fig_5">Figure 6</ref>, we further evaluate the effect of the components of our MG-VTON. It shows that the multi-pose composition mask loss, the perceptual loss, and the pose in the refinement render stage, and the warping module in Warp-GAN are all important to enhance the performance.</p><p>We also conduct an experiment to verify the effect of the human parsing in our MG-VTON. As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, there is a positive correlation between the quality of the human parsing with that of the result. We further to verify the effect of the synthesized human parsing by manipulating the desired pose and clothes, as illustrated in <ref type="figure" target="#fig_7">Figure 8</ref>. We manipulate the human parsing instead of the person image directly, and we can synthesize the person image in an easier and more effective way. Furthermore, we introduce an experiment that trained on our collected dataset MPV and test on the DeepFashion dataset to verify the generalization of the proposed model. As the <ref type="figure" target="#fig_4">Figure 5</ref> shown, our model </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we make the first attempt to investigate the multi-pose guided virtual try-on system, which enables clothes transferred onto a person image under diverse poses. We propose a Multi-pose Guided Virtual Try-on Network (MG-VTON) that generates a new person image after fitting the desired clothes into the input image and manipulating human poses. Our MG-VTON decomposes the virtual try-on task into three stages, incorporates a human parsing model is to guide the image synthesis, a Warp-GAN learns to synthesize the realistic image by alleviating misalignment caused by diverse pose, and a refinement render recovers the texture details. We construct a new dataset for the multi-pose guided virtual try-on task covering person images with more poses and clothes diversity. Extensive experiments demonstrate that our MG-VTON significantly outperforms all state-of-the-art methods both qualitatively and quantitatively with promising performances. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Some results of our model by manipulating both various clothes and diverse poses. The input image of the clothes and poses are shown in the first row, while the input images of the person are shown in the first column. The results manipulated by both clothes and pose are shown in the other columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The network architecture of the proposed MG-VTON. (a)(b): The conditional parsing learning module consists of a poseclothes-guided network that predicts the human parsing, which helps to generate high-quality person image. (c)(d): The Warp-GAN learns to generate the realistic image by using a warping features strategy due to the misalignment caused by the diversity of pose. (e): The refinement render network learns the pose-guided composition mask that enhances the visual quality of the synthesized image. (f): The geometric matching network learns to estimate the transformation mapping conditioned on the body shape and clothes mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F</head><label></label><figDesc>(G, D) = E M,C,P ∼pdata [log(1 − D(G(M, C, P ), M, C, P ))] + E St,M,C,P ∼pdata [log D(S t , M, C, P )] + E St,M,C,P ∼pdata [ S t − G(M, C, P ) 1 ] + E St,M,C,P ∼pdata [L parsing (S t , G(M, C, P ))],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualized comparison with other methods on our collected dataset MPV. MG-VTON (w/o Render) is the model where the refinement render is removed. The model where the multi-pose composition mask is removed denotes as MG-VTON (w/o Mask).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Some results from our model trained on MPV and tested on DeepFashion, which synthesizes the realistic image and captures the desired pose and clothes well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Ablation study on our collected dataset MPV. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Effect of the quality of human parsing. The quality of human parsing significantly affects the quality of the synthesized image in the virtual try-on task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Effect of clothes and pose for the human parsing, which is manipulating by the pose and the clothes. captures the target pose and clothes well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Test results of our MG-VTON on MPV dataset. Test results of our MG-VTON, trained on MPV dataset, test on DeepFashion dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Comparisons on MPV and DeepFashion.</figDesc><table><row><cell></cell><cell>MPV</cell><cell></cell><cell>DeepFashion</cell></row><row><cell>Model</cell><cell>SSIM</cell><cell>IS</cell><cell>IS</cell></row><row><cell>VITON [8]</cell><cell cols="3">0.639 2.394 ± 0.205 2.302 ± 0.116</cell></row><row><cell>CP-VTON [29]</cell><cell cols="3">0.705 2.519 ± 0.107 2.459 ± 0.212</cell></row><row><cell cols="4">MG-VTON (w/o Render) 0.754 2.694 ± 0.119 2.813 ± 0.047</cell></row><row><cell>MG-VTON (w/o Mask)</cell><cell cols="3">0.733 3.309 ± 0.137 3.368 ± 0.055</cell></row><row><cell>MG-VTON (Ours)</cell><cell cols="3">0.744 3.154 ± 0.142 3.030 ± 0.057</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Pairwise comparison on MPV and DeepFashion. Each cell lists the percentage where our MG-VTON is preferred over the other method. Chance is at 50%.</figDesc><table><row><cell></cell><cell cols="2">VITON CP-VTON</cell><cell>MG-VTON</cell><cell>MG-VTON</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(w/o Render) (w/o Mask)</cell></row><row><cell>MPV</cell><cell>83.1%</cell><cell>85.9%</cell><cell>82.4%</cell><cell>84.6%</cell></row><row><cell cols="2">DeepFashion 88.9%</cell><cell>83.3%</cell><cell>84.6%</cell><cell>75.5%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthesizing images of humans in unseen poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Principal warps: Thin-plate splines and the decomposition of deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A variational u-net for conditional appearance and shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Viton: An image-based virtual try-on network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The conditional analogy gan: Swapping fashion articles on people images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jetchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bergmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCVW</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05192</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepwrinkles: Accurate and realistic clothing modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Laehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A generative model of people in clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disentangled person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clothcap: Seamless 4d clothing capture and retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised person image synthesis in arbitrary poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative adversarial text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Virtual fitting by single-shot body shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sugita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perbet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3d Body Scanning Technologies</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="406" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deformable gans for pose-based human image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuiliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00055</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward characteristic-preserving image-based virtual try-on network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Skeleton-aided articulated motion generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detailed, accurate, human shape estimation from clothed 3d scan sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Be your own prada: Fashion synthesis with structural coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q X W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
