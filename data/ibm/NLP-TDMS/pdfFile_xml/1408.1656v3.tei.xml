<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fast and Accurate Unconstrained Face Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">A Fast and Accurate Unconstrained Face Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Unconstrained face detection</term>
					<term>normalized pixel difference</term>
					<term>deep quadratic tree</term>
					<term>AdaBoost</term>
					<term>cascade classifier !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method to address challenges in unconstrained face detection, such as arbitrary pose variations and occlusions. First, a new image feature called Normalized Pixel Difference (NPD) is proposed. NPD feature is computed as the difference to sum ratio between two pixel values, inspired by the Weber Fraction in experimental psychology. The new feature is scale invariant, bounded, and is able to reconstruct the original image. Second, we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations, so that complex face manifolds can be partitioned by the learned rules. This way, only a single soft-cascade classifier is needed to handle unconstrained face detection. Furthermore, we show that the NPD features can be efficiently obtained from a look up table, and the detection template can be easily scaled, making the proposed face detector very fast. Experimental results on three public face datasets (FDDB, GENKI, and CMU-MIT) show that the proposed method achieves state-of-the-art performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The objective of face detection is to find and locate faces in an image. It is the first step in automatic face recognition applications. Face detection has been well studied for frontal and near frontal faces. The Viola and Jones' face detector <ref type="bibr" target="#b0">[1]</ref> is the most well known face detection algorithm, which is based on Haar-like features and cascade AdaBoost <ref type="bibr" target="#b1">[2]</ref> classifier. However, in unconstrained scenes such as faces in a crowd, state-of-the-art face detectors fail to perform well due to large pose variations, illumination variations, occlusions, expression variations, out-of-focus blur, and low image resolution. For example, the Viola-Jones face detector fails to detect most of the face images in the Face Detection Data set and Benchmark (FDDB) database <ref type="bibr" target="#b2">[3]</ref> (examples shown in <ref type="figure">Fig. 1</ref>) due to the difficulties mentioned above. In this paper, we refer to face detection with arbitrary facial variations as the unconstrained face detection problem. We are interested in face detection in unconstrained scenarios such as video surveillance or images captured by hand-held devices.</p><p>Numerous face detection methods have been developed following Viola and Jones' work <ref type="bibr" target="#b0">[1]</ref>, mainly focusing on extracting different types of features and developing different cascade structures. A variety of complex features <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="figure">Fig. 1</ref>. Face images annotated (red ellipses) in the FDDB database <ref type="bibr" target="#b2">[3]</ref>.</p><p>[12], <ref type="bibr" target="#b12">[13]</ref> have been proposed to replace the Haarlike features used in <ref type="bibr" target="#b0">[1]</ref>. While these methods can improve the face detection performance to some extent, they generate a very large number (hundreds of thousands) of features and the resulting systems take too much time to train. Another development in face detection has been to learn different cascade structures for multiview face detection, such as parallel cascade <ref type="bibr" target="#b13">[14]</ref>, pyramid architecture <ref type="bibr" target="#b14">[15]</ref>, and Width-First-Search (WFS) tree <ref type="bibr" target="#b15">[16]</ref>. All these methods need to learn one cascade classifier for each specific facial view (or view range). In unconstrained scenarios, however, it is not easy to define all possible views of a face, and the computational cost increases with an increasing number of classifiers in complex cascade structures. Moreover, these approaches require manual labeling of face pose in each training image.</p><p>While some of the available methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> can handle multiview faces, they are not able arXiv:1408.1656v3 [cs.CV] 7 Sep 2015 to simultaneously consider other challenges such as occlusion. In fact, since these methods require partitioning multiview data into known poses, occlusion is not easy to handle in this way. On the other hand, while several studies addressed face detection under occlusion <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, they constrained themselves to detect only frontal faces under occlusion. As discussed in <ref type="bibr" target="#b21">[22]</ref>, a robust face detection algorithm should be effective under arbitrary variations in pose and occlusion, which remains an unresolved challenging problem.</p><p>In this paper, we are interested in developing effective features and robust classifiers for unconstrained face detection with arbitrary facial variation. First, we propose a simple pixel-level feature, called the Normalized Pixel Difference (NPD). An NPD is computed as the ratio of the difference between any two pixel intensity values to the sum of their values, in the same form as the Weber Fraction in experimental psychology <ref type="bibr" target="#b22">[23]</ref>. The NPD feature has several desirable properties, such as scale invariance, boundedness, and ability to reconstruct the original image. we further show that NPD features can be obtained from a look up table, and the resulting face detection template can be easily scaled for multiscale face detection.</p><p>Secondly, we propose a deep quadratic tree learning method and construct a single soft-cascade AdaBoost classifier to handle complex face manifolds and arbitrary pose and occlusion conditions. While individual NPD features may have "weak" discriminative ability, our work indicates that a subset of NPD features can be optimally learned and combined to construct more discriminative features in a deep quadratic tree. In this way, different types of faces can be automatically divided into different leaves of a tree classifier, and the complex face manifold in a high dimensional space can be partitioned in the learning process. This is the "divide and conquer" strategy to tackle unconstrained face detection in a single classifier, without pre-labeling of views in the training set of face images. The resulting face detector is robust to variations in pose, occlusion, and illumination, as well as to blur and low image resolution.</p><p>The novelty of this work is summarized as follows:</p><p>• A new type of feature, called NPD is proposed, which is efficient to compute and has several desirable properties, including scale invariance, boundedness, and enabling reconstruction of the original image.</p><p>• A deep quadratic tree learner is proposed to learn and combine an optimal subset of NPD features to boost their discriminability. In this way, only a single soft-cascade AdaBoost classifier is needed to handle unconstrained faces with occlusions and arbitrary viewpoints, without pose labeling or clustering in the training stage.</p><p>The advantages of the proposed approach include:</p><p>• The NPD feature evaluation is extremely fast, requiring a single memory access using a look up table.</p><p>• Multiscale face detection can be easily achieved by applying pre-scaled detection templates.</p><p>• The unconstrained face detector does not depend on pose specific cascade structure design; pose labeling or clustering in the training stage is also not required.</p><p>• The face detector is able to handle illumination variations, pose variations, occlusions, outof-focus blur, and low resolution face images in unconstrained scenarios. The source code of the proposed method is available in http://www.cbsr.ia.ac.cn/users/scliao/ projects/npdface/. The remainder of this paper is organized as follows. In Section 2 we review the related work. In Section 3 we introduce the NPD feature space. The proposed NPD based face detection method is presented in Section 4. Experimental results are provided in Section 5. Finally, we summarize the contributions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>As indicated in a survey of face detection methods <ref type="bibr" target="#b23">[24]</ref>, the most popular face detection methods are appearance based, which use local feature representation and classifier learning. Viola and Jones' face detector <ref type="bibr" target="#b0">[1]</ref> was the first one to apply rectangular Haar-like features in a cascaded AdaBoost classifier for real-time face detection. Many approaches have been proposed around the Viola-Jones detector to advance the state of the art in face detection. Lienhart and Maydt <ref type="bibr" target="#b3">[4]</ref> proposed an extended set of Haar-like features, where 45 • rotated rectangular features were introduced. Li et al. <ref type="bibr" target="#b4">[5]</ref> proposed another extension of Haar-like features, where the rectangles can be spatially set apart with a flexible distance. A similar feature, called the diagonal filter was also proposed by Jones and Viola <ref type="bibr" target="#b5">[6]</ref>. Various other local texture features have been introduced for face detection, such as the modified census transform <ref type="bibr" target="#b6">[7]</ref>, local binary pattern (LBP) <ref type="bibr" target="#b7">[8]</ref>, MB-LBP <ref type="bibr" target="#b10">[11]</ref>, LBP histogram <ref type="bibr" target="#b9">[10]</ref>, and the locally assembled binary feature <ref type="bibr" target="#b11">[12]</ref>. These features have been shown to be robust to illumination variations. Mita et al. <ref type="bibr" target="#b8">[9]</ref> proposed the joint Haarlike features to capture the co-occurrence of effective Haar-like features. Huang et al. <ref type="bibr" target="#b15">[16]</ref> proposed a sparse feature set in a granular space, where granules were represented by rectangles, and each individual sparse feature was learned as a combination of granules. A problem with the approaches in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b15">[16]</ref> is that the joint feature space is very large, making the optimal combination a difficult task.</p><p>While more sophisticated features may provide better discrimination power than Haar-like features for the face detection task, they generally increase the computational cost. In contrast, ordinal relationships among image regions are simple yet effective image features <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Sinha <ref type="bibr" target="#b24">[25]</ref> studied several robust ordinal relationships in face images and developed a face detection method accordingly. Liao et al. <ref type="bibr" target="#b27">[28]</ref> further showed that ordinal features can be effectively learned by AdaBoost classifier for face recognition. Sadr et al. <ref type="bibr" target="#b25">[26]</ref> showed that pixelwise ordinal features (POF), i.e. ordinal relationship (x &gt; y) between any two pixels, can faithfully encode image structures. Lepetit and Fua <ref type="bibr" target="#b28">[29]</ref> applied POF features in random trees for keypoint recognition. Shotton <ref type="bibr" target="#b31">[32]</ref> applied POF features in random forests for image categorization and segmentation. For facial analysis, Baluja et al. <ref type="bibr" target="#b26">[27]</ref> showed that POF features are good enough for discriminating between five facial orientations, a relatively simpler task than face detection. Wang et al. <ref type="bibr" target="#b30">[31]</ref> applied the random forest classifier together with POF features for facial landmark localization. Abramson and Steux <ref type="bibr" target="#b29">[30]</ref> proposed a pixel control point based feature for face detection, where each feature is associated with two sets of pixel locations (control points).</p><p>Besides different feature representations, some researchers have also tried different AdaBoost algorithms and weak classifiers. For weak classifiers utilized in boosting, Lienhart et al. <ref type="bibr" target="#b32">[33]</ref> and Brubaker et al. <ref type="bibr" target="#b33">[34]</ref> have shown that classification and regression trees (CART) <ref type="bibr" target="#b34">[35]</ref> work better than simple decision stumps. In this paper, we show that the optimal ordinal/contrastive features and their combinations can be learned by integrating the proposed NPD features in a deep quadratic tree. In this way, unconstrained face variations can be automatically partitioned into different leaves of the learned quadratic tree classifier.</p><p>Given that the original Viola-Jones face detector has limitations for multiview face detection <ref type="bibr" target="#b23">[24]</ref>, various cascade structures have been proposed to tackle multiview face detection <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Jones and Viola <ref type="bibr" target="#b5">[6]</ref> extended their face detector by training one face detector for each specific pose. To avoid evaluating all face detectors on each scanning subwindow, they developed a pose estimation step (similar to Rowley et al. <ref type="bibr" target="#b35">[36]</ref>) before face detection, and then only the face detector trained on that estimated pose was applied. In this two-stage detection structure, if the pose estimation is not reliable, the face is not likely to be detected in the second stage. Wu et al. <ref type="bibr" target="#b13">[14]</ref> proposed a parallel cascade structure for multiview face detection, where all face detectors tuned to different views have to be evaluated for each scanning window; they did use the first few cascade layers of all face detectors to estimate the pose for speedup. Li and Zhang <ref type="bibr" target="#b14">[15]</ref> proposed a coarse-to-fine pyramid architecture for multiview face detection, where the entire range of face poses was divided into increasingly smaller subranges, resulting in a more efficient detection structure. Huang et al. proposed a WFS tree based multiview face detection approach, which also works in a coarse-to-fine manner. They proposed the Vector Boost algorithm for multiclass learning, which is well suited for multiview pose estimation. However, all these methods need to learn a cascade classifier for each specific view (or view range) of a face, requiring an input face image to go through different branches of the detection structure. Hence, their computational cost generally increases with the number of classifiers in complex cascade structures. Moreover, these approaches require manual labeling of the face pose in each training image.</p><p>Instead of designing a detection structure, Lin and Liu <ref type="bibr" target="#b18">[19]</ref> proposed to learn the multiview face detector as a single cascade classifier. They derived a multiclass boosting algorithm, called MBHBoost by sharing features among different classes. This is a simpler approach to multiview face detection than designing complex cascade structures. Nevertheless, it still requires manual labeling of poses. In uncontrolled environments, however, it is not easy to define specific views of a face by discretizing the pose space, because a face could be in arbitrary pose simultaneously in yaw (out-of-plane), roll (in-plane), and pitch (up-anddown) angles. To avoid manual labeling, Seemann et al. <ref type="bibr" target="#b36">[37]</ref> suggested learning viewpoint clusters automatically for object detection. However, for human faces, Kim and Cipolla <ref type="bibr" target="#b37">[38]</ref> showed that clustering by traditional techniques like K-Means does not result in categorized poses. They hence proposed a multiclassifier boosting (MCBoost) for human perceptual clustering of object images, which showed promise for clustering face poses. However, the clusters are not always related to pose variations; in addition to different pose clusters, they also obtained clusters with various illumination variations.</p><p>Face detection in presence of occlusion is also an important issue in unconstrained face detection, but it has received less attention compared to multiview face detection. This is probably because, compared to pose variations, it is more difficult to categorize arbitrary occlusions into predefined classes. Hotta <ref type="bibr" target="#b16">[17]</ref> proposed a local kernel based SVM method for face detection, which was better than global kernel based SVM in detecting occluded frontal faces. Lin et al. <ref type="bibr" target="#b17">[18]</ref> considered 8 kinds of manually defined facial occlusions by training 8 additional cascade classifiers besides the standard face detector. Lin and Liu <ref type="bibr" target="#b18">[19]</ref> further proposed the MBHBoost algorithm to handle faces with one of 12 in-plane rotations or one of 8 types of occlusions, with each kind of rotation and occlusion treated as a different class. Chen et al. <ref type="bibr" target="#b19">[20]</ref> proposed a modified Viola-Jones face detector, where the trained detector was divided into sub-classifiers related to several predefined local patches, and the outputs of sub-classifiers were fused. Goldmann et al. <ref type="bibr" target="#b20">[21]</ref> proposed a component-based approach for face detection, where the two eyes, nose, and mouth were detected separately, and further connected in a topology graph. However, none of the above methods considered face detection with both occlusions and pose variations simultaneously in unconstrained scenarios. As discussed in <ref type="bibr" target="#b21">[22]</ref>, a robust face detector should be effective under arbitrary variations in pose and occlusion, which has not yet been solved.</p><p>Recently, unconstrained face detection has gained attention. Jain and Learned-Miller <ref type="bibr" target="#b2">[3]</ref> developed the FDDB database and benchmark for the development of unconstrained face detection algorithms. This database contains images collected from the Internet, and presents challenging scenarios for face detection. Subburaman and Marcel <ref type="bibr" target="#b38">[39]</ref> proposed a fast bounding box estimation technique for face detection, where the bounding box is predicted by small patch based local search. Jain and Learned-Miller <ref type="bibr" target="#b39">[40]</ref> proposed an online domain adaption approach to improve the performance of the Viola-Jones face detector on the FDDB database. Li et al. <ref type="bibr" target="#b12">[13]</ref> proposed the use of SURF feature <ref type="bibr" target="#b40">[41]</ref> in an AdaBoost cascade, and area under the curve (AUC) criterion to speed up the face detector training. Shen et al. <ref type="bibr" target="#b41">[42]</ref> proposed an exemplar-based face detection approach, which retrieves images from a large annotated face dataset; facial landmark locations are inferred from the annotations. This method is further improved in <ref type="bibr" target="#b42">[43]</ref> by boosting. Li et al. <ref type="bibr" target="#b43">[44]</ref> proposed a probabilistic elastic part (PEP) model to adapt any pre-trained face detector to a specific image collection like FDDB by an additional post-processing classifier. Zhu and Ramanan <ref type="bibr" target="#b44">[45]</ref> proposed to jointly detect a face, estimate its pose, and localize face landmarks in the wild by a Deformable Parts-based Model (DPM), which was further improved in <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b46">[47]</ref>. Chen et al. <ref type="bibr" target="#b47">[48]</ref> proposed to combine the face detection and landmark estimation tasks in a joint cascade framework to refine face detection by precise landmark detections. Yang et al. <ref type="bibr" target="#b48">[49]</ref> investigated the use of channel features for face detection, which achieves promising performance. Despite the availability of these methods for unconstrained face detection, the detection accuracy is still not satisfactory, especially when the detector is required to have low false alarms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NORMALIZED PIXEL DIFFERENCE FEA-TURE SPACE</head><p>The Normalized Pixel Difference (NPD) feature between two pixels in an image is defined as</p><formula xml:id="formula_0">f (x, y) = x − y x + y ,<label>(1)</label></formula><p>where x, y ≥ 0 are intensity values of the two pixels 1 , and f (0, 0) is defined as 0 when x = y = 0.</p><p>1. For ease of representation, sometimes we also denote x and y as pixels instead of pixel values. We use subscripts to differentiate between pixel and pixel values only when pixel locations are under discussion.</p><p>The NPD feature measures the relative difference between two pixel values. The sign of f (x, y) indicates the ordinal relationship between the two pixels x and y, and the magnitude of f (x, y) measures the relative difference (as a percentage of the joint intensity x + y) between x and y. Note that the definition f (0, 0) 0 is reasonable because, in this case, there is no difference between the two pixels x and y. Compared to the absolute difference |x − y|, NPD is invariant to scale change of the pixel intensities.</p><p>Weber, a pioneer in experimental psychology, stated that the just-noticeable difference in the magnitude change of a stimulus is proportional to the magnitude of the stimulus, rather than its absolute value <ref type="bibr" target="#b22">[23]</ref>. This is known as the Weber's Law. In other words, the human perception of difference in stimulus is often measured as a fraction of the original stimulus, that is, in a form ∆I/I, which is called the Weber Fraction. Chen et al. <ref type="bibr" target="#b49">[50]</ref> proposed a local image descriptor, called Weber's Law Descriptor for face recognition, which was computed from Weber Fractions of pixels in a 3 × 3 window. The proposed feature in Eq. <ref type="formula" target="#formula_0">(1)</ref> has also been used in other fields such as remote sensing, where the Normalized Difference Vegetation Index (NDVI) <ref type="bibr" target="#b50">[51]</ref> is defined as the difference to sum ratio between the visible red and the near infrared spectra to estimate the green vegetation coverage.</p><p>The NPD feature has a number of desirable properties. First, the NPD feature is antisymmetric, so either f (x, y) or f (y, x) is adequate for feature representation, resulting in a reduced feature space. Therefore, in an s × s image patch (vectorized as p × 1, where p = s · s), NPD feature f (x i , x j ) for pixel pairs 1 ≤ i &lt; j ≤ p is computed, resulting in d = p(p − 1)/2 features. For example, in a 20×20 face template, there are (20 × 20) × (20 × 20 − 1)/2 = 79, 800 NPD features in total. We call the resulting feature space the NPD feature space, denoted as Ω npd (∈ R d ).</p><p>Second, the sign of f (x, y) is an indicator of the ordinal relationship between x and y. Ordinal relationship has been shown to be an effective encoding for object detection and recognition <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref> because ordinal relationship encodes the intrinsic structure of an object image and it is invariant under various illumination changes <ref type="bibr" target="#b24">[25]</ref>. However, simply using the sign to encode the ordinal relationship is likely to be sensitive to noise when x and y have similar values. In the next section we will show how to learn robust ordinal/contrastive relationships with NPD features.</p><p>Third, the NPD feature is scale invariant, which is expected to be robust against illumination changes. This is important for image representation, since illumination change is always a troublesome issue for both object detection and recognition.</p><p>Fourth, as shown in Appendix A, the NPD feature f(x,y) is bounded in [-1,1]. The bounded property makes the NPD feature amenable to histogram binning or threshold learning in tree-based classifiers <ref type="bibr" target="#b0">[1]</ref>.  </p><formula xml:id="formula_1">f = (f (x 1 , x 2 ), f (x 1 , x 3 ), . . . ,f (x p−1 , x p )) T ∈ Ω npd , the original image I = (x 1 , x 2 , . . . , x p ) T can be reconstructed up to a scale factor.</formula><p>The proof of Theorem 1 is shown in Appendix B, which also gives a linear-time approach to reconstruct the original image up to a scale factor. Theorem 1 states that each point in the feature space Ω npd corresponds to a group of intensity-scaled images in the original pixel intensity space. In contrast, the scale invariance property says that all intensity-scaled images are "compressed" to a point in the bounded feature space Ω npd . Therefore, Ω npd is a feature space which is invariant to scale variations, but it carries all the necessary information from the original space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NPD FOR FACE DETECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Deep Quadratic Tree</head><p>The classic Viola-Jones face detector <ref type="bibr" target="#b0">[1]</ref> learns representative features by boosted stumps. A stump is a basic tree classifier with one threshold that splits a node in two leaves. There are two limitations with stumps. First, this shallow structure cannot capture interactions between different feature dimensions. Second, the simple thresholding ignores higher-order information contained in a feature. Therefore, in this paper, we consider a quadratic splitting strategy and a deeper tree structure. Specifically, for a feature x, we consider the tree node splitting as</p><formula xml:id="formula_2">(ax 2 + bx + c) &lt; t,<label>(2)</label></formula><p>where a, b, c are constants w.r.t. x, and t is the splitting threshold. With appropriate coefficients, this corresponds to checking whether x is in a range [θ 1 , θ 2 ] or not, where θ 1 and θ 2 are two learned thresholds. Compared to the original linear splitting x &lt; t, Eq.</p><p>(2) considers both the first-order and second-order information of x, enabling a better interpretation of the splitting rule. Particularly, for the proposed NPD feature, three kinds of object structures can be learned:</p><formula xml:id="formula_3">−1 ≤ x − y x + y ≤ θ &lt; 0,<label>(3)</label></formula><formula xml:id="formula_4">0 &lt; θ ≤ x − y x + y ≤ 1,<label>(4)</label></formula><formula xml:id="formula_5">θ 1 ≤ x − y x + y ≤ θ 2 ,<label>(5)</label></formula><p>where θ 1 &lt; 0 and θ 2 &gt; 0. Eq. (3) applies if the object pixel x is notably darker than pixel y (e.g. f 1 in <ref type="figure" target="#fig_2">Fig. 3</ref>), while Eq. (4) covers the case when pixel x is notably brighter than pixel y (e.g. f 2 in <ref type="figure" target="#fig_2">Fig. 3</ref>). These two kinds of structures can also be learned by a classic stump. They are also known as ordinal relationships similar as in <ref type="bibr" target="#b24">[25]</ref>, except that a better threshold is learned instead of the default threshold 0. In contrast, if Eq. (5) does not hold, then there will be a notable edge or contrast between pixels x and y (e.g. f 3 and f 4 in <ref type="figure" target="#fig_2">Fig. 3</ref>), but the polarity is uncertain.</p><p>For example, f 3 in <ref type="figure" target="#fig_2">Fig. 3</ref> represents a notable edge between the face and background, but the background pixel can be either darker or brighter than the face. This kind of contrastive structure can only be learned by a quadratic splitting.</p><p>In practice, instead of solving Eq. (2) for quadratic splitting, we propose to quantize the feature range into L discrete bins (e.g. L=256 in this paper), and do an exhaustive search to determine the two optimal thresholds, where the weighted mean square error is applied as the optimal splitting criterion. Thanks to the bounded property of the proposed NPD feature, this quantization can be easily done. Besides, we build an L-bin histogram of the sample weights, and apply a one-dimensional integral technique similar as in <ref type="bibr" target="#b0">[1]</ref> to speed up the splitting.</p><p>Furthermore, we apply the quadratic splitting to learn a deep tree (e.g. depth of eight in this paper), instead of a stump or a shallow tree for face detection. This way, several NPD features are optimally combined together to represent the intrinsic face structure. An example is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The proposed deep quadratic tree is well suited for face detection with arbitrary pose variations, since similar views can be clustered in the same leaf node of the tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Face Detector</head><p>Given that the proposed NPD features contain redundant information, we also apply the AdaBoost algorithm to select the most discriminative features and construct strong classifiers <ref type="bibr" target="#b0">[1]</ref>. We adopt the Gentle AdaBoost algorithm <ref type="bibr" target="#b1">[2]</ref> to learn the NPD feature based deep quadratic trees.</p><p>As in <ref type="bibr" target="#b0">[1]</ref>, a cascade classifier is further learned for rapid face detection. We only learn one single cascade classifier for unconstrained face detection robust to occlusions and pose variations. This implementation has the advantage that there is no need to label the pose of each face image manually or cluster the poses before training the detector. In the learning process, the algorithm automatically divides the whole face manifold into several sub-manifolds by the deep quadratic trees. Besides, we adopt the soft cascade structure <ref type="bibr" target="#b51">[52]</ref> for efficient training and early rejection of negative samples. Specifically, soft cascade can be regarded as a single AdaBoost classifier with one exit per weak classifier. In each iteration, a deep quadratic tree is learned as the weak classifier, and a threshold of the current AdaBoost classifier is also learned for rejecting nonfaces. Finally, the learned deep quadratic trees and thresholds are aggregated sequentially to represent an ensemble <ref type="bibr" target="#b1">[2]</ref>.</p><p>Below is a summary of how the proposed method handles the unconstrained face detection problem. • Occlusion. In contrast to Haar-like features that are sensitive to occlusions because of large support <ref type="bibr" target="#b17">[18]</ref>, NPD features are computed by only two pixel values, making them robust to occlusion.</p><p>• Illumination. Since NPD features are scale invariant, they are robust to illumination changes.</p><p>• Blur or low image resolution. Because the NPD features involve only two pixel values, they do not require rich texture information on the face. This makes NPD features effective in handling blurred or low resolution face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Implementation Details</head><p>we used the Annotated Facial Landmarks in the Wild (AFLW) database <ref type="bibr" target="#b52">[53]</ref> for training our unconstrained face detector. The AFLW database contains 25,993 face annotations in 21,997 real-world images collected from Flickr. This is an unconstrained face database including large face variations in pose, illumination, expression, ethnicity, age, gender, etc. We cropped 21,730 face images from AFLW. Together with their mirrored images and perturbations in positions, we had 217,300 face images in total for training. Some examples are shown in <ref type="figure" target="#fig_4">Fig. 4 (left)</ref>. For bootstrapping nonface images, we also used the AFLW images, but masked the facial regions with random images containing no faces, as shown in <ref type="figure" target="#fig_4">Fig. 4 (right)</ref>. We used a detection template of 24 × 24 pixels. We set the maximum depth of the tree classifiers to be learned as eight, so that at most eight NPD features need to be evaluated for each tree classifier. In the soft cascade training, we set the threshold of each exit as the minimal score of positive samples, i.e. we did not reject positive samples during training. Our final detector contains 1,226 deep quadratic trees, and 46,401 NPD features. Nevertheless, the average number of feature evaluations per detection window is only 114.5 considering stagewise nonface rejection, which is quite reasonable.</p><p>For an analysis, we also trained a near frontal face detector using the proposed NPD features and the classic cascade of regression trees (CART <ref type="bibr" target="#b34">[35]</ref>) with depth of four. A subset of the training data 2 in <ref type="bibr" target="#b12">[13]</ref> was used, including 12,102 face images and 12,315 nonface images. The detection template is 20 × 20 pixels. The detector cascade contains 15 stages, and for each stage, the target false accept rate was 0.5, with a detection rate of 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Detector Speed Up</head><p>To further speed up the learned NPD detector for face detection, we develop the following two techniques. First, for 8-bit gray images, we build a 256 × 256 look up table to store pre-computed NPD features. This way, computing f (x, y) in Eq. 1 only requires one memory access from the look up table.</p><p>Second, the learned face detection template (e.g. 20 × 20 used in this paper) can be easily scaled to enable multiscale face detection. So, we pre-compute multiscale detection templates and apply them to detect faces at various scales. This way, iterative rescaling of images for multiscale detection is avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We evaluate the performance of the NPD face detector on three public-domain databases, FDDB <ref type="bibr" target="#b2">[3]</ref>, GENKI <ref type="bibr" target="#b53">[54]</ref>, and CMU-MIT <ref type="bibr" target="#b35">[36]</ref>. We also provide an analysis of the proposed method, report the face detection speed, and report unconstrained face detection performances under illumination variations, pose variations, occlusion, and blur, respectively.</p><p>In the test stage, a scale factor of 1.2 was set for multiscale detection. A postprocessing method similar to the OpenCV face detection module was implemented, which merges nearby detections by the disjoint set algorithm. For each detected face, we summarized the scores of AdaBoost classifiers in all stages of the cascade to be the final score; this score was used to generate the Receiver Operating Characteristic (ROC) curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on FDDB Database</head><p>The FDDB dataset <ref type="bibr" target="#b2">[3]</ref> covers challenging scenarios for face detection. Images in FDDB comes from the Faces in the Wild dataset <ref type="bibr" target="#b54">[55]</ref>, which is a large collection of Internet images collected from the Yahoo News. It contains 2,845 images with a total of 5,171 faces, with a wide range of challenging scenarios including arbitrary pose, occlusions, different lightings, expressions, low resolutions, and out-of-focus faces. All faces in the database have been annotated with elliptical regions. <ref type="figure">Fig. 1</ref> shows some examples of the annotated faces from the FDDB database.</p><p>For benchmark evaluation, Jain and Learned-Miller <ref type="bibr" target="#b2">[3]</ref> provided an evaluation code for a comparison of different face detection algorithms. There are two metrics for performance evaluation based on ROC: discrete score metric and continuous score metric, which correspond to coarse match (similar to previous evaluations in the face detection literature) and precise match, respectively, between the detection and the ground truth. The database is divided into 10 subsets for performance evaluation, and the obtained detection results are accumulated to generate the ROC curve <ref type="bibr" target="#b2">3</ref> .</p><p>We compared our method with state-of-the-art results reported on the FDDB website <ref type="bibr" target="#b3">4</ref> . <ref type="table" target="#tab_0">Table 1</ref> shows a comprehensive comparison of detection rates of various algorithms on the FDDB database at FP=0, 10, and 100, where methods marked with a 9 were trained on the same AFLW database <ref type="bibr" target="#b52">[53]</ref> as ours. It can be observed that the proposed method outperforms most of the baseline methods except four methods <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> published recently. The proposed NPD face detector is the second best one at FP=0 for the discrete metric and the third best one for the continuous metric. Specifically, the NPD detector detects about 3. According to <ref type="bibr" target="#b2">[3]</ref>, ten ROC curves should be obtained and averaged for the final performance report, however, what is actually done in the FDDB results webpage (Footnote 4) is that all detection results of the 10 subsets are first merged, and then a single ROC curve is evaluated. We followed the latter one for consistency to existing results. 4. http://vis-www.cs.umass.edu/fddb/results.html 54% of the annotated FDDB faces in coarse sense (50% overlap with ground truth) without any false alarms.</p><p>The ROC curves of recent methods are depicted in <ref type="figure" target="#fig_5">Fig. 5</ref> for the discrete score metric and in <ref type="figure" target="#fig_6">Fig. 6</ref> for the continuous score metric. In both Figs. 5 and 6, the curve labels in the legend are sorted in descending order of the detection rates at zero false positives (FP=0). It can be observed that the proposed NPD detector is among the top performers for the discrete metric, though it is not as good as the four recent methods for the continuous metric. However, note that the FDDB database uses ellipses for groundtruth of face annotations, and several methods (e.g. Yan-DPM <ref type="bibr" target="#b45">[46]</ref>, and HeadHunter <ref type="bibr" target="#b46">[47]</ref>) output similar elliptical detections to improve the performance especially with the continuous metric. The proposed detector outputs square detections, followed by a 20% horizontal expansion and 50% vertical expansion as suggested in <ref type="bibr" target="#b42">[43]</ref>. This processing is not as good as making elliptical detections, but is still better than the original square detections.</p><p>Compared to recent methods, the Joint Cascade algorithm <ref type="bibr" target="#b47">[48]</ref> is the most competitive one to us in terms of accuracy and speed (see Sec. 5.6). However, the Joint Cascade method used a sophisticated postprocessing classifier to remove hard negatives and hence improved the results. Other methods are not efficient as compared in Sec. 5.6. Especially, the DPM based methods <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> are known to be quite slow. The method of Zhu-Ramanan <ref type="bibr" target="#b44">[45]</ref> has the advantage of learning from only hundreds of face images and it jointly outputs face bounding box, pose, and landmarks. But it requires manual landmark and pose annotations as face prior knowledge before train-  <ref type="bibr" target="#b2">[3]</ref> with the discrete score metric. ing. The performance of the Zhu-Ramanan model is quite impressive considering such a small training data. However, the runtime cost of their model is very expensive. As reported in <ref type="bibr" target="#b42">[43]</ref>, for a 1480 × 986 image, Zhu and Ramanan's detector takes 231 seconds to run and allocates up to 2GB memory. In contrast, our model is more efficient, requiring only a few milliseconds per image and only 50MB of memory as discussed in Sec. 5.6. <ref type="figure">Fig. 7</ref> shows some examples of detected faces in the FDDB database by the proposed NPD method. Many rotated, occluded, and out-of-focus faces can be successfully detected by the proposed method. Some faces (e.g. the 2nd image in row 1, and the 4th image in row 3 in <ref type="figure">Fig. 7</ref>) that are not annotated in the ground truth can still be detected by the proposed method. However, there are a number of faces that cannot be detected by the proposed method, especially in very crowded scenes (see the 1st image and the 3rd image in row 1, and the last two images in row 5 in <ref type="figure">Fig. 7)</ref>. Therefore, unconstrained face detection in crowded scenes is still very challenging and deserves more attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on GENKI Database</head><p>The GENKI database <ref type="bibr" target="#b53">[54]</ref> was collected by the Machine Perception Laboratory, University of California, San Diego. We evaluated the current release of the GENKI database, GENKI-R2009a, on its SZSL subset, which contains 3,500 images collected from the Internet. These images include a wide range of backgrounds, illumination conditions, geographical locations, personal identity, and ethnicity. Some examples of face images from the GENKI database are shown in <ref type="figure">Fig. 9</ref>, with labeled detections by the proposed NPD method. Most images in the GENKI dataset contain only a single face. In that sense, the GENKI dataset is not as challenging as the FDDB dataset. Some of the images in the GENKI-SZSL dataset contain faces that are not labeled, therefore they are not suitable for the face detection evaluation task. After removing such unlabeled images, we are left with 3,270 images for face detection evaluation. We evaluated our unconstrained face detector, as well as the Viola-Jones face detector implemented in OpenCV 2.4, and a commercial face detector, PittPatt <ref type="bibr" target="#b59">[60]</ref>. We again used the benchmark evaluation code in <ref type="bibr" target="#b2">[3]</ref> for performance evaluation, but slightly modified the code for allowing ground truth annotations as rectangles. The ROC curves of the three methods are shown in <ref type="figure" target="#fig_7">Fig. 8</ref> for both the discrete and continuous score metrics. The results show that the proposed NPD face detector significantly outperforms both the Viola-Jones and PittPatt face detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on CMU-MIT Database</head><p>The CMU-MIT face dataset <ref type="bibr" target="#b35">[36]</ref> is one of the early benchmarks for face detection. The CMU-MIT frontal face data set contains 130 gray-scale images with a total of 511 faces, most of which are not occluded. We applied our frontal NPD face detector described in Subsection 4.2.1 on this database. We also used the modified benchmark evaluation code from <ref type="bibr" target="#b2">[3]</ref> with the discrete score metric for performance evaluation. <ref type="figure">Fig. 10</ref> shows the ROC curves for the proposed NPD face detector, the Soft cascade method <ref type="bibr" target="#b51">[52]</ref>, the SURF cascade method <ref type="bibr" target="#b12">[13]</ref>, and the Viola-Jones detector <ref type="bibr" target="#b0">[1]</ref>.</p><p>The results show that, compared to the Viola-Jones frontal face detector, the NPD detector performs better when the number of false positives, FP &lt; 50, while it is slightly worse than Viola-Jones at higher FPs. Compared to the SURF cascade detector, the NPD <ref type="figure">Fig. 7</ref>. Detected faces in the FDDB database <ref type="bibr" target="#b2">[3]</ref> by the proposed NPD method. Green boxes are detections by the NPD detector, while red ellipses are ground truth annotations.</p><p>detector is better when FP &lt; 3, but SURF cascade method outperforms NPD at higher FPs. Note that the SURF cascade method uses a face template of size 40 × 40 pixels, which is four times larger than our face detection template (20 × 20 pixels). Generally, a larger face template contains more features for face description, but is computationally more expensive and may have a limitation in detecting blurred faces. In addition, the proposed NPD method is not as good as the Soft cascade, the state-of-the-art method on the CMU-MIT dataset. Still, the proposed NPD method can detect about 80% of the frontal faces without any false positives, which is promising. Some of the detected faces in the CMU-MIT dataset by the proposed NPD method are shown in <ref type="figure">Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis of the Proposed Face Detector</head><p>Since the proposed face detector is a combination of the NPD features and tree classifiers, it is instructive to determine the contribution of each of these two components. In the following, we trained all compared face detectors on the same training set <ref type="bibr" target="#b12">[13]</ref> and cascade training settings described in Section 4.2.1. First, we fixed the classic regression tree (CART <ref type="bibr" target="#b34">[35]</ref>) based weak learner with depth of four, and compared the proposed NPD feature to three <ref type="figure">Fig. 9</ref>. Detected faces in the GENKI-SZSL dataset <ref type="bibr" target="#b53">[54]</ref> by the proposed NPD method. <ref type="figure">Fig. 11</ref>. Detected faces in the CMU-MIT dataset <ref type="bibr" target="#b35">[36]</ref> by the proposed NPD method. <ref type="figure">Fig. 10</ref>. ROC curves for face detection on the CMU-MIT dataset <ref type="bibr" target="#b35">[36]</ref>.</p><p>other local features, namely Haar-like features <ref type="bibr" target="#b0">[1]</ref>, LBP <ref type="bibr" target="#b60">[61]</ref>, and POF <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Since LBP is a discrete label, we treated it as a categorical variable in the regression tree learning, that is, for branching at each tree node, the algorithm finds the optimal criterion that splits the discrete LBP codes into two groups. Using the same training set <ref type="bibr" target="#b12">[13]</ref>, we trained the three detectors using Haar, LBP, and POF, respectively. The model complexity of these detectors is summarized in <ref type="table" target="#tab_1">Table 2</ref>. It can be observed that, the CART based NPD model is more efficient than the POF model, though it requires slightly more feature evaluations than the Haar and LBP models. However, it should be noted that the computation of Haar-like features requires computing integral images, while for LBP, each feature needs to compare 8 pairs of pixels and convert the resulting binary string to the corresponding decimal number. In contrast, using look up tables as aforementioned, computing the NPD feature requires only one memory access. The four detectors with different local features were tested on the FDDB database, resulting in ROC curves shown in <ref type="figure" target="#fig_0">Fig. 12</ref> for both the discrete and continuous score metrics. The NPD detector performs better than the Haar, LBP, and POF detectors with the same CART based weak learners. The performance improvements due to NPD features over Haar, LBP, and POF features are about 6%, 19%, and 15%, respectively, for discrete metric, and about 4%, 13%, and 10%, respectively, for continuous metric, at FP=1. NPD is better than POF, because with NPD features the regression tree learns optimal thresholds to form more robust ordinal rules. NPD performs better than Haar and LBP, especially at low false positives, indicating that combining optimal pixel-level features in regression trees provides better discrimination between faces and nonfaces.</p><p>We also tried a variation of NPD, defined as f (x, y) = x−y √ x 2 +y 2 and denoted as NPD2. The comparison on FDDB are illustrated in <ref type="figure" target="#fig_0">Fig. 12</ref>, showing that the performance of NPD is slightly better than that of NPD2. Therefore, given that NPD is simpler than NPD2, we prefer the formulation of Eq. (1).</p><p>Next, we fixed the NPD feature representation and the classic cascade architecture <ref type="bibr" target="#b0">[1]</ref>, and compared three different weak learners, namely the stump classifier <ref type="bibr" target="#b0">[1]</ref>, the classic regression tree CART, and the proposed deep quadratic tree (DQT). Both CART and DQT were with depth of four. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the stump based detector requires much more weak classifiers than CART, indicating that combining NPD features in a deeper regression tree is much more effective. Furthermore, <ref type="table" target="#tab_1">Table 2</ref> shows that using CART does not increase the average computation cost compared to stump w.r.t. average feature evaluations. In addition, the proposed DQT based learner further reduces the number of weak classifiers and average feature evaluations required. The three face detectors were tested on the FDDB database, resulting in ROC curves shown in <ref type="figure" target="#fig_2">Fig. 13</ref> for both the discrete score metric and continuous score metric. As illustrated, using CART instead of stump classifier improves the face detection performance by about 0% -17% for discrete metric and 0% -11% for continuous metric. The improvement is larger at smaller false positives. This verifies that tree classifiers help to optimally combine NPD features for the complex unconstrained face detection task. Besides, the DQT based detector further improves the performance, due to its quadratic splitting capability compared to linear splitting.</p><p>Finally, with NPD+DQT, we compared the soft cascade detector <ref type="bibr" target="#b51">[52]</ref> and the classic cascade detector <ref type="bibr" target="#b0">[1]</ref>, as shown in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="figure" target="#fig_2">Fig. 13</ref>. Clearly, with comparable performance, soft cascade further reduces the model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluation Under Specific Challenges</head><p>In the following, we evaluate how the proposed NPD face detector performs under illumination variation, pose variation, occlusion, and blur (or low resolution). Note that these four challenges are often encountered   simultaneously in an image. In our selection of the four subsets, one per specific challenge, we focused on the main source of variation in each image. For each challenge, we selected 100 images from the FDDB database <ref type="bibr" target="#b2">[3]</ref> (examples are shown in <ref type="figure" target="#fig_4">Fig. 14)</ref>, and ran our unconstrained NPD face detector (trained with AFLW) on each subset separately. <ref type="figure" target="#fig_5">Fig. 15</ref> shows that the NPD face detector performs the best on the pose and illumination subsets, thanks to the scale-invariant NPD features and the deep quadratic trees. For the occlusion and blur subsets, the performance largely drops. These results indicate that occlusion and blur are the two major challenges for unconstrained face detection, which have not been well addressed in the literature.</p><p>The NPD face detector is also compared with the Viola-Jones face detector implemented in OpenCV 2.4, and the commercial face detector PittPatt on the four subsets of FDDB discussed above. The resulting ROC curves with the discrete score metric are shown in <ref type="figure" target="#fig_6">Fig. 16</ref>. These plots show that the proposed NPD face detector outperforms both the Viola-Jones and the PittPatt face detectors on all the four subset-    s. The reasons for the superior performance of the proposed method under illumination variations, pose variations, occlusions, and blur, were discussed in Subsection 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Detection Speed</head><p>For handheld devices like mobile phones, the available resources for computation and memory are rather limited. Therefore, face detector's complexity and detection speed are very important for embedded systems. In this subsection, we report the detection speed of the proposed NPD face detector, compared with the Viola-Jones 5 face detector in OpenCV 2.4, which is known to be optimized for speed. The proposed NPD face detector is implemented in C++, which requires about 50MB of memory in runtime. The model size of our frontal detector is 41KB, while that of the unconstrained detector is 831KB. Two platforms were selected for this evaluation: (i) a normal desktop PC with the Intel Core i5-2400 @3.1GHz CPU (4 cores, 4 threads), and (ii) a netbook with Intel Atom N450 @1.6GHz processor (1 core, 2 threads), to simulate low-end devices. For face detection evaluation, a video clip of the movie "Jobs" was used. This video clip shows a busy campus, with each frame containing from one to tens of faces. The length of the video clip is about 2 minutes, containing 3,950 frames in total. The original resolution is 1280 × 720. To test the detection speed at various resolutions, the original video clip was cropped and resized to 1920 × 1080, 800×600, and 640×480. In this evaluation, the minimal face size to detect was set to 40 × 40 pixels for frontal detector as in <ref type="bibr" target="#b12">[13]</ref> and 80×80 for unconstrained detector as in <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, and the scaling factor was 1.2. The multi threading technique was enabled in both NPD and OpenCV detectors for parallel computation.</p><p>Note that we only calculated the face detection time, regardless of the video decoding time.</p><p>The test results (in terms of Frame Per Second, FPS) of frontal detectors are shown in <ref type="table" target="#tab_2">Table 3</ref>. The detection parameters of the SURF cascade method <ref type="bibr" target="#b12">[13]</ref> are the same as our algorithm, except that authors in <ref type="bibr" target="#b12">[13]</ref> used an i7@3.2GHz CPU (4 cores, 8 threads) for the desktop computer. It can be observed that the NPD detector is much faster than both the OpenCV and SURF cascade detectors. On Atom N450 processor, the detection speed of the NPD detector is about 9 times faster than the detection speed of the OpenCV detector; on i5 processor the speed of the NPD detector is about 7 times the speed of the OpenCV detector. <ref type="table" target="#tab_2">Table 3</ref> shows that our frontal face detector can run in real-time (29.6 FPS) on i5 desktop PC for processing 1920 × 1080 high definition videos, and 177.6FPS for VGA videos. On the low-end Atom platform, the NPD detector can run in near real-time (19.4 FPS) for VGA videos. The reasons for the high processing speed of NPD are two folds. First, the NPD feature is simple, involving only two pixels. Further with the look up table technique, the evaluation of each NPD feature requires only one memory access. Second, the NPD feature can be easily scaled to various sizes of detection templates. Therefore, pre-calculating and storing multiscale templates can speed up detection because rescaling the input image is avoided.</p><p>Next, we also evaluate our unconstrained detector and compare it to recent methods, as shown in Table 4. The proposed method is much faster than Yan-DPM <ref type="bibr" target="#b45">[46]</ref> and ACF <ref type="bibr" target="#b48">[49]</ref> with either a single thread or multi-threads. The NPD detector achieves similar speed as that of Joint Cascade method <ref type="bibr" target="#b47">[48]</ref>. Using multi-thread i5 CPU, we are able to achieve 70FPS for unconstrained face detection on VGA frames, which is slower than the frontal detector, but still quite efficient.  <ref type="bibr" target="#b5">6</ref> 16.2 n/a @3.1GHz</p><p>1280 × 720 63.3 8.9 n/a (4 cores, 4 threads) 1920 × 1080 29. <ref type="bibr" target="#b5">6</ref> 3.6 n/a * "n/a" means results are not reported in <ref type="bibr" target="#b12">[13]</ref> for the SURF detector. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SUMMARY AND FUTURE WORK</head><p>We have proposed a fast and accurate method for face detection in cluttered scenes. First, a simple feature called NPD is proposed, which has properties of scale invariance, boundedness, and reconstruction ability. Second, we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations. As a result, a single soft-cascade AdaBoost classifier is able to achieve promising results for face detection with large pose variations and occlusions. Evaluations on three public face databases show that the proposed method achieves state-of-the-art performance for unconstrained face detection, and an analysis show that occlusions and blur are two big challenges for face detection. The proposed detector is also efficient, about 6 times faster than the Viola-Jones face detector implemented in OpenCV 2.4. It is interesting to apply the proposed NPD feature and the classifier learning method for other tasks such as face attribute classification and pedestrian detection. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>A plot of the NPD function f (x, y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>shows that f (x, y) is a bounded function and it defines a nonlinear surface.Theorem 1 (Reconstruction): Given the NPD feature vector</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Learning and combining NPD features in a deep quadratic tree. Left: four NPD features are automatically selected in the learning process. Right: the four features are optimally combined in a deep quadratic tree for face/nonface prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>Pose. Pose variations are handled by learning NPD features in boosted deep quadratic trees, where different views can be automatically partitioned into different leaves of the trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Example face (left) and nonface (right) images from AFLW<ref type="bibr" target="#b52">[53]</ref> for face detector training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>ROC curves of recent methods on the FDDB database</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>ROC curves of recent methods on the FDDB database<ref type="bibr" target="#b2">[3]</ref> with the continuous score metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>ROC curves for face detection on the GENKI-SZSL dataset<ref type="bibr" target="#b53">[54]</ref> with (a) discrete and (b) continuous score metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Comparison of different features in CART based face detector on the FDDB database<ref type="bibr" target="#b2">[3]</ref> with (a) discrete and (b) continuous score metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Comparison of NPD face detectors based on different weak learners on the FDDB database<ref type="bibr" target="#b2">[3]</ref> with (a) discrete and (b) continuous score metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Example images and annotated faces for four subsets extracted from the FDDB database<ref type="bibr" target="#b2">[3]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>ROC curves of the proposed NPD face detector on the four subsets extracted from the FDDB database<ref type="bibr" target="#b2">[3]</ref> with (a) discrete and (b) continuous score metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 16 .</head><label>16</label><figDesc>ROC curves for face detection on four subsets from the FDDB database<ref type="bibr" target="#b2">[3]</ref> with the discrete score metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Shengcai</head><label></label><figDesc>Liao received the B.S. degree in mathematics and applied mathematics from the Sun Yat-sen University in 2005, and the Ph.D. degree from the Institute of Automation, Chinese Academy of Sciences (CASIA) in 2010. He was a Postdoc in the Dept. of Computer Science and Engineering, Michigan State University during 2010-2012, and he is now an Associate Professor in CASIA. His research interests include face recognition and video surveillance. He was awarded the Excellence Paper of Motorola Best Student Paper and the 1st Place Best Biometrics Paper award at the International Conference on Biometrics in 2006 and 2007, respectively. He was also awarded the best reviewer award in IJCB 2014. Anil K. Jain is a university distinguished professor in the Department of Computer Science and Engineering at Michigan State University. His research interests include pattern recognition and biometric authentication. He served as the editor-in-chief of the IEEE TPAMI (1991-1994). The holder of six patents in the area of fingerprints, he is the author of a number of books, including Handbook of Fingerprint Recognition (2009), Handbook of Biometrics (2011), Handbook of Multibiometrics (2006), Handbook of Face Recognition (2005), BIOMETRICS: Personal Identification in Networked Society (1999), and Algorithms for Clustering Data (1988). He served as a member of the Defense Science Board and The National Academies committees on Whither Biometrics and Improvised Explosive Devices. Dr. Jain received the 1996 IEEE TNN Outstanding Paper Award and the Pattern Recognition Society best paper awards in 1987, 1991, and 2005. He is a fellow of the AAAS, ACM, IAPR, and SPIE. He has received Fulbright, Guggenheim, Alexander von Humboldt, IEEE Computer Society Technical Achievement, IEEE Wallace McDowell, ICDM Research Contributions, and IAPR King-Sun Fu awards. Stan Z. Li received the B.Eng. degree from Hunan University, Changsha, China, the M.Eng. degree from the National University of Defense Technology, China, and the Ph.D. degree from Surrey University, Surrey, U.K. He is currently a Professor and the Director of Center for Biometrics and Security Research (CBSR), CASIA. He worked at Microsoft Research Asia as a researcher from 2000 to 2004. Prior to that, he was an Associate Professor at Nanyang Technological University, Singapore. His research interest includes pattern recognition and machine learning, image and vision processing, face recognition, biometrics, and intelligent video surveillance. He has published over 200 papers in international journals and conferences, and authored and edited eight books. Dr. Li was an Associate Editor of the IEEE TPAMI and is acting as the Editor-in-Chief for the Encyclopedia of Biometrics. He served as a co-chair for the International Conference on Biometrics 2007 and 2009, and has been involved in organizing other international conferences and workshops in the fields of his research interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Comparison of detection rates (%) with both discrete and continuous metrics on the FDDB database [3]* .38 81.67 43.27 55.83 60.43 9 NPD</figDesc><table><row><cell cols="3">Discrete Metric</cell><cell cols="3">Continuous Metric</cell></row><row><cell>FP = 0</cell><cell>FP = 10</cell><cell>FP = 100</cell><cell>FP = 0</cell><cell>FP = 10</cell><cell>FP = 100</cell></row><row><cell cols="6">54.15 9 Boosted Examplar [43] 52.47 69.29 80.82 37.07 48.75 56.87 72.31 77.97 40.64 53.93 58.04 9 Yan-DPM [46] 51.61 75.98 81.36 43.70 63.48 67.70 9 HeadHunter [47] 34.94 72.58 83.41 28.71 58.94 67.28 Joint Cascade [48] 33.44 78.84 83.91 29.84 69.15 73.06 Zhu-Ramanan [45] 27.38 63.88 73.08 21.25 48.62 55.40 SURF-Multiview [56] 12.40 69.43 80.60 8.49 46.82 54.37 VJGPR [40] 4.58 15.76 51.00 2.95 10.20 33.16 Mikolajczyk et al. [57] [3] 3.25 10.23 33.28 2.10 6.61 21.67 Viola-Jones [1] [3] 1.39 10.02 32.64 0.90 6.48 21.26 9 XZJY [42] 0.31 7.91 67.51 0.19 4.99 43.40 9 Koestinger et al. [58] 0.19 21.47 57.03 0.14 15.38 40.55 Segui et al. [59] 0.00 15.08 67.94 0.00 9.78 43.76 PEP [44] n/a 8.43 73.35 n/a 5.38 47.30 Subburaman-Marcel [39] n/a 0.54 17.25 n/a 0.36 11.27</cell></row></table><note>9 ACF-Multiscale [49] 58.21 75* Red numbers represents the best results, while blue numbers are the second best results. Methods marked by 9 used the same AFLW [53] training data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Comparison of detector complexity.</figDesc><table><row><cell>Haar LBP POF 150 108 276 1,763 1,269 3,082 1,597 2,035 1,929 NPD Stump CART DQT DQT-Soft 1,597 176 140 72 1,018 #fea. evals. 33.9 30.4 44.3 #trees #features 36.5 34.4 23.5 18.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Speed (FPS) of frontal face detectors.</figDesc><table><row><cell>CPU Atom N450 @1.6GHz (1 core, 2 threads) 1920 × 1080 3.0 Resolution NPD OpenCV SURF [13]* 640 × 480 19.4 2.1 5.8 800 × 600 12.1 1.3 n/a 1280 × 720 6.8 0.7 n/a 0.3 n/a i5-2400 640 × 480 177.6 24.4 71.3 800 × 600 112.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Speed (FPS) of unconstrained face detectors.</figDesc><table><row><cell>NPD i5@3.1GHz X5650@2.66GHz Yan-DPM [46] JCascade [48] ACF [49] @2.93GHz i7@3.9GHz 4 6 n/a 4 1 4 1 12 1 1 8 Speed 29.28 70.06 CPU Cores Threads 5 25 34.97 15 42</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">. https://sites.google.com/site/leeplus/publications/ facedetectionusingsurfcascade</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">. We have tested four models of the Viola-Jones face detector provided in OpenCV 2.4, and found that the "haarcascade frontalface alt" model is the fastest, which was selected here for comparison.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A BOUNDEDNESS OF NPD</head><p>Lemma 1 (Boundedness): ∀x, y ≥ 0, the NPD feature f(x,y) is well bounded in <ref type="bibr">[-1,1]</ref>. In addition, f (x, y) = 1 if and only if x &gt; 0 and y = 0; and f (x, y) = −1 if and only if x = 0 and y &gt; 0.</p><p>Proof: From the definition of NPD we know that x ≥ 0, y ≥ 0, and f (0, 0) = 0 ∈ [−1, 1]. When either x or y is nonzero, for example, y ≥ 0 but x &gt; 0, Eq. (1) can be reformulated as</p><p>The inequality in Eq. (a) holds because y ≥ 0, and the last equality holds if and only if x &gt; 0 and y = 0. Similarly, when x ≥ 0 but y &gt; 0, Eq. (1) can be reformulated as</p><p>The inequality in Eq. (b) holds because x ≥ 0, and the last equality holds if and only if x = 0 and y &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B PROOF OF THEOREM 1</head><p>Denote</p><p>Equivalently,</p><p>Therefore, we have the following set of linear equations</p><p>is a sparse d × p matrix with each row containing at most two nonzero entries. Furthermore, from the formulation of F we know that each row of F contains at least one nonzero entry, because (f ij − 1) = (f ij + 1) always holds for all i and j. Without loss of generality, let's assume f 12 + 1 = 0. Then it follows that f 1j +1 = 0, ∀j. Because if ∃j such that f 1j +1 = 0, then from Lemma 1 we know that x 1 = 0. This will further lead to f 12 +1 = 0, which violates the assumption that f 12 +1 = 0. Therefore, the first p−1 rows in the matrix F are linearly independent of each other. We will further prove that rank(F) = p − 1. In fact, any row of the matrix F can be linearly expressed by the first p − 1 rows. To show this, let's denote the row containing f ij −1 and f ij +1 by r ij . We will show that</p><p>holds for all i &gt; 1 and j &gt; i. In fact, it is easy to verify that the above equation holds for all columns of r ij , r 1i , and r 1j after the first column. So, we only need to show that, for the first column, we have</p><p>which is equivalent to</p><p>This can be verified by substituting each feature with its definition in Eq. (1). Given that rank(F) = p − 1, we know that the nullspace of F contains only one nonzero vector, which is a solution to Eq. (e). Furthermore, from Lemma 1 we can infer that (f ij −1)(f ij +1) ≤ 0, hence Eq. (d) tells that x i x j ≥ 0, ∀i, j. Consequently, Eq. (e) always has a nonnegative solutionx, and all solutions to Eq. (e) must be cx, where c is a scale factor.</p><p>Given this proof, we make four observations below:</p><p>• For a solution, c can be any real value, but to satisfy the constraint that all pixel intensity values are nonnegative, c should be positive.</p><p>• The solution to Eq. (e) spans a one-dimensional subspace (the nullspace).</p><p>• A specific solution can be obtained by assigning x 1 = 1 and solving for the other variables from the first p − 1 rows of Eq. (e) in linear time.</p><p>• When the original image is x = 0, it can also be reconstructed by cx wherex i = 1, ∀i, and c = 0. However, in this case a solution with c &gt; 0 is not generally regarded as a scaled version of the original image x = 0.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Additive logistic regression: a statistical view of boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="374" />
			<date type="published" when="2000-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">FDDB: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An extended set of Haar-like features for rapid object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maydt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistical learning of multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Conference on Computer Vision</title>
		<meeting>the 7th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mitsubishi Electric Research Lab TR-2003-96</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face detection with the modified census transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Froba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the 6th IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face detection using improved LBP under bayesian framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Image and Graphics</title>
		<meeting>the 3rd International Conference on Image and Graphics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint Haar-like features for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Computer Vision</title>
		<meeting>the 10th IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1619" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection using spatial histogram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="327" to="341" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face detection based on multi-block LBP representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IAPR/IEEE International Conference on Biometrics</title>
		<meeting>the IAPR/IEEE International Conference on Biometrics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Locally assembled binary (LAB) feature with feature-centric cascade for fast and accurate face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face detection using SURF cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV BeFIT workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast rotation invariant multi-view face detection based on real AdaBoost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Floatboost learning and statistical face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1112" to="1123" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High-performance rotation invariant multiview face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="671" to="686" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A robust face detector under partial occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast object detection with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="402" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust face detection with multi-class boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modification of the adaboost-based detector for partially occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Components and their topology for robust face detection in the presence of partial occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Goldmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Monich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="559" to="569" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting faces in images: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="58" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tastsinn und gemeingefühl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Weber</surname></persName>
		</author>
		<editor>Handwörterbuch der Physiologie, R. Wagner</editor>
		<imprint>
			<date type="published" when="1846" />
			<publisher>Vieweg</publisher>
			<biblScope unit="page" from="481" to="588" />
			<pubPlace>Ed. Brunswick</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A survey of recent advances in face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno>MSR-TR-2010-66</idno>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Qualitative representations for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biologically Motivated Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward the fidelity of local ordinal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sadr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thoresz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient face orientation discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="589" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face recognition using ordinal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st IAPR International Conference on Biometrics</title>
		<meeting>the 1st IAPR International Conference on Biometrics<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Keypoint recognition using randomized trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1465" to="1479" />
			<date type="published" when="2006-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Yet even faster (YEF) real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ghorayeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Systems Technologies and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="102" to="112" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">2D face fittingassisted 3D face reconstruction for pose-robust face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soft Computing-A Fusion of Foundations, Methodologies and Applications</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="417" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic texton forests for image categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Empirical analysis of detection cascades of boosted classifiers for rapid object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuranov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pisarevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th DAGM Symposium on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the design of cascades of boosted ensembles for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mullin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<idno>GIT-GVU-05-28</idno>
	</analytic>
	<monogr>
		<title level="j">Georgia Institute of Technology</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rotation invariant neural network-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-aspect detection of articulated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MCBoost: Multiple classifier boosting for perceptual co-clustering of images and visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast bounding box estimation based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Subburaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Face Detection: Where we are and what next</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Online domain adaptation of a pre-trained cascade of classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Speeded-up robust features (SURF)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detecting and aligning faces by image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient boosted exemplar-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Probabilistic elastic part model for unsupervised face detector adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The fastest deformable part model for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Biometrics (IJCB)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">WLD: A robust local image descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1705" to="1720" />
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Preprocessing transformations and their effects on multispectral recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kriegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Malila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nalepka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Symposium on Remote Sensing of Environment</title>
		<meeting>the Sixth International Symposium on Remote Sensing of Environment</meeting>
		<imprint>
			<date type="published" when="1969" />
			<biblScope unit="page" from="97" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Robust object detection via soft cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">The MPLab GENKI Database, GENKI-SZSL Subset</title>
		<ptr target="http://mplab.ucsd.edu" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Whos in the picture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning SURF cascade for fast and accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Human detection based on a probabilistic assembly of robust part detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Robust face detection by simple means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM Computer Vision in Applications Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An integrated approach to contextual face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seguí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vitrià</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition Applications and Methods</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<ptr target="http://www.pittpatt.com" />
		<title level="m">PittPatt Software Developer Kit, Pittsburgh Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
