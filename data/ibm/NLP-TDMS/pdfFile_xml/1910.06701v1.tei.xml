<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NumNet: Machine Reading Comprehension with Numerical Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu</forename><surname>Ran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
							<email>yankailin@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pattern Recognition Center</orgName>
								<address>
									<addrLine>WeChat AI</addrLine>
									<region>Tencent Inc</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NumNet: Machine Reading Comprehension with Numerical Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Numerical reasoning, such as addition, subtraction, sorting and counting is a critical skill in human's reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems. To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage. Our system achieves an EM-score of 64.56% on the DROP dataset, outperforming all existing machine reading comprehension models by considering the numerical relations among numbers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine reading comprehension (MRC) aims to infer the answer to a question given the document. In recent years, researchers have proposed lots of MRC models <ref type="bibr" target="#b0">(Chen et al., 2016;</ref><ref type="bibr" target="#b3">Dhingra et al., 2017;</ref><ref type="bibr" target="#b1">Cui et al., 2017;</ref><ref type="bibr" target="#b19">Seo et al., 2017)</ref> and these models have achieved remarkable results in various public benchmarks such as SQuAD <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref> and RACE <ref type="bibr" target="#b14">(Lai et al., 2017)</ref>. The success of these models is due to two reasons: (1) Multi-layer architectures which allow these models to read the document and the question iteratively for reasoning; (2) Attention mechanisms which would enable these models to focus on the part related to the question in the document.</p><p>However, most of existing MRC models are still weak in numerical reasoning such as addition, subtraction, sorting and counting <ref type="bibr" target="#b4">(Dua et al., 2019)</ref>, which are naturally required when reading financial news, scientific articles, etc. <ref type="bibr" target="#b4">Dua et al. (2019)</ref> proposed a numerically-aware QANet * indicates equal contribution (NAQANet) model, which divides the answer generation for numerical MRC into three types: (1) extracting spans; (2) counting; (3) addition or subtraction over numbers. NAQANet makes a pioneering attempt to answer numerical questions but still does not explicitly consider numerical reasoning.</p><p>To tackle this problem, we introduce a novel model NumNet that integrates numerical reasoning into existing MRC models. A key problem to answer questions requiring numerical reasoning is how to perform numerical comparison in MRC systems, which is crucial for two common types of questions:</p><p>(1) Numerical Comparison: The answers of the questions can be directly obtained via performing numerical comparison, such as sorting and comparison, in the documents. For example, in <ref type="table" target="#tab_0">Table 1</ref>, for the first question, if the MRC system knows the fact that "49 &gt; 47 &gt; 36 &gt; 31 &gt; 22", it could easily extract that the second longest field goal is 47-yard.</p><p>(2) Numerical Condition: The answers of the questions cannot be directly obtained through simple numerical comparison in the documents, but often require numerical comparison for understanding the text. For example, for the second question in <ref type="table" target="#tab_0">Table 1</ref>, an MRC system needs to know which age group made up more than 7% of the population to count the group number.</p><p>Hence, our NumNet model considers numerical comparing information among numbers when answering numerical questions. As shown in <ref type="figure">Figure</ref> 1, NumNet first encodes both the question and passages through an encoding module consisting of convolution layers, self-attention layers and feed-forward layers as well as a passage-question attention layer. After that, we feed the question and passage representations into a numericallyaware graph neural network (NumGNN) to further arXiv:1910.06701v1 [cs.CL] 15 Oct 2019</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Passage Answer</head><p>What is the second longest field goal made?</p><p>... The Seahawks immediately trailed on a scoring rally by the Raiders with kicker Sebastian Janikowski nailing a 31-yard field goal ... Then in the third quarter Janikowski made a 36-yard field goal. Then he made a 22-yard field goal in the fourth quarter to put the Raiders up 16-0 ... The Seahawks would make their only score of the game with kicker Olindo Mare hitting a 47-yard field goal. However, they continued to trail as Janikowski made a 49-yard field goal, followed by RB Michael Bush making a 4-yard TD run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>47-yard</head><p>How many age groups made up more than 7% of the population?</p><p>Of Saratoga Countys population in 2010, 6.3% were between ages of 5 and 9 years, 6.7% between 10 and 14 years, 6.5% between 15 and 19 years, 5.5% between 20 and 24 years, 5.5% between 25 and 29 years, 5.8% between 30 and 34 years, 6.6% between 35 and 39 years, 7.9% between 40 and 44 years, 8.5% between 45 and 49 years, 8.0% between 50 and 54 years, 7.0% between 55 and 59 years, 6.4% between 60 and 64 years, and 13.7% of age 65 years and over ... integrate the comparison information among numbers into their representations. Finally, we utilize the numerically-aware representation of passages to infer the answer to the question. The experimental results on a public numerical MRC dataset DROP <ref type="bibr" target="#b4">(Dua et al., 2019)</ref> show that our NumNet model achieves significant and consistent improvement as compared to all baseline methods by explicitly performing numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Machine Reading Comprehension</head><p>Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail <ref type="bibr" target="#b5">(Hermann et al., 2015)</ref>, SQuAD <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref>, RACE <ref type="bibr" target="#b14">(Lai et al., 2017)</ref>, Trivi-aQA <ref type="bibr" target="#b8">(Joshi et al., 2017)</ref> and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader <ref type="bibr" target="#b9">(Kadlec et al., 2016)</ref>, BiDAF <ref type="bibr" target="#b19">(Seo et al., 2017)</ref>, Interactive AoA Reader <ref type="bibr" target="#b1">(Cui et al., 2017)</ref>, Gated Attention Reader <ref type="bibr" target="#b3">(Dhingra et al., 2017)</ref>, R-Net <ref type="bibr" target="#b22">(Wang et al., 2017a)</ref>, DCN <ref type="bibr" target="#b24">(Xiong et al., 2017)</ref>, QANet <ref type="bibr" target="#b25">(Yu et al., 2018)</ref>, and achieve promising results in most existing public MRC datasets.</p><p>Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works <ref type="bibr" target="#b0">(Chen et al., 2016;</ref><ref type="bibr" target="#b20">Sugawara et al., 2018;</ref><ref type="bibr" target="#b10">Kaushik and Lipton, 2018)</ref> classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC system. The most relevant work to ours is NAQANet <ref type="bibr" target="#b4">(Dua et al., 2019)</ref>, which adapts the output layer of QANet <ref type="bibr" target="#b25">(Yu et al., 2018)</ref> to support predicting answers based on counting and addition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Arithmetic Word Problem Solving</head><p>Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. <ref type="bibr" target="#b6">Hosseini et al. (2014)</ref> proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, <ref type="bibr" target="#b18">Roy and Roth (2015)</ref> proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. <ref type="bibr" target="#b12">Koncel-Kedziorski et al. (2015)</ref> further <ref type="figure">Figure 1</ref>: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from "6" to "5" denotes "6" is greater than "5". And the reasoning module leverages a numerically-aware graph neural network to perform numerical reasoning on the graph. As numerical comparison is modeled explicitly in our model, it is more effective for answering questions requiring numerical reasoning such as addition, counting, or sorting over numbers.</p><p>formalized the AWP problem as that of generating and scoring equation trees via integer linear programming. <ref type="bibr" target="#b23">Wang et al. (2017b)</ref> and <ref type="bibr" target="#b16">Ling et al. (2017)</ref> proposed sequence to sequence solvers for the AWP problems, which are capable of generating unseen expressions and do not rely on sophisticated manual features. <ref type="bibr" target="#b21">Wang et al. (2018)</ref> leveraged deep Q-network to solve the AWP problems, achieving a good balance between effectiveness and efficiency. However, all the existing AWP systems are only trained and validated on small benchmark datasets. <ref type="bibr" target="#b7">Huang et al. (2016)</ref> found that the performance of these AWP systems sharply degrades on larger datasets. Moreover, from the perspective of NLP, MRC problems are more challenging than AWP since the passages in MRC are mostly real-world texts which require more complex skills to be understood. Above all, it is nontrivial to adapt most existing AWP models to the MRC scenario. Therefore, we focus on enhancing MRC models with numerical reasoning abilities in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we will introduce the framework of our model NumNet and provide the details of the proposed numerically-aware graph neural network (NumGNN) for numerical reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><p>An overview of our model NumNet is shown in <ref type="figure">Figure 1</ref>. We compose our model with encoding module, reasoning module and prediction module. Our major contribution is the reasoning module, which leverages a NumGNN between the encoding module and prediction module to explicitly consider the numerical comparison information and perform numerical reasoning. As NAQANet has been shown effective for handling numerical MRC problem <ref type="bibr" target="#b4">(Dua et al., 2019)</ref>, we leverage it as our base model and mainly focus on the design and integration of the NumGNN in this work.</p><p>Encoding Module Without loss of generality, we use the encoding components of QANet and NAQANet to encode the question and passage into vector-space representations. Formally, the question Q and passage P are first encoded as:</p><formula xml:id="formula_0">Q = QANet-Emb-Enc(Q), (1) P = QANet-Emb-Enc(P ),<label>(2)</label></formula><p>and then the passage-aware question representation and the question-aware passage representation are computed as:</p><formula xml:id="formula_1">Q = QANet-Att(P , Q),<label>(3)</label></formula><formula xml:id="formula_2">P = QANet-Att(Q, P ),<label>(4)</label></formula><p>where QANet-Emb-Enc(·) and QANet-Att(·) denote the "stacked embedding encoder layer" and "context-query attention layer" of QANet respectively. The former consists of convolution, selfattention and feed-forward layers. The latter is a passage-question attention layer.Q andP are used by the following components.</p><p>Reasoning Module First we build a heterogeneous directed graph G = (V ; E), whose nodes (V ) are corresponding to the numbers in the question and passage, and edges (E) are used to encode numerical relationships among the numbers. The details will be explained in Sec. 3.2. Then we perform reasoning on the graph based on a graph neural network, which can be formally denoted as:</p><formula xml:id="formula_3">M Q = QANet-Mod-Enc(W MQ ), (5) M P = QANet-Mod-Enc(W MP ), (6) U = Reasoning(G; M Q , M P ), (7)</formula><p>where W M is a shared weight matrix, U is the representations of the nodes corresponding to the numbers, QANet-Mod-Enc(·) is the "model encoder layer" defined in QANet which is similar to QANet-Emb-Enc(·), and the definition of Reasoning(·) will be given in Sec. 3.3.</p><p>Finally, as U only contains the representations of numbers, to tackle span-style answers containing non-numerical words, we concatenate U with M P to produce numerically-aware passage representation M 0 . Formally,</p><formula xml:id="formula_4">M num [i] = U [I(i)] if w p i is a number 0 , M 0 = W 0 [M P ; M num ] + b 0 , (8) M 0 = QANet-Mod-Enc(M 0 ),<label>(9)</label></formula><p>where [·; ·] denotes matrix concatenation, W [k] denotes the k-th column of a matrix W , 0 is a zero vector, I(i) denotes the node index corresponding to the passage word w p i which is a number, W 0 is a weight matrix, and b 0 is a bias vector.</p><p>Prediction Module Following NAQANet <ref type="bibr" target="#b4">(Dua et al., 2019)</ref>, we divide the answers into four types and use a unique output layer to calculate the conditional answer probability Pr(answer|type) for each type :</p><p>• Passage span: The answer is a span of the passage, and the answer probability is defined as the product of the probabilities of the start and end positions.</p><p>• Question span: The answer is a span of the question, and the answer probability is also defined as the product of the probabilities of the start and end positions.</p><p>• Count: The answer is obtained by counting, and it is treated as a multi-class classification problem over ten numbers (0-9), which covers most of the Count type answers in the DROP dataset.</p><p>• Arithmetic expression: The answer is the result of an arithmetic expression. The expression is obtained in three steps: (1) extract all numbers from the passage;</p><p>(2) assign a sign (plus, minus or zero) for each number; (3) sum the signed numbers 1 .</p><p>Meanwhile, an extra output layer is also used to predict the probability Pr(type) of each answer type.</p><p>At training time, the final answer probability is defined as the joint probability over all feasible answer types, i.e., type Pr(type) Pr(answer|type). Here, the answer type annotation is not required and the probability Pr(type) is learnt by the model. At test time, the model first selects the most probable answer type greedily and then predicts the best answer accordingly.</p><p>Without loss of generality, we leverage the definition of the five output layers in <ref type="bibr" target="#b4">(Dua et al., 2019)</ref>, with M 0 and Q as inputs. Please refer to the paper for more details due to space limitation.</p><p>Comparison with NAQANet The major difference between our model and NAQANet is that NAQANet does not have the reasoning module, i.e., M 0 is simply set as M P . As a result, numbers are treated as common words in NAQANet except in the prediction module, thus NAQANet may struggle to learn the numerical relationships between numbers, and potentially cannot well generalize to unseen numbers. However, as discussed in Sec. 1, the numerical comparison is essential for answering questions requiring numerical reasoning. In our model, the numerical relationships are explicitly represented with the topology of the graph and a NumGNN is used to perform numerical reasoning. Therefore, our Num-Net model can handle questions requiring numerical reasoning more effectively, which is verified by the experiments in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Numerically-aware Graph Construction</head><p>We regard all numbers from the question and passage as nodes in the graph for reasoning 2 . The set of nodes corresponding to the numbers occurring in question and passage are denoted as V Q and V P respectively. And we denote all the nodes as V = V Q ∪ V P , and the number corresponding to a node v ∈ V as n(v).</p><p>Two sets of edges are considered in this work:</p><formula xml:id="formula_5">• Greater Relation Edge ( − → E ): For two nodes v i , v j ∈ V , a directed edge − → e ij = (v i , v j )</formula><p>pointing from v i to v j will be added to the graph if n(v i ) &gt; n(v j ), which is denoted as solid arrow in <ref type="figure">Figure 1</ref>.</p><p>• Lower or Equal Relation Edge (</p><formula xml:id="formula_6">← − E ): For two nodes v i , v j ∈ V , a directed edge ← − e ij = (v j , v i ) will be added to the graph if n(v i ) ≤ n(v j )</formula><p>, which is denoted as dashed arrow in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretically,</head><p>− → E and ← − E are complement to each other . However, as a number may occur several times and represent different facts in a document, we add a distinct node for each occurrence in the graph to prevent potential ambiguity. Therefore, it is more reasonable to use both − → E and ← − E in order to encode the equal information among nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Numerical Reasoning</head><p>As we built the graph G = (V , E), we leverage NumGNN to perform reasoning, which is corresponding to the function Reasoning(·) in Eq. 7. The reasoning process is as follows:</p><p>Initialization For each node v P i ∈ V P , its representation is initialized as the corresponding column vector of M P . Formally, the initial representation is v P i = M P [I P (v P i )], where I P (v P i ) denotes the word index corresponding to v P i . Similarly, the initial representation v Q j for a node v Q j ∈ V Q is set as the corresponding column vector of M Q . We denote all the initial node representations as v 0 = {v P i } ∪ {v Q j }. One-step Reasoning Given the graph G and the node representations v, we use a GNN to perform reasoning in three steps:</p><p>(1) Node Relatedness Measure: As only a few numbers are relevant for answering a question generally, we compute a weight for each node to by-pass irrelevant numbers in reasoning. Formally, the weight for node v i is computed as:</p><formula xml:id="formula_7">α i = sigmoid(W v v[i] + b v ),<label>(10)</label></formula><p>we also add nodes for them in the graph.</p><p>where W v is a weight matrix, and b v is a bias.</p><p>(2) Message Propagation: As the role a number plays in reasoning is not only decided by itself, but also related to the context, we propagate messages from each node to its neighbors to help to perform reasoning. As numbers in question and passage may play different roles in reasoning and edges corresponding to different numerical relations should be distinguished, we use relationspecific transform matrices in the message propagation. Formally, we define the following propagation function for calculating the forward-pass update of a node:</p><formula xml:id="formula_8">v i = 1 |N i |   j∈N i α j W r ji v[j]   ,<label>(11)</label></formula><p>where v i is the message representation of node v i , r ji is the relation assigned to edge e ji , W r ji are relation-specific transform matrices, and</p><formula xml:id="formula_9">N i = {j|(v j , v i ) ∈ E} is the neighbors of node v i .</formula><p>For each edge e ji , r ji is determined by the following two attributes:</p><p>• Number relation: &gt; or ≤;</p><p>• Node types: the two nodes of the edge corresponding to two numbers that: (1) both from the question (q-q); (2) both from the passage (p-p); (3) from the question and the passage respectively (q-p); (4) from the passage and the question respectively (p-q).</p><p>Formally, r ij ∈ {&gt;, ≤} × {q-q, p-p, q-p, p-q}.</p><p>(3) Node Representation Update: As the message representation obtained in the previous step only contains information from the neighbors, it needs to be fused with the node representation to combine with the information carried by the node itself, which is performed as:</p><formula xml:id="formula_10">v i = ReLU(W f v i + v i + b f ),<label>(12)</label></formula><p>where W f is a weight matrix, and b f is a bias vector.</p><p>We denote the entire one-step reasoning process (Eq. 10-12) as a single function v = Reasoning-Step(G, v).</p><p>As the graph G constructed in Sec. 3.2 has encoded the numerical relations via its topology, the reasoning process is numerically-aware.</p><p>Multi-step Reasoning By single-step reasoning, we can only infer relations between adjacent nodes. However, relations between multiple nodes may be required for certain tasks, e.g., sorting. Therefore, it is essential to perform multi-step reasoning, which can be done as follows</p><formula xml:id="formula_12">v t = Reasoning-Step(v t−1 ),<label>(14)</label></formula><p>where t ≥ 1. Suppose we perform K steps of reasoning, v K is used as U in Eq. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>We evaluate our proposed model on DROP dataset <ref type="bibr" target="#b4">(Dua et al., 2019)</ref>, which is a public numerical MRC dataset. The DROP dataset is constructed by crowd-sourcing, which asks the annotators to generate question-answer pairs according to the given Wikipedia passages, which require numerical reasoning such as addition, counting, or sorting over numbers in the passages. There are 77, 409 training samples, 9, 536 development samples and 9, 622 testing samples in the dataset. In this paper, we adopt two metrics including Exact Match (EM) and numerically-focused F1 scores to evaluate our model following <ref type="bibr" target="#b4">Dua et al. (2019)</ref>. The numerically-focused F1 is set to be 0 when the predicted answer is mismatched for those questions with the numeric golden answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>For comparison, we select several public models as baselines including semantic parsing models:</p><p>• Syn Dep <ref type="bibr" target="#b4">(Dua et al., 2019)</ref>, the neural semantic parsing model (KDG) <ref type="bibr" target="#b13">(Krishnamurthy et al., 2017)</ref> with Stanford dependencies based sentence representations;</p><p>• OpenIE <ref type="bibr" target="#b4">(Dua et al., 2019)</ref>, KDG with open information extraction based sentence representations;</p><p>• SRL <ref type="bibr" target="#b4">(Dua et al., 2019)</ref>, KDG with semantic role labeling based sentence representations; and traditional MRC models:</p><p>• BiDAF <ref type="bibr" target="#b19">(Seo et al., 2017)</ref>, an MRC model which utilizes a bi-directional attention flow network to encode the question and passage;</p><p>• QANet <ref type="bibr" target="#b25">(Yu et al., 2018)</ref>, which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage;</p><p>• BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>, a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently; and numerical MRC models:</p><p>• NAQANet <ref type="figure" target="#fig_0">(Dua et al., 2019)</ref>, a numerical version of QANet model.</p><p>• NAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. "2.5"), richer arithmetic expression, data augmentation, etc. The enhancements are also used in our NumNet model and the details are given in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Settings</head><p>In this paper, we tune our model on the development set and use a grid search to determine the optimal parameters. The dimensions of all the representations (e.g., Q, P , M Q , M P , U , M 0 , M 0 and v) are set to 128. If not specified, the reasoning step K is set to 3. Since other parameters have little effect on the results, we simply follow the settings used in <ref type="bibr" target="#b4">(Dua et al., 2019)</ref>. We use the Adam optimizer (Kingma and Ba, 2015) with β 1 = 0.8, β 2 = 0.999, = 10 −7 to minimize the objective function. The learning rate is 5 × 10 −4 , L2 weight decay λ is 10 −7 and the maximum norm value of gradient clipping is 5. We also apply exponential moving average with a decay rate 0.9999 on all trainable variables. The model is trained with a batch size of 16 for 40 epochs. Passages and questions are trimmed to 400 and 50 tokens respectively during training, and trimmed to 1, 000 and 100 tokens respectively during prediction 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Overall Results</head><p>The performance of our NumNet model and other baselines on DROP dataset are shown in <ref type="table">Table 2</ref>. From the results, we can observe that:</p><p>(1) Our NumNet model achieves better results on both the development and testing sets on DROP dataset as compared to semantic parsing-based models, traditional MRC models and even numerical MRC models NAQANet and NAQANet+. The reason is that our NumNet model can make full use of the numerical comparison information over  <ref type="table">Table 2</ref>: Overall results on the development and test set. The evaluation metrics are calculated as the maximum over a golden answer set. All the results except "NAQANet+" and "NumNet" are obtained from <ref type="bibr" target="#b4">(Dua et al., 2019)</ref>.</p><p>numbers in both question and passage via the proposed NumGNN module.</p><p>(2) Our implemented NAQANet+ has a much better performance compared to the original version of NAQANet. It verifies the effectiveness of our proposed enhancements for baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of GNN Structure</head><p>In this part, we investigate the effect of different GNN structures on the DROP development set. The results are shown in <ref type="table">Table 3</ref>. The "Comparison", "Number" and "ALL" are corresponding to the comparing question subset 4 , the numbertype answer subset, and the entire development set, respectively 5 . If we replace the proposed numerically-aware graph (Sec. 3.2) with a fully connected graph, our model fallbacks to a traditional GNN, denoted as "GNN" in the table. Moreover, "-question num" denotes the numbers in the question is not included in the graph, and "-≤ type edge" and "-&gt; type edge" denote edges of ≤ and &gt; types are not adopted respectively.  <ref type="table">Table 3</ref>: Performance with different GNN structure. "Comparison", "Number" and "ALL" denote the comparing question subset, the number-type answer subset, and the entire development set, respectively.  <ref type="table">Table 3</ref>, our proposed NumGNN leads to statistically significant improvements compared to traditional GNN on both EM and F1 scores especially for comparing questions. It indicates that considering the comparing information over numbers could effectively help the numerical reasoning for comparing questions. Moreover, we find that the numbers in the question are often related to the numerical reasoning for answering the question, thus considering numbers in questions in NumGNN achieves better performance. And the results also justify that encoding "greater relation" and "lower or equal relation" simultaneously in the graph also benefits our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effect of GNN Layer Number</head><p>The number of NumGNN layers represents the numerical reasoning ability of our models. A Klayer version has the ability for K-step numerical inference. In this part, we additionally perform experiments to understand the values of the numbers of NumGNN layers. From <ref type="figure" target="#fig_0">Figure 2</ref>, we could observe that:</p><p>(1) The 2-layer version of NumNet achieves the best performance for the comparing questions. From careful analysis, we find that most compar-  Of Saratoga Countys population in 2010, 6.3% were between ages of 5 and 9 years, 6.7% between 10 and 14 years, 6.5% between 15 and 19 years, ... , 7.9% between 40 and 44 years, 8.5% between 45 and 49 years, 8.0% between 50 and 54 years, 7.0% between 55 and 59 years, 6.4% between 60 and 64 years, and 13.7% of age 65 years and over ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="40">and older</head><p>19 and younger ing questions only require at most 2-step reasoning (e.g., "Who was the second oldest player in the MLB, Clemens or Franco?"), and therefore the 3-layer version of NumNet is more complex but brings no gains for these questions.</p><p>(2) The performance of our NumNet model on the overall development set is improved consistently as the number of GNN layers increases. The reason is that some of the numerical questions require reasoning over many numbers in the passage, which could benefit from the multi-step reasoning ability of multi-layer GNN. However, further investigation shows that the performance gain is not stable when K ≥ 4. We believe it is due to the intrinsic over smoothing problem of GNNs <ref type="bibr" target="#b15">(Li et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Case Study</head><p>We further give some examples to show why incorporating comparing information over numbers in the passage could help numerical reasoning in MRC in <ref type="table" target="#tab_4">Table 4</ref>. For the first case, we observe that NAQANet+ gives a wrong prediction, and we find that NAQANet+ will give the same prediction for the question "Which age group is smaller: under the age of 18 or 18 and 24?". The reason is that NAQANet+ cannot distinguish which one is larger for 10.1% and 56.2%. For the second case, NAQANet+ cannot recognize the second longest field goal is 22-yard and also gives a wrong prediction. For these two cases, our NumNet model could give the correct answer through the numeric reasoning, which indicates the effectiveness of our NumNet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Error Analysis</head><p>To investigate how well our NumNet model handles sorting/comparison questions and better understand the remaining challenges, we perform an error analysis on a random sample of NumNet predictions. We find that:</p><p>(1) Our NumNet model can answer about 76% of sorting/comparison questions correctly, which indicates that our NumNet model has achieved numerical reasoning ability to some extend.</p><p>(2) Among the incorrectly answered sorting/comparison questions, the most ones (26%) are those whose golden answers are multiple nonadjacent spans (row 1 in <ref type="table" target="#tab_5">Table 5</ref>), and the second most ones (19%) are those involving comparison with an intermediate number that does not literally occur in the document/question but has to be de-rived from counting or arithmetic operation (row 1 in <ref type="table" target="#tab_5">Table 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Discussion</head><p>By combining the numerically-aware graph and the NumGNN together, our NumNet model achieves the numerical reasoning ability. On one hand, the numerically-aware graph encodes numbers as nodes and relationships between them as the edges, which is required for numerical comparison. On the other hand, through one-step reasoning, our NumGNN could perform comparison and identify the numerical condition. After multiple-step reasoning, our NumGNN could further perform sorting.</p><p>However, since the numerically-aware graph is pre-defined, our NumNet is not applicable to the case where an intermediate number has to be derived (e.g., from arithmetic operation) in the reasoning process, which is a major limitation of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>Numerical reasoning skills such as addition, subtraction, sorting and counting are naturally required by machine reading comprehension (MRC) problems in practice. Nevertheless, these skills are not taken into account explicitly for most existing MRC models. In this work, we propose a numerical MRC model named NumNet which performs explicit numerical reasoning while reading the passages. To be specific, NumNet encodes the numerical relations among numbers in the question and passage into a graph as its topology, and leverages a numerically-aware graph neural network to perform numerical reasoning on the graph. Our NumNet model outperforms strong baselines with a large margin on the DROP dataset.</p><p>In the future, we will explore the following directions: (1)As we use a pre-defined reasoning graph in our model, it is incapable of handling reasoning process which involves intermediate numbers that not presented in the graph. How to incorporate dynamic graph into our model is an interesting problem.</p><p>(2) Compared with methods proposed for arithmetic word problems (AWPs), our model has better natural language understanding ability. However, the methods for AWPs can handle much richer arithmetic expressions. Therefore, how to combine both of their abilities to develop a more powerful numerical MRC model is an interesting future direction. (3) Symbolic reasoning plays a crucial role in human reading comprehension. Our work integrates numerical reasoning, which is a special case of symbolic reasoning, into traditional MRC systems. How to incorporate more sophisticated symbolic reasoning abilities into MRC systems is also a valuable future direction.  (1) The uses of real number and richer arithmetic expression are crucial for answering numerical questions: both EM and F1 drop drastically by up to 15 − 21 points if they are removed.</p><p>(2) The passage-preferred strategy and data augmentation are also necessary components that contribute significant improvements for those comparing questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Effect of GNN layer numbers (# L).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example questions from the DROP dataset which require numerical comparison. We highlight the relevant parts in the passage to infer the answer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>75.91 67.77 67.78 61.90 65.16 NumGNN 74.53 80.36 69.74 69.75 64.54 68.02 -question num 74.84 80.24 68.42 68.43 63.78 67.17 -≤ type edge 74.89 80.51 68.48 68.50 63.66 67.06 -&gt; type edge 74.86 80.19 68.77 68.78 63.64 66.96</figDesc><table><row><cell>Method</cell><cell cols="2">Comparison</cell><cell cols="2">Number</cell><cell cols="2">ALL</cell></row><row><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>GNN</cell><cell>69.86</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The median age in the city was 22.1 years. 10.1% of residents were under the age of 18; 56.2% were between the ages of 18 and 24; 16.1% were from 25 to 44; 10.5% were from 45 to 64; and 7% were 65 years of age or older. The gender makeup of the city was 64.3% male and 35.7% female. The Vikings would draw first blood with a 26-yard field goal by kicker Ryan Longwell. In the second quarter, Carolina got a field goal with opposing kicker John Kasay. The Vikings would respond with another Longwell field goal (a 22-yard FG) ... In OT, Longwell booted the game-winning 19-yard field goal to give Minnesota the win. It was the first time in Vikings history that a coach ...</figDesc><table><row><cell>Question &amp; Answer</cell><cell>Passage</cell><cell cols="2">NAQANet+ NumNet</cell></row><row><cell>Q: Which age group is</cell><cell></cell><cell>under the</cell><cell>18 and 24</cell></row><row><cell>larger: under the age</cell><cell></cell><cell>age of 18</cell></row><row><cell>of 18 or 18 and 24?</cell><cell></cell><cell></cell></row><row><cell>A: 18 and 24</cell><cell></cell><cell></cell></row><row><cell>Q: How many more</cell><cell cols="2">... 26-19 = 7</cell><cell>26-22 = 4</cell></row><row><cell>yards was Longwell's</cell><cell></cell><cell></cell></row><row><cell>longest field goal over</cell><cell></cell><cell></cell></row><row><cell>his second longest one?</cell><cell></cell><cell></cell></row><row><cell>A: 26-22=4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Cases from the DROP dataset. We demonstrate the predictions of NAQANet+ and our NumNet model. Note that the two models only output the arithmetic expressions but we also provide their results for clarity.</figDesc><table><row><cell>Question</cell><cell>Passage</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Typical error examples. Row 1: the answer is multiple nonadjacent spans; Row 2: Intermediate numbers are involved in reasoning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>75.62 66.92 66.94 61.11 64.54 -real number 66.87 73.25 45.82 45.85 47.82 51.22 -richer arithmetic expression 68.62 74.55 52.48 52.51 52.02 55.32 -passage-preferred 64.06 72.34 66.46 66.47 59.64 63.34 -data augmentation 65.28 71.81 67.05 67.07 61.21 64.60</figDesc><table><row><cell>Method</cell><cell cols="2">Comparison</cell><cell cols="2">Number</cell><cell>ALL</cell></row><row><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>NAQANet+</cell><cell>69.11</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Baseline enhancements ablation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As few samples require multiplication/division expression in the DROP dataset, we simply adapt the module proposed<ref type="bibr" target="#b4">(Dua et al., 2019)</ref> and leave multiplication/division expression handling as future work.2 As a number in the question may serve as a critical comparison condition (refer to the second example inTable 1),</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Trimming passages/questions introduces little impact because it only affects about 8% of the samples.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We find that many comparing questions in the DROP dataset are biased, of which the answers are the former candidates in the questions. Hence, we employ crowdsourced workers to identify and rewrite all comparing questions to construct an enhanced development set. Specially, for those comparing questions containing answer candidates, we also ask the crowdsourced workers to swap the candidates manually to enlarge the dataset. 5 Note that the "ALL" result is not the average of "Comparison" and "Number". It is the performance on the entire development set which also includes questions of selection type, coreference resolution type, etc.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank all anonymous reviewers for their insightful comments, and thank Yan Zhang for her help on improving the presentation of <ref type="figure">Figure 1.</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Baseline Enhancements</head><p>The major enhancements leveraged by our implemented NAQANet+ model include:</p><p>(1) "real number": Unlike NAQANet only considers integer numbers, we also consider real numbers.</p><p>(2) "richer arithmetic expression": We conceptually append an extra number "100" to the passage to support arithmetic expressions like "100-25", which is required for answering questions such as "How many percent were not American?".</p><p>(3) "passage-preferred": If an answer is both a span of the question and the passage, we only propagate gradients through the output layer for processing "Passage span" type answers.</p><p>(4) "data augmentation": The original questions in the DROP dataset are generated by crowdsourced workers. For the comparing questions which contain answer candidates, we observe that the workers frequently only change the incorrect answer candidate to generate a new question. For example, "How many from the census is bigger: Germans or English?" whose golden answer is "Germans" is modified to "How many from the census is bigger: Germans or Irish?". This may introduce undesired inductive bias to the model. Therefore, we propose to augment the training dataset with new questions automatically generated by swapping the candidate answers, e.g., "How many from the census is bigger: English or Germans?" is added to the training dataset.</p><p>We further conduct ablation studies on the enhancements. And the validation scores on the development set are shown in <ref type="table">Table 6</ref>. As can be seen from <ref type="table">Table 6:</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A thorough examination of the CNN/Daily Mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1223</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2358" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1055</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gatedattention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1168</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1832" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1246</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to solve arithmetic word problems with verb categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1058</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="523" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How well do computers solve math word problems? Large-scale dataset construction and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1084</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="887" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bajgar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1086</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="908" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How much reading does reading comprehension require? A critical investigation of popular benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5010" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2015 : International Conference on Learning Representations</title>
		<meeting>ICLR 2015 : International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parsing algebraic word problems into equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siena</forename><surname>Dumas Ang</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00160</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1516" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Program induction by rationale generation: Learning to solve and explain algebraic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04146</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Solving general arithmetic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1743" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2017: the 5th International Conference on Learning Representations</title>
		<meeting>ICLR 2017: the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What makes reading comprehension questions easier?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saku</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4208" to="4219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Math-DQN: Solving arithmetic word problems via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep neural solver for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1088</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2017: the 5th International Conference on Learning Representations</title>
		<meeting>ICLR 2017: the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">QANet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2018: the 6th International Conference on Learning Representations</title>
		<meeting>ICLR 2018: the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
