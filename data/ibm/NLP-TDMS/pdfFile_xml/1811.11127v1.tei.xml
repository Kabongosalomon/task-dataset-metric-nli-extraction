<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unprocessing Images for Learned Raw Denoising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unprocessing Images for Learned Raw Denoising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning techniques work best when the data used for training resembles the data used for evaluation. This holds true for learned single-image denoising algorithms, which are applied to real raw camera sensor readings but, due to practical constraints, are often trained on synthetic image data. Though it is understood that generalizing from synthetic to real images requires careful consideration of the noise properties of camera sensors, the other aspects of an image processing pipeline (such as gain, color correction, and tone mapping) are often overlooked, despite their significant effect on how raw measurements are transformed into finished images. To address this, we present a technique to "unprocess" images by inverting each step of an image processing pipeline, thereby allowing us to synthesize realistic raw sensor measurements from commonly available Internet photos. We additionally model the relevant components of an image processing pipeline when evaluating our loss function, which allows training to be aware of all relevant photometric processing that will occur after denoising. By unprocessing and processing training data and model outputs in this way, we are able to train a simple convolutional neural network that has 14%-38% lower error rates and is 9×-18× faster than the previous state of the art on the Darmstadt Noise Dataset <ref type="bibr" target="#b29">[30]</ref>, and generalizes to sensors outside of that dataset as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Traditional single-image denoising algorithms often analytically model properties of images and the noise they are designed to remove. In contrast, modern denoising methods often employ neural networks to learn a mapping from noisy images to noise-free images. Deep learning is capable of representing complex properties of images and noise, but training these models requires large paired datasets. As a result, most learning-based denoising techniques rely on synthetic training data. Despite significant work on designing neural networks for denoising, recent benchmarks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>   <ref type="figure">Figure 1</ref>. An image from the Darmstadt Noise Dataset <ref type="bibr" target="#b29">[30]</ref>, where we present (a) the noisy input image, (b) the ground truth noisefree image, (c) the output of the previous state-of-the-art algorithm, and (d) the output of our model. All four images were converted from raw Bayer space to sRGB for visualization. Alongside each result are three cropped sub-images, rendered with nearestneighbor interpolation. See the supplement for additional results.</p><p>reveal that deep learning models are often outperformed by traditional, hand-engineered algorithms when evaluated on real noisy raw images.</p><p>We propose that this discrepancy is in part due to unrealistic synthetic training data. Many classic algorithms generalize poorly to real data due to assumptions that noise is additive, white, and Gaussian <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38]</ref>. Recent work has identified this inaccuracy and shifted to more sophisticated noise models that better match the physics of image formation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>. However, these techniques do not consider the many steps of a typical image processing pipeline.</p><p>One approach to ameliorate the mismatch between synthetic training data and real raw images is to capture noisy and noise-free image pairs using the same camera being targeted by the denoising algorithm <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37]</ref>. However, capturing noisy and noise-free image pairs is difficult, requiring long exposures or large bursts of images, and postprocessing to combat camera motion and lighting changes. Acquiring these image pairs is expensive and time consuming, a problem that is exacerbated by the large amounts of training data required to prevent over-fitting when training neural networks. Furthermore, because different camera sensors exhibit different noise characteristics, adapting a learned denoising algorithm to a new camera sensor may require capturing a new dataset.</p><p>When properly modeled, synthetic data is simple and effective. The physics of digital sensors and the steps of an imaging pipeline are well-understood and can be leveraged to generate training data from almost any image using only basic information about the target camera sensor. We present a systematic approach for modeling key components of image processing pipelines, "unprocessing" generic Internet images to produce realistic raw data, and integrating conventional image processing operations into the training of a neural network. When evaluated on real noisy raw images in the Darmstadt Noise Dataset <ref type="bibr" target="#b29">[30]</ref>, our model has 14%-38% lower error rates and is 9×-18× faster than the previous state of the art. A visualization of our model's output can be seen in <ref type="figure">Figure 1</ref>. Our unprocessing and processing approach also generalizes images captured from devices which were not explicitly modeled when generating our synthetic training data. This paper proceeds as follows: In Section 2 we review related work. In Section 3 we detail the steps of a raw image processing pipeline and define the inverse of each step. In Section 4 we present procedures for unprocessing generic Internet images into synthetic raw data, modifying training loss to account for raw processing, and training our simple and effective denoising neural network model. In Section 5 we demonstrate our model's improved performance on the Darmstadt Noise Dataset <ref type="bibr" target="#b29">[30]</ref> and provide an ablation study isolating the relative importance of each aspect of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single image denoising has been the focus of a significant body of research in computer vision and image processing. Classic techniques such as anisotropic diffusion <ref type="bibr" target="#b28">[29]</ref>, total variation denoising <ref type="bibr" target="#b33">[34]</ref>, and wavelet coring <ref type="bibr" target="#b37">[38]</ref> use hand-engineered algorithms to recover a clean signal from noisy input, under the assumption that both signal and noise exhibit particular statistical regularities. Though simple and effective, these parametric models are limited in their capacity and expressiveness, which led to increased interest in nonparametric, self-similarity-driven techniques such as BM3D <ref type="bibr" target="#b8">[9]</ref> and non-local means <ref type="bibr" target="#b4">[5]</ref>. The move from simple, analytical techniques towards datadriven approaches continued in the form of dictionarylearning and basis-pursuit algorithms such as KSVD <ref type="bibr" target="#b1">[2]</ref> and Fields-of-Experts <ref type="bibr" target="#b32">[33]</ref>, which operate by finding image representations where sparsity holds or statistical regularities are well-modeled. In the modern era, most single-image denoising algorithms are entirely data-driven, consisting of deep neural networks trained to regress from noisy images to denoised images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Most classic denoising work was done under the assumption that image noise is additive, white, and Gaussian. Though convenient and simple, this model is not realistic, as the stochastic process of photons arriving at a sensor is better modeled as "shot" and "read" noise <ref type="bibr" target="#b18">[19]</ref>. The overall noise can more accurately be modeled as containing both Gaussian and Poissonian signal-dependent components <ref type="bibr" target="#b13">[14]</ref> or as being sampled from a heteroscedastic Gaussian where variance is a function of intensity <ref type="bibr" target="#b19">[20]</ref>. An alternative to analytically modeling image noise is to use examples of real noisy and noise-free images. This can be done by capturing datasets consisting of pairs of real photos, where one image is a short exposure and therefore noisy, and the other image is a long exposure and therefore largely noise-free <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>. These datasets enabled the observation that recent learned techniques trained using synthetic data were outperformed by older models, such as BM3D <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>. As a result, recent work has demonstrated progress by collecting this real, paired data not just for evaluation, but for training models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37]</ref>. These approaches show great promise, but applying such a technique to a particular camera requires the laborious collection of large amounts of perfectly-aligned training data for that camera, significantly increasing the burden on the practitioner compared to the older techniques that required only synthetic training data or calibrated parameters. Additionally, it is not clear how this dataset acquisition procedure could be used to capture subjects where small motions are pervasive, such as water, clouds, foliage, or living creatures. Recent work suggests that multiple noisy images of the same scene can be used as training data instead of paired noisy and noise-free images <ref type="bibr" target="#b23">[24]</ref>, but this does not substantially mitigate the limi-  <ref type="figure">Figure 2</ref>. A visualization of our data pipeline and network training procedure. sRGB images from the MIR Flickr dataset <ref type="bibr" target="#b25">[26]</ref> are unprocessed, and realistic shot and read noise is added to synthesize noisy raw input images. Noisy images are fed through our denoising neural network, and the outputs of that network and the noise-free raw images then undergo raw processing before L1 loss is computed. See Sections 3 and 4 for details.</p><p>tations or the labor requirements of these large datasets of real photographs. Though it is generally understood that correctly modeling noise during image formation is critical for learning an effective denoising algorithm <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>, a less well-explored issue is the effect of the image processing pipeline used to turn raw sensor readings into a finished image. Modern image processing pipelines (well described in <ref type="bibr" target="#b20">[21]</ref>) consist of several steps which transform image intensities, therefore effecting both how input noise is scaled or modified and how the final rendered image appears as a function of the raw sensor measurements. In this work we model and invert these same steps when synthesizing training data for our model, and demonstrate that doing so significantly improves denoising performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Raw Image Pipeline</head><p>Modern digital cameras attempt to render a pleasant and accurate image of the world, similar to that perceived by the human eye. However, the raw sensor data from a camera does not yet resemble a photograph, and many processing stages are required to transform its noisy linear intensities into their final form. In this section, we describe a conventional image processing pipeline, proceeding from sensor measurement to a final image. To enable the generation of realistic synthetic raw data, we also describe how each step in our pipeline can be inverted. Through this procedure we are able to turn generic Internet images into training pairs that well-approximate the Darmstadt Noise Dataset <ref type="bibr" target="#b29">[30]</ref>, and generalize well to other raw images. See <ref type="figure">Figure 2</ref> for an overview of our unprocessing steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Shot and Read Noise</head><p>Though the noise in a processed image may have very complex characteristics due to nonlinearities and correlation across pixel values, the noise in raw sensor data is well understood. Sensor noise primarily comes from two sources: photon arrival statistics ("shot" noise) and imprecision in the readout circuitry ("read" noise) <ref type="bibr" target="#b18">[19]</ref>. Shot noise is a Poisson random variable whose mean is the true light intensity (measured in photoelectrons). Read noise is an approximately Gaussian random variable with zero mean and fixed variance. We can approximate these together as a single heteroscedastic Gaussian and treat each observed intensity y as a random variable whose variance is a function of the true signal x:</p><formula xml:id="formula_0">y ∼ N (µ = x, σ 2 = λ read + λ shot x).<label>(1)</label></formula><p>Parameters λ read and λ shot are determined by sensor's analog and digital gains. For some digital gain g d , analog gain g a , and fixed sensor readout variance σ 2 r , we have</p><formula xml:id="formula_1">λ read = g 2 d σ 2 r , λ shot = g d g a .<label>(2)</label></formula><p>These two gain levels are set by the camera as a direct function of the ISO light sensitivity level chosen by the user or  <ref type="figure">Figure 3</ref>. Shot and read noise parameters from the Darmstadt dataset <ref type="bibr" target="#b29">[30]</ref>. The size of each circle indicates how many images in the dataset shared that shot/read noise pair. To choose the noise level for each synthetic training image, we randomly sample shot and read noise parameters from the distribution shown in red.</p><p>by some auto exposure algorithm. Thus the values of λ read and λ shot can be calculated by the camera for a particular exposure and are usually stored as part of the metadata accompanying a raw image file.</p><p>To choose noise levels for our synthetic images, we model the joint distribution of different shot/read noise parameter pairs in our real raw images and sample from that distribution. For the Darmstadt Noise Dataset <ref type="bibr" target="#b29">[30]</ref>, a reasonable sampling procedure of shot/read noise factors is</p><formula xml:id="formula_2">log (λ shot ) ∼ U(a = log(0.0001), b = log(0.012)) log (λ read ) | log (λ shot ) ∼ N (µ = 2.18 log (λ shot ) + 1.2, σ = 0.26). (3)</formula><p>See <ref type="figure">Figure 3</ref> for a visualization of this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Demosaicing</head><p>Each pixel in a conventional camera sensor is covered by a single red, green, or blue color filter, arranged in a Bayer pattern, such as R-G-G-B. The process of recovering all three color measurements for each pixel in the image is the well-studied problem of demosaicing <ref type="bibr" target="#b14">[15]</ref>. The Darmstadt dataset follows the convention of using bilinear interpolation to perform demosaicing, which we adopt. Inverting this step is trivial-for each pixel in the image we omit two of its three color values according to the Bayer filter pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Digital Gain</head><p>A camera will commonly apply a digital gain to all image intensities, where each image's particular gain is selected by the camera's auto exposure algorithm. These auto exposure algorithms are usually proprietary "black boxes" and are difficult to reverse engineer for any individual image. But to invert this step for a pair of synthetic and real datasets, a reasonable heuristic is to simply find a single global scaling that best matches the marginal statistics of all image intensities across both datasets. To produce this scaling, we assume that our real and synthetic image intensities are both drawn from different exponential distributions: </p><formula xml:id="formula_3">p(x; λ) = λe −λx<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">White Balance</head><p>The image recorded by a camera is the product of the color of the lights that illuminate the scene and the material colors of the objects in the scene. One goal of a camera pipeline is to undo some of the effect of illumination, producing an image that appears to be lit under "neutral" illumination. This is performed by a white balance algorithm that estimates a per-channel gain for the red and blue channels of an image using a heuristic or statistical approach <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4]</ref>. Inverting this procedure from synthetic data is challenging because, like auto exposure, the white balance algorithm of a camera is unknown and therefore difficult to reverse engineer. However, raw image datasets such as Darmstadt record the white balance metadata of their images, so we can synthesize somewhat realistic data by simply sampling from the empirical distribution of white balance gains in that dataset: a red gain in [1.9, 2.4] and a blue gain in [1.5, 1.9], sampled uniformly and independently.</p><p>When synthesizing training data, we sample inverse digital and white balance gains and take their product to get a per-channel inverse gain to apply to our synthetic data. This inverse gain is almost always less than unity, which means that naïvely gaining down our synthetic imagery will result in a dataset that systematically lacks highlights and contains almost no clipped pixels. This is problematic, as correctly handling saturated image intensities is critical when denoising. To account for this, instead of applying our inverse gain 1 /g to some intensity x with a simple multiplication, we apply a highlight-preserving transformation f (x, g) that is linear when g ≤ 1 or x ≤ t for some threshold t = 0.9, but is a cubic transformation when g &gt; 1 and x &gt; t:</p><formula xml:id="formula_4">α(x) = max(x − t, 0) 1 − t 2 (5) f (x, g) = max x g , (1 − α(x)) x g + α(x)x<label>(6)</label></formula><p>This transformation is designed such that f (x, g) = x /g when x ≤ t, f (1, g) = 1 when g ≤ 1, and f (x, g) is continuous and differentiable. This function is visualized in <ref type="figure" target="#fig_2">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Color Correction</head><p>In general, the color filters of a camera sensor do not match the spectra expected by the sRGB color space. To address this, a camera will apply a 3 × 3 color correction matrix (CCM) to convert its own "camera space" RGB color measurements to sRGB values. The Darmstadt dataset consists of four cameras, each of which uses its own fixed CCM when performing color correction. To generate our synthetic data such that it will generalize to all cameras in the dataset, we sample random convex combinations of these four CCMs, and for each synthetic image, we apply the inverse of a sampled CCM to undo the effect of color correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Gamma Compression</head><p>Because humans are more sensitive to gradations in the dark areas of images, gamma compression is typically used to allocate more bits of dynamic range to low intensity pixels. We use the same standard gamma curve as <ref type="bibr" target="#b29">[30]</ref>, while taking care to clamp the input to the gamma curve with = 10 −8 to prevent numerical instability during training:</p><formula xml:id="formula_5">Γ(x) = max(x, ) 1 /2.2<label>(7)</label></formula><p>When generating synthetic data, we apply the (slightly approximate, due to ) inverse of this operator:  <ref type="figure">Figure 5</ref>. Histograms for each color channel of (a) sRGB images from the MIR Flickr dataset, (b) unprocessed images created following the procedure enumerated in Section 4.1 and detailed in Section 3, and (c) real raw images from the Darmstadt dataset. Note that the distributions of real raw intensities and our unprocessed intensities are similar.</p><formula xml:id="formula_6">Γ −1 (y) = max(y, ) 2.2<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Tone Mapping</head><p>While high dynamic range images require extreme tone mapping <ref type="bibr" target="#b10">[11]</ref>, even standard low-dynamic-range images are often processed with an S-shaped curve designed to match the "characteristic curve" of film <ref type="bibr" target="#b9">[10]</ref>. More complex edge-aware local tone mapping may be performed, though reverse-engineering such an operation is difficult <ref type="bibr" target="#b27">[28]</ref>. We therefore assume that tone mapping is performed with a simple "smoothstep" curve, and we use the inverse of that curve when generating synthetic data.</p><formula xml:id="formula_7">smoothstep(x) = 3x 2 − 2x 3 (9) smoothstep −1 (y) = 1 2 − sin sin −1 (1 − 2y) 3<label>(10)</label></formula><p>where both are only defined on inputs in [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model</head><p>Now that we have defined each step of our image processing pipeline and each step's inverse, we can construct our denoising neural network model. The input and groundtruth used to train our network is synthetic data that has been unprocessed using the inverse of our image processing pipeline, where the input image has additionally been corrupted by noise. The output of our network and the ground-truth are processed by our pipeline before evaluating the loss being minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Unprocessing Training Images</head><p>To generate realistic synthetic raw data, we unprocess images by sequentially inverting image processing transformations, as summarized in <ref type="figure">Figure 2</ref>. This consists of inverting, in order, tone mapping (Section 3.7), applying gamma decompression (Section 3.6), applying the sRGB to camera RGB color correction matrix (Section 3.5), and inverting white balance gains (Section 3.4) and digital gain (Section 3.3). The resulting synthetic raw image is used as the noise-free ground truth during training, and shot and read noise (Section 3.1) is added to create the noisy network input. Our synthetic raw images more closely resemble real raw intensities, as demonstrated in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Processing Raw Images</head><p>Since raw images ultimately go through an image processing pipeline before being viewed, the output images from our model should also be subject to such a pipeline before any loss is evaluated. We therefore apply raw processing to the output of our model, which in order consists of applying white balance gains (Section 3.4), naïve bilinear demosaicing (Section 3.2), applying a color correction matrix to convert from camera RGB to sRGB (Section 3.5), and gamma compression (Section 3.6). This simplified image processing pipeline matches that used in the Darmstadt Noise Dataset benchmark <ref type="bibr" target="#b29">[30]</ref> and is a good approximation for general image pipelines. We apply this processing to the network's output and to the ground truth noise-free image before computing our loss. Incorporating this pipeline into training allows the network to reason about how downstream processing will impact the desired denoising behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Architecture</head><p>Our denoising network takes as input a noisy raw image in the Bayer domain and outputs a reduced noise image in the same domain. As an additional input, we pass the network a per-pixel estimate of the standard deviation of noise in the input image, based on its shot and read noise parameters. This information is concatenated to the input as 4 additional channels-one for each of the R-G-G-B Bayer planes. We use a U-Net architecture <ref type="bibr" target="#b31">[32]</ref> with skip connections between encoder and decoder blocks at the same scale (see <ref type="figure" target="#fig_4">Figure 6</ref> for details), with box downsampling when encoding, bilinear upsampling when decoding, and the PReLU <ref type="bibr" target="#b21">[22]</ref> activation function. As in <ref type="bibr" target="#b40">[41]</ref>, instead of directly predicting a denoised image, our model predicts a residual that is added back to the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training</head><p>To create our synthetic training data, we start with the 1 million images of the MIR Flickr extended dataset <ref type="bibr" target="#b25">[26]</ref>, setting aside 5% of the dataset for validation and 5% for testing. We downsample all images by 2× using a Gaussian kernel (σ = 1) to reduce the effect of noise, quantization, JPEG compression, demosaicing, and other artifacts. We then take random 128 × 128 crops of each image, with random horizontal and vertical flips for data augmentation. We synthesize noisy and clean raw training pairs by applying the unprocessing steps described in Section 4.1. We train using Adam <ref type="bibr" target="#b22">[23]</ref> with a learning rate of 10 −4 , β 1 = 0.9, β 2 = 0.999, = 10 −7 , and a batch size of 16. Our models and ablations are trained to convergence over approximately 3.5 million steps on a single NVIDIA Tesla P100 GPU, which takes ∼3 days.</p><p>We train two models, one targeting performance on sRGB error metrics, and another targeting performance on raw error metrics. For our "sRGB" model the network output and synthetic ground-truth are both transformed to sRGB space before computing the loss, as described in Section 4.2. Our "Raw" model instead computes the loss directly between our network output and our raw synthetic ground-truth, without this processing. For both experiments we minimize L 1 loss between the output and ground-truth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>To evaluate our technique we use the Darmstadt Noise Dataset <ref type="bibr" target="#b29">[30]</ref>, a benchmark of 50 real high-resolution images where each noisy high-ISO image is paired with a (nearly) noise-free low-ISO ground-truth image. The Darmstadt dataset represents a significant improvement upon earlier benchmarks for denoising, which tended to rely on synthetic data and synthetic (and often unrealistic) noise models. Additional strengths of the Darmstadt dataset are that it includes images taken from four different standard consumer cameras of natural "in the wild" scene content, where the camera metadata has been captured and the camera noise properties have been carefully calibrated, and where the image intensities are presented as raw unprocessed linear intensities. Another valuable property of this dataset is that evaluation on the dataset is restricted through a carefully controlled online submission system: the entire dataset is the test set, with the ground-truth noise-free images completely hidden from the public, and the frequency of submissions to the dataset is limited. As a result, overfitting to the test set of this benchmark is difficult. Though this approach is common for object recognition <ref type="bibr" target="#b12">[13]</ref> and stereo <ref type="bibr" target="#b34">[35]</ref> challenges, it is not common in the context of image denoising.</p><p>The performance of our model on the Darmstadt dataset  <ref type="table">Table 1</ref>. Performance of our model and its ablations on the Darmstadt Noise Dataset <ref type="bibr" target="#b29">[30]</ref> compared to all published techniques at the time of submission, taken from https://noise.visinf.tu-darmstadt.de/benchmark/, and sorted by sRGB PSNR. For baseline methods that have been benchmarked with and without a variance stabilizing transformation (VST), we report whichever version performs better and indicate accordingly in the algorithm name. We report baseline techniques that use either raw or sRGB data as input, and because this benchmark does not evaluate sRGB-input techniques in terms of raw output, the raw error metrics are missing for those techniques. For each technique and metric we report relative improvement in parenthesis, which is done by turning PSNR into RMSE and SSIM into DSSIM and then computing the reduction in error relative to the best-performing models. Ablations of our model are presented in a separate sub- with respect to prior work is shown in <ref type="table">Table 1</ref>. The Darmstadt dataset as presented by <ref type="bibr" target="#b29">[30]</ref> separates its evaluation into multiple categories: algorithms that do and do not use a variance stabilizing transformation, and algorithms that use linear Bayer sensor readings or that use bilinearly demosaiced sRGB images as input. Each algorithm that operates on raw input is evaluated both on raw Bayer images, and on their denoised Bayer outputs after conversion to sRGB space. Following the procedure of the Darmstadt dataset, we report PSNR and SSIM for each technique, on raw and sRGB outputs. Some algorithms only operate on sRGB inputs; to be as fair as possible to all prior work, we present these models, reporting their evaluation in sRGB space. For algorithms which have been evaluated with and without a variance stabilizing transformation (VST), we include whichever version performs better.</p><p>The two variants of our model (one targeting sRGB and the other targeting raw) produce significantly higher PSNRs and SSIMs than all baseline techniques across all outputs, with each model variant outperforming the other for the domain that it targets. Relative improvements on PSNR and SSIM are difficult to judge, as both metrics are designed to saturate as errors become small. To help with this, alongside each error we report the relative reduction in error of the best-performing model with respect to that model, in parentheses. This was done by converting PSNR into RMSE (RMSE ∝ √ 10 −PSNR/10 ) and converting SSIM into DSSIM (DSSIM = (1−SSIM)/2) and then computing each relative reduction in error.</p><p>We see that our models produce a 14% and 25% reduction in error on the two raw metrics compared to the next best performing technique (N3Net <ref type="bibr" target="#b30">[31]</ref>), and a 21% and 38% reduction in error on the two sRGB metrics compared to the two next best performing techniques (N3Net <ref type="bibr" target="#b30">[31]</ref> and  CBDNet <ref type="bibr" target="#b17">[18]</ref>). Visualizations of our model's output compared to other methods can be seen in <ref type="figure">Figure 1</ref> and in the supplement. Our model's improved performance appears to be partly due to the decreased low-frequency chroma artifacts in its output compared to our baselines.</p><p>To verify that our approach generalizes to other datasets and devices, we evaluated our denoising method on raw images from the HDR+ dataset <ref type="bibr" target="#b20">[21]</ref>. Results from these evaluations are provided in <ref type="figure" target="#fig_6">Figure 7</ref> and in the supplemental material.</p><p>Separately from our two primary models of interest, we present an ablation study of "Our Model (sRGB)," in which we remove one or more model components. "No CCM, WB, Gain" indicates that when generating synthetic training data we did not perform the unprocessing steps of sRGB to camera RGB CCM inversion, or inverting white balance and digital gain. "No Tone Mapping, Gamma" indicates that we did not perform the unprocessing steps of inverting tone mapping or gamma decompression. "No Unprocessing" indicates that we did not perform any unprocessing steps, and "4× bigger" indicates that we quadrupled the number of channels in each conv layer. "Noise-blind" indicates that the noise level was not provided as input to the network. "AWGN" indicates that instead of using our more realistic noise model when synthesizing training data, we use additive white Gaussian noise with σ sampled uniformly between 0.001 and 0.15 (the range reported in <ref type="bibr" target="#b29">[30]</ref>). "No Residual Output" indicates that our model architecture directly predicts the output image, instead of predicting a residual that is added to the input.</p><p>We see from this ablation study that removing any of our proposed model components reduces quality. Performance is most sensitive to our modeling of noise, as using Gaussian noise significantly decreases performance. Unprocess-ing also contributes substantially, especially when evaluated on sRGB metrics, albeit slightly less than a realistic noise model. Notably, increasing the network size does not make up for the omission of unprocessing steps. Our only ablation study that actually removes a component of our neural network architecture (the residual output block) results in the smallest decrease in performance. <ref type="table">Table 1</ref> also includes runtimes for as many models as we were able to find. Many of these runtimes were produced on different hardware platforms with different timing conventions, so we detail how these numbers were produced here. The runtime of our model is 22ms for the 512×512 images of the Darmstadt dataset, using our TensorFlow implementation running on a single NVIDIA GeForce GTX 1080Ti GPU, excluding the time taken for data to be transferred to the GPU. We report the mean over 100 runs. The runtime for DnCNN is taken from <ref type="bibr" target="#b40">[41]</ref>, which reports a runtime on a GPU (Nvidia Titan X) of 60ms for a 512×512 image, also not including GPU memory transfer times. The runtime for N3Net <ref type="bibr" target="#b30">[31]</ref> is taken from that paper, which reports a runtime of 3.5× that of <ref type="bibr" target="#b40">[41]</ref>, suggesting a runtime of 210ms. In <ref type="bibr" target="#b5">[6]</ref> they report a runtime of 60 seconds on a 512×512 image for a CPU implementation, and note that their runtime is less than that of KSVD <ref type="bibr" target="#b1">[2]</ref>, which we note accordingly. The runtime for CBDNet was taken from <ref type="bibr" target="#b17">[18]</ref>, and the runtimes for BM3D, TNRD, TWSC, and MCWNNM were taken from <ref type="bibr" target="#b38">[39]</ref>. We were unable to find reported runtimes for the remaining techniques in <ref type="table">Table 1</ref>, though in <ref type="bibr" target="#b29">[30]</ref> they note that "many of the benchmarked algorithms are too slow to be applied to megapixel-sized images". Our model is the fastest technique by a significant margin: 9× faster than N3Net <ref type="bibr" target="#b30">[31]</ref> and 18× faster than CBDnet <ref type="bibr" target="#b17">[18]</ref>, the next two best performing techniques after our own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Runtimes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a technique for "unprocessing" generic images into data that resembles the raw measurements captured by real camera sensors, by modeling and inverting each step of a camera's image processing pipeline. This allowed us to train a convolutional neural network for the task of denoising raw image data, where we synthesized large amounts of realistic noisy/clean paired training data from abundantly available Internet images. Furthermore, by incorporating standard image processing operations into the learning procedure itself, we are able to train a network that is explicitly aware of how its output will be processed before it is evaluated. When our resulting learned model is applied to the Darmstadt Noise Dataset <ref type="bibr" target="#b29">[30]</ref> it achieves 14%-38% lower error rates and 9×-18× faster runtimes than the previous state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Noisy Input, PSNR = 18.76 (b) Ground Truth (c) N3Net [31], PSNR = 32.42 (d) Our Model, PSNR = 35.35</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The function f (x, g) (defined in Equation 6) we use for gaining down synthetic image intensities x while preserving highlights, for a representative set of gains {g}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The network structure of our model. Input to the network is a 4-channel noisy mosaic image concatenated with a 4-channel noise level map, and output is a 4-channel denoised mosaic image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>An image from the HDR+ dataset<ref type="bibr" target="#b20">[21]</ref>, where we present (a) the noisy input image and (b) the output of our model, in the same format asFigure 1. See the supplement for additional results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>for x ≥ 0. The maximum likelihood estimate of the scale parameter λ is simply the inverse of the sample mean, and scaling x is equivalent to an inverse scaling of λ. This means that we can match two sets of intensities that are both exponentially distributed by using the ratio of the sample means of both sets. When using our synthetic data and the Darmstadt dataset, this scaling ratio is 1.25. For more thorough data augmentation and to ensure that our model observes pixel intensities throughout [0, 1] during training, rather than applying this constant scaling, we sample inverse gains from a normal distribution centered at 1/1.25 = 0.8 with standard deviation of 0.1, resulting in inverse gains roughly spanning [0.5, 1.1].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>table .</head><label>.</label><figDesc>The top three techniques for each metric (ignoring ablations) are color-coded. Runtimes are presented when available (see Section 5.1).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K-Svd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Sig. Proc</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Renoir -a dataset for real lowlight noise image reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.8230</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast fourier color constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Tsai</surname></persName>
		</author>
		<idno>2017. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Image denoising: Can plain neural networks compete with BM3D? CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On learning optimized reaction diffusion processes for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sensitometry of photographic emulsions and a survey of the characteristics of plates and films of American manufacture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Govt. Print. Off</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Recovering high dynamic range radiance maps from photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Practical poissonian-gaussian noise modeling and fitting for single-image raw-data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trimeche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep joint demosaicking and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computational color constancy: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gijsenij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Weighted nuclear norm minimization with application to image denoising. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04686</idno>
		<title level="m">Toward convolutional blind denoising of real photographs</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">In Computer Vision: A Reference Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Photon, poisson noise</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noiseoptimal capture for high dynamic range photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Burst photography for high dynamic range and low-light imaging on mobile cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<title level="m">Noise2Noise: Learning image restoration without clean data. ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic estimation and removal of noise from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">New trends and ideas in visual concept detection: The MIR Flickr Retrieval Evaluation Initiative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ACM MIR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Burst denoising with kernel prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Local laplacian filters: Edge-aware image processing with a laplacian pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<title level="m">Neural nearest neighbors networks. NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fields of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Nonlinear total variation based noise removal algorithms. Phys. D, 1992</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepisp: Toward learning an end-to-end image processing pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Noise removal via bayesian wavelet coring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A trilateral weighted sparse coding scheme for real-world image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multi-channel weighted nuclear norm minimization for real color image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
