<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
							<email>mark.mcdonnell@unisa.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesham</forename><surname>Mostafa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runchun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Van Schaik</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Computational Learning Systems Laboratory</orgName>
								<orgName type="institution">University of South</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Mawson Lakes SA</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute for Neural Computation</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">International Centre for Neuromorphic Systems</orgName>
								<address>
									<addrLine>Western Sydney University Penrith NSW</addrLine>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> Batch-normalization (BN)  <p>layers are thought to be an integrally important layer type in today's state-of-the-art deep convolutional neural networks for computer vision tasks such as classification and detection. However, BN layers introduce complexity and computational overheads that are highly undesirable for training and/or inference on low-power custom hardware implementations of real-time embedded vision systems such as UAVs, robots and Internet of Things (IoT) devices. They are also problematic when batch sizes need to be very small during training, and innovations such as residual connections introduced more recently than BN layers could potentially have lessened their impact. In this paper we aim to quantify the benefits BN layers offer in image classification networks, in comparison with alternative choices. In particular, we study networks that use shifted-ReLU layers instead of BN layers. We found, following experiments with wide residual networks applied to the ImageNet, CIFAR 10 and CIFAR 100 image classification datasets, that BN layers do not consistently offer a significant advantage. We found that the accuracy margin offered by BN layers depends on the data set, the network size, and the bit-depth of weights. We conclude that in situations where BN layers are undesirable due to speed, memory or complexity costs, that using shifted-ReLU layers instead should be considered; we found they can offer advantages in all these areas, and often do not impose a significant accuracy cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Following its introduction in 2015 <ref type="bibr" target="#b0">[1]</ref>, Batch Normalization (BN) layers rapidly became a default layer type in state-of-theart deep convolutional neural networks (CNNs). In particular for CNNs designed as image classifiers, batch-nomalization is essential for state of the art accuracy on difficult datasets like Imagenet <ref type="bibr" target="#b1">[2]</ref>. However, BN layers have several disadvantages such as:</p><p>• BN layers typically increase the GPU memory requirements and computational load during training, due to bydefault storage of feature maps for every layer, for use in gradient calculations 1 ; additional multiplications needed for BN layers also slows down inference; • BN layers and the computation of their parameters are implemented slightly differently in different popular deep learning libraries, resulting in difficulty replicating results and in model portability; • BN layers are not well-suited to custom-hardware implementations of training, such as those that use a pipeline approach where all processing needs to be done on one sample independently of all other samples. • BN layers create challenges for efficient training when batches are split across multiple GPUs, and as a result sometimes non-exact approximations are used <ref type="bibr" target="#b2">[3]</ref>; • BN layers can be less effective for models that have to be trained using small batches <ref type="bibr" target="#b3">[4]</ref>, such as when input images or model sizes are large enough to fill up GPU RAM; • BN layers can be problematic for datasets with high class-imbalance or when samples in a minibatch are not independent <ref type="bibr" target="#b4">[5]</ref>. The last four disadvantages are due to the fact that BN layers require calculation of batch-wise statistics, namely the mean and variance of every channel, calculated over all locations in the channel's feature map and all samples in a minibatch, at each point in a network where a BN layer is used. These statistics are less robust for both smaller numbers of samples in a minibatch, and if a batch includes rarely chosen or nonindependent samples <ref type="bibr" target="#b4">[5]</ref>.</p><p>Some of the potential challenges of BN layers are not as frequently of importance for inference, such as minibatch size. However in the near future, it has been predicted that there will be many applications where it will be desirable to train deep neural networks on low-power custom hardware, such as when networks need to be updated frequently using new data, and communication to a data center is too slow or unavailable <ref type="bibr" target="#b5">[6]</ref>.</p><p>For all these reasons, it is desirable to know whether BN layers really are generally necessary for best performance, or whether for particular datasets state-of-the-art accuracy can be achieved without them. However, our main motivation is drawn from seeking to design deep CNN classifiers implemented in custom hardware, that run as fast and efficiently as possible in an embedded system. It is already well-established that convolutional layers can be implemented without use of multipliers, by reducing representation precision of weights or feature maps to 1 bit, hence potentially saving large amounts of chip space and power usage <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>. However, existing investigations in this area often still use BN layers, which if implemented exactly introduce significant complexity, including mandating the use of minibatches, and the need for multipliers, and it is desirable to know whether they can be removed or replaced.</p><p>In this paper, we train deep CNNs with and without BN layers, and analyze the resulting accuracy changes.</p><p>The paper is structured as follows. In Section II we review the BN layer definition, and discuss relevant prior research related to the goals of this paper. Then, in Section III, we describe the CNN architectures we chose to use for this study, how we vary the use of BN layers in this architecture, and adaptations we need to introduce to enable them to work effectively. Next, Section IV contains our results. Finally, we discuss the implications and significance of our results in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELEVANT PREVIOUS RESEARCH</head><p>Depth is crucial for effective learning <ref type="bibr" target="#b11">[11]</ref> in modern neural networks as it allows a network to learn a hierarchical representation of the data by successively composing simpler features into more complex features. Depth, however, poses a number of challenges when training neural networks. The large number of parameters in deep networks typically restricts the optimization methods that could be feasibly used to first order methods like stochastic gradient descent. The loss surface of a deep neural network, however, is highly non-convex and there is no guarantee that starting from an initial parameter point and traversing this loss surface using gradient descent will land in a good local minimum that allows the network to generalize well on unseen data <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>. A considerable number of techniques and tricks have been developed to allow gradient descent to practically succeed in training deep networks, ranging from random parameter initialization strategies <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>, unsupervised pre-training <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>, or augmenting gradient descent with gradient history information to more effectively traverse the loss surface <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>.</p><p>The performance of gradient descent is highly dependent on the way in which the optimization problem is parameterized. For example, by re-parameterizing a deep neural network, i.e, by changing the scale and shift of the network's parameters, we can drastically change the curvature of the loss surface <ref type="bibr" target="#b21">[21]</ref> and the behavior of gradient descent. One of the famous deleterious effects of bad parameterization is the vanishing and exploding gradients problem that plagued early neural networks <ref type="bibr" target="#b22">[22]</ref>. To combat such problems, and to yield a more favorable parameterization for gradient descent in general, a broad class of techniques attempt to re-parameterize deep neural networks through various forms of normalization. Such normalization techniques operate on various quantities in the network, such as activations or weights.</p><p>Perhaps the most popular normalization technique is batchnormalization <ref type="bibr" target="#b0">[1]</ref>, which is described in more detail in the next subsection. Batch-normalization not only accelerates gradient descent learning, it also has a beneficial regularization effect <ref type="bibr" target="#b23">[23]</ref>. Batch-normalization operates on pre-activations (the inputs to neurons before the activation function is applied); it rescales and shifts the pre-activations of each single neuron so that they have zero mean and unity variance across the minibatch samples, in each channel. During training, the network's response to an example thus depends on the other examples that accompany it in a mini-batch. This dependence, however, could prove undesirable in some situations <ref type="bibr" target="#b24">[24]</ref>. Batchnormalization requires the use of a mini-batch that is large enough to yield reliable pre-activation statistics. Increasing mini-batch size, however, often leads to a decrease in the network's generalization performance <ref type="bibr" target="#b25">[25]</ref>.</p><p>A related normalization technique that does not depend on mini-batch statistics is layer normalization <ref type="bibr" target="#b26">[26]</ref>. Layer normalization normalizes the pre-activations of neurons in a layer so that they have zero mean and unity variance. These statistics are calculated across all pre-activations in a layer, unlike batch-normalization which calculates them for each neuron individually across the mini-batch. One downside, however, is that different neurons, especially in convolutional layers, can have widely different input statistics, so normalizing all of them using the same coefficients is poorly motivated. Instead of normalizing the activations, ref <ref type="bibr" target="#b27">[27]</ref> normalizes by the norm of the input weight vector of each neuron. An extra parameter is introduced for each weight vector to explicitly control its length. The performance of weight normalization closely matches that of batch-normalization while avoiding its major downside: the dependence on mini-batch statistics. Another innovation introduced by <ref type="bibr" target="#b27">[27]</ref> is the "mean-only BN" layer, in which layer inputs are centered according to the mean of a batch, in conjunction with weight normalization.</p><p>Recently, yet another method was introduced and shown to outperform BN: group normalization <ref type="bibr" target="#b3">[4]</ref> while avoiding some of its downsides.</p><p>The choice of activation function has a large impact on the performance of gradient descent or backpropagation. When errors are backpropagated through a layer, they are scaled by the derivative of the activation function of the neurons in that layer. If these derivatives are small, then errors are effectively blocked from propagating backwards. Saturating activation functions such as the logistic sigmoid, or hyperbolic tan, are particularly vulnerable to this effect as their derivatives are very small when their input is far from zero <ref type="bibr" target="#b28">[28]</ref>. The use of non-saturating activation functions such as Rectified Linear Units (ReLUs) has partially alleviated this problem. A ReLU activation is zero for negative inputs and the identity for positive inputs. ReLUs, however, cannot have an output that is zero mean across the training examples as they do not produce negative outputs. Exponential Linear Units (ELUs) <ref type="bibr" target="#b29">[29]</ref> address this issue by having a saturating negative output if the input is less than zero. ELUs have been shown to perform extremely well without any form of explicit normalization suggesting that they intrinsically normalize the levels of activation in the network.</p><p>However, shifted Rectified Linear Units (sReLUs) offer nearly all the same benefits as ELUs, but without the need to calculate exponentials, and given our motivation of minimizing computational load, they are our focus here instead of ELUs. Indeed, our results in <ref type="figure" target="#fig_4">Figures 5 and 6</ref> show that networks using sReLU and ELU activations do not have significant differences in accuracy.</p><p>For similar reasons, we aim to establish if we can avoid alternatives to BN like group normalization <ref type="bibr" target="#b3">[4]</ref>, where complexity and additional computation is introduced for computing statistics and carrying out normalizations by non-constant factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Review of BN layers</head><p>Batch-normalization (BN) layers have been shown to help deep neural networks produce better accuracy following training. The usual explanation for this is that they help "reduce internal covariate shift" <ref type="bibr" target="#b0">[1]</ref>. This view has been recently challenged <ref type="bibr" target="#b30">[30]</ref>. However it is clear that the use of BN layers enables higher learning rates, and hence faster convergence during training, and diminishes the importance of good initial conditions for convolutional layers <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>BN layers are applied to a minibatch of feature maps, which typically can be represented as a 4-axis tensor, x ∈ R K,H,W,C , where K is the size of the minibatch, H and W are the height and width of the feature maps, and C is the number of channels. BN layers operate on a per-channel basis, with channel-specific gain and shift parameters, and make use of channel-specific calculations of the mean and variance of the inputs to the channel in a minibatch. Mathematically, if x k,h,w,c is the value of the feature map at location h, w in the c-th channel of the k-th sample, then BN layers transform that value according tô</p><formula xml:id="formula_0">x k,h,w,c = g c x k,h,w,x − µ c σ 2 c + + o c ,<label>(1)</label></formula><p>where µ c and σ 2 c are calculated as</p><formula xml:id="formula_1">µ c = 1 KHW k h w x k,h,w,c<label>(2)</label></formula><p>and</p><formula xml:id="formula_2">σ 2 c = 1 KHW k h w (x k,h,w,c − µ c ) 2 .<label>(3)</label></formula><p>The remaining parameters are:</p><p>• gain, g c , and shift o c , which are usually learned in the same manner as weights in convolutional layers; • , which ensures division by zero cannot happen in the event of zero variance, and can also act like a regularizer; it is not widely appreciated that accuracy can depend on the exact choice of and that inference performance is sensitive to inadvertent changes in compared to training. It is also noteworthy that some popular deep learning libraries define differently, by taking it outside the square root, which changes subtely but sometimes significantly the performance of otherwise identical models.</p><p>There are two major differences between BN layers and other layers: 1) calculation of layer outputs depend on all samples in a minibatch and cannot be computed independently for each sample; 2) During training, minibatch means and variances are calculated. For inference, each mean and variance is an additional parameter that forms part of the trained model, but unlike most parameters derived from training data, these are not learned, but rather are computed. Most popular libraries compute these values during training by calculating exponential moving averages over batches as training progresses. We have found that this can sometimes give misleading and sub-par performance during monitoring on a validation set as training progresses, because parameters change over the window in which averages are created. Instead, we favour calculation of batch means and variances using multiple training batches while training is frozen, as described in Algorithm 2 in the original BN paper <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. BN layer scale and shift can be detrimental</head><p>Recent work demonstrated the surprising result that not learning BN scale and shift parameters can actually be beneficial, leading to reduced error rates for CIFAR 10 and CIFAR 100 in a wide residual network <ref type="bibr" target="#b10">[10]</ref>. This result was found to be the case for both full-precision networks, and for "1-bitper-weight" versions of the same networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Shifted Rectified Linear Unit and Exponential Linear Unit</head><p>It has already been shown that removal of BN layers can somewhat be compensated for by using exponential linear units (ELUs) <ref type="bibr" target="#b29">[29]</ref> instead of the combination of BN and ReLU. The mathematical definition of these two activation functions are</p><formula xml:id="formula_3">ELU(x) = xI(x) + (exp(x) − 1)(1 − I(x)),<label>(4)</label></formula><p>where I(x) is the Heaviside step function, and sReLU(x) = max(−1, x).</p><p>Both can be generalized to a different minimum output value, but we consider only the case of −1. See <ref type="figure">Figure 1</ref> for plots of the sReLU and ELU characteristics, in comparison with ReLU. When ELUs were introduced, it was shown that they outperform shifted rectified linear units (sReLUs) <ref type="bibr" target="#b29">[29]</ref>. However, in our experiments with more recently advanced networks and training methods not available at the time of <ref type="bibr" target="#b29">[29]</ref>, we did not observe any significant difference between ELUs and sReLUs. Moreover, ELUs are computationally expensive compared with sReLU, both because sReLUs do not require calculations of exponentials in the forward pass, nor storage of their values for use in the backward pass during training. We show here that shifted ReLUs are an effective and efficient alternative to ELUs, and seek to quantify how much accuracy penalty is incurred by replacing BN layers. ReLU(x) sReLU(x) ELU(x) <ref type="figure">Fig. 1</ref>. Shifted Rectified Linear Unit (sReLU) activation function. The sReLU activation function lets negative inputs pass through, between 0 and some negative constant, in this case equal to −1. While the Exponential Linear Unit (ELU) is more popular, we have found sReLU to be equally effective, and less computationally demanding, due to avoiding calculation of an exponential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline network architecture and training</head><p>Our experiments are based on wide residual networks [31] (see <ref type="figure" target="#fig_1">Figure 2</ref>) that use the post-activation architecture <ref type="bibr" target="#b32">[32]</ref>.</p><p>Using the nomenclature of <ref type="bibr" target="#b31">[31]</ref>, for CIFAR 10 and CIFAR 100 we used depth 20 and widths of 4× and 10× and for ImageNet depth 18 and widths 1× and 2.5×, meaning in both cases that the first layer of convolutional weights had either 64 or 160 output channels. The design we used has a few differences to <ref type="bibr" target="#b31">[31]</ref>, such as that the final weights layer is a 1 × 1 convolutional layer applied before the global average pooling layer-see <ref type="figure" target="#fig_1">Figure 2</ref> which illustrates this aspect, and the overall design. More details are described in <ref type="bibr" target="#b10">[10]</ref>. Note also that in the design of <ref type="bibr" target="#b10">[10]</ref> a BN layer was applied to the RGB input channels prior to the first convolution layer. Here we do the same for all variations, including the sReLU one. Provided that the shift and scale factors are not learned, this BN layer is equivalent to preprocessing raw data, and does not need to be considered to form a network layer.</p><p>Training was carried out similarly to <ref type="bibr" target="#b10">[10]</ref>, i.e. we used backpropagation and stochastic gradient descent, with minibatches of size 125 samples, momentum of 0.9 and weight decay with a value of 0.0005. No biases are used. Weights were initialized using the method of <ref type="bibr" target="#b15">[15]</ref>, while BN gains were initialized to 1 and shifts to 0. No convolutional layer biases were used. Unlike <ref type="bibr" target="#b10">[10]</ref>, we did not use a warm restart learning rate schedule, as we found this could sometimes lead to divergence following a restart with the sReLU networks. However, we did use a cosine learning-rate decay schedule, starting at an initial value of 0.1 and finishing at 10 −5 after 300 epochs (CIFAR) or 60 epochs (ImageNet). For networks where BN layers were used, we computed the mean and variance statistics following the end of training, by calculating averages over all minibatches in 1 epoch, with learning turned off.</p><p>During training, each image selected for a minibatch was augmented using standard methods as in <ref type="bibr" target="#b10">[10]</ref>. In addition, cutout augmentation <ref type="bibr" target="#b33">[33]</ref> was used for CIFAR 10 and CIFAR 100, with the same design as in <ref type="bibr" target="#b10">[10]</ref>, with a patch size of 18 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 1-bit-per-weight networks</head><p>As well as training networks with the usual 32 bit floating point precision for all variables, including learned weights, we also trained networks using a method for enabling storage of learned weights and inference to take place using 1-bit values. We followed the method of <ref type="bibr" target="#b10">[10]</ref>; differences in training are summarised for the case of sReLUs in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>We emphasize that the main motivation of this paper is to examine whether methods for enabling reduced-precision representations in deep neural networks, such as <ref type="bibr" target="#b10">[10]</ref>, still work effectively when batch-normalization layers are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Shifted ReLUs</head><p>We conducted experiments using the same architecture as in  <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training innovations for networks with BN layers removed</head><p>For the model where all BN layers but the final one were removed, we found we did not need to change any aspect of training relative to our baseline models.</p><p>However, without any BN layers, we found that training diverged, or converged very slowly. A partial remedy to this problem is to simply reduce the learning rate, a fact consistent with one of the advantages of using BN layers, i.e. that the initial learning rate can be set higher <ref type="bibr" target="#b0">[1]</ref> than in networks without BNs. However, this leads to slow convergence and increased error rates.</p><p>We investigated why this was the case. We found, surprisingly, that using just a single BN layer between the final convolutional layer and the global average pooling layer was sufficient to enable the network to be trained exactly like the baseline models.</p><p>Based on this observation, we hypothesized that the main reason that networks with BN layers enable a larger learning rate is due to the standarized scaling the final BN layer imparts on gradients from the output prior to backpropagation to weight layers. Indeed, we observed that without a final BN layer, the distribution of gradients at the output early in training have a longer tail then when BN is included.</p><p>We therefore introduced in our shifted ReLU networks a constant scaling layer to replace the BN layer between the final convolutional layer and global average pooling layer. For simplicity we have set the constant scaling identically for all channels (which in our design at this point in the network is equal to the number of classes).</p><p>Due to the linearity of the global average pooling layer, changing this scaling is equivalent to changing the temperature in the softmax layer, from its default value of 1. That is, our We change a block of this form:</p><p>To this form: <ref type="figure" target="#fig_2">Fig. 3</ref>. Changes when training for 1-bit-per-weight. When we train 1-bitper-weight networks following the method of <ref type="bibr" target="#b10">[10]</ref>, we apply the sign operator to full-precision copies of weights during training, and then scale by a constant equal to the initial standard deviation of the weights according to the method of <ref type="bibr" target="#b15">[15]</ref>. approach corresponds to employing a final softmax layer of the form</p><formula xml:id="formula_5">SM i (x) := exp xi T N j=1 exp xj T ,<label>(6)</label></formula><p>where T is the temperature-see, e.g. <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref>. We found a value for the temperature between approximately 30 and 100 to enable training to take place identically to the baseline models, including the same high initial learning rate. Lower values of T tended to result in failure to converge shortly after training commenced.</p><p>Note that although the gradient propagated back is linearly scaled by 1/T , changing from the default of T = 1 is not equivalent to simply changing the learning rate by a factor of T . This is due to the nonlinearity in the softmax layer. Increasing T has the effect of moving softmax outputs away from 0 or 1, thereby increasing the entropy of the output vector. In turn, this means gradients propagating backwards early in training have a distribution with lower standard deviation.</p><p>That this temperature scaling is all that is needed to ensure a high learning rate suggests that the main problem with larger learning rate for models without BN layers is simply that early in training, the gradients calculated at the output of the network are too high for high learning rates, and that the normalization of the final BN layer compensates for this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>We trained the following variations of our wide residual networks on both CIFAR 10 and CIFAR 100, all for both width 4 and width 10.</p><p>1) Baseline 1: networks as in <ref type="figure" target="#fig_1">Figure 2</ref>, with conventional BN layers where scales and shifts are learned. 2) Baseline 2: the same as Baseline 1, but without any scales and shifts learned, as in <ref type="bibr" target="#b10">[10]</ref>.  <ref type="table" target="#tab_1">Tables I and II, while Figures 5 and 6</ref> show the mean and spread of accuracies for width-4 ResNets for CIFAR 10 and CIFAR 100 over 10 runs.</p><p>For ImageNet, we compared width-1 and width-2.5 networks, and an ensemble of 3 width-1 networks, for the case of Baseline 1, and case 3 in the above list. The results are summarised in <ref type="table" target="#tab_1">Table III.</ref> Our conclusions drawn from the results are left for Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The impact of removing BN layers</head><p>Our results indicate the following:      longer the case for CIFAR 10, which might be because here we do not use a ReLU applied to the input. From these observations, we propose the following:</p><p>• Conclusion 1: Removing all BN layers can be expected to cause an accuracy penalty, but this penalty is po-tentially small, depending on the dataset and network architecture. • Conclusion 2: Removing all but the final BN layer is a potentially viable option for getting the benefits of removing most BN layers, but without as much accuracy loss as removing all. • Conclusion 3: There is no consistently best way to design networks with BNs; in some cases learning scales and shifts is beneficial, but in other cases it causes a big drop in performance. • Conclusion 4: There is less impact on accuracy in the case of 1-bit-per-weight compared with full precision weights. Indeed, for CIFAR 10, sReLU networks and 1-bit-per-weight had an accuracy gap no larger than 0.2%, and only 0.3% for width-2.5 ImageNet with centre cropping, suggesting sReLU as a viable option when 1bit-per-weight is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Significance for custom hardware implementations</head><p>Hardware acceleration of CNNs is necessary to provide realtime embedded vision systems, e.g., UAVs and Internet of Things (IoT) devices, because existing systems using CPUs are too slow. To increase speed, most software-based CNNs use GPUs. However, it is difficult to deploy GPUs in embedded systems, since they consume a significant amount of power. Thus, custom rather than general-purpose hardware-based CNNs are desired for low-power and real-time embedded vision systems.</p><p>In hardware-based acceleration systems, most power consumption comes from the computation modules, e.g., the multipliers and adders, and from accessing of the data, particularly the weights. By using binary weights, the multiplier can be replaced by a multiplexer, which consumes orders of magnitude less power. In modern CMOS technologies, e.g., 28nm, a binary convolutional operation with multiplexers achieves a power efficiency up to 230 1b-TOPS/W <ref type="bibr" target="#b37">[36]</ref>. More importantly, using binary weights enables use of only on-chip memories, e.g., SRAMs, to store these weights. Accessing data stored in external memories would consume an order of magnitude more power (10×) than the computation itself <ref type="bibr" target="#b38">[37]</ref>.</p><p>The use of batch-normalization layers might maintain maximum classification accuracy but at the cost of extra silicon area and computation and thus more power consumption. Particularly, it will require significant amount of silicon area to implement the nonlinear square and square-root operations in the conventional batch-normalization. What makes it worse is that these operations impede low bit-width quantization techniques <ref type="bibr" target="#b39">[38]</ref>. One can easily implement the shifted ReLU activation function with a tiny silicon area, while achieving comparable accuracies to networks with BN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Future work</head><p>In future, it will be valuable to try to devise new lowcomplexity methods that narrow the accuracy gap between networks that use BN layers, and the ones outlined here.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 ,</head><label>2</label><figDesc>but where (i) all BN layers except the final one closest to the output are replaced by shifted ReLUs and (ii) where all BN layers are replaced by shifted ReLUs. The architecture for the latter case is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 )</head><label>3</label><figDesc>A single BN layer at the end of the networks only (no scale and shift learned), with all other BN-ReLU combinations replaced by sReLU. 4) No BN layers -all replaced by sReLU, as in Figure 4. 5) No BN layers -all replaced by ELU [29]. 6) No BN layers -all replaced by sReLU, except for a 'mean-only BN' layer at the end of the network [27], without a learned bias. Our results for width-4 and width-10 ResNets are summarized in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Wide ResNet architecture for CIFAR when all BN layers are replaced by sReLUs. The architecture is identical to that ofFigure 2except that (i) all BNs have been removed and ReLUs have been replaced by shifted ReLUs (sRELU); (ii) a scale layer (multiplies all inputs by a constant) has been inserted before the global average pooling (GAP) layer; and (iii) the input is now normalized using pre-preprocessing applied to each of the RGB channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Spread of results: CIFAR 10, Width 4. The circle markers show the mean from 10 repeated runs for each of the 4 model types, using different random seeds for each repeat, but the same seed for each model. The error bars indicate the maximum and minimum errors over the 10 repeated runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Spread of results: CIFAR 100, Width 4. The circle markers show the mean from 10 repeated runs for each of the 4 model types, using different random seeds for each repeat, but the same seed for each model. The error bars indicate the maximum and minimum errors over the 10 repeated runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Wide ResNet architecture for Baseline CIFAR models where BN layers are used. This architecture is nearly identical to that of<ref type="bibr" target="#b10">[10]</ref>, except here there is no optional ReLU applied to the input. Note the ordering of the final layers, where global average pooling (GAP) is used after a final 1×1 convolutional layer, that reduces the number of channels to equal the number of classes, and then feeds directly to the softmax output (SM).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Post-activation Residual Block:</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Repeat 9 times</cell><cell></cell><cell></cell><cell></cell></row><row><cell>input</cell><cell>BN, conv</cell><cell>BN, ReLU, conv</cell><cell>BN, ReLU, conv</cell><cell>+</cell><cell>BN, ReLU, conv</cell><cell>BN, GAP, SM</cell></row><row><cell></cell><cell></cell><cell cols="2">Downsampling Residual Blocks</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>3x3 avg pool</cell><cell>double channels</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>stride 2</cell><cell cols="2">using zero padding</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>BN, ReLU, conv</cell><cell cols="2">BN, ReLU, stride 2 conv double channels</cell><cell>+</cell><cell></cell></row><row><cell cols="2">Full precision (32-bit weights) Fig. 2. 1-bit-per-weight: sReLU, conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>sReLU, scaled 1-bit conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc>CIFAR 10: Test-set error-rates. THE ENTRIES FOR RESNET 20-4 NETWORKS ARE THE MEAN VALUE FROM 10 REPEATS. THE ENTRIES FOR RESNET 20-10 NETWORKS ARE FOR SINGLE RUNS. THE FINAL COLUMN SHOWS THE DIFFERENCE BETWEEN THE ERRORS FOR SRELU ONLY NETWORKS COMPARED WITH THE BEST RESULT IN EACH ROW, AS INDICATED IN BOLD FONT. Test-set error-rates. THE ENTRIES FOR RESNET 20-4 NETWORKS ARE THE MEAN VALUE FROM 10 REPEATS. THE ENTRIES FOR RESNET 20-10 NETWORKS ARE FOR SINGLE RUNS. THE FINAL COLUMN SHOWS THE DIFFERENCE BETWEEN THE ERRORS FOR SRELU ONLY NETWORKS COMPARED WITH THE BEST RESULT IN EACH ROW, AS INDICATED IN BOLD FONT.For CIFAR, there is no clear advantage in using ELU layers rather than sReLU. The impact of removing BN is larger for full-precision weights compared with 1-bit-per weight. Baseline 1, where gains and shifts are learned is about as good for CIFAR 10 as Baseline 2 where they are not, but Baseline 2 is clearly better for CIFAR 100. Using a single BN-layer at the end of networks was mostly markedly better than using all sReLUs, and in some cases nearly as good as the best baseline. The mean-only-BN model outperforms the All ReLU and All ELU networks for CIFAR 100 and 1-bit per weight, halving the error rate gap to Baseline 2.</figDesc><table><row><cell>Model</cell><cell cols="2">Baseline 1 Baseline 2</cell><cell cols="3">Final BN only sReLU only sReLU gap</cell></row><row><cell>ResNet 20-4, 32 bit weights</cell><cell>3.97%</cell><cell>3.80%</cell><cell>4.42 %</cell><cell>4.67%</cell><cell>0.87%</cell></row><row><cell>ResNet 20-10, 32 bit weights</cell><cell>3.29%</cell><cell>3.57%</cell><cell>3.79%</cell><cell>4.36%</cell><cell>1.07%</cell></row><row><cell>ResNet 20-4, 1 bit weights</cell><cell>4.51%</cell><cell>4.65%</cell><cell>4.47%</cell><cell>4.66%</cell><cell>0.19%</cell></row><row><cell>ResNet 20-10, 1 bit weights</cell><cell>3.83%</cell><cell>3.65%</cell><cell>4.00%</cell><cell>3.74%</cell><cell>0.09%</cell></row><row><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR 100: Model</cell><cell cols="2">Baseline 1 Baseline 2</cell><cell cols="3">Final BN only sReLU only sReLU gap</cell></row><row><cell>ResNet 20-4, 32 bit weights</cell><cell>22.10%</cell><cell>19.53%</cell><cell>20.18%</cell><cell>22.24%</cell><cell>2.69%</cell></row><row><cell>ResNet 20-10, 32 bit weights</cell><cell>20.99%</cell><cell>17.05%</cell><cell>18.25%</cell><cell>20.97%</cell><cell>3.92%</cell></row><row><cell>ResNet 20-4, 1 bit weights</cell><cell>22.82%</cell><cell>22.07%</cell><cell>22.31%</cell><cell>23.69%</cell><cell>1.60%</cell></row><row><cell>ResNet 20-10, 1 bit weights</cell><cell>20.61%</cell><cell>18.22%</cell><cell>19.42%</cell><cell>20.74%</cell><cell>2.52%</cell></row><row><cell cols="3">• The importance of BN layers is data-set and model-size</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">dependent. Our findings show that the accuracy loss when</cell><cell cols="3">For width 4 networks, the variations within models across</cell></row><row><cell cols="3">replacing all BN layers with shifted ReLUs is larger on</cell><cell cols="3">runs for CIFAR 10 and 1-bit-per-weight indicated that</cell></row><row><cell cols="3">CIFAR 100 than on CIFAR 10. For all three datasets, and</cell><cell cols="3">any model could produce best results. For CIFAR 100,</cell></row><row><cell cols="3">especially ImageNet, the accuracy loss is very large for</cell><cell cols="3">the first three models exhibited this effect, but the sReLU</cell></row><row><cell cols="3">width-1 (non-wide), but not for wide variants, indicating</cell><cell cols="3">network clearly had a small gap in performance compared</cell></row><row><cell cols="2">BN is much more important for smaller models.</cell><cell></cell><cell cols="3">to other models. For 32-bit-per-weight models, gaps were</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">larger, but the top two models for CIFAR 10 were the two</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">baselines, while for CIFAR 100 they were Baseline 2 and</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">the final BN only model.</cell><cell></cell></row></table><note>••••••• Consistent with [10], for CIFAR 100, Baseline 1 where scales and shifts are learned causes a dramatic drop in accuracy relative to Baseline 2. Unlike [10], this is no</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc>ImageNet: Validation-set top-5 error-rates. CENTRE CROP MEANS A SINGLE 224 × 224 CROP FROM THE CENTRE OF THE VALIDATION IMAGE WAS USED. MULTICROP MEANS 25 DIFFERENT CROPS WERE RUN USED AND THEIR PREDICTIONS AVERAGED BEFORE CLASSIFYING, SIMILAR TO<ref type="bibr" target="#b32">[32]</ref>.</figDesc><table><row><cell cols="3">Bits per weight</cell><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell># Learned parameters</cell><cell>Test mode</cell><cell>Baseline 1 Final BN only</cell><cell>Gap</cell></row><row><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet 18-1</cell><cell>11.5M</cell><cell>centre crop</cell><cell>12.41%</cell><cell>15.50%</cell><cell>3.01%</cell></row><row><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet 18-1</cell><cell>11.5M</cell><cell>multi crop</cell><cell>9.03%</cell><cell>14.70%</cell><cell>5.67%</cell></row><row><cell></cell><cell>32</cell><cell></cell><cell cols="5">Ensemble of 3 ResNet 18-1</cell><cell>34.5M</cell><cell>centre crop</cell><cell>10.70%</cell><cell>14.00%</cell><cell>3.30%</cell></row><row><cell></cell><cell>32</cell><cell></cell><cell cols="5">Ensemble of 3 ResNet 18-1</cell><cell>34.5M</cell><cell>multi crop</cell><cell>7.95%</cell><cell>12.30%</cell><cell>4.35%</cell></row><row><cell></cell><cell>32</cell><cell></cell><cell></cell><cell cols="3">ResNet 18-2.5</cell><cell>70M</cell><cell>centre crop</cell><cell>9.20%</cell><cell>9.51%</cell><cell>0.3%</cell></row><row><cell></cell><cell>32</cell><cell></cell><cell></cell><cell cols="3">ResNet 18-2.5</cell><cell>70M</cell><cell>multi crop</cell><cell>6.91%</cell><cell>8.83%</cell><cell>1.92%</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet 18-1</cell><cell>11.5M</cell><cell>centre crop</cell><cell>17.55%</cell><cell>23.66%</cell><cell>6.11%</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet 18-1</cell><cell>11.5M</cell><cell>multi crop</cell><cell>12.80%</cell><cell>19.94%</cell><cell>7.14%</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell cols="5">Ensemble of 3 ResNet 18-1</cell><cell>34.5M</cell><cell>centre crop</cell><cell>15.48%</cell><cell>22.18%</cell><cell>6.70%</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell cols="5">Ensemble of 3 ResNet 18-1</cell><cell>34.5M</cell><cell>multi crop</cell><cell>11.38%</cell><cell>18.85%</cell><cell>7.47%</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="3">ResNet 18-2.5</cell><cell>70M</cell><cell>centre crop</cell><cell>11.51%</cell><cell>11.81%</cell><cell>0.3%</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="3">ResNet 18-2.5</cell><cell>70M</cell><cell>multi crop</cell><cell>8.48%</cell><cell>10.03%</cell><cell>1.58%</cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Error rate (%)</cell><cell>4 4.5 3.5</cell><cell></cell><cell></cell><cell></cell><cell cols="3">CIFAR 10, 32 bits</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CIFAR 10, 1 bit</cell></row><row><cell></cell><cell>3</cell><cell>Baseline 1</cell><cell>Baseline 2</cell><cell>Final BN only</cell><cell>All sReLU</cell><cell>All ELU</cell><cell>Mean-only-BN</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This extra storage can be avoided by recomputing BN layer outputs from its input feature map, but this slows down training slightly.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by a Discovery Project funded by the Australian Research Council (project number DP170104600).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<ptr target="http://arxiv.org/abs/1502.03167" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>arxiv.1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Group Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno>abs/1702.03275</idno>
		<ptr target="http://arxiv.org/abs/1702.03275" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;1MB model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>abs/1602.07360</idno>
		<ptr target="http://arxiv.org/abs/1602.07360" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BinaryConnect: Training Deep Neural Networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<idno>abs/1511.00363</idno>
		<ptr target="http://arxiv.org/abs/1511.00363" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural networks are robust to weight binarization and other non-linear distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
		<idno>abs/1606.01981</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<ptr target="http://arxiv.org/abs/1606.01981" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">XNOR-Net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1603.05279</idno>
		<ptr target="http://arxiv.org/abs/1603.05279" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training wide residual networks for deployment using a single bit for each weight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
		</author>
		<idno>arxiv: 1802.08530</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Arous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="192" to="204" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>see</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ser. ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ser. ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sharp minima can generalize for deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04933</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On the importance of single directions for generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06959</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04836</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade, this book is an outgrowth of a 1996 NIPS workshop</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>abs/1511.07289</idno>
		<ptr target="http://arxiv.org/abs/1511.07289" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>arxiv.1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>arxiv.1603.05027</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<ptr target="http://arxiv.org/abs/1708.04552" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1706.04599</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<ptr target="http://arxiv.org/abs/1706.04599" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Binareye: An always-on energy-accuracy-scalable binary CNN processor with all memory on chip in 28nm CMOS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bankman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Murmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verhelst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Custom Integrated Circuits Conference</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Eyeriss: An energyefficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISSCC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="262" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Brein memory: A single-chip binary/ternary reconfigurable inmemory deep neural network accelerator achieving 1.4 TOPS at 0.6 W</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ueyoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Orimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yonekawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takamaeda-Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ikebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kuroda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Motomura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
