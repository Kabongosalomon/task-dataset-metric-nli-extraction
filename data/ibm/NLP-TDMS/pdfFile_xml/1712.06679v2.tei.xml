<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqiang</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
							<email>3dymeng@mail.xjtu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In real-world crowd counting applications, the crowd densities vary greatly in spatial and temporal domains. A detection based counting method will estimate crowds accurately in low density scenes, while its reliability in congested areas is downgraded. A regression based approach, on the other hand, captures the general density information in crowded regions. Without knowing the location of each person, it tends to overestimate the count in low density areas. Thus, exclusively using either one of them is not sufficient to handle all kinds of scenes with varying densities. To address this issue, a novel end-to-end crowd counting framework, named DecideNet (DEteCtIon and Density Estimation Network) is proposed. It can adaptively decide the appropriate counting mode for different locations on the image based on its real density conditions. DecideNet starts with estimating the crowd density by generating detection and regression based density maps separately. To capture inevitable variation in densities, it incorporates an attention module, meant to adaptively assess the reliability of the two types of estimations. The final crowd counts are obtained with the guidance of the attention module to adopt suitable estimations from the two kinds of density maps. Experimental results show that our method achieves state-of-the-art performance on three challenging crowd counting datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The crowd counting task in the computer vision community aims at obtaining number of individuals appearing in specific scenes. It is the essential building block for highlevel crowd analysis, including crowd monitoring <ref type="bibr" target="#b2">[3]</ref>, scene understanding <ref type="bibr" target="#b40">[41]</ref> and public safety management <ref type="bibr" target="#b4">[5]</ref>.</p><p>Various methods have been proposed to tackle this problem. They could generally be classified into detection and regression based approaches. The detection based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b12">13]</ref> employ object detectors to lo-  <ref type="figure">Figure 1</ref>. Ablation studies of detection and regression based crowd counting on the ShanghaiTech PartB (SHB) dataset <ref type="bibr" target="#b41">[42]</ref>. Detection reliability decreases along with the increased crowd density, resulting in underestimated counts in those areas. Counts from density estimation tend to be overestimated in scenes with low densities. (a) Visualization of the detection results on a image from a Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> detector. (b) The density map on the same image from a CNN regression network <ref type="bibr" target="#b27">[28]</ref>. (c) The median object detection scores from the detector used in (a) versus the ground-truth counts. (d) The predictions from the network used in (b) versus the true crowd counts.</p><p>calize the position for each person. The number of detections is then treated as the crowd count. Early works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10]</ref> employ low-level features as region descriptors, followed by a classifier for classification. Benefiting from the recent progress in object detection using deep neural networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12]</ref>, in ideal images with relatively large individual sizes and sparse crowd densities, detection based counting could surpass human performance. Different from crowd counting by detection, regression based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b1">2]</ref> obtain the crowd count without explicitly detecting and localizing each individual. Pre-liminary works directly learn the mapping between features of image patches to crowd counts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40]</ref>. Recent regression based works improve the performance with Convolutional Neural Network (CNN) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> to output density maps of image patches. Integrating over the map will give the count for the patch. Regression based methods usually work well in crowded patches since they can capture the general density information by benefiting from the rich context in local patches.</p><p>In real-world counting applications, the crowd density varies enormously in spatial and temporal domains. In the spatial aspect, even in a same image, the density in some regions may be much higher than those of others. In some background regions, there may even be no person present. Meanwhile, it is also natural for the crowd volume to change along with time: a business street may have very high crowd volumes during the workdays, while the weekends counterparts are much lower. Intuitively, here comes a question: can crowd counting exclusively based on either regression or detection be enough to simultaneously handle high and low density scenes?</p><p>To answer this question, we study the performance of two types of approaches on the ShanghaiTech PartB (SHB) dataset <ref type="bibr" target="#b41">[42]</ref> collected from real street scenes with great variation in crowd densities. The result is illustrated in <ref type="figure">Figure 1</ref>. <ref type="figure">Figure 1(a)</ref> gives the detections from a fine-tuned Faster R-CNN head detector on a specific image: with the distance to the camera increasing, the crowd density and the number of missed detections rises. <ref type="figure">Figure 1</ref>(c) shows the relationship between median detection scores and ground-truth counts for 10,000 image patches with sizes of 256 × 128. It is clear that the score drops rapidly with the rise of the groundtruth count. We may therefore find that the reliability of detection based counting, reflected by the detection score, is highly correlated to the crowd density. In scenes with sparse crowds, the estimations are reliable, and the detection scores are also higher than those of congested scenes. On the other hand, in crowded scenes, the corresponding object sizes tend to be very small. Detection in these scenes is less reliable, leading to low detection scores and recall rates. Consequently, the predicted crowd counts will be underestimated; while the regression based counting methods could perform better on these occasions. <ref type="figure">Figure 1</ref>(b) provides the crowd density map visualization on the same image in (a), outputted by a 5-layer CNN based regression network with similar structure employed in <ref type="bibr" target="#b27">[28]</ref>. We find that the estimations in remote congested areas are quite reasonable. However, in background regions near the camera viewpoint, there exist false alarm hot spots on the pavement. The relationship between ground-truth counts and corresponding predictions is plotted in <ref type="figure">Figure 1(d)</ref>. Note that the prediction dots for patches with lower ground-truth counts are mostly above the dashed line. This indicates that prediction counts in these scenes are mostly larger than the ground-truth. Hence, being not aware of the location of each individual, and directly applying the regression based approaches to low density scenes may lead to overestimated results.</p><p>Based on the above ablation analysis, we may find that the detection and regression based counting approaches have their different strengths on different crowd densities. The regression based method is preferred for congested scenes. Without localization information for each person, applying them to low density scenes tends to overestimate counts. The detection based approach could localize and count each person precisely on these occasions since they are expected settings for object detectors. However, its reliability degenerates in crowded scenes due to small target sizes and occlusion.</p><p>Therefore, we may find that a conventional crowd counting method which only relies on either detection or regression is limited when handling real scenes with unavoidable density variations. An ideal counting method, on the other hand, should have an adaptive ability to choose the appropriate counting mode according to the crowd density: in low density scenes, it is expected to count by localizing as an object detector; whereas in congested scenes, it should behave in a regression manner. Motivated by this understanding, we propose a novel crowd counting framework named as DecideNet (DEteCtIon and Density Estimation Network), as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. To the best of our knowledge, Deci-deNet is the first framework, which is capable of perceiving the crowd density for each pixel in a scene and adaptively deciding the relative weights for detection and regression based estimations.</p><p>In detail, for a given scene, the DecideNet first estimates two kinds of crowd densities maps by detecting individuals and regressing pixel-wise densities, respectively. To capture the subtle variation in crowd densities, an attention module QualityNet is proposed to assess the reliability of two types of density maps with the additional supervision of detection scores. The final count is obtained under guidance from QualityNet to allocate adaptive attention weights for the two density maps. Parameters in our proposed DecideNet are end-to-end learnable by minimizing a joint loss function.</p><p>In summary, we make the following contributions:</p><p>• We find that real-world crowd counting occasions are frequently faced with great density variations. While existing estimation methods, which either rely exclusively on detection or regression, are unable to provide precise estimations along the whole density range. • Based on the complementary property of two types of crowd counting methods, we design a novel framework DecideNet, which can capture this variation and estimate optimal counts by assigning adaptive weights for both detection and regression based estimations.</p><p>• Experimental results reveal that our method achieves state-of-the-art performance on public datasets with varying crowd densities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Crowd counting by detection. Early works addressing the crowd counting problem major follow the counting by detection framework. Region proposal generators <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33]</ref> are firstly used to propose potential regions that include persons. Low-level features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref> are then used for feature representation. Different binary classifiers including Naive Bayes <ref type="bibr" target="#b3">[4]</ref>, Random Forest <ref type="bibr" target="#b22">[23]</ref> and their variations <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b10">11]</ref> are trained with these features. The crowd count is the number of positive samples outputted by the classifier on a test image. Global detection scores are employed to estimate crowd densities and utilized for object tracking in <ref type="bibr" target="#b25">[26]</ref>. Recent approaches seek the end-to-end crowd counting solution by CNN based object detectors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7]</ref> and greatly improve the counting accuracy. Though detection based crowd counting is successful for scenes with low crowd density, its performance on highly congested environments is still problematic. On these occasions, usually only partial of the whole objects are visible, posing great challenge to object detectors for localization. Therefore, part and shape based models are introduced in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39]</ref>, where ensembles of classifiers are built for specific body parts and regions. Although these methods mitigate the issue in some degree, counting in evident crowded scenes still remains challenging, since objects in those areas are too small to be detected.</p><p>Crowd counting by regression. Different from counting by detection, counting by regression estimates crowd counts without knowing the location of each person. Preliminary works employ edge and texture features such as HOG and LBP to learn the mapping from image patterns to corresponding crowd counts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40]</ref>. Multi-source information is utilized <ref type="bibr" target="#b14">[15]</ref> to regress the crowd counts in extreme dense crowd images. An end-to-end CNN model adopted from AlexNet is constructed <ref type="bibr" target="#b35">[36]</ref> recently for counting in extreme crowd scenes. Later, instead of direct regressing the count, the spatial information of crowds are taken into consideration by regressing the CNN feature maps as crowd density maps <ref type="bibr" target="#b40">[41]</ref> . Observing that the densities and appearances of image patches are of large variations, a multicolumn CNN architecture is developed for density map regression <ref type="bibr" target="#b41">[42]</ref>. Three CNN columns with different receptive fields are explicitly constructed for counting crowds with robustness to density and appearance changes. Similar frameworks are also developed in <ref type="bibr" target="#b21">[22]</ref>, where a Hydra-CNN architecture is designed to estimate crowd densities in a variety of scenes. Better performance can be obtained by further exploiting switching structures <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17]</ref> or con-textual correlations using LSTM <ref type="bibr" target="#b28">[29]</ref>. Though counting by regression is reliable in crowded settings, without object location information, their predictions for low density crowds tend to be overestimated. The soundness of such kind of methods relies on the statistical stability of data, while in such scenarios the instance number is too small to help explore the its intrinsic statistical principle. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Crowd Counting by DecideNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem formulation</head><p>Our solution formulates the crowd counting task as a density map estimation problem. It requires N training images I 1 , I 2 , · · · , I N as inputs. For a specific image I i , a collection of c i 2D points P gt i = {P 1 , P 2 , · · · , P ci } is provided by the dataset <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, indicating the ground-truth head positions in the image I i . The ground-truth crowd density map D gt i of I i is generated by convolving annotated points with a Gaussian kernel N gt (p|µ, σ 2 ) <ref type="bibr" target="#b21">[22]</ref>. Therefore, the density at a specific pixel p of I i could be obtained by considering the effects from all the Gaussian functions centered by annotation points, i.e.,</p><formula xml:id="formula_0">∀p ∈ I i , D gt i (p|I i ) = P ∈P gt i N gt (p|µ = P, σ 2 ). (1)</formula><p>Summing over the density values of all pixels over the entire image I i , the total person count c i of I i can be acquired:</p><formula xml:id="formula_1">p∈Ii D gt i (p|I i ) = c i .</formula><p>For a counting model parameterized by Ω, its objective is to learn a non-linear mapping for I i , whereas the difference between the prediction density map D out i (p|I i ) and the ground-truth D gt i (p|I i ) is minimized.</p><p>Traditional crowd counting by density estimation methods regress density maps by minimizing the pixel-wise Euclidean loss to the ground-truth <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b21">22]</ref>. However, as we have analyzed in introduction, counting by purely regression would result in the overestimation problem on occasions with low crowd densities. Oppositely, counting by detection works comparably better in those scenes, since low crowd density is the expecting environment to an object detector.</p><p>In practical applications, the crowd density varies both spatially and temporally. Hence, deciding the crowd counts exclusively based on either regression or detection is insufficient. DecideNet is motivated by their complementary property to address this problem. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, instead of counting people either by merely regressing density maps, or applying an object detector over the whole image, DecideNet simultaneously estimates crowd counting with both detection and regression modules. Later, an attention block is utilized to decide which estimation result should be adopted for a specific pixel. Three CNN blocks are included in our framework: the RegNet, the DetNet and the Quali-tyNet, parameterized by Ω = (Ω det , Ω reg , Ω qua ). The parameters for three CNN blocks could be jointly learned on the training set. The RegNet block counts crowds in the absence of localizing each individual. Without knowing the specific location of each head in the input image patch, it directly estimates the crowd density for all the pixels in I i with a fully convolutional network:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The RegNet block</head><formula xml:id="formula_2">F reg (I i |Ω reg ) = D reg i (p|Ω reg , I i ).<label>(2)</label></formula><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the RegNet block consists of 5 convolutional layers. Because it is designed to capture the general crowd density information, larger filters' receptive fields will grasp more contextual details, which is more beneficial for modeling the density maps. Therefore, in our implemented RegNet block, the "conv1" layer has 20 filters with a 7×7 kernel size. 40 filters with a 5×5 kernel size are set as the "conv2" layer. In order to capture scale and orientation invariant person density features, the "conv1" and "conv2" layers are followed by two 2 × 2 max-pooling layers. The "conv3" and "conv4" layers both have 5 × 5 filter sizes with 20 and 10 filters, respectively. Since the density estimation result could be viewed as a CNN feature map with only one channel, we add a "conv5" layer with only one filter and a "1 × 1" filter size. This layer is responsible to return the regression based crowd density map D reg i , in which value on each pixel represents the estimated count at that point. A ReLU unit is applied after the "conv5" layer ensuring that the output density map will not contain negative values. To handle varying perspectives, crowd densities and appearances, existing density estimation methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17]</ref> consist of several CNN structures like the RegNet block. However, without the prior knowledge about the exact position of each person in the image patches, the network purely decides the crowd density based on the raw image pixels. This regression methodology may be accurate in image patches with relatively large crowd densities, while it tends to overestimate the crowd counts in sparse or even "noperson" (background) scenes. In our proposed DecideNet architecture, the DetNet is designed to address this issue by generating the "location aware" detection based density map D det i . The motivation is intuitive and simple: sparse and non-crowded image patches are expected settings for present CNN based object detectors. Therefore, compared to use regression networks to count on these patches, using the prior knowledge from outputs of object detectors should substantially relieve the overestimation problem.</p><p>The DetNet block, illustrated in <ref type="figure">Figure 4</ref> is built based on the above assumption. It could be viewed as an extension of the Faster-RCNN network <ref type="bibr" target="#b13">[14]</ref> for head detection on the basis of the ResNet-101 architecture. To be specific, we design a Gaussian convolutional layer and plug it after  is not accurate on crowded occasions due to the low detection confidence resulted from the small object size and occlusion. On the contrary, the regression based map D reg i , which is unaware of individual locations, is the preferred estimation for these scenes: the full convolutional network is capable of capturing rich context crowd density information. Intuitively, one may think that fusing D reg i and D det i by applying average or max pooling <ref type="bibr" target="#b36">[37]</ref> may obtain better results on varying density crowds. Nevertheless, even in the same scene, the density may differ significantly in different parts or time intervals. Therefore, the importance between D det i and D reg i also changes correspondingly for instant pixel values in I i . In DecideNet, we propose an attention block QualityNet, shown in <ref type="figure" target="#fig_5">Figure 5</ref> to model the selection process for optimal counting estimations. It captures the different importance weight of two density maps by dynamically assessing the qualities of them for each pixel.</p><p>For a given I i , the QualityNet block firstly upsamples D det i and D reg i to the same size of I i . Then D det i , D reg i and I i are stacked together as the QualityNet input with 5 channels. Four fully convolutional layers and a pixel-wise sigmoid layer is followed to output a probabilistic attention map K i (p|Ω qua , I i ). We define the specific value of K i (p|Ω qua , I i ) at the pixel p reflects the importance of the detection based density map D det i , compared to the regression counterpart D reg i . As a result, the QualityNet block could decide the relative reliability (i.e., the quality) between D det i and D reg i . A higher K i (p|Ω qua , I i ) at pixel p means a higher attention we should rely on the detection, rather than the regression density estimation for p. Hence, we could further define the final density map estimation D f inal i (p|I i ) as a weighted sum between two density maps D reg i and D det i , guided by the attention map K i :</p><formula xml:id="formula_3">D f inal i (p|I i ) =K i (p|Ω qua , I i ) D det i (p|Ω det , I i )+ (J − K i (p|Ω qua , I i )) D reg i (p|Ω reg , I i ),<label>(4)</label></formula><p>whereas is the Hadamard product for two matrices and the J is an all-one-matrix with the same size of K i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model Learning</head><p>Parameters of DecideNet Ω consist of three parts: Ω reg , Ω det and Ω qua . Hence, we generalize the training process as a multi-task learning problem. The overall loss function L decide , is given by Eq. <ref type="formula" target="#formula_4">(5)</ref>:</p><formula xml:id="formula_4">L decide = L reg + L det + L qua ,<label>(5)</label></formula><p>whereas the L reg , L det and L qua are the losses for Reg-Net, DetNet and QualityNet, respectively. L decide could be optimized via Stochastic Gradient Descent with annotated training data. In each iteration, gradients for L reg , L det and L qua are alternatively calculated and employed to update corresponding parameters. To be specific, for the loss of the RegNet component, we employ the pixel-wise mean square error as the loss function. That is:</p><formula xml:id="formula_5">L reg = 1 N i p∈Ii D reg i (p|Ω reg , I i ) − D gt i (p|I i ) 2 ,<label>(6)</label></formula><p>whereas N is the total number of training images.</p><p>For the DetN et block, different from the regression counterpart, the responses on the density map D det i mostly concentrate on the detected head centers. Directly minimizing the difference between D det i and D gt i involves in overwhelmed negative pixel samples, i.e., background pixels without head detections. Hence, instead of using the pixel-wise Euclidean loss as error measurement, we employ the bounding boxes as supervision. In this way, optimizing Ω det is equivalent to minimizing the classification and localization error in the original Faster R-CNN <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_6">L det = 1 N i L cls (P det i , P gt i |Ω det ) + L loc (P det i , P gt i |Ω det ) .<label>(7)</label></formula><p>Due to the fact that only the centers of individuals' heads are provided as the annotation on crowd density estimation datasets, we manually label the bounding boxes on partial of the training set points. Later, we employ the average width and height of them for the bounding box supervision in Eq. <ref type="bibr" target="#b6">(7)</ref>. The loss function L qua for the attention module Quali-tyNet should measure two kinds of errors. One is the difference between the final crowd density map D f inal i and the ground-truth density map D gt i . This error is similar to that we have defined in L reg . The second error measures the quality of the output probabilistic map K i in QualityNet. Recall that K i (p|Ω qua , I i ) is the confidence of how reliable the detection result is at pixel p in the image I i . As we analyzed in <ref type="figure">Figure 1(c)</ref>, this confidence could be reflected by the object detection score S det (p|I i ) at p. Therefore, we employ the Euclidean distances between the probabilistic attention map K i and object detection score map S det as the second error component in L qua . From another perspective, this error could be considered as a regularization term over the QualityNet parameters Ω qua , by incorporating detection scores as prior information. In experiment evaluation, we will show that this regularization is indispensably beneficial to the performance of our proposed DecideN et architecture. Since the object detection qualities are brought into this loss function, we name it as the "quality-aware" loss. The final formulation of this loss L qua is defined as following:</p><formula xml:id="formula_7">L qua = 1 N i p∈Ii D f inal i (p|Ω qua , I i ) − D gt i (p|I i ) 2 + λ K i (p|Ω qua , I i ) − S det (p|I i ) 2 ,<label>(8)</label></formula><p>where λ is the hyper-parameter to balance the importance between two errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation settings</head><p>Our proposed method is evaluated on three major crowd counting datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> collected from real-world surveillance cameras. For all datasets, DecideNet is optimized with 40k steps of iterations. We set the initial learning rate at 0.005 and cut it by half in each 10k steps. Then the best model is selected over the validation data. Instead of sending the whole image to DecideNet during training, we follow the strategy used in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22]</ref> to crop images into 4 × 3 patches. In this way, the number of samples for training the regression network is boosted. Each patch is then augmented by random vertical and horizontal flipping with a probability of 0.5. We also add uniform noise ranging in [−5, 5] on each pixel in the patch with a probability of 0.5 for data augmentation. To optimize the parameters for the RegNet and the QualityNet, the ground-truth density maps are obtained by applying the Gaussian kernel N gt (p|µ, σ 2 ) with σ = 4.0 and a window size of 15. In each iteration, the object detection score map S det (p|I i ) is acquired by evaluating I i on the DetNet. For each pixel p in the detected bounding boxes, the value of S det (p|I i ) is filled with corresponding detection score. For the rest of pixels which are not included in any bounding boxes, they are filled with a default value set at 0.1. The score map is downsampled to the same size of K i in order to calculate the "quality-aware" loss L qua . We follow the convention of existing works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b22">23]</ref> to use the mean absolute error (MAE) and mean squared error (MSE) as the evaluation metric. The MAE metric reveals the accuracy of the algorithm for crowd estimation, while the MSE metric indicates the robustness of estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">The Mall dataset</head><p>The Mall dataset <ref type="bibr" target="#b5">[6]</ref> contains 2000 frames, collected in a shopping mall. Each frame has a fixed resolution of 320 × 240. We follow the pre-defined settings to use the first 800 frames as the training set and the rest 1200 frames as the test set. The validation set is selected randomly from 100 images in the training set. We compare our DecideNet with both detection based approaches: SquareChn Detector <ref type="bibr" target="#b0">[1]</ref>, R-FCN <ref type="bibr" target="#b6">[7]</ref>, Faster R-CNN <ref type="bibr" target="#b24">[25]</ref>; and regression based approaches: Count Forest <ref type="bibr" target="#b22">[23]</ref>, Exemplary Density <ref type="bibr" target="#b37">[38]</ref>, Boosting CNN <ref type="bibr" target="#b34">[35]</ref>, MoCNN <ref type="bibr" target="#b16">[17]</ref>, Weighted VLAD <ref type="bibr" target="#b29">[30]</ref>. The evaluation results are exhibited in <ref type="table" target="#tab_0">Table 1</ref>. From <ref type="table" target="#tab_0">Table 1</ref>, we can observe the detection based approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref> generally perform worse than the regression counterparts. Even the most recent CNN based object detectors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> still have a large performance gap to the CNN based regression approaches <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30]</ref>. Our proposed DecideNet obtains the minimum error on both MAE and MSE metrics. Compared to the best approach "Boosting CNN", which based on regression, DecideNet reveals 0.49 point improvement on MAE metric. This is achieved without using the ensemble scheme employed by the "MoCNN" and "Boosting CNN" methods. Moreover, the MSE metric of the DecideNet is merely 1.90. This is significantly lower than other state-of-the-art methods, which either use detection or regression approach. This gain rationally results from our density estimations formulated from both detection and regression results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">The ShanghaiTech PartB dataset</head><p>Method MAE MSE R-FCN <ref type="bibr" target="#b6">[7]</ref> 52.35 70.12 Faster R-CNN <ref type="bibr" target="#b24">[25]</ref> 44.51 53.22 Cross-scene <ref type="bibr" target="#b40">[41]</ref> 32.00 49.80 M-CNN <ref type="bibr" target="#b41">[42]</ref> 26  We also perform the evaluation experiments on the Shang-haiTech PartB (SHB) <ref type="bibr" target="#b41">[42]</ref> crowd counting dataset, which is among the largest datasets captured in real outdoor scenes. It consists of 716 images taken from business streets in Shanghai, in which 400 of them are pre-defined training set and the rest are the test set. Compared to the Mall dataset, it poses very diverse scene and perspective types over greatly changing crowd densities. We use 50 randomly selected images in the training set for validation. Since the resolution of each image is 768×1024, the patches are cropped from the original image with a size of 256 × 256 during training. Our evaluation result and the comparison to other state-of-the-art methods are shown in <ref type="table" target="#tab_2">Table 2</ref>. Due to the large variation in density and object size on the SHB dataset, the detection based approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> perform worse than the others relying on regression. Specifically, the ensemble and fusion strategy is employed by the M-CNN <ref type="bibr" target="#b41">[42]</ref>, Switching-CNN <ref type="bibr" target="#b27">[28]</ref>, CP-CNN <ref type="bibr" target="#b30">[31]</ref> in <ref type="table" target="#tab_2">Table 2</ref>. Compared to the Mall dataset, the challenging SHB dataset leads to much higher MAE and MSE on all the methods. Even though, our proposed method (DecideNet; DecideNet+R3, which trained with an additional R3 stream in Switching-CNN) is very competitive to existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">The WorldExpo'10 dataset</head><p>The WorldExpo'10 dataset <ref type="bibr" target="#b40">[41]</ref> includes 1132 annotated video sequences collected from 103 different scenes in the World Expo 2010 event. There are a total number of 3980 frames with sizes normalized to 576 × 720. The patch size we used for training is 144 × 144. The training set consists of 3380 frames and the rests are used for testing. Since the Region Of Interest (ROI) are provided for test scenes (S1-S5), we follow the fashion of previous method <ref type="bibr" target="#b31">[32]</ref> to only count persons within the ROI area. We use the same metric, namely MAE, suggested by the author <ref type="bibr" target="#b40">[41]</ref> for evaluation. The results of our proposed approach on each test scene and the comparisons to other methods are listed in <ref type="table">Table 3</ref>.  <ref type="table">Table 3</ref>. Comparison results of different methods on 5 scenes in the WorldExpo'10 dataset.</p><p>From <ref type="table">Table 3</ref>, we can notice that our proposed approach achieves an average MAE at 9.23 across all 5 scenes. This is the best performance among those obtained by all compared methods, revealing 0.17 improvement on the second best "Switching-CNN" approach. It is not that significant, because our error on S4 is a little bit higher. The reason may lie on the fact that people in S4 majorly gather in crowds at remote areas, posing great challenge for the DetN et to output meaningful estimations. Therefore, the estimation on S4 are mostly relied on the outputs from RegN et. While without the ensemble regression structure, using the RegN et only may not be able to exhibit the superior counting precision. We can also notice that the prediction counts of different state-of-the-art methods alter considerably on the 5 scenes, revealing different approaches have their own strengths to specific scenes. However, DecideNet obtains three minimum MAE errors when compared to other approaches. This indicates DecideNet having a good generalization ability and prediction robustness on different scenes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Effects of different components in DecideNet</head><p>To analyze effects of each components of the proposed Deci-deNet, we conduct ablation studies on the Mall and SHB dataset. The qualitative results are listed in <ref type="table" target="#tab_5">Table 4</ref>, which shows several interesting observations. First, using the estimations exclusively from either the RegNet ("RegNet only"), or DetNet ("DetNet only") only obtains fair results on both datasets. The estimations from the RegNet have lower error than the detection counterparts. This is possibly due to the fact that most of the image regions are with high crowd density on both datasets. Further, late fusion by averaging two classes of density maps ("RegNet+DetNet (Late Fusion)") exhibits improvements than "RegNet only" and "Det-Net only" on that SHB dataset. While on the Mall dataset, it only achieves a mediocre result between two kinds of density estimations. This indicates that direct late fusion is not robust enough to obtain better results across all kinds of datasets. Second, with DecideNet, even training without the object detection scores regularization ("RegNet+DetNet+QualityNet"), we obtain significant MAE and MSE decrease as compared with those obtained by the previous methods. Compared to late fusion, it almost decreases the MAE by half on two datasets, revealing the power of the attention mechanism. Last but not least, adopting the "quality-aware" loss during training ("RegNet+DetNet+QualityNet (quality-aware loss)"), the MAE and MSE errors are further reduced on two datasets. In particular, the MSE decreases from 41.86 to 31.98 on SHB dataset: this shows that the loss can substantially increase the prediction stability on challenging datasets with great variations.</p><p>In <ref type="figure" target="#fig_6">Figure 6</ref>, we show the relationships between different crowd count predictions and the ground-truth crowd counts on the test sets of two datasets. Note that the horizontal axes "image id" are sorted in ascending order by the number of ground-truth crowd counts. Clearly, when the numbers of ground-truth crowd count are small, the regression based results from the RegNet overestimate the estimations: the blue lines are above the ground-truth red lines in the first half part of the horizontal axis in both figures. On the opposite, the detection based result curves (the green lines) fit the red lines well at that region on two datasets. However, when the numbers of ground-truth count increase, the estimations of the detection based density map become considerably lower than the red lines, particularly after the second half parts of the horizontal axis. The blue lines fit the ground-truth lines best in the middle part of the horizontal axes. This verifies our observation that regression based estimations are more suitable for high crowded patches. Directly applying the late fusion (the purple curves) helps to a certain extent, while its predicted counts are not stable along all images. At last, the cyan lines, which represent DecideNet outputs, indicate the smallest differences to the ground-truth curves along all parts of the horizontal axes. That is, the DecideNet trained with "quality-aware" loss exhibits the best estimation results for all kinds of crowd densities on two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Visualization on density maps</head><p>To better understand what is learned in our proposed model, we visualize three categories of crowd density maps in the SHB dataset from three blocks: RegNet, DetNet and QuialityNet in  Predicted count (GT: 266) 293.6 230.0 252.5 <ref type="figure">Figure 7</ref>. The visualization results of three types of density maps on the SHB dataset (best viewed in color).</p><p>We can discover that the outputs of regression based density maps D reg i (on the most left column) exhibit diffused density estimations along the image regions. For the remote areas with highly congested crowds, such predictions from the RegNet are reliable. However, when it comes to the nearby regions with lower crowd density, the results are not satisfactory: some single person bodies are erroneously predicted with very high density. The prediction counts of the D reg i are also larger than the ground-truth (GT) counts, implying the occurrence of overestimation issue. Compared to D reg i , the detection based density maps D det i (the middle column) are very different: the predicted peak regions are concentrated on the center of heads. This is resulted from the fact that these maps are generated from outputs of head detectors. We can further observe that the detection based density results are pretty good in nearby low density regions of the given image, while not all the heads are marked with high prediction peaks in the remote areas. The underestimated predicted counts of D det i also reflect this phenomenon. With the attention information from the Qual-ityNet, final density maps in the right column reveal very good characteristics: in the nearby region, the estimation prefers the detection results. Persons in those areas share very similar estimation patterns with D det i . Oppositely, in remote and congested regions, instead of the "concentrated dot" patterns, the density maps are diffused. DecideNet considers the regression based results D reg i are more reliable for those cases. This confirms that the Quali-tyNet block is able to assess the reliability of the corresponding density map value for a specific pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, a novel end-to-end crowd counting architecture named DecideNet has been proposed. It is motivated by the complementary performance of detection and regression based counting methods under situations with varying crowd densities. To the best of our knowledge, DecideNet is the first framework to estimate crowd counts via adaptively adopting detection and regression based count estimations under the guidance from the attention mechanism. We evaluate the framework on three challenging crowd counting benchmarks collected from real-world scenes with high variation in crowd densities. Experimental results confirm that our method obtains the state-of-the-art performance on three public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgment</head><p>Jiang Liu and Alexander Hauptmann are supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) contract number D17PC00340. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation/herein. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government. Chenqiang Gao and Deyu Meng are supported by the China NSFC projects with No.61571071, No.61661166011, No.61721002.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of our proposed DecideNet. Image patches are sent to the RegNet and DetNet blocks for two types of density maps D reg i and D det i estimation. The final density map D f inal i is outputted by the QualityNet, which adaptively decides the attention weight between two density maps for each pixel. Three blocks are jointly learned on the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The RegNet block consisting of 5 fully convolutional layers. It outputs the crowd density map D reg i of each pixel in image patches without predicting the head locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3. 3 .Figure 4 .</head><label>34</label><figDesc>The DetNet block box and score ROI pooling box and score box and score Gaussian convolution Faster R-CNN with ResNet101 backbone … * The proposed DetNet block is built upon the Faster R-CNN network. A Gaussian convolutional layer is plugged after the bounding box outputs to generate the detection based crowd density map D det i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the bounding box outputs of the original Faster-RCNN network. The Gaussian convolutional layer employs a constant Gaussian function N det (p|µ = P, σ 2 ), to convolve over the centers of detected bounding boxes P det i on the original image patch. The detection based density map D det i is obtained by this layer, i.e., D det i (p|Ω det , I i ) = P ∈P det i N det (p|µ = P, σ 2 ). (3) Since the pixel values of D det i are obtained by considering the impact from the points in detection output P det i , D det i is a "location aware" density map. Compared to D reg i from the output of RegN et, responses of D det i are more concentrated on specific head locations. The difference between them is obvious in D reg i of Figure 3 and D det i of Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The QualityNet block: stacking two density maps and the original image Ii as input, it outputs a probabilistic attention map Ki(p|Ωqua, Ii). The final density estimation D f inal i is jointly determined by Ki, D reg i and D det i . Herein, we have described the details about obtaining two kinds of density maps: D reg i and D det i for a given image I i . The detection based map D det i employs object detection results for density estimation. Therefore, it could count persons precisely in sparse density scenes by localizing their head positions. However, counting via D det i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Prediction and the ground-truth crowd counts on the test sets of the Mall (left) and SHB (right) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 7 (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison results of different methods on the Mall dataset. The MAE and MSE error of our proposed DecideNet is significant lower than other approaches.</figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell>MSE</cell></row><row><cell>SquareChn Detector [1]</cell><cell>20.55</cell><cell>439.1</cell></row><row><cell>R-FCN [7]</cell><cell>6.02</cell><cell>5.46</cell></row><row><cell>Faster R-CNN [25]</cell><cell>5.91</cell><cell>6.60</cell></row><row><cell>Count Forest [23]</cell><cell>4.40</cell><cell>2.40</cell></row><row><cell>Exemplary Density [38]</cell><cell>1.82</cell><cell>2.74</cell></row><row><cell>Boosting CNN [35]</cell><cell>2.01</cell><cell>N/A</cell></row><row><cell>MoCNN [17]</cell><cell>2.75</cell><cell>13.40</cell></row><row><cell>Weighted VLAD [30]</cell><cell>2.41</cell><cell>9.12</cell></row><row><cell>DecideNet</cell><cell>1.52</cell><cell>1.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison results of different methods on the Shang-haiTech PartB dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Qualitative results of different DecideNet components on the Mall and SHB dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Regression based density map Detection based density map Final density map</head><label></label><figDesc></figDesc><table><row><cell>Predicted count (GT: 429) 464.3</cell><cell>367.2</cell><cell>403.1</cell></row><row><cell>Predicted count (GT: 293) 305.5</cell><cell>219.4</cell><cell>284.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ten years of pedestrian detection, what have we learned? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>abs/1411.4304</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crowdnet: a deep convolutional network for dense crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-S</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian poisson regression for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Videos from the 2013 boston marathon: An event reconstruction dataset for synchronization and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hough forests for object detection, tracking, and action recognition. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">People counting based on head detection combining adaboost and cnn in crowded surveillance environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Marked point processes for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Beyond counting: Comparisons of density maps for crowd analysis tasks-counting, detection, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10118</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mixture of counting cnns: Adaptive integration of cnns specialized to specific appearance for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09393</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pedestrian detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimation of number of people in crowded scenes using perspective transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguiness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00220</idno>
		<title level="m">Fully convolutional crowd counting on highly congested scenes</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kozakaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Density-aware person detection and tracking in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting pedestrians by learning shapelet features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sabzmeydani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end crowd counting via joint learning local and global count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Crowd counting via weighted vlad on dense attribute feature maps. TCVST</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey of recent advances in cnn-based single image crowd counting and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to count with cnn boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A novel learningbased frame pooling method for event detection. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast visual object counting via example-based density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Detection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors. IIJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Crowd density estimation based on rich features and random projection forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Singleimage crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Segmentation and tracking of multiple humans in crowded environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
