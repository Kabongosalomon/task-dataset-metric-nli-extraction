<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
							<email>irie@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>D-52056</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
							<affiliation key="aff1">
								<address>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
							<affiliation key="aff1">
								<address>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bruguier</surname></persName>
							<affiliation key="aff1">
								<address>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rybach</surname></persName>
							<affiliation key="aff1">
								<address>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff1">
								<address>
									<postCode>94043</postCode>
									<settlement>Google, Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: End-to-end speech recognition</term>
					<term>word-pieces</term>
					<term>graphemes</term>
					<term>phonemes</term>
					<term>sequence-to-sequence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In conventional speech recognition, phoneme-based models outperform grapheme-based models for non-phonetic languages such as English. The performance gap between the two typically reduces as the amount of training data is increased. In this work, we examine the impact of the choice of modeling unit for attention-based encoder-decoder models. We conduct experiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using various target units (phoneme, grapheme, and word-piece); across all tasks, we find that grapheme or word-piece models consistently outperform phoneme-based models, even though they are evaluated without a lexicon or an external language model. We also investigate model complementarity: we find that we can improve WERs by up to 9% relative by rescoring N-best lists generated from a strong word-piece based baseline with either the phoneme or the grapheme model. Rescoring an N-best list generated by the phonemic system, however, provides limited improvements. Further analysis shows that the word-piece-based models produce more diverse N-best hypotheses, and thus lower oracle WERs, than phonemic models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sequence-to-sequence learning <ref type="bibr" target="#b1">[2]</ref> based on encoder-decoder attention models <ref type="bibr" target="#b2">[3]</ref> has become popular for both machine translation <ref type="bibr" target="#b3">[4]</ref> and speech recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Such models are typically trained to output character-based units: graphemes, byte-pair encodings (BPEs) <ref type="bibr" target="#b9">[10]</ref>, or wordpieces <ref type="bibr" target="#b10">[11]</ref>, which allow the model to directly map the framelevel input audio features to the output word sequence, without using a hand-crafted pronunciation lexicon. Thus, when using such character-based output units, end-to-end speech recognition models <ref type="bibr" target="#b11">[12]</ref> jointly learn the acoustic model, pronunciation model, and language model within a single neural network. In fact, such models outperform conventional hybrid recognizers <ref type="bibr" target="#b12">[13]</ref> when trained on sufficiently large amounts of data <ref type="bibr" target="#b8">[9]</ref>.</p><p>One of the main advantages of character-based sequenceto-sequence models lies in their simplicity: both for training, as well as decoding. In fact, the use of characters as units for acoustic modeling has a long history for conventional HMM-based automatic speech recognition (ASR) systems (e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, inter alia). In the context of conventional ASR systems, for non-phonetic languages such as English, where the correspondence between orthography and pronunciation is less *Work performed during internship at Google. We thank Tara Sainath and Yu Zhang for helpful discussion, and Jinxi Guo for sharing his language model setup. An initial version of this paper appears as a pre-print <ref type="bibr" target="#b0">[1]</ref>. clear, previous works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> have found that phoneme-based models outperform grapheme-based models; grapheme-based systems approach the performance of phoneme-based systems only when much larger amounts of training training data are available <ref type="bibr" target="#b15">[16]</ref>. It is therefore, natural to ask whether similar observations also apply to recently proposed attention-based encoder-decoder models: specifically, how do attention-based encoder-decoder models perform when using phonemes instead of character-based output units? To the best of our knowledge, this question has only been empirically investigated in the setting where a large amount of labeled training data are available. In previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, it has been empirically shown that the grapheme-based encoder-decoder models outperform the phoneme-based approach, while <ref type="bibr" target="#b16">[17]</ref> find that use of lexica is still useful for recognizing rare words such as named entities.</p><p>In this work, we first investigate whether the previous result <ref type="bibr" target="#b16">[17]</ref> which establishes the dominance of lexicon-free graphemic models over the phoneme-based models also hold on tasks with smaller amounts of training data. We carry out evaluations on the three subsets of the LibriSpeech task <ref type="bibr" target="#b18">[19]</ref>: 100hr, 460hr, and 960hr, where we find that grapheme or word-piece models do indeed consistently outperform phoneme-based models, even when training data is limited. In Sec. 6, we further investigate the benefits offered by phonemic models by studying the complementarity of different units. In experimental evaluations, we find that simple N-best list rescoring results in large improvements in WER. Finally, we conduct a detailed analysis of the differences in the hypotheses produced by the models with various output units, in terms of quality of the top hypotheses, as well as the oracle error rate of the N-best list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sequence-to-Sequence Speech Models</head><p>All our models are Listen, Attend, and Spell (LAS) <ref type="bibr" target="#b11">[12]</ref> speech models. The LAS model, which is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, has encoder, attention, and decoder modules. The encoder transforms the input frame-level audio feature sequence into a sequence of hidden activations. The attention module summarizes the encoder sequence into a single vector for each prediction step, and finally, the decoder models the distribution of the output sequence conditioned on the history of previously predicted labels. Both the encoder and the decoder are modeled using recurrent neural networks, and thus the entire model can be jointly optimized. We refer the interested reader to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20]</ref> for more details. Standard LAS models use character-based output units: grapheme <ref type="bibr" target="#b11">[12]</ref>, word-piece <ref type="bibr" target="#b8">[9]</ref> or BPE <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Phonemic Sequence-to-Sequence Model</head><p>The phonemic LAS model can be obtained by using phonemes as the output unit. Phonemes are natural labels for acoustic modeling of non-phonetic languages. The use of a pronunciation lexicon can also ease integration of completely new words or named entities 1 <ref type="bibr" target="#b21">[22]</ref>. However, by using a pronunciation lexicon, we give up the end-to-end approach, which introduces complications for both training and decoding.</p><p>For training, words with multiple pronunciation variants cause a problem, since there is no unique mapping from such a word to its corresponding phoneme sequence. While we can potentially obtain the correct pronunciation variant by generating alignments, we skip this extra effort by choosing a pronunciation simply by randomly choosing one of the pronunciations for each word to define a unique mapping. In addition, we include an unknown token UNK as a part of the phoneme vocabulary and use it to represent words which are not included in the lexicon. We use a dedicated end-of-word token EOW (as part of the phoneme inventory) to model word boundaries, as in <ref type="bibr" target="#b16">[17]</ref>, which we find improves performance.</p><p>To deal with the ambiguity of homophones 2 during decoding, we incorporate a (word-based) n-gram language model. We use a general weighted finite-state transducer (WFST) decoder to perform a beam search. The lexicon and language model (LM) are represented as WFST L and G respectively and combined by means of FST composition as the search network L • G <ref type="bibr" target="#b23">[24]</ref>. The search process then explores partial path hypotheses which are constrained by the search network and scored by both the LAS model and the n-gram language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LibriSpeech Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>The LibriSpeech task <ref type="bibr" target="#b18">[19]</ref> has three subsets with different amounts of transcribed training data: 100hr, 460hr, and 960hr. A lexicon with pronunciations for 200K words is officially distributed. The development and test data are both split into clean and other subsets, each of them consisting of about 5 to 6 hours of audio. The number of unique words observed in each subset as well as the out-of-vocabulary (OOV; unseen in training data) rate is summarized in <ref type="table" target="#tab_0">Table 1</ref>. For language modeling, extra text-only data of about 800M words is also available, along with an officially distributed 3-gram word LM; we use the unpruned 3-gram LM for decoding the phonemic LAS models. In contrast, the grapheme and word-piece models are evaluated without a lexicon or a language model (unless otherwise indicated). We train word piece models <ref type="bibr" target="#b10">[11]</ref> of size 16K (16,384) on each training subset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Models and training</head><p>We use 80-dimensional log-mel features with deltas and accelerations as the frame-level audio input features. Reducing input frame rate in the encoder is important for successfully training sequence-to-sequence speech models, especially for tasks such as LibriSpeech which feature long utterances (∼15s). Thus, following <ref type="bibr" target="#b24">[25]</ref>, our encoder layers include two layers of 3×3 convolution with 32 channels with a stride of 2, which results in a total time reduction factor of 4. We consider three model (small, medium, and large) which differ in terms of the sizes of model components. On top of the convolutional layers, the encoder contains 3 (small) or 4 (medium and large) layers of bidirectional LSTMs <ref type="bibr" target="#b25">[26]</ref>, with either 256 (small), 512 (medium), or 1024 (large) LSTM <ref type="bibr" target="#b26">[27]</ref> cells in each layer. A projection layer and batch normalization are applied after each LSTM encoder layer <ref type="bibr" target="#b24">[25]</ref>. The decoder consists of 1 (small) or 2 (medium and large) LSTM layers, and uses additive attention as described in <ref type="bibr" target="#b19">[20]</ref>. We train all models using 16 GPUs by asynchronous stochastic gradient descent with Adam optimizer <ref type="bibr" target="#b27">[28]</ref> from random initialization without any special pre-training method <ref type="bibr" target="#b2">3</ref> for about 80 epochs. We use open-source Tensorflow Lingvo toolkit <ref type="bibr" target="#b28">[29]</ref> for all experiments. Our grapheme and wordpiece based baseline configurations are publicly available online <ref type="bibr" target="#b3">4</ref> where further details about the models can be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Standalone Performance Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baseline model performance on 960hr</head><p>The WER performance of grapheme and word-piece based models is summarized in <ref type="table" target="#tab_1">Table 2</ref>. For both graphemes and word-pieces, we present the performance for small, medium and large model sizes (as shown by different numbers of parameters) as described in Sec 4.2. The difference of number of parameters between different units only comes from the unit-level vocabulary size. As can be seen in <ref type="table" target="#tab_1">Table 2</ref>, models benefit from the additional parameters and the best WERs are obtained for the large word-piece model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Phonemic model performance on 960hr</head><p>For phoneme based models, we first check the phoneme error rates (PER) in order to make sure that the models are reasonable <ref type="bibr" target="#b4">5</ref> . The WER performance results for decoding with the lexicon and the 3-gram word LM (88M n-grams) is shown in <ref type="table" target="#tab_2">Table 3</ref>. We observe that despite the use of an external LM which is trained on much more data than the transcribed acoustic training data, the phonemic system performs worse than the best graphemic model <ref type="bibr" target="#b5">6</ref> . It is nevertheless interesting to examine examples where the phonemic model outperforms the best word-piece model. In <ref type="table" target="#tab_3">Table 4</ref>, we present some illustrative examples. In addition, we find that decoding the graphemic model with the 3-gram word LM does not give improvement.  In <ref type="table" target="#tab_2">Table 3</ref>, we also include the WERs from previous work on LibriSpeech 960hr; for fair comparison, systems which employ data augmentation <ref type="bibr" target="#b30">[31]</ref> are excluded. Our word-piece model performs better than the previously reported sequenceto-sequence model in <ref type="bibr" target="#b20">[21]</ref> while the performance is behind the conventional hybrid system with an n-gram LM <ref type="bibr" target="#b29">[30]</ref>. We note that our word-piece models simply trained using the crossentropy criterion (without e.g. minimum word error rate training <ref type="bibr" target="#b31">[32]</ref>) is competitive with Sabour et al.'s model trained with optimal completion distillation <ref type="bibr" target="#b32">[33]</ref>, which is reported to give 4.5% and 13.3% on the test-clean and test-other sets. For further comparison, we also report the WERs of our best wordpiece model combined with an LSTM language model <ref type="bibr" target="#b33">[34]</ref> by shallow fusion <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>  <ref type="bibr" target="#b6">7</ref> . The LSTM LM consists of one input linear layer of dimension 1024 and 2 LSTM layers with 2048 nodes <ref type="bibr" target="#b26">[27]</ref>. The LM weight of 0.35 is found to be optimal for dev-clean and dev-other WERs. We obtain similar relative improvements reported in <ref type="bibr" target="#b20">[21]</ref> and achieve WERs of 3.6% on the test-clean, and 10.3% on the test-other set, which reduces the performance gap from the best hybrid system reported in <ref type="bibr" target="#b29">[30]</ref>. <ref type="bibr" target="#b4">5</ref> By increasing the model size from 7 M to 35 M, then to 130 M, we improve the PERs (%) from (3.2, 9.7, 3.2, 9.9), to (2.8, 8.9, 3.0, 9.1), then to (2.4, 7.9, 2.5, 7.7) on the dev-clean/other, test-clean/other sets. <ref type="bibr" target="#b5">6</ref> This is similar to what is reported in <ref type="bibr" target="#b16">[17]</ref>. Though, we note that we get about 2% absolute degradation in WERs with a model trained without EOW compared with the model with EOW. <ref type="bibr" target="#b6">7</ref> In our experiments, we find it crucial to constrain the emission of end-of-sentence (EOS) tokens <ref type="bibr" target="#b34">[35]</ref> to penalize short sentences (rather than applying length normalization) in shallow fusion: we only allow the model to emit EOS when its score is within 1.0 of the top hypothesis. We check that tuning such an EOS emission constraint does not improve the baseline systems without language model nor beam search with the WFST decoder for phonemic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on 100hr and 460hr tasks</head><p>We conduct the same experiments in the 100hr and 460hr conditions. For each unit, we obtain the best performance for the large models for the 460hr scenario, whereas for the 100hr case, the medium model perform the best. The results are summarized in <ref type="table" target="#tab_4">Table 5</ref>. We find that even in the small dataset scenarios with higher OOV rates, graphemic and word-piece based models outperform the phonemic system. We also note that the performance of attention-based models dramatically degrades when the amount of training data is reduced, unlike conventional hybrid approach <ref type="bibr" target="#b18">[19]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Rescoring Experiments</head><p>While Sec. 5 focuses on the comparison of models with different output units, our goal is to ideally get benefits from different model units. We consider two methods for combining LAS models with different output units. The first approach is simple N-best list rescoring. We generate a N-best list from one LAS model, convert the corresponding word sequences to the rescorer LAS model's unit, score them, and combine the scores by log-linear interpolation to get new scores. However, rescoring is limited to the hypotheses generated by one LAS model. Therefore, we also carry out union of N-best list with crossrescoring: we independently generate N-best lists from two LAS models, rescore the hypotheses generated by one model using the other model and vice versa, to get the 1-best from the union of the rescored (up to) 2N hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">N-best Rescoring results</head><p>We carry out the N-best rescoring of our best word-piece based model in the 960hr scenario by a graphemic, and a phonemic model. In all following experiments, the interpolation weights are optimized to obtain the best dev-clean WER (which typically also gives the best dev-other WER). The WERs are presented in the upper part of <ref type="table" target="#tab_5">Table 6</ref>. We obtain improvements of 9% in both cases on the test-clean set; on the test-other set, we obtain an 8% relative with the phonemic model and a 9% relative with the graphemic model. Thus, it can be noted that rescoring is a simple method for using a phonemic model without an additional language model. To determine if gains by the graphemic and phonemic models are additive, we combine the scores from all models, which obtains only slight improvements of up to 0.1 absolute as shown in <ref type="table" target="#tab_5">Table 6</ref> (+ Both). In <ref type="table" target="#tab_6">Table 7</ref>, we again show some illustrative examples where the phonemic model outperforms the combination of word-piece and grapheme based models only. It is for example interesting to observe that the correct spelling "bartley" is in the N-best hypotheses of the word-piece model, and that the phonemic model helps recognize it correctly.</p><p>In the other direction, we also rescore the N-best list generated by the phonemic system by the word-piece model. The results are shown in the lower part of <ref type="table" target="#tab_5">Table 6</ref>. We find that the improvements are limited (only up to 4% relative). In fact, the 30-best list generated by a phonemic system has much higher oracle WERs than the 8-best list of the word-piece model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Union of N-best lists with cross-rescoring results</head><p>The examples in <ref type="table" target="#tab_3">Table 4</ref> show some complementarity between the word-piece 1-best hypothesis and the phonemic one. To evaluate the potential value of hypotheses generated by the phonemic model, we decode a N-best list from the word-piece based and phoneme based models independently, rescore the respective hypotheses (cross-rescoring), and take the 1-best from the 2N hypotheses (union). In <ref type="table" target="#tab_7">Table 8</ref>, we observe that we only obtain marginal improvements on the test-other set, compared with rescoring the 8-best word-piece hypotheses. For a fairer comparison, we also carry out rescoring of 16-best lists generated by the word-piece model by the phonemic model. We find that such an approach is slightly better than the union. This suggests that decoding from the phonemic model has limited benefits for the LibriSpeech task. The oracle WERs are much worse for the phonemic system than the word-piece model ( <ref type="table" target="#tab_5">Table 6</ref>). We observe that the diversity of hypotheses in the N-best list generated by the phonemic system is mainly based on homophones, rather than difficult words (i.e. words with unusual pronunciation  <ref type="figure">Figure 2</ref>: LAS model with an auxiliary decoder: main decoder operates on graphemes and the auxiliary decoder predicts phonemes; dashed lines represent state copying for initialization at each word boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Rescoring with an auxiliary decoder</head><p>Finally, we examine a model with two decoders operating on different units but using a single encoder. Such a model can be convenient for model combination (e.g., rescoring or potentially also decoding from two decoders operating on different units and combining hypotheses in a word synchronous fashion). The design of the model is illustrated in <ref type="figure">Figure 2</ref>. The main decoder (grapheme, in the example) works exactly as in the baseline LAS model (Sec. 2; <ref type="figure" target="#fig_0">Figure 1</ref>). The auxiliary decoder (phoneme, in the example) is designed such that it predicts only the next word as a sequence of the auxiliary units.</p><p>We use separate parameters for the auxiliary attention and initialize all recurrent states of the auxiliary component at each word boundary by those of the main decoder (i.e. the prediction from the auxiliary decoder is conditioned on the word sequence generated thus far from the main decoder). The model is trained in two stages; the main decoder and the encoder are first trained, and their parameters are not modified during the training of the auxiliary components. In experiments, we use wordpieces for the main decoder, and phonemes for the auxiliary decoder. <ref type="table" target="#tab_9">Table 9</ref> shows improvements by rescoring with an auxiliary phoneme decoder of the two decoder-model. We obtain improvements despite small number of additional parameters (30M) corresponding to the phonemic 2-layer LSTM decoder and the attention layer, however rescoring with an independent phoneme model (as in <ref type="table" target="#tab_5">Table 6</ref>) gives larger improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Our experiments on different LibriSpeech subsets show that word-piece and grapheme based models consistently outperform phoneme based models. Therefore, the dominance of character-based model units in the LAS speech model is not due to the amount of training data. This indicates that this behavior is more likely related to the model itself (e.g., the decoder is conditioned on all predecessor labels). Furthermore, we find that the word-piece based attention models can achieve a relatively low oracle WER with only 8-best hypotheses and rescoring that N-best hypotheses using graphemic or phonemic models gives good improvements. Future work will examine whether streaming end-to-end approaches (e.g., RNN-T <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>) show similar trends.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>LAS model. arXiv:1902.01955v2 [cs.CL] 23 Jul 2019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Out-of-vocabulary (OOV) rates (%) with respect to the vocabulary (unique word list) in different data scenarios, and with respect to the pronunciation lexicon.</figDesc><table><row><cell cols="2">Training Vocab. data (h) Size</cell><cell cols="4">dev clean other clean other test</cell></row><row><cell>100 460 960</cell><cell>34 K 66 K 89 K</cell><cell>2.5 0.9 0.6</cell><cell>2.5 1.2 0.8</cell><cell>2.4 1.0 0.6</cell><cell>2.8 1.3 0.8</cell></row><row><cell>Lexicon</cell><cell>200 K</cell><cell>0.3</cell><cell>0.6</cell><cell>0.4</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>WERs (%) for grapheme and word-piece models.</figDesc><table><row><cell>Unit</cell><cell>Param.</cell><cell cols="4">dev clean other clean other test</cell></row><row><cell>Grapheme</cell><cell>7 M 35 M 130 M</cell><cell>7.6 5.3 5.3</cell><cell>20.5 15.6 15.2</cell><cell>7.9 5.6 5.5</cell><cell>21.3 15.8 15.3</cell></row><row><cell>Word-Piece</cell><cell>20 M 60 M 180 M</cell><cell>5.8 4.9 4.4</cell><cell>16.0 14.0 13.2</cell><cell>6.1 5.0 4.7</cell><cell>16.4 14.1 13.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>WERs (%) for the 960hr dataset.</figDesc><table><row><cell>Unit</cell><cell>LM</cell><cell cols="4">dev clean other clean other test</cell></row><row><cell>Phoneme Grapheme Word-Piece 16K</cell><cell>3-gram None None</cell><cell>5.6 5.3 4.4</cell><cell>15.8 15.2 13.2</cell><cell>6.2 5.5 4.7</cell><cell>15.8 15.3 13.4</cell></row><row><cell>Word-Piece 16K</cell><cell>LSTM</cell><cell>3.3</cell><cell>10.3</cell><cell>3.6</cell><cell>10.3</cell></row><row><cell>BPE 10K [21]</cell><cell>None LSTM</cell><cell>4.9 3.5</cell><cell>14.4 11.5</cell><cell>4.9 3.8</cell><cell>15.4 12.8</cell></row><row><cell>Hybrid system [30]</cell><cell cols="2">N-gram 3.4 LSTM 3.1</cell><cell>8.8 8.3</cell><cell>3.6 3.5</cell><cell>8.9 8.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Examples where the phonemic system's 1-best wins against the word-piece model's 1-best. Phoneme Word-Piece when did you come bartley when did you come partly kirkland jumped for the jetty kerklin jumped for the jetty man's eyes remained fixed man's eyes were made fixed</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>WERs for the 460hr and 100hr scenarios.</figDesc><table><row><cell>Train data</cell><cell>Unit</cell><cell cols="4">dev clean other clean other test</cell></row><row><cell>460hr</cell><cell>Phoneme Grapheme Word-Piece</cell><cell>7.6 6.4 5.7</cell><cell>27.3 23.5 21.8</cell><cell>8.5 6.8 6.5</cell><cell>27.8 24.1 22.5</cell></row><row><cell>100hr</cell><cell>Phoneme Grapheme Word-Piece</cell><cell>13.8 11.6 12.7</cell><cell>38.9 36.1 33.9</cell><cell>14.3 12.0 12.9</cell><cell>40.9 38.0 35.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>WER (%) results for N-best list rescoring. Oracle WERs are shown in parentheses.</figDesc><table><row><cell></cell><cell>clean</cell><cell>dev</cell><cell>other</cell><cell>clean</cell><cell>test</cell><cell>other</cell></row><row><cell>Word-Piece + Phoneme + Grapheme + Both</cell><cell cols="6">4.4 (2.4) 13.2 (9.2) 4.7 (2.6) 13.4 (9.1) 4.1 12.4 4.3 12.4 4.0 12.3 4.3 12.3 3.9 12.2 4.3 12.2</cell></row><row><cell>Phoneme + Word-Piece</cell><cell cols="6">5.6 (4.9) 15.8 (14.4) 6.2 (5.5) 15.8 (14.7) 5.4 15.5 6.0 15.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Examples where Word-Piece+Grapheme+Phoneme (WP+G+P) wins over Word-Piece+Grapheme (WP+G). . lettuce leaf with mayonnaise ... ... lettuce leaf with mayonna is ... the manager fell to his musings the manager felt of his musings what a fuss is made about you what are fusses made about you ... eyes blazed with indignation ... eyes blaze of indignation</figDesc><table><row><cell></cell><cell>WP+G+P</cell><cell>WP+G</cell></row><row><cell>..</cell><cell>oh bartley did you write to me</cell><cell>oh bartly did you write to me</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>WERs (%) results for union of N-best lists with crossrescoring. Oracle WERs are shown in parentheses.</figDesc><table><row><cell>Num. hyp.</cell><cell>clean</cell><cell>dev</cell><cell>other</cell><cell>clean</cell><cell>test</cell><cell>other</cell></row><row><cell>Word-Piece 8 + Phoneme Union 16</cell><cell cols="6">4.4 (2.4) 13.2 (9.2) 4.7 (2.6) 13.4 (9.1) 4.1 12.4 4.3 12.4 4.1 12.4 4.3 12.3</cell></row><row><cell>Word-Piece 16 + Phoneme</cell><cell cols="6">4.4 (2.0) 13.2 (8.3) 4.7 (2.2) 13.4 (8.1) 4.0 12.3 4.3 12.2</cell></row><row><cell cols="7">6.3. Why is Oracle WER So High for Phonemic System?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>). For example, on the reference utterance "bozzle had always waited upon him with a decent coat and a well brushed hat and clean shoes", where bozzle is not in the training data, the word-piece based model fills the 8-best beam by proposing different spellings for bozzle such as {basil, bazil, basle, bosel, bosal, bosell, bossel}, which is a reasonable way to model the ambiguity. The phoneme system, instead, only produces {bazil, basil} as a substitution for bozzle and lists homophones for shoes, {shoes, shews, shoos, shues, shooes} instead. Homophone distinction might still be inefficient for a phonemic system as the phonemic LAS model gives them all the same score, and a single parameter is used to weight the external LM for the entire search. Addressing this issue might be crucial to improve the phonemic system.</figDesc><table><row><cell cols="4">Auxiliary Decoder</cell><cell cols="3">k ae1 t EOW</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">&lt;s&gt; k ae1 t</cell></row><row><cell>h</cell><cell cols="2">e _</cell><cell>c</cell><cell>a</cell><cell>t</cell><cell>_</cell></row><row><cell>Auxiliary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>t</cell><cell>h</cell><cell cols="2">e _</cell><cell>c</cell><cell>a</cell><cell>t</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>WERs (%) for rescoring with an auxiliary decoder.</figDesc><table><row><cell>dev clean other clean other Param. test Total</cell></row><row><cell>Word-Piece (WP) WP + Auxiliary phoneme 4.3 13.0 4.6 13.1 210 M 4.4 13.2 4.7 13.4 180 M WP + Phoneme 4.1 12.4 4.3 12.4 310 M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This might not be as relevant for LibriSpeech evaluation as we use the official LibriSpeech lexicon without modification.<ref type="bibr" target="#b1">2</ref> By choosing phonemes as output units, we are giving up the standalone recognition using the attention-based model. Also, while acoustic modeling motivates the use of phonemes, the ability of the decoder as a language model can possibly be weaker compared with characterbased units, since the phoneme-level language model can be viewed as a subword-level class-based language model<ref type="bibr" target="#b22">[23]</ref> where the clusters are formed based on the phonemic similarity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We find training to be stable across repeated runs. We avoid plateaus at the beginning of training (which we often observe) by tuning the initial learning rate. We find that our models achieve the best WER on the dev-clean, earlier than on the dev-other set.<ref type="bibr" target="#b3">4</ref> https://github.com/tensorflow/lingvo</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Model unit exploration for sequence-to-sequence speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01955</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>Int. Conf. on Learning Representations (ICLR)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comparison of sequence-to-sequence models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="939" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint ctc-attention based endto-end speech recognition using multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech and Signal essing (ICASSP)<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03" />
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring neural transducers for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU<address><addrLine>Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving attention based sequence-to-sequence models for end-toend english conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="761" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-03" />
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: a neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<title level="m">Connectionist Speech Recognition: A Hybrid Approach</title>
		<meeting><address><addrLine>Norwell, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kanthak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech and Signal essing (ICASSP)<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05" />
			<biblScope unit="page" from="845" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Grapheme based speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Killer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech</title>
		<meeting>Eurospeech<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Revisiting graphemes with increasing amounts of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-04" />
			<biblScope unit="page" from="4449" to="4452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">No need for a lexicon? evaluating the value of the pronunciation lexica in end-to-end models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Schogol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="page" from="5859" to="5863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A comparison of modeling units in sequence-to-sequence speech recognition with the Transformer on Mandarin Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06239</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lib-riSpeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence models can directly translate foreign speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="2625" to="2629" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<meeting><address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="7" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Phoebe: Pronunciation-aware contextualization for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brighton, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Speech recognition with weighted finite-state transducers,&quot; in Handbook of Speech Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="559" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech and Signal essing (ICASSP)<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03" />
			<biblScope unit="page" from="4845" to="4849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Lingvo: a modular and scalable framework for sequence-to-sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The CAPIO 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandrashekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<idno>arXiv preprint:1801.00059</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Minimum word error rate training for attention-based sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="page" from="4839" to="4843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimal completion distillation for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09" />
			<biblScope unit="page" from="194" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="523" to="527" />
			<pubPlace>Stockholm, Sweden</pubPlace>
		</imprint>
	</monogr>
	<note>in Internspeech</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A comparison of techniques for language model integration in encoder-decoder speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Representation Learning Workshop, Int. Conf. on Machine Learning (ICML)</title>
		<meeting><address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU<address><addrLine>Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
