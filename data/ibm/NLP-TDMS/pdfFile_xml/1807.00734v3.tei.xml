<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The relativistic discriminator: a key element missing from standard GAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
							<email>alexia.jolicoeur-martineau@mail.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Lady Davis Institute Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The relativistic discriminator: a key element missing from standard GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In standard generative adversarial network (SGAN), the discriminator D estimates the probability that the input data is real. The generator G is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. We show that this property can be induced by using a "relativistic discriminator" which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b0">[Hong et al., 2017]</ref> form a broad class of generative models in which a game is played between two competing neural networks, the discriminator D and the generator G. D is trained to discriminate real from fake data, while G is trained to generate fake data that D will mistakenly recognize as real. In the original GAN by <ref type="bibr" target="#b1">Goodfellow et al. [2014]</ref>, which we refer to as Standard GAN (SGAN), D is a classifier, thus it is predicting the probability that the input data is real. When D is optimal, the loss function of SGAN is approximately equal to the Jensen-Shannon divergence (JSD) <ref type="bibr" target="#b1">[Goodfellow et al., 2014]</ref>. SGAN has two variants for the generator loss functions: saturating and non-saturating. In practice, the former has been found to be very unstable, while the latter has been found to more stable <ref type="bibr" target="#b1">[Goodfellow et al., 2014]</ref>. Under certain conditions,  proved that, if real and fake data are perfectly classified, the saturating loss has zero gradient and the non-saturating loss has non-zero but volatile gradient. In practice, this mean that the discriminator in SGAN often cannot be trained to optimality or with a too high learning rate; otherwise, gradients may vanish and, if so, training will stop. This problem is generally more noticeable in high-dimensional setting (e.g., high resolution images and discriminator architectures with high expressive power) given that there are more degrees of freedom available to reach perfect classification of the training set.</p><p>To improve on SGAN, many GAN variants have been suggested using different loss functions and discriminators that are not classifiers (e.g., LSGAN <ref type="bibr" target="#b3">[Mao et al., 2017]</ref>, WGAN ). Although these approaches have partially succeeded in improving stability and data quality, the large-scale study by <ref type="bibr" target="#b5">Lucic et al. [2017]</ref> suggests that these approaches do not consistently improve on SGAN. Additionally, some of the most successful approaches, such as WGAN-GP <ref type="bibr" target="#b6">[Gulrajani et al., 2017]</ref>, are much more computationally demanding than SGAN.</p><p>Many of the recent successful GANs variants have been based on Integral probability metrics (IPMs) <ref type="bibr" target="#b7">[Müller, 1997]</ref> (e.g., WGAN , WGAN-GP <ref type="bibr" target="#b6">[Gulrajani et al., 2017]</ref>, Sobolev GAN , Fisher GAN ). In IPM-based GANs, the discriminator is real-valued and constrained to a specific class of function so that it does not grow too quickly; this act as a form of regularization which prevents D from becoming too strong (i.e., almost perfectly classifying real from fake data). In practice, we generally observe that the discriminator of IPM-based GANs can be trained for many iterations without causing vanishing gradients.</p><p>IPM constraints have been shown to be similarly beneficial in non-IPM-based GANs. The constraint of WGAN (i.e., Lipschitz discriminator) has been shown to be beneficial in other GANs through spectral normalization <ref type="bibr" target="#b10">[Miyato et al., 2018]</ref>. The constraint of WGAN-GP (i.e., discriminator with gradient norm equal to 1 around real and fake data) has been shown to be beneficial in SGAN <ref type="bibr" target="#b11">[Fedus et al., 2017]</ref> (along with a very similar gradient penalty by <ref type="bibr" target="#b12">Kodali et al. [2017]</ref>). Although this shows that certain IPM constraints improve the stability of GANs, it does not explain why IPMs generally provide increased stability over other metrics/divergences in GANs (e.g., JSD for SGAN, f -divergences for f -GANs <ref type="bibr" target="#b13">[Nowozin et al., 2016]</ref>).</p><p>In this paper, we argue that non-IPM-based GANs are missing a key ingredient, a relativistic discriminator, which IPM-based GANs already possess. We show that a relativistic discriminator is necessary to make GANs analogous to divergence minimization and produce sensible predictions based on the a priori knowledge that half of the samples in the mini-batch are fake. We provide empirical evidence showing that GANs with a relativistic discriminator are more stable and produce data of higher quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative adversarial networks</head><p>GANs can be defined very generally in terms of the discriminator in the following way:</p><formula xml:id="formula_0">L D = E xr∼P f 1 (D(x r )) + E z∼Pz f 2 (D(G(z))) ,<label>(1)</label></formula><p>and</p><formula xml:id="formula_1">L G = E xr∼P [g 1 (D(x r ))] + E z∼Pz [g 2 (D(G(z)))] ,<label>(2)</label></formula><p>wheref 1 ,f 2 ,g 1 ,g 2 are scalar-to-scalar functions, P is the distribution of real data, P z is generally a multivariate normal distribution centered at 0 with variance 1, D(x) is the discriminator evaluated at x, G(z) is the generator evaluated at z (Q is the distribution of fake data, thus of G(z)). Note that, through the paper, we refer to real data as x r and fake data as x f . Without loss of generality, we assume that both L D and L G are loss functions to be minimized.</p><p>Most GANs can be separated into two classes: non-saturating and saturating loss functions. GANs with the saturating loss are such thatg 1 =−f 1 andg 2 =−f 2 , while GANs with the non-saturating loss are such thatg 1 =f 2 andg 2 =f 1 . Saturating GANs are most intuitive as they can be interpreted as alternating between maximizing and minimizing the same loss function. After training D to optimality, the loss function is generally an approximation of a divergence (e.g., Jensen-Shannon divergence (JSD) for SGAN <ref type="bibr" target="#b1">[Goodfellow et al., 2014]</ref>, f -divergences for F-GANs <ref type="bibr" target="#b13">[Nowozin et al., 2016]</ref>, and Wassertein distance for WGAN ). Thus, training G to minimize L G can be roughly interpreted as minimizing the approximated divergence (although this is not technically true; see <ref type="bibr" target="#b14">Jolicoeur-Martineau [2018]</ref>). On the other hand, non-saturating GANs can be thought as optimizing the same loss function, but swapping real data with fake data (and vice-versa). In this article, unless otherwise specified, we assume a non-saturating loss for all GANs. SGAN assumes a cross-entropy loss, i.e.,f 1 (D(x)) = − log(D(x)) andf 2 (D(x)) = − log(1 − D(x)), where D(x) = sigmoid(C(x)), and C(x) is the non-transformed discriminator output (which we call the critic as per ). In most GANs, C(x) can be interpreted as how realistic the input data is; a negative number means that the input data looks fake (e.g., in SGAN, D(x) = sigmoid(−5) = 0), while a positive number means that the input data looks real (e.g., in SGAN, D(x) = sigmoid(5) = 1).</p><p>In SGAN, the discriminator is said to output the probability that the input data is real. This is because minimizing the cross-entropy is equivalent to maximizing the log-likelihood of a Bernoulli variable. Thus, the output of D is approximately Bernoulli distributed and representative of a probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Integral probability metrics</head><p>IPMs are statistical divergences represented mathematically as:</p><formula xml:id="formula_2">IP M F (P||Q) = sup C∈F E x∼P [C(x)] − E x∼Q [C(x)],</formula><p>where F is a class of real-valued functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IPM-based GANs can be defined using equation 1 and 2 assumingf</head><formula xml:id="formula_3">1 (D(x)) =g 2 (D(x)) = −D(x) andf 2 (D(x)) =g 1 (D(x)) = D(x), where D(x) = C(x) (i.e.</formula><p>, no transformation is applied). It can be observed that both discriminator and generator loss functions are unbounded and would diverge to −∞ if optimized directly. However, IPMs assume that the discriminator is of a certain class of function that does not grow too quickly which prevent the loss functions from diverging. Each IPM applies a different constraint to the discriminator (e.g., WGAN assumes a Lipschitz D, WGAN-GP assumes that D has gradient norm equal to 1 around real and fake data).</p><p>3 Missing property of SGAN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Missing property</head><p>We argue that the key missing property of SGAN is that the probability of real data being real (D(x r )) should decrease as the probability of fake data being real (D(x f )) increase. We provide three arguments suggesting that SGAN should have this property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prior knowledge argument</head><p>With adequate training, the discriminator is able to correctly classify most real samples as real and most fake samples as not real. Subsequently, after the generator is trained to "fool" the discriminator into thinking that fake samples are real samples, the discriminator classify most samples, real or fake, as real. This behavior is illogical considering the a priori knowledge that half of the samples in the mini-batch are fake, as we explain below.</p><p>After training the generator, given that both real and fake samples look equally real, the critic values (C(x)) of real and fake data may be very close, i.e., C(x f ) ≈ C(x r ) for most x r and x f . Considering the fact that the discriminator is always shown half real data and half fake data, if the discriminator perceive all samples shown as equally real, it should assume that each sample has probability .50 of being real. However, in SGAN and other non-IPM-based GANs, we implicitly assume that the discriminator does not know that half the samples are fake. If the discriminator doesn't know, it could be possible that all samples shown are real. Thus, if all samples look real, it would be reasonable to assume that they are indeed all real (D(x) ≈ 1 for all x).</p><p>Assuming that the generator is trained with a strong learning rate or for many iterations; in addition to both real and fake samples being classified as real, fake samples may appear to be more realistic than real samples, i.e., C(x f ) &gt; C(x r ) for most x r and x f . In that case, considering that half of the samples are fake, the discriminator should assign a higher probability of being fake to real samples rather than classify all samples are real. Real data Fake data <ref type="figure">Figure 1</ref>: Expected discriminator output of the real and fake data for the a) direct minimization of the Jensen-Shannon divergence, b) actual training of the generator to minimize its loss function, and c) ideal training of the generator to minimize its loss function (lines are dotted when they cross beyond the equilibrium to signify that this may or may not be necessary).</p><p>In summary, by not decreasing D(x r ) as D(x f ) increase, SGAN completely ignores the a priori knowledge that half of the mini-batch samples are fake. Unless one makes the task of the discriminator more difficult (using regularization or lower learning rates), the discriminator does not make reasonable predictions. On the other hand, IPM-based GANs implicitly account for the fact that some of the samples must be fake because they compare how realistic real data is compared to fake data. This provides an intuitive argument to why the discriminator in SGAN (and GANs in general) should depends on both real and fake data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Divergence minimization argument</head><p>In SGAN, we have that the discriminator loss function is equal to the Jensen-Shannon divergence (JSD) <ref type="bibr" target="#b1">[Goodfellow et al., 2014]</ref>. Therefore, calculating the JSD can be represented as solving the following maximum problem:</p><formula xml:id="formula_4">JSD(P||Q) = 1 2 log(4) + max D:X→[0,1] E xr∼P [log(D(x r ))] + E x f ∼Q [log (1 − D(x f ))] . (3)</formula><p>The JSD is minimized (JSD(P||Q) = 0) when D(x r ) = D(x f ) = 1 2 for all x r ∈ P and x f ∈ Q and maximized (JSD(P||Q) = log(2)) when D(x r ) = 1, D(x f ) = 0 for all x r ∈ P and x f ∈ Q. Thus, if we were directly minimizing the divergence from maximum to minimum, we would expect D(x r ) to smoothly decrease from 1 to .50 for most x r and D(x f ) to smoothly increase from 0 to .50 for most x f <ref type="figure">(Figure 1a</ref>). However, when minimizing the saturating loss in SGAN, we are only increasing D(x f ), we are not decreasing D(x r ) ( <ref type="figure">Figure 1b</ref>). Furthermore, we are bringing D(x f ) closer to 1 rather than .50. This means that SGAN dynamics are very different from the minimization of the JSD. To bring SGAN closer to divergence minimization, training the generator should not only increase D(x f ) but also decrease D(x r ) ( <ref type="figure">Figure 1c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Gradient argument</head><p>Let's compare the gradient steps of standard GAN and IPM-based GANs for further insight. It can be shown that the gradients of the discriminator and generator in non-saturating SGAN are respectively:</p><formula xml:id="formula_5">∇ w L GAN D = −E xr∼P [(1 − D(x r ))∇ w C(x r )] + E x f ∼Q θ [D(x f )∇ w C(x f )] , (4) ∇ θ L GAN G = −E z∼Pz [(1 − D(G(z)))∇ x C(G(z))J θ G(z)] ,<label>(5)</label></formula><p>where J is the Jacobian.</p><p>It can be shown that the gradients of the discriminator and generator in IPM-based GANs are respectively:</p><formula xml:id="formula_6">∇ w L IP M D = −E xr∼P [∇ w C(x r )] + E x f ∼Q θ [∇ w C(x f )],<label>(6)</label></formula><formula xml:id="formula_7">∇ θ L IP M G = −E z∼Pz [∇ x C(G(z))J θ G(z)],<label>(7)</label></formula><p>where C(x) ∈ F (the class of functions assigned by the IPM).</p><p>From these equations, it can be observed that SGAN leads to the same dynamics as IPM-based GANs when we have that:</p><formula xml:id="formula_8">1. D(x r ) = 0, D(x f ) = 1 in the discriminator step of SGAN 2. D(x f ) = 0 in the generator step of SGAN. 3. C(x) ∈ F</formula><p>Assuming that the discriminator and generator are trained to optimality in each step and that it is possible to perfectly distinguish real from the fake data (strong assumption, but generally true early in training); we have that D(x r ) = 1, D(x f ) = 0 in the generator step and that D(x r ) = 1, D(x f ) = 1 in the discriminator step for most x r and x f <ref type="figure">(Figure 1b)</ref>. Thus, the only missing assumption is that D(x r ) = 0 in the discriminator step.</p><p>This means that SGAN could be equivalent to IPM-based GANs, in certain situations, if the generator could indirectly influence D(x r ). Considering that IPM-based GANs are generally more stable than SGAN, it would be reasonable to expect that making SGAN closer to IPM-based GANs could improve its stability.</p><p>In IPMs, both real and fake data equally contribute to the gradient of the discriminator's loss function. However, in SGAN, if the discriminator reach optimality, the gradient completely ignores real data. This means that if D(x r ) does not indirectly change when training the discriminator to reduce D(x f ) (which might happens if real and fake data have different supports or if D has a very large capacity), the discriminator will stop learning what it means for data to be "real" and training will focus entirely on fake data. In which case, fake samples will not become more realistic and training will get stuck. On the other hand, if D(x r ) always decreases when D(x f ) increases, real data will always be incorporated in the gradient of the discriminator loss function. In our experiments, we observe that GANs with this property are able to learn in very difficult settings whereas traditional GANs become stuck early in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relativistic standard GAN</head><p>In standard GAN, the discriminator can be defined, in term of the non-transformed layer C(x), as D(x) = sigmoid(C(x)). A simple way to make discriminator relativistic (i.e., having the output of D depends on both real and fake data) is to sample from real/fake data pairsx = (x r , x f ) and define it as D(x) = sigmoid(C(x r ) − C(x f )).</p><p>We can interpret this modification in the following way: the discriminator estimates the probability that the given real data is more realistic than a randomly sampled fake data. Similarly, we can define D rev (x) = sigmoid(C(x f ) − C(x r )) as the probability that the given fake data is more realistic than a randomly sampled real data. An interesting property of this discriminator is that we do not need to include D rev in the loss function through log(1 − D rev (x)) because we have that</p><formula xml:id="formula_9">1 − D rev (x) = 1 − sigmoid(C(x f ) − C(x r )) = sigmoid(C(x r ) − C(x f )) = D(x); thus, log(D(x)) = log(1 − D rev (x)).</formula><p>The discriminator and generator (non-saturating) loss functions of the Relativistic Standard GAN (RSGAN) can be written as:</p><formula xml:id="formula_10">L RSGAN D = −E (xr,x f )∼(P,Q) [log(sigmoid(C(x r ) − C(x f )))] .<label>(8)</label></formula><formula xml:id="formula_11">L RSGAN G = −E (xr,x f )∼(P,Q) [log(sigmoid(C(x f ) − C(x r )))] .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relativistic GANs</head><p>More generally, we consider any discriminator defined as a(C(x r )−C(x f )), where a is the activation function, to be relativistic. This means that almost any GAN can have a relativistic discriminator. This forms a new class of models which we call Relativistic GANs (RGANs).</p><p>Most GANs can be parametrized very generally in terms of the critic:</p><formula xml:id="formula_12">L GAN D = E xr∼P [f 1 (C(x r ))] + E x f ∼Q [f 2 (C(x f ))]<label>(10)</label></formula><p>and</p><formula xml:id="formula_13">L GAN G = E xr∼P [g 1 (C(x r ))] + E x f ∼Q [g 2 (C(x f ))] ,<label>(11)</label></formula><p>where f 1 , f 2 , g 1 , g 2 are scalar-to-scalar functions. If we use a relativistic discriminator, these GANs now have the following form:</p><formula xml:id="formula_14">L RGAN D = E (xr,x f )∼(P,Q) [f 1 (C(x r ) − C(x f ))] + E (xr,x f )∼(P,Q) [f 2 (C(x f ) − C(x r ))]<label>(12)</label></formula><p>and</p><formula xml:id="formula_15">L RGAN G = E (xr,x f )∼(P,Q) [g 1 (C(x r ) − C(x f ))] + E (xr,x f )∼(P,Q) [g 2 (C(x f ) − C(x r ))]</formula><p>. <ref type="formula" target="#formula_0">(13)</ref> IPM-based GANs represent a special case of RGAN where f 1 (y) = g 2 (y) = −y and f 2 (y) = g 1 (y) = y. Importantly, g 1 is normally ignored in GANs because its gradient is zero since the generator does not influence it. However, in RGANs, g 1 is influenced by fake data, thus by the generator. Therefore, g 1 generally has a non-zero gradient and needs to be specified in the generator loss. This means that in most RGANs (except in IPM-based GANs because they use the identity function), the generator is trained to minimize the full loss function envisioned rather than only half of it.</p><p>The formulation of RGANs can be simplified when we have the following two properties:</p><p>(1) f 2 (−y) = f 1 (y) and <ref type="formula" target="#formula_1">(2)</ref> the generator assumes a non-saturating loss (g 1 (y) = f 2 (y) and g 2 (y) = f 1 (y)). These two properties are observed in standard GAN, LSGAN using symmetric labels (e.g., -1 and 1), IPM-based GANs, etc. With these two properties, RGANs with non-saturating loss can be formulated simply as:</p><formula xml:id="formula_16">L RGAN * D = E (xr,x f )∼(P,Q) [f 1 (C(x r ) − C(x f ))]<label>(14)</label></formula><p>and</p><formula xml:id="formula_17">L RGAN * G = E (xr,x f )∼(P,Q) [f 1 (C(x f ) − C(x r ))] .<label>(15)</label></formula><p>Algorithm 1 shows how to train RGANs of this form.</p><p>Algorithm 1 Training algorithm for non-saturating RGANs with symmetric loss functions Require: The number of D iterations n D (n D = 1 unless one seeks to train D to optimality), batch size m, and functions f which determine the objective function of the discriminator (f is f 1 from equation 10 assuming that f 2 (−y) = f 1 (y), which is true for many GANs). while θ has not converged do for t = 1, . . . , n D do</p><formula xml:id="formula_18">Sample {x (i) } m i=1 ∼ P Sample {z (i) } m i=1 ∼ P z Update w using SGD by ascending with ∇ w 1 m m i=1 f (C w (x (i) ) − C w (G θ (z (i) ))) end for Sample {x (i) } m i=1 ∼ P Sample {z (i) } m i=1 ∼ P z Update θ using SGD by ascending with ∇ θ 1 m m i=1 f (C w (G θ (z (i) )) − C w (x (i) )) end while</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relativistic average GANs</head><p>Although the relative discriminator provide the missing property that we want in GANs (i.e., G influencing D(x r )), its interpretation is different from the standard discriminator. Rather than measuring "the probability that the input data is real", it is now measuring "the probability that the input data is more realistic than a randomly sampled data of the opposing type (fake if the input is real or real if the input is fake)". To make the relativistic discriminator act more globally, as in its original definition, our initial idea was to focus on the average of the relativistic discriminator over random samples of data of the opposing type. This can be conceptualized in the following way:</p><formula xml:id="formula_19">P (x r is real) := E x f ∼Q [P (x r is more real than x f )] = E x f ∼Q [sigmoid(C(x r ) − C(x f ))] = E x f ∼Q [D(x r , x f )], P (x f is real) := E xr∼P [P (x f is more real than x r )] = E xr∼P [sigmoid(C(x f ) − C(x r ))] = E xr∼P [D(x f , x r )], where D(x r , x f ) = sigmoid(C(x r ) − C(x f )).</formula><p>Then, the following loss function for D could be applied:</p><formula xml:id="formula_20">L D = −E xr∼P log E x f ∼Q [D(x r , x f )] ) − E x f ∼Q [log (1 − E xr∼P [D(x f , x r )])] .<label>(16)</label></formula><p>The main problem with this idea is that it would require looking at all possible combinations of real and fake data in the mini-batch. This would transform the problem from O(m) to O(m 2 ) complexity, where m is the batch size. This is problematic; therefore, we do not use this approach.</p><p>Instead, we propose to use the Relativistic average Discriminator (RaD) which compares the critic of the input data to the average critic of samples of the opposite type. The discriminator loss function for this approach can be formulated as:</p><formula xml:id="formula_21">L RaSGAN D = −E xr∼P log D (x r ) ) − E x f ∼Q log 1 −D(x f ) ,<label>(17)</label></formula><p>whereD</p><formula xml:id="formula_22">(x) = sigmoid(C(x) − E x f ∼Q C(x f )) if x is real sigmoid(C(x) − E xr∼P C(x r )) if x is fake.<label>(18)</label></formula><p>RaD has a more similar interpretation to the standard discriminator than the relativistic discriminator. With RaD, the discriminator estimates the probability that the given real data is more realistic than fake data, on average. This approach has O(m) complexity. <ref type="table" target="#tab_0">Table 1</ref> shows an intuitive and memeful visual representation of how this approach works.</p><p>As before, we can generalize this approach to work with any GAN loss function using the following formulation:</p><formula xml:id="formula_23">L RaGAN D = E xr∼P f 1 C(x r ) − E x f ∼Q C(x f ) ) + E x f ∼Q [f 2 (C(x f ) − E xr∼P C(x r ))] . (19) L RaGAN G = E xr∼P g 1 C(x r ) − E x f ∼Q C(x f ) ) + E x f ∼Q [g 2 (C(x f ) − E xr∼P C(x r ))] .<label>(20)</label></formula><p>We call this general approach Relativistic average GAN (RaGAN). See Algorithm 2 for how to train non-saturating RaGANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Experiments were conducted on the CIFAR-10 dataset <ref type="bibr" target="#b15">[Krizhevsky, 2009]</ref> and the CAT dataset <ref type="bibr" target="#b16">[Zhang et al., 2008]</ref>. Code was written in Pytorch <ref type="bibr" target="#b17">[Paszke et al., 2017]</ref> and models were trained using the Adam optimizer [Kingma and Ba, 2014] for 100K generator iterations with seed 1 (which shows that we did not fish for the best seed, instead, we selected the seed a priori). We report the Fréchet Inception Distance (FID) <ref type="bibr" target="#b19">[Heusel et al., 2017]</ref>, a measure that is generally better correlated with data quality than the Inception Distance <ref type="bibr" target="#b20">[Salimans et al., 2016]</ref>  <ref type="bibr" target="#b21">[Borji, 2018]</ref>; lower FID means that the generated images are of better quality.</p><p>For the models architectures, we used the standard CNN described by <ref type="bibr" target="#b10">Miyato et al. [2018]</ref> on CIFAR-10 and a relatively standard DCGAN architecture <ref type="bibr" target="#b22">[Radford et al., 2015]</ref> on CAT (see Appendix).</p><p>We also provide the source code required to replicate all analyses presented in this paper (See our repository: www.github.com/AlexiaJM/RelativisticGAN).  <ref type="figure">sigmoid(C(x r )</ref>)) versus the Relativistic average Discriminator (RaD) (P (x r is real|C(x f )) = sigmoid(C(x r ) − C(x f ))). Breads represent real images, while dogs represent fake images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenario</head><p>Absolute probability Relative probability (Standard GAN) (Relativistic average Standard GAN)</p><p>Real image looks real and fake images look fake</p><formula xml:id="formula_24">C(x r ) = 8 C(x f ) = −5 P (x r is bread) = 1 P (x r is bread|C(x f )) = 1</formula><p>Real image looks real but fake images look similarly real on average C(x r ) = 8</p><p>C(x f ) = 7 P (x r is bread) = 1 P (x r is bread|C(x f )) = .73</p><p>Real image looks fake but fake images look more fake on average</p><formula xml:id="formula_25">C(x r ) = −3 C(x f ) = −5 P (x r is bread) = .05 P (x r is bread|C(x f )) = .88</formula><p>Algorithm 2 Training algorithm for non-saturating RaGANs Require: The number of D iterations n D (n D = 1 unless one seek to train D to optimality), batch size m, and functions f 1 and f 2 which determine the objective function of the discriminator (see equation 10). while θ has not converged do for t = 1, . . . , n D do</p><formula xml:id="formula_26">Sample {x (i) } m i=1 ∼ P Sample {z (i) } m i=1 ∼ P z Let C w (x r ) = 1 m m i=1 C w (x (i) ) Let C w (x f ) = 1 m m i=1 C w (G θ (z (i) )) Update w using SGD by ascending with ∇ w 1 m m i=1 f 1 (C w (x (i) ) − C w (x f )) + f 2 (C w (G θ (z (i) )) − C w (x r )) end for Sample {x (i) } m i=1 ∼ P Sample {z (i) } m i=1 ∼ P z Let C w (x r ) = 1 m m i=1 C w (x (i) ) Let C w (x f ) = 1 m m i=1 C w (G θ (z (i) )) Update θ using SGD by ascending with ∇ θ 1 m m i=1 f 1 (C w (G θ (z (i) )) − C w (x r )) + f 2 (C w (x (i) ) − C w (x f )) end while</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Easy/stable experiments</head><p>In these analyses, we compared standard GAN (SGAN), least-squares GAN (LSGAN), Wassertein GAN improved (WGAN-GP), Hinge-loss GAN (HingeGAN) <ref type="bibr" target="#b10">[Miyato et al., 2018]</ref>, Relativistic SGAN (RSGAN), Relativistic average SGAN (RaSGAN), Relativistic average LSGAN (RaLSGAN), and Relativistic average HingeGAN (RaHingeGAN) using the standard CNN architecture on stable setups (See Appendix for details on the loss functions used). Additionally, we tested RSGAN and RaSGAN with the same gradient-penalty as WGAN-GP (named RSGAN-GP and RaSGAN-GP respectively).</p><p>We used the following two known stable setups: (DCGAN setup) lr = .0002, n D = 1, β 1 = .50 and β 2 = .999 <ref type="bibr" target="#b22">[Radford et al., 2015]</ref>, and (WGAN-GP setup) lr = .0001, n D = 5, β 1 = .50 and β 2 = .9 <ref type="bibr" target="#b6">[Gulrajani et al., 2017]</ref>, where lr is the learning rate, n D is the number of discriminator updates per generator update, and β 1 , β 2 are the ADAM momentum parameters. For optimal stability, we used batch norm <ref type="bibr" target="#b23">[Ioffe and Szegedy, 2015]</ref> in G and spectral norm <ref type="bibr" target="#b10">[Miyato et al., 2018]</ref> in D.</p><p>Results are presented in <ref type="table">Table 2</ref>. We observe that RSGAN and RaSGAN generally performed better than SGAN. Similarly, RaHingeGAN performed better than HingeGAN. RaLSGAN performed on par with LSGAN, albeit sightly worse. WGAN-GP performed poorly in the DCGAN setup, but very well in the WGAN-GP setup. RasGAN-GP performed poorly; however, RSGAN-GP performed better than all other loss functions using only one discriminator update per generator update. Importantly, the resulting FID of 25.60 is on par with the lowest FID obtained for this architecture using spectral normalization, as reported by <ref type="bibr" target="#b10">Miyato et al. [2018]</ref> (25.5). Overall, these results show that using a relativistic discriminator generally improve data generation quality and that RSGAN works very well in conjunction with gradient penalty to obtain state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hard /unstable experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">CIFAR-10</head><p>In these analyses, we compared SGAN, LSGAN, WGAN-GP, RSGAN, RaSGAN, RaLSGAN, and RaHingeGAN with the standard CNN architecture on unstable setups in CIFAR-10. Unless otherwise specified, we used lr = .0002, β 1 = .5, β 2 = .999, n D = 1, and batch norm <ref type="bibr" target="#b23">[Ioffe and Szegedy, 2015]</ref> in G and D. We tested the following four unstable setups: (1) lr = .001, (2) β 1 = .9, β 2 = .9, <ref type="table">Table 2</ref>: Fréchet Inception Distance (FID) at exactly 100k generator iterations on the CIFAR-10 dataset using stable setups with different GAN loss functions. We used spectral norm in D and batch norm in G. All models were trained using the same a priori selected seed (seed=1). lr = .0002 lr = .0001 β = (.50, .999) β = (.50, .9) Loss n D = 1 n D = 5  <ref type="table">Table 3</ref>: Fréchet Inception Distance (FID) at exactly 100k generator iterations on the CIFAR-10 dataset using instable setups with different GAN loss functions. Unless otherwise specified, we used lr = .0002, β = (.50, .999), n D = 1, and batch norm (BN) in D and G. All models were trained using the same a priori selected seed (seed=1). (3) no batch norm in G or D, and (4) all activation functions replaced with Tanh in both G and D (except for the output activation function of D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>Results are presented in <ref type="table">Table 3</ref>. We observe that RaLSGAN performed better than LSGAN in all setups. RaHingeGAN performed slightly worse than HingeGAN in most setups. RSGAN and RaSGAN performed better than SGAN in two out of four setups, although differences were small. WGAN-GP generally performed poorly which we suspect is due to the single discriminator update per generator update. Overall, this provide good support for the improved stability of using the relative discriminator with LSGAN, but not with HingeGAN and SGAN. Although results are worse for the relativistic discriminator in some settings, differences are minimal and probably reflect natural variations.</p><p>It is surprising to observe low FID for SGAN without batch normalization considering its well-known difficulty with this setting . Given these results, we suspected that CIFAR-10 may be too easy to fully observe the stabilizing effects of using the relative discriminator. Therefore, our next analyses were done on the more difficult CAT dataset with high resolution pictures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">CAT</head><p>CAT is a dataset containing around 10k pictures of cats with annotations. We cropped the pictures to the faces of the cats using those annotations. After removing outliers (hidden faces, blurriness, etc.), the CAT dataset contained 9304 images ≥ 64x64, 6645 images ≥ 128x128, and 2011 images ≥ 256x256. Previous analyses 1 showed that the CAT dataset is particularly difficult in high-dimensions; SGAN generally has vanishing/exploding gradients with 64x64 images and is unable to generate 128x128 images without using certain tricks (e.g., unequal learning rates, Lipschitz discriminator, gradient penalty, etc.); this makes this dataset perfect for testing the stability of different GAN loss functions.</p><p>We trained different GAN loss functions on 64x64, 128x128, 256x256 images. For 256x256 images, we compared RaGANs to known stable approaches: SpectralSGAN (SGAN with spectral normalization in D) and WGAN-GP. Although some approaches were able to train on 256x256 images, they did so with significant mode collapse. To alleviate this problem, for 256x256 images, we packed the discriminator  (i.e., D took a concatenated pair of images instead of a single image). We looked at the mimimum, maximum, mean and standard deviation (SD) of the FID at 20k, 30k, ..., 100k generator iterations; results are presented in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>Overall, we observe lower minimum FID, maximum FID, mean and standard deviation (sd) for RGANs and RaGANs than their non-relativistic counterparts <ref type="figure">(SGAN, LSGAN, RaLSGAN)</ref>.</p><p>In 64x64 resolution, both SGAN and LSGAN generated images with low FID, but they did so in a very unstable matter. For example, SGAN went from a FID of 17.50 at 30k iterations, to 310.56 at 40k iterations, and back to 27.72 at 50k iterations. Similarly, LSGAN went from a FID of 20.27 at 20k iterations, to 224.97 at 30k iterations, and back to 51.98 at 40k iterations. On the other hand, RaGANs were much more stable (lower max and SD) while also resulting in lower minimum FID.</p><p>Using gradient-penalty did not improve data quality; however, it reduced the SD lower than without gradient penalty, thus increasing stability further.</p><p>SGAN was unable to converge on 128x128 or bigger images and LSGAN was unable to converge on 256x256 images. Meanwhile, RaGANs were able to generate plausible images with low FID in all resolutions. Although SpectralSGAN and WGAN-GP were able to generate 256x256 images of cats, the samples they generated were of poor quality (high FID). Thus, in this very difficult setting, relativism provided a greater improvement in quality than gradient penalty or spectral normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>In this paper, we proposed the relativistic discriminator as a way to fix and improve on standard GAN. We further generalized this approach to any GAN loss and introduced a generally more stable variant called RaD. Our results suggest that relativism significantly improve data quality and stability of GANs at no computational cost. Furthermore, using a relativistic discriminator with other tools of the trade (spectral norm, gradient penalty, etc.) may lead to better state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><formula xml:id="formula_27">A Gradient step A.1 SGAN ∇ w L GAN D = −∇ w E xr∼P [log D(x r )] − ∇ w E x f ∼Q θ [log(1 − D(x f ))] = −∇ w E xr∼P log e C(xr) e C(xr) + 1 − ∇ w E x f ∼Q θ log 1 − e C(x f ) e C(x f ) + 1 = −∇ w E xr∼P C(x r ) − log e C(xr) + 1 − ∇ w E x f ∼Q θ log(1) − log e C(x f ) + 1 = −E xr∼P [∇ w C(x r )] + E xr∼P e C(xr) e C(xr) + 1 ∇ w C(x r ) + E x f ∼Q θ e C(x f ) e C(x f ) + 1 ∇ w C(x f ) = −E xr∼P [∇ w C(x r )] + E xr∼P [D(x r )∇ w C(x r )] + E x f ∼Q θ [D(x f )∇ w C(x f )] = −E xr∼P [(1 − D(x r ))∇ w C(x r )] + E x f ∼Q θ [D(x f )∇ w C(x f )] ∇ θ L GAN G = −∇ θ E z∼Pz [log D(G(z))] = −∇ θ E z∼Pz log e C(G(z)) e C(G(z)) + 1 = −∇ θ E z∼Pz C(G(z)) − log e C(G(z)) + 1 = −E z∼Pz ∇ x C(G(z))J θ G(z) − e C(G(z)) e C(G(z)) + 1 ∇ x C(G(z))J θ G(z) = −E z∼Pz [(1 − D(G(z)))∇ x C(G(z))J θ G(z)]</formula><p>A.2 IPM-based GANs</p><formula xml:id="formula_28">∇ w L IP M D = −∇ w E xr∼P [C(x r )] + ∇ w E x f ∼Q θ [C(x f )] = −E xr∼P [∇ w C(x r )] + E x f ∼Q θ [∇ w C(x f )] ∇ θ L IP M G = −∇ θ E z∼Pz [C(G(z))] = −E z∼Pz [∇ x C(G(z))J θ G(z)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Simplified form of relativistic saturating and non-saturating GANs</head><p>Assuming f 2 (−y) = f 1 (y), we have that</p><formula xml:id="formula_29">L RGAN D = E (xr,x f )∼(P,Q) [f 1 (C(x r ) − C(x f ))] + E (xr,x f )∼(P,Q) [f 2 (C(x f ) − C(x r ))] = E (xr,x f )∼(P,Q) [f 1 (C(x r ) − C(x f ))] + E (xr,x f )∼(P,Q) [f 1 (C(x r ) − C(x f ))] = 2E (xr,x f )∼(P,Q) [f 1 (C(x r ) − C(x f ))] .</formula><p>If g 1 (y) = −f 1 (y) and g 2 (y) = −f 2 (y) (saturating GAN), we have that </p><formula xml:id="formula_30">L RGAN −S G = E (xr,x f )∼(P,Q) [g 1 (C(x r ) − C(x f ))] + E (xr,x f )∼(P,Q) [g 2 (C(x f ) − C(x r ))] = −E (xr,x f )∼(P,Q) [f 1 (C(x r ) − C(x f ))] − E (xr,x f )∼(P,Q) [f 2 (C(x f ) − C(x r ))] = −E (xr,x f )∼(P,Q) [f 1 (C(x r ) − C(x f ))] − E (xr,<label>x</label></formula><p>C.3 RaSGAN </p><formula xml:id="formula_32">L RaSGAN D = −E xr∼P log D (x r ) − E x f ∼Q log 1 −D(x f ) (25) L RaSGAN G = −E x f ∼Q log D (x f ) − E xr∼P log 1 −D(x r ) (26) D(x r ) = sigmoid C(x r ) − E x f ∼Q C(x f ) D (x f ) = sigmoid (C(x f ) − E xr∼P C(x r )) C.4 LSGAN L LSGAN D = E xr∼P (C(x r ) − 0) 2 + E x f ∼Q (C(x f ) − 1) 2 (27) L LSGAN G = E x f ∼Q (C(x f ) − 0) 2 (28) C.5 RaLSGAN L RaLSGAN D = E xr∼P (C(x r ) − E x f ∼Q C(x f ) − 1) 2 + E x f ∼Q (C(x f ) − E xr∼P C(x r ) + 1) 2 (29) L RaLSGAN G = E x f ∼P (C(x f ) − E xr∼P C(x r ) − 1) 2 + E xr∼P (C(x r ) − E x f ∼Q C(x f ) + 1) 2<label>(30)</label></formula><formula xml:id="formula_33">L RaSGAN G = −E x f ∼Q log D (x f ) − E xr∼P log 1 −D(x r ) (40) D(x r ) = sigmoid C(x r ) − E x f ∼Q C(x f ) D (x f ) = sigmoid (C(x f ) − E xr∼P C(x r ))</formula><p>Px is the distribution ofx = </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>f )∼(P,Q) [f 1 (C(x r ) − C(x f ))] = −2E (xr,x f )∼(P,Q) [f 1 (C(x r ) − C(x f ))] .If g 1 (y) = f 2 (y) and g 2 (y) = f 1 (y) (non-saturating GAN), we have thatL RGAN −N S G = E (xr,x f )∼(P,Q) [g 1 (C(x r ) − C(x f ))] + E (xr,x f )∼(P,Q) [g 2 (C(x f ) − C(x r ))] = E (xr,x f )∼(P,Q) [f 2 (C(x r ) − C(x f ))] + E (xr,x f )∼(P,Q) [f 1 (C(x f ) − C(x r ))] = E (xr,x f )∼(P,Q) [f 1 (C(x f ) − C(x r ))] + E (xr,x f )∼(P,Q) [f 1 (C(x f ) − C(x r ))] = 2E (xr,x f )∼(P,Q) [f 1 (C(x f ) − C(x r ))] . xr∼P [log (sigmoid(C(x r )))] − E x f ∼Q [log (1 − sigmoid(C(x f )))] x f ∼Q [log (sigmoid(C(x f )))] (xr,x f )∼(P,Q) [log(sigmoid(C(x r ) − C(x f )))](23)L RSGAN G = −E (xr,x f )∼(P,Q) [log(sigmoid(C(x f ) − C(x r )))]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>===</head><label></label><figDesc>E xr∼P [max(0, 1 − C(x r ))] + E x f ∼Q [max(0, 1 + C(x f ))] E xr∼P max(0, 1 −D(x r )) + E x f ∼Q max(0, 1 +D(x f )) E x f ∼P max(0, 1 −D(x f )) + E xr∼Q max(0, 1 +D(x r )) (34) D(x r ) = C(x r ) − E x f ∼Q C(x f ) D(x f ) = C(x f ) − E xr∼P C(x r ) C.8 WGAN-GP L W GAN −GP D = −E xr∼P [C(x r )] + E x f ∼Q [C(x f )] + λEx ∼Px (||∇xC(x) || 2 − 1) x f ∼Q [C(x f )] (36) Px is the distribution ofx = x r + (1 − )x f , where x r ∼ P, x f ∼ Q, ∼ U [0, 1]. (xr,x f )∼(P,Q) [log(sigmoid(C(x r ) − C(x f )))] + λEx ∼Px (||∇xC(x) || 2 − 1) (xr,x f )∼(P,Q) [log(sigmoid(C(x f ) − C(x r )))](38)Px is the distribution ofx =x r + (1 − )x f , where x r ∼ P, x f ∼ Q, ∼ U [0, 1]. C.10 RaSGAN-GP L RaSGAN D = −E xr∼P log D (x r ) −E x f ∼Q log 1 −D(x f ) +λEx ∼Px (||∇xC(x) || 2 − 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>x r + (1 − )x f , where x r ∼ P, x f ∼ Q, ∼ U [0, 1].64x64 cats with RaLSGAN (FID = 11.97) Figure 4: 256x256 cats with GAN (5k iterations) Figure 5: 256x256 cats with LSGAN (5k iterations) Figure 6: 256x256 cats with RaSGAN (FID = 32.11) Figure 7: 256x256 cats with RaLSGAN (FID = 35.21) Figure 8: 256x256 cats with SpectralSGAN (FID = 54.73) Figure 9: 256x256 cats with WGAN-GP (FID &gt; 100)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A illustrative example of the discriminator's output in standard GAN as traditionally defined (P (x r is real) =</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Minimum (min), maximum (max), mean, and standard deviation (SD) of the Fréchet Inception Distance (FID) calculated at 20k, 30k . . . , 100k generator iterations on the CAT dataset with different GAN loss functions. The hyperparameters used were lr = .0002, β = (.50, .999), n D = 1, and batch norm (BN) in D and G. All models were trained using the same a priori selected seed (seed=1).</figDesc><table><row><cell>Loss</cell><cell>Min</cell><cell>Max</cell><cell>Mean</cell><cell>SD</cell></row><row><cell></cell><cell cols="3">64x64 images (N=9304)</cell><cell></cell></row><row><cell>SGAN</cell><cell cols="3">16.56 310.56 52.54</cell><cell>96.81</cell></row><row><cell>RSGAN</cell><cell>19.03</cell><cell>42.05</cell><cell>32.16</cell><cell>7.01</cell></row><row><cell>RaSGAN</cell><cell>15.38</cell><cell>33.11</cell><cell>20.53</cell><cell>5.68</cell></row><row><cell>LSGAN</cell><cell cols="3">20.27 224.97 73.62</cell><cell>61.02</cell></row><row><cell>RaLSGAN</cell><cell>11.97</cell><cell>19.29</cell><cell>15.61</cell><cell>2.55</cell></row><row><cell>HingeGAN</cell><cell>17.60</cell><cell>50.94</cell><cell>32.23</cell><cell>14.44</cell></row><row><cell>RaHingeGAN</cell><cell>14.62</cell><cell>27.31</cell><cell>20.29</cell><cell>3.96</cell></row><row><cell>RSGAN-GP</cell><cell>16.41</cell><cell>22.34</cell><cell>18.20</cell><cell>1.82</cell></row><row><cell>RaSGAN-GP</cell><cell>17.32</cell><cell>22</cell><cell>19.58</cell><cell>1.81</cell></row><row><cell cols="4">128x128 images (N=6645)</cell><cell></cell></row><row><cell>SGAN 2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RaSGAN</cell><cell>21.05</cell><cell>39.65</cell><cell>28.53</cell><cell>6.52</cell></row><row><cell>LSGAN</cell><cell>19.03</cell><cell>51.36</cell><cell>30.28</cell><cell>10.16</cell></row><row><cell>RaLSGAN</cell><cell>15.85</cell><cell>40.26</cell><cell>22.36</cell><cell>7.53</cell></row><row><cell cols="4">256x256 images (N=2011)</cell><cell></cell></row><row><cell>SGAN 2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RaSGAN</cell><cell cols="3">32.11 102.76 56.64</cell><cell>21.03</cell></row><row><cell cols="2">SpectralSGAN 54.08</cell><cell>90.43</cell><cell>64.92</cell><cell>12.00</cell></row><row><cell>LSGAN 2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RaLSGAN</cell><cell cols="3">35.21 299.52 70.44</cell><cell>86.01</cell></row><row><cell>WGAN-GP</cell><cell cols="4">155.46 437.48 341.91 101.11</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As reported on https://ajolicoeur.wordpress.com/cats. 2 Didn't converge, became stuck in the first few iterations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">See https://github.com/AlexiaJM/RelativisticGAN/tree/master/images/full_minibatch for all cats of the mini-batch.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 3: 128x128 cats with RaLSGAN (FID = 15.85)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Future research is needed to fully understand the mathematical implications of adding relativism to GANs. Furthermore, our experiments were limited to certain loss functions using only one seed, due to computational constraints. More experiments are required to determine which relativistic GAN loss function is best over a wide-range of datasets and hyperparameters. We greatly encourage researchers and machine learning enthusiasts with greater computing power to experiment further with our approach.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">How generative adversarial nets and its variants work: An overview of gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uiwon</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyoon</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05914</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<title level="m">Towards principled methods for training generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Are gans created equal? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10337</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integral probability metrics and their generating classes of functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04894</idno>
		<title level="m">Anant Raj, and Yu Cheng. Sobolev gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">I</forename><surname>Fisher Gan. In</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6845-fisher-gan.pdf" />
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2513" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Many paths to equilibrium: Gans do not need to decrease adivergence at every step</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08446</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">How to train your dragan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07215</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><forename type="middle">D</forename><surname>Tomioka. F-Gan ; D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6066-f-gan-training-generative-neural-samplers-using-variational-divergence-minimization.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02145</idno>
		<title level="m">Gans beyond divergence minimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cat head detection-how to effectively exploit shape and texture features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="802" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pros and cons of gan evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03446</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pacgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04086</idno>
		<title level="m">The power of two samples in generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="64" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="128" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="256" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">Conv2d 3x3</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="256" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">128-&gt;512 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias, 512-&gt;256 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias, 256-&gt;128 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias, 128-&gt;64 BN and ReLU ConvTranspose2d 4x4</title>
		<imprint>
			<biblScope unit="page" from="64" to="67" />
		</imprint>
	</monogr>
	<note>ConvTranspose2d 4x4, stride 1, pad 0, no bias. pad 1, no bias</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<idno>64-&gt;128 BN and LeakyReLU 0.2</idno>
		<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<idno>128-&gt;256 BN and LeakyReLU 0.2</idno>
		<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<idno>256-&gt;512 BN and LeakyReLU 0.2</idno>
		<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="512" to="513" />
		</imprint>
	</monogr>
	<note>no bias</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">128-&gt;1024 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias, 1024-&gt;512 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias, 512-&gt;256 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias, 256-&gt;128 BN and ReLU ConvTranspose2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="64" to="67" />
		</imprint>
	</monogr>
	<note>ConvTranspose2d 4x4, stride 1, pad 0, no bias. no bias, 128-&gt;64 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<idno>64-&gt;128 BN and LeakyReLU 0.2</idno>
		<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<idno>128-&gt;256 BN and LeakyReLU 0.2</idno>
		<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<idno>256-&gt;512 BN and LeakyReLU 0.2</idno>
		<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<idno>512-&gt;1024 BN and LeakyReLU 0.2</idno>
		<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1024" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">128-&gt;1024 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias, 1024-&gt;512 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias, 512-&gt;256 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias, 256-&gt;128 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias, 128-&gt;64 BN and ReLU ConvTranspose2d 4x4, stride 2, pad 1, no bias, 64-&gt;32 BN and ReLU ConvTranspose2d 4x4</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>ConvTranspose2d 4x4, stride 1, pad 0, no bias. no bias, 64-&gt;3 Tanh Discriminator (PACGAN2 [Lin et al.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">x 1 ∈ R 3x256x256 , x 2 ∈ R 3x256x256</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Concatenate</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>x 1 , x 2 ] ∈ R 6x256x256</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="32" to="64" />
		</imprint>
	</monogr>
	<note>no bias</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<idno>64-&gt;128 BN and LeakyReLU 0.2</idno>
		<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<idno>128-&gt;256 BN and LeakyReLU 0.2</idno>
		<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<idno>256-&gt;512 BN and LeakyReLU 0.2</idno>
		<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<idno>512-&gt;1024 BN and LeakyReLU 0.2</idno>
		<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<title level="m">Conv2d 4x4</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1024" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">This shows a selection of cats from certain models. Images shown are from the lowest FID registered at every 10k generator iterations. Given space constraint, with higher resolutions cats, we show some of the nicer looking cats for each approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Samples</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>there are evidently some worse looking cats 3</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
