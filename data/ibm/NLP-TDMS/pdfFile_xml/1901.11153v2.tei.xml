<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research § Peng Cheng Laboratory †</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research § Peng Cheng Laboratory †</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research § Peng Cheng Laboratory †</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research § Peng Cheng Laboratory †</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
							<email>s.zhang@hit.edu.cn‡zhoushangchen@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research § Peng Cheng Laboratory †</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/project/pix2vox</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-ofthe-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D reconstruction is an important problem in robotics, CAD, virtual reality and augmented reality. Traditional methods, such as Structure from Motion (SfM) <ref type="bibr" target="#b13">[14]</ref> and Simultaneous Localization and Mapping (SLAM) <ref type="bibr" target="#b5">[6]</ref>, match image features across views. However, establishing feature correspondences becomes extremely difficult when multiple viewpoints are separated by a large margin due to local appearance changes or self-occlusions <ref type="bibr" target="#b11">[12]</ref>. To overcome these limitations, several deep learning based approaches, including 3D-R2N2 <ref type="bibr" target="#b1">[2]</ref>, LSM <ref type="bibr" target="#b8">[9]</ref>, and 3DensiNet <ref type="bibr" target="#b27">[28]</ref>, have been proposed to recover the 3D shape of an object and obtained promising results.</p><p>To generate 3D volumes, 3D-R2N2 <ref type="bibr" target="#b1">[2]</ref> and LSM <ref type="bibr" target="#b8">[9]</ref> formulate multi-view 3D reconstruction as a sequence learning problem and use recurrent neural networks (RNNs) to fuse multiple feature maps extracted by a shared encoder from input images. The feature maps are incrementally refined when more views of an object are available. However, RNN-based methods suffer from three limitations. First, when given the same set of images with different orders, RNNs are unable to estimate the 3D shape of an object consistently results due to permutation variance <ref type="bibr" target="#b26">[27]</ref>. Second, due to long-term memory loss of RNNs, the input images cannot be fully exploited to refine reconstruction results <ref type="bibr" target="#b14">[15]</ref>. Last but not least, <ref type="bibr">RNN-</ref>  <ref type="figure">Figure 2</ref>: An overview of the proposed Pix2Vox. The network recovers the shape of 3D objects from arbitrary (uncalibrated) single or multiple images. The reconstruction results can be refined when more input images are available. Note that the weights of the encoder and decoder are shared among all views. without parallelization <ref type="bibr" target="#b7">[8]</ref>.</p><p>To address the issues mentioned above, we propose Pix2Vox, a novel framework for single-view and multi-view 3D reconstruction that contains four modules: encoder, decoder, context-aware fusion, and refiner. The encoder and decoder generate coarse 3D volumes from multiple input images in parallel, which eliminates the effect of the orders of input images and accelerates the computation. Then, the context-aware fusion module selects high-quality reconstructions from all coarse 3D volumes and generates a fused 3D volume, which fully exploits information of all input images without long-term memory loss. Finally, the refiner further correct wrongly recovered parts of the fused 3D volumes to obtain a refined reconstruction. To achieve a good balance between accuracy and model size, we implement two versions of the proposed framework: Pix2Vox-F and Pix2Vox-A ( <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>The contributions can be summarized as follows:</p><p>• We present a unified framework for both single-view and multi-view 3D reconstruction, namely Pix2Vox. We equip Pix2Vox with well-designed encoder, decoder, and refiner, which shows a powerful ability to handle 3D reconstruction in both synthetic and realworld images.</p><p>• We propose a context-aware fusion module to adaptively select high-quality reconstructions for each part from different coarse 3D volumes in parallel to produce a fused reconstruction of the whole object. To the best of our knowledge, it is the first time to exploit context across multiple views for 3D reconstruction.</p><p>• Experimental results on the ShapeNet <ref type="bibr" target="#b33">[34]</ref> and Pix3D <ref type="bibr" target="#b22">[23]</ref> datasets demonstrate that the proposed approaches outperform state-of-the-art methods in terms of both accuracy and efficiency. Additional experiments also show its strong generalization abilities in reconstructing unseen 3D objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single-view 3D Reconstruction Theoretically, recovering 3D shape from single-view images is an ill-posed problem.</p><p>To address this issue, many attempts have been made, such as ShapeFromX <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>, where X may represent silhouettes <ref type="bibr" target="#b3">[4]</ref>, shading <ref type="bibr" target="#b16">[17]</ref>, and texture <ref type="bibr" target="#b30">[31]</ref>. However, these methods are barely applicable to use in the real-world scenarios, because all of them require strong presumptions and abundant expertise in natural images <ref type="bibr" target="#b35">[36]</ref>. With the success of generative adversarial networks (GANs) <ref type="bibr" target="#b6">[7]</ref> and variational autoencoders (VAEs) <ref type="bibr" target="#b10">[11]</ref>, 3D-VAE-GAN <ref type="bibr" target="#b32">[33]</ref> adopts GAN and VAE to generate 3D objects by taking a single-view image as input. However, 3D-VAE-GAN requires class labels for reconstruction. MarrNet <ref type="bibr" target="#b31">[32]</ref> reconstructs 3D objects by estimating depth, surface normals, and silhouettes of 2D images, which is challenging and usually leads to severe distortion <ref type="bibr" target="#b24">[25]</ref>. OGN <ref type="bibr" target="#b23">[24]</ref> and O-CNN <ref type="bibr" target="#b29">[30]</ref> use octree to represent higher resolution volumetric 3D objects with a limited memory budget. However, OGN representations are complex and consume more computational resources due to the complexity of octree representations. PSGN <ref type="bibr" target="#b4">[5]</ref> and 3D-LMNet <ref type="bibr" target="#b12">[13]</ref> generate point clouds from single-view images. However, the points have a large degree of freedom in the point cloud representation because of the limited connections between points. Consequently, these methods cannot recover 3D volumes accurately <ref type="bibr" target="#b28">[29]</ref>.</p><p>Multi-view 3D Reconstruction SfM <ref type="bibr" target="#b13">[14]</ref> and SLAM <ref type="bibr" target="#b5">[6]</ref> methods are successful in handling many scenarios. These methods match features among images and estimate the camera pose for each image. However, the matching process becomes difficult when multiple viewpoints are separated by a large margin. Besides, scanning all surfaces of an object before reconstruction is sometimes impossible, which leads to incomplete 3D shapes with occluded or hollowed-out areas <ref type="bibr" target="#b34">[35]</ref>. Powered by large-scale datasets of 3D CAD models (e.g., ShapeNet <ref type="bibr" target="#b33">[34]</ref>), deep-learning- . To reduce the model size, the refiner is removed in Pix2Vox-F.</p><formula xml:id="formula_0">224×224×3 Image 3 % ×64 conv2D 3 % ×64 conv2D 3 % ×128 conv2D 2 % MaxPool 3 % ×128 conv2D 3 % ×256 conv2D 2 % MaxPool 3 % ×256 conv2D 3 % ×256 conv2D 2 % MaxPool 3 % ×512 conv2D 3 % ×512 conv2D 3 % ×512</formula><p>based methods have been proposed for 3D reconstruction. Both 3D-R2N2 <ref type="bibr" target="#b1">[2]</ref> and LSM <ref type="bibr" target="#b8">[9]</ref> use RNNs to infer 3D shape from single or multiple input images and achieve impressive results. However, RNNs are time-consuming and permutation-variant, which produce inconsistent reconstruction results. 3DensiNet <ref type="bibr" target="#b27">[28]</ref> uses max pooling to aggregate the features from multiple images. However, max pooling only extracts maximum values from features, which may ignore other valuable features that are useful for 3D reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The proposed Pix2Vox aims to reconstruct the 3D shape of an object from either single or multiple RGB images. The 3D shape of an object is represented by a 3D voxel grid, where 0 is an empty cell and 1 denotes an occupied cell. The key components of Pix2Vox are shown in <ref type="figure">Figure  2</ref>. First, the encoder produces feature maps from input images. Second, the decoder takes each feature map as input and generates a coarse 3D volume correspondingly. Third, single or multiple 3D volumes are forwarded to the contextaware fusion module, which adaptively selects high-quality reconstructions for each part from coarse 3D volumes to obtain a fused 3D volume. Finally, the refiner with skipconnections further refines the fused 3D volume to generate the final reconstruction result. <ref type="figure" target="#fig_2">Figure 3</ref> shows the detailed architectures of Pix2Vox-F and Pix2Vox-A. The former involves much fewer parameters and lower computational complexity, while the latter has more parameters, which can construct more accurate 3D shapes but has higher computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Encoder</head><p>The encoder is to compute a set of features for the decoder to recover the 3D shape of the object. The first nine convolutional layers, along with the corresponding batch normalization layers and ReLU activations of a VGG16 <ref type="bibr" target="#b20">[21]</ref> pretrained on ImageNet <ref type="bibr" target="#b2">[3]</ref>, are used to extract a 512 × 28 × 28 feature tensor from a 224 × 224 × 3 image. This feature extraction is followed by three sets of 2D convolutional layers, batch normalization layers and ELU layers to embed semantic information into feature vectors. In Pix2Vox-F, the kernel size of the first convolutional layer is 1 2 while the kernel sizes of the other two are 3 2 . The number of output channels of the convolutional layer starts with 512 and decreases by half for the subsequent layer and ends up with 128. In Pix2Vox-A, the kernel sizes of the three convolutional layers are 3 2 , 3 2 , and 1 2 , respectively. The output channels of the three convolutional layers are 512, 512, and 256, respectively. After the second convolutional layer, there is a max pooling layer with kernel sizes of 3 2 and 4 2 in Pix2Vox-F and Pix2Vox-A, respectively. The feature vectors produced by Pix2Vox-F and Pix2Vox-A are of sizes 2048 and 16384, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Decoder</head><p>The decoder is responsible for transforming information of 2D feature maps into 3D volumes. There are five 3D transposed convolutional layers in both Pix2Vox-F and Pix2Vox-A. Specifically, the first four transposed convolutional layers are of a kernel size of 4 3 , with stride of 2 and padding of 1. There is an additional transposed convolutional layer with a bank of 1 3 filter. Each transposed convolutional layer is followed by a batch normalization layer and a ReLU activation except for the last layer followed by a sigmoid function. In Pix2Vox-F, the numbers of output channels of the transposed convolutional layers are 128, 64, 32, 8, and 1, re- It aims to select high-quality reconstructions for each part to construct the final results. The objects in the bounding box describe the procedure score calculation for a coarse volume v c n . The other scores are calculated according to the same procedure. Note that the weights of the context scoring network are shared among different views. spectively. In Pix2Vox-A, the numbers of output channels of the five transposed convolutional layers are 512, 128, 32, 8, and 1, respectively. The decoder outputs a 32 3 voxelized shape in the object's canonical view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Context-aware Fusion</head><p>From different viewpoints, we can see different visible parts of an object. The reconstruction qualities of visible parts are much higher than those of invisible parts. Inspired by this observation, we propose a context-aware fusion module to adaptively select high-quality reconstruction for each part (e.g., table legs) from different coarse 3D volumes. The selected reconstructions are fused to generate a 3D volume of the whole object ( <ref type="figure">Figure 4</ref>).</p><p>As shown in <ref type="figure">Figure 5</ref>, given coarse 3D volumes and the corresponding context, the context-aware fusion module generates a score map for each coarse volume and then fuses them into one volume by the weighted summation of all coarse volumes according to their score maps. The spatial information of voxels is preserved in the context-aware fusion module, and thus Pix2Vox can utilize multi-view information to recover the structure of an object better.</p><p>Specifically, the context-aware fusion module generates the context c r of the r-th coarse volume v c r by concatenating the output of the last two layers in the decoder. Then, the context scoring network generates a score m r for the context of the r-th coarse voxel. The context scoring network is composed of five sets of 3D convolutional layers, each of which has a kernel size of 3 3 and padding of 1, followed by a batch normalization and a leaky ReLU activation. The numbers of output channels of convolutional layers are 9, 16, 8, 4, and 1, respectively. The learned score m r for context c r are normalized across all learnt scores. We choose softmax as the normalization function. Therefore, the score s (i,j,k) r at position (i, j, k) for the r-th voxel can be calculated as</p><formula xml:id="formula_1">s (i,j,k) r = exp m (i,j,k) r n p=1 exp m (i,j,k) p<label>(1)</label></formula><p>where n represents the number of views. Finally, the fused voxel v f is produced by summing up the product of coarse voxels and the corresponding scores altogether. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Refiner</head><p>The refiner can be seen as a residual network, which aims to correct wrongly recovered parts of a 3D volume. It follows the idea of a 3D encoder-decoder with the U-net connections <ref type="bibr" target="#b17">[18]</ref>. With the help of the U-net connections between the encoder and decoder, the local structure in the fused volume can be preserved. Specifically, the encoder has three 3D convolutional layers, each of which has a bank of 4 3 filters with padding of 2, followed by a batch normalization layer, a leaky ReLU activation and a max pooling layer with a kernel size of 2 3 . The numbers of output channels of convolutional layers are 32, 64, and 128, respectively. The encoder is finally followed by two fully connected layers with dimensions of 2048 and 8192. The decoder consists of three transposed convolutional layers, each of which has a bank of 4 3 filters with padding of 2 and stride of 1.</p><p>Except for the last transposed convolutional layer that is followed by a sigmoid function, other layers are followed by a batch normalization layer and a ReLU activation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Loss Function</head><p>The loss function of the network is defined as the mean value of the voxel-wise binary cross entropies between the reconstructed object and the ground truth. More formally, it can be defined as</p><formula xml:id="formula_2">= 1 N N i=1 [gt i log(p i ) + (1 − gt i ) log(1 − p i )]<label>(3)</label></formula><p>where N denotes the number of voxels in the ground truth. p i and gt i represent the predicted occupancy and the corresponding ground truth. The smaller the value is, the closer the prediction is to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>Datasets We evaluate the proposed Pix2Vox-F and Pix2Vox-A on both synthetic images of objects from the ShapeNet <ref type="bibr" target="#b33">[34]</ref> dataset and real images from the Pix3D <ref type="bibr" target="#b22">[23]</ref> dataset. More specifically, we use a subset of ShapeNet consisting of 13 major categories and 43,783 3D models following the settings of <ref type="bibr" target="#b1">[2]</ref>. As for Pix3D, we use the 2,894 untruncated and unoccluded chair images following the settings of <ref type="bibr" target="#b22">[23]</ref>. Evaluation Metrics To evaluate the quality of the output from the proposed methods, we binarize the probabilities at a fixed threshold of 0.3 and use intersection over union (IoU) as the similarity measure. More formally,</p><formula xml:id="formula_3">IoU = i,j,k I(p (i,j,k) &gt; t)I(gt (i,j,k) ) i,j,k I I(p (i,j,k) &gt; t) + I(gt (i,j,k) )<label>(4)</label></formula><p>where p (i,j,k) and gt (i,j,k) represent the predicted occupancy probability and the ground truth at (i, j, k), respectively. I(·) is an indicator function and t denotes a voxelization threshold. Higher IoU values indicate better reconstruction results. <ref type="figure">Figure 6</ref>: Single-view (left) and multi-view (right) reconstructions on the ShapeNet testing set. GT represents the ground truth of the 3D object. Note that DRC <ref type="bibr" target="#b25">[26]</ref> is trained/tested per category.</p><formula xml:id="formula_4">Input GT 3D-R2N2 OGN DRC Pix2Vox-F Pix2Vox-A Multi-view Inputs (3 views) GT 3D-R2N2 Pix2Vox-F Pix2Vox-A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use 224 × 224 RGB images as input to train the proposed methods with a shape batch size of 64. The output voxelized reconstruction is 32 3 in size. We implement our network in PyTorch <ref type="bibr" target="#b15">[16]</ref> and train both Pix2Vox-F and Pix2Vox-A using an Adam optimizer <ref type="bibr" target="#b9">[10]</ref> with a β 1 of 0.9 and a β 2 of 0.999. The initial learning rate is set to 0.001 and decayed by 2 after 150 epochs. First, we train both networks except the context-aware fusion feeding with a single-view image for 250 epochs. Then, we train the whole network jointly feeding with random numbers of input images for 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Reconstruction of Synthetic Images</head><p>To evaluate the performance of the proposed methods in handling synthetic images, we compare our methods against several state-of-the-art methods on the ShapeNet testing set. To make a fair comparison, all methods are compared with the same input images for all experiments except PSGN <ref type="bibr" target="#b4">[5]</ref>. Although PSGN uses much more data during training, Pix2Vox-A still performs better in recovering the 3D shape of an object. <ref type="table" target="#tab_0">Table 1</ref> shows the performance of single-view reconstruction, while <ref type="table" target="#tab_1">Table 2</ref> shows the mean IoU scores of multi-view reconstruction with different numbers of views.</p><p>The single-view reconstruction results of Pix2Vox-F and Pix2Vox-A significantly outperform other methods <ref type="table" target="#tab_0">(Table  1)</ref>. Pix2Vox-A increases IoU over 3D-R2N2 by 18%. In multi-view reconstruction, Pix2Vox-A consistently outperforms 3D-R2N2 in all numbers of views ( <ref type="table" target="#tab_1">Table 2</ref>). The IoU of Pix2Vox-A is 13% higher than that of 3D-R2N2. <ref type="table">Table 3</ref>: Single-view reconstruction on Pix3D compared using Intersection-over-Union (IoU). The best number is highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>IoU 3D-R2N2 <ref type="bibr" target="#b1">[2]</ref> 0.136 DRC <ref type="bibr" target="#b25">[26]</ref> 0.265 Pix3D (w/o Pose) <ref type="bibr" target="#b22">[23]</ref> 0.267 Pix3D (w/ Pose) <ref type="bibr" target="#b22">[23]</ref> 0.282 Pix2Vox-F 0.271 Pix2Vox-A 0.288 <ref type="figure">Figure 6</ref> shows several reconstruction examples from the ShapeNet testing set. Both Pix2Vox-F and Pix2Vox-A are able to recover the thin parts of objects, such as lamps and table legs. Compare with Pix2Vox-F, we also observe that higher dimensional feature maps in Pix2Vox-A do contribute to 3D reconstruction. Moreover, in multi-view reconstruction, both Pix2Vox-A and Pix2Vox-F produce better results than 3D-R2N2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Reconstruction of Real-world Images</head><p>To evaluate the performance on of the proposed methods on real-world images, we test our methods for single-view reconstruction on the Pix3D dataset.</p><p>We use the pipeline of RenderForCNN <ref type="bibr" target="#b21">[22]</ref> to generate 60 images for each 3D CAD model in the ShapeNet dataset. We perform quantitative evaluation of the resulting models on real-world RGB images using the Pix3D dataset. Besides, we augment our training data by random color and light jittering. First, the images are cropped according to the bounding box of the objects within the image. Then, these cropped images are rescaled as required by each reconstruction network. The mean IoU of the Pix3D dataset is reported in <ref type="table">Table  3</ref>. The experimental results indicate Pix2Vox-A outperform the competing approaches on the Pix3D testing set without estimating the pose of an object. The qualitative analysis is given in <ref type="figure" target="#fig_5">Figure 7</ref>, which indicate that the proposed methods are more effective in handling real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Reconstruction of Unseen Objects</head><p>In order to test how well our methods can generalize to unseen objects, we conduct additional experiments on ShapeNetCore <ref type="bibr" target="#b33">[34]</ref>. We use Mitsuba 1 to render objects in the remaining 44 categories of ShapeNetCore from 24 random views along with voxel representations. All pretrained models have never "seen" either the objects in these categories or the labels of objects before. More specifically, all models are trained on the 13 major categories of ShapeNet renderings provided by <ref type="bibr" target="#b1">[2]</ref> and tested on the remaining 44 categories of ShapeNetCore with the same input images. The reconstruction results of 3D-R2N2 are obtained with the released pretrained model.</p><p>Several reconstruction results are presented in <ref type="figure">Figure  8</ref>. The reconstruction IoU of 3D-R2N2 on unseen objects 1 https://www.mitsuba-renderer.org</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>GT 3D-R2N2 Pix2Vox-F Pix2Vox-A <ref type="figure">Figure 8</ref>: Reconstruction on unseen objects of ShapeNet from 5-view images. GT represents the ground truth of the 3D object.</p><p>is 0.120, while Pix2Vox-F and Pix2Vox-A are 0.209 and 0.227, respectively. Experimental results demonstrate that 3D-R2N2 can hardly recover the shape of unseen objects. In contrast, Pix2Vox-F and Pix2Vox-A show satisfactory generalization abilities to unseen objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>In this section, we validate the context-aware fusion and the refiner by ablation studies. Context-aware fusion To quantitatively evaluate the context-aware fusion, we replace the context-aware fusion in Pix2Vox-A with the average fusion, where the fused voxel v f can be calculated as <ref type="table" target="#tab_1">Table 2</ref> shows that the context-aware fusion performs better than the average fusion in selecting the high-quality reconstructions for each part from different coarse volumes.</p><formula xml:id="formula_5">v f (i,j,k) = 1 n n r=1 v r (i,j,k)<label>(5)</label></formula><p>To make a further comparison with RNN-based fusion, we remove the context-aware fusion and add an 3D convolutional LSTM <ref type="bibr" target="#b1">[2]</ref> after the encoder. To fit the input of the 3D convolutional LSTM, we add an additional fullyconnected layer with a dimension of 1024 before it. As shown in <ref type="figure" target="#fig_6">Figure 9a</ref>, both the average fusion and contextaware fusion consistently outperform the RNN-based fusion in all numbers of views.  Refiner Pix2Vox-A uses a refiner to further refine the fused 3D volume. For single-view reconstruction on ShapeNet, the IoU of Pix2Vox-A is 0.661. In contrast, the IoU of Pix2Vox-A without the refiner decreases to 0.636. Removing refiner causes considerable degeneration for the reconstruction accuracy. As shown in <ref type="figure" target="#fig_6">Figure 9b</ref>, as the number of views increases, the effect of the refiner becomes weaker. The ablation studies indicate that both the context-aware fusion and the refiner play important roles in our framework for the performance improvements against previous stateof-the-art methods. <ref type="table" target="#tab_2">Table 4</ref> and <ref type="figure" target="#fig_0">Figure 1</ref> show the numbers of parameters of different methods. There is an 80% reduction in parameters in Pix2Vox-F compared to 3D-R2N2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Space and Time Complexity</head><p>The running times are obtained on the same PC with an NVIDIA GTX 1080 Ti GPU. For more precise timing, we exclude the reading and writing time when evaluating the forward and backward inference time. Both Pix2Vox-F and Pix2Vox-A are about 8 times faster in forward inference than 3D-R2N2 in single-view reconstruction. In backward inference, Pix2Vox-F and Pix2Vox-A are about 24 and 4 times faster than 3D-R2N2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Discussion</head><p>To give a detailed analysis of the context-aware fusion module, we visualized the score maps of three coarse volumes when reconstructing the 3D shape of a table from 3view images, as shown in <ref type="figure">Figure 4</ref>. The reconstruction quality of the table tops on the right is clearly of low quality, and the score of the corresponding part is lower than those in the other two coarse volumes. The fused 3D volume is obtained by combining the selected high-quality reconstruction parts, where bad reconstructions can be eliminated effectively by our scoring scheme.</p><p>Pix2Vox recovers the 3D shape of an object without knowing camera parameters. To further demonstrate the superior ability of the context-aware fusion in multi-view stereo (MVS) systems <ref type="bibr" target="#b19">[20]</ref>, we replace the RNN with the context-aware fusion in LSM <ref type="bibr" target="#b8">[9]</ref>. Specifically, we remove the recurrent fusion and add the context-aware fusion to combine the 3D volume reconstruction of each view. Experimental results show that the IoU is increased by about 2% on the ShapeNet testing set, which indicate that the contextaware fusion also helps MVS systems to obtain better reconstruction results.</p><p>Although our methods outperform state-of-the-arts, the reconstruction results of our methods are still with a low resolution. We can further improve the reconstruction resolutions in the future work by introducing GANs <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Works</head><p>In this paper, we propose a unified framework for both single-view and multi-view 3D reconstruction, named Pix2Vox. Compared with existing methods that fuse deep features generated by a shared encoder, the proposed method fuses multiple coarse volumes produced by a decoder and preserves multi-view spatial constraints better. Quantitative and qualitative evaluation for both single-view and multi-view reconstruction on the ShapeNet and Pix3D benchmarks indicate that the proposed methods outperform state-of-the-arts by a large margin. Pix2Vox is computationally efficient, which is 24 times faster than 3D-R2N2 in terms of backward inference time. In future work, we will work on improving the resolution of the reconstructed 3D objects. In addition, we also plan to extend Pix2Vox to reconstruct 3D objects from RGB-D images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Forward inference time, model size, and IoU of state-of-the-arts and our methods for single-view 3D reconstruction on the ShapeNet testing set. The radius of each circle represents the size of the corresponding model. Pix2Vox outperforms state-of-the-arts in forward inference time and reaches the best balance between accuracy and model size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>based methods are timeconsuming since input images are processed sequentially arXiv:1901.11153v2 [cs.CV] 29 Jul 2019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The network architecture of (top) Pix2Vox-F and (bottom) Pix2Vox-A. The EDLoss and the RLoss are defined as Equation 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Visualization of the score maps in the contextaware fusion module. The context-aware fusion module generates higher scores for high-quality reconstructions, which can eliminate the effect of the missing or wrongly recovered parts. An overview of the context-aware fusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Reconstruction on the Pix3D testing set from single-view images. GT represents the ground truth of the 3D object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>The IoU on ShapeNet testing set. (a) Effects of the context aware fusion and the number of views on the evaluation IoU. (b) Effects of the refiner network and the number of views on the evaluation IoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Single-view reconstruction on ShapeNet compared using Intersection-over-Union (IoU). The best number for each category is highlighted in bold. Note that DRC<ref type="bibr" target="#b25">[26]</ref> is trained/tested per category and PSGN<ref type="bibr" target="#b4">[5]</ref> takes object masks as an additional input. Besides, PSGN uses 220k 3D CAD models while the remaining methods use only 44k 3D CAD models during training.</figDesc><table><row><cell>Category</cell><cell>3D-R2N2 [2]</cell><cell>OGN [24]</cell><cell>DRC [26]</cell><cell>PSGN [5]</cell><cell>Pix2Vox-F</cell><cell>Pix2Vox-A</cell></row><row><cell>airplane</cell><cell>0.513</cell><cell>0.587</cell><cell>0.571</cell><cell>0.601</cell><cell>0.600</cell><cell>0.684</cell></row><row><cell>bench</cell><cell>0.421</cell><cell>0.481</cell><cell>0.453</cell><cell>0.550</cell><cell>0.538</cell><cell>0.616</cell></row><row><cell>cabinet</cell><cell>0.716</cell><cell>0.729</cell><cell>0.635</cell><cell>0.771</cell><cell>0.765</cell><cell>0.792</cell></row><row><cell>car</cell><cell>0.798</cell><cell>0.828</cell><cell>0.755</cell><cell>0.831</cell><cell>0.837</cell><cell>0.854</cell></row><row><cell>chair</cell><cell>0.466</cell><cell>0.483</cell><cell>0.469</cell><cell>0.544</cell><cell>0.535</cell><cell>0.567</cell></row><row><cell>display</cell><cell>0.468</cell><cell>0.502</cell><cell>0.419</cell><cell>0.552</cell><cell>0.511</cell><cell>0.537</cell></row><row><cell>lamp</cell><cell>0.381</cell><cell>0.398</cell><cell>0.415</cell><cell>0.462</cell><cell>0.435</cell><cell>0.443</cell></row><row><cell>speaker</cell><cell>0.662</cell><cell>0.637</cell><cell>0.609</cell><cell>0.737</cell><cell>0.707</cell><cell>0.714</cell></row><row><cell>rifle</cell><cell>0.544</cell><cell>0.593</cell><cell>0.608</cell><cell>0.604</cell><cell>0.598</cell><cell>0.615</cell></row><row><cell>sofa</cell><cell>0.628</cell><cell>0.646</cell><cell>0.606</cell><cell>0.708</cell><cell>0.687</cell><cell>0.709</cell></row><row><cell>table</cell><cell>0.513</cell><cell>0.536</cell><cell>0.424</cell><cell>0.606</cell><cell>0.587</cell><cell>0.601</cell></row><row><cell>telephone</cell><cell>0.661</cell><cell>0.702</cell><cell>0.413</cell><cell>0.749</cell><cell>0.770</cell><cell>0.776</cell></row><row><cell>watercraft</cell><cell>0.513</cell><cell>0.632</cell><cell>0.556</cell><cell>0.611</cell><cell>0.582</cell><cell>0.594</cell></row><row><cell>Overall</cell><cell>0.560</cell><cell>0.596</cell><cell>0.545</cell><cell>0.640</cell><cell>0.634</cell><cell>0.661</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>average</cell></row></table><note>Multi-view reconstruction on ShapeNet compared using Intersection-over-Union (IoU). The best results for different numbers of views are highlighted in bold. The marker † indicates that the context-aware fusion is replaced with the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Memory usage and running time on ShapeNet dataset. Note that backward time is measured in single-view reconstruction with a batch size of 1.</figDesc><table><row><cell>Methods</cell><cell cols="4">3D-R2N2 OGN Pix2Vox-F Pix2Vox-A</cell></row><row><cell>#Parameters (M)</cell><cell>35.97</cell><cell>12.46</cell><cell>7.41</cell><cell>114.24</cell></row><row><cell>Memory (MB)</cell><cell>1407</cell><cell>793</cell><cell>673</cell><cell>2729</cell></row><row><cell>Training (hours)</cell><cell>169</cell><cell>192</cell><cell>12</cell><cell>25</cell></row><row><cell>Backward (ms)</cell><cell>312.50</cell><cell>312.25</cell><cell>12.93</cell><cell>72.01</cell></row><row><cell>Forward, 1-view (ms)</cell><cell>73.35</cell><cell>37.90</cell><cell>9.25</cell><cell>9.90</cell></row><row><cell>Forward, 2-views (ms)</cell><cell>108.11</cell><cell>N/A</cell><cell>12.05</cell><cell>13.69</cell></row><row><cell>Forward, 4-views (ms)</cell><cell>112.36</cell><cell>N/A</cell><cell>23.26</cell><cell>26.31</cell></row><row><cell>Forward, 8-views (ms)</cell><cell>117.64</cell><cell>N/A</cell><cell>52.63</cell><cell>55.56</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the National Natural Science Foundation of China under Project No. 61772158, 61702136, 61872112 and U1711265. We gratefully acknowledge Prof. Junbao Li and Huanyu Liu for providing additional GPU hours for this research. We would also like to thank Prof. Wangmeng Zuo, Jiapeng Tang, and anonymous reviewers for their valuable feedbacks and help during this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape, illumination, and reflectance from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1670" to="1687" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3D-R2N2: A unified approach for single and multi-view 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Human shape from silhouettes using generative HKS descriptors and cross-modal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dibra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual simultaneous localization and mapping: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fuentes-Pacheco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Ascencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rendón-Mancha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="81" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Single stream parallelization of generalized LSTM-like RNNs on a GPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1312.6114</idno>
		<title level="m">Auto-encoding variational bayes. arXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">3D-LMNet: Latent embedding matching for accurate and diverse 3D point cloud reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mandikal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Özyeil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Voroninski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">305364</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Discriminative shape from shading in uncalibrated illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIC-CAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D reconstruction by shadow carving: Theory and practical evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="305" to="336" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A comparison and evaluation of multi-view stereo reconstruction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Render for CNN: viewpoint estimation in images using cnns trained with rendered 3d model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pix3d: Dataset and methods for single-image 3d shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3D outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning Single-view 3D Reconstruction of Objects and Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3DensiNet: A robust neural network architecture towards 3D volumetric object prediction from 2D image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pixel2Mesh: Generating 3D mesh models from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">O-CNN: octree-based convolutional neural networks for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<idno>72:1-72:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recovering surface shape and orientation from texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="17" to="45" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">MarrNet: 3D shape reconstruction via 2.5D sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dense 3D object reconstruction from a single depth view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2868195</idno>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">RealPoint3D: An efficient generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="57539" to="57549" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
