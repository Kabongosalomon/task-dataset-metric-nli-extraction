<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Feature Aggregation Networks for Video Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Bruno Kessler</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
							<email>sergio@maia.ub.es</email>
							<affiliation key="aff2">
								<orgName type="department">Computer Vision Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
							<email>lanz@fbk.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Bruno Kessler</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Feature Aggregation Networks for Video Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most action recognition methods base on a) a late aggregation of frame level CNN features using average pooling, max pooling, or RNN, among others, or b) spatio-temporal aggregation via 3D convolutions. The first assume independence among frame features up to a certain level of abstraction and then perform higher-level aggregation, while the second extracts spatiotemporal features from grouped frames as early fusion. In this paper we explore the space in between these two, by letting adjacent feature branches interact as they develop into the higher level representation. The interaction happens between feature differencing and averaging at each level of the hierarchy, and it has convolutional structure that learns to select the appropriate mode locally in contrast to previous works that impose one of the modes globally (e.g. feature differencing) as a design choice. We further constrain this interaction to be conservative, e.g. a local feature subtraction in one branch is compensated by the addition on another, such that the total feature flow is preserved. We evaluate the performance of our proposal on a number of existing models, i.e. TSN, TRN and ECO, to show its flexibility and effectiveness in improving action recognition performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Video action recognition requires strong temporal reasoning. Temporal reasoning in deep networks can be implemented by 3D (space+time) convolutions, temporal (average) pooling, or recurrent layers that aggregate frame-level or spatio-temporal representations of an input sequence. The effectiveness of temporal reasoning hereby depends on the representation upon which the aggregation is performed.</p><p>While spatio-temporal features are more effective than frame-level features in video recognition tasks, they are also more costly to compute and harder to train, including a large number of parameters. Furthermore, even with a deep backbone the receptive field of spatio-temporal features are confined to the small fixed-size frame window from which they were computed. Since more abstract representations benefit from larger temporal context, a gradual increase of the temporal extend of receptive fields in the higher layers of the feature extraction backbone would be desired instead.</p><p>A feasible solution to increase the temporal size of receptive fields is to let features from adjacent branches interact. Temporal Segment Network (TSN) <ref type="bibr" target="#b0">[1]</ref> showed that it is useful to apply the difference of frames as input to a CNN for video action recognition. Temporal Difference Network (TDN) <ref type="bibr" target="#b1">[2]</ref> extended this idea to compute the difference of frame level features obtained from each layer in a CNN. Such hardcoded approach considers that there is a significant variation of information in adjacent frames. This is however not always guaranteed in practice. It is therefore desirable to let a network decide when to subtract the features from adjacent frames.</p><p>In this paper we present a hierarchical feature aggregation scheme that is lightweight and can be plugged into any deep architecture with CNN backbone. In particular, when plugged into TSN, a performance gain of 24.2% is obtained on Something-v1 <ref type="bibr" target="#b2">[3]</ref> dataset with an addition of only 1.14% parameters and 1.04% Floating Point Operations (FLOPs). The main idea, as illustrated in <ref type="figure" target="#fig_0">Fig. 1a</ref>, is to transfer features between adjacent branches at each layer in a way that adjacent features interact as they develop into the higher level representation. The amount of feature transfer is controlled through a convolutional gate that decides to what amount a feature is subtracted or added into the adjacent branch, as detailed in <ref type="figure" target="#fig_0">Fig. 1b</ref>. Through end-to-end training, the network learns to route features through the hierarchy, hereby causing the temporal extend of receptive fields to grow with depth and adapt to the input. We evaluate our scheme on a number of existing models, TSN, TRN and ECO, and show its flexibility and effectiveness in improving action recognition performance.</p><p>The rest of the paper is organized as follows. Sec. II reviews work on action recognition most related to our proposal. Sec. III presents the hierarchical feature aggregation network. Experimental results are reported in Sec. IV and Sec. V concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Inspired by the performance improvements obtained by CNNs on image recognition <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, many deep learning based techniques have been developed for video action recognition. The most effective and simple extension was developed by At each layer the feature from a CNN block is gated and its residual is transferred to the adjacent branch. Gating is learnt jointly with the network during training, and decides locally whether, and to what extend, a feature differencing or feature averaging is performed. The temporal extend of receptive fields increases with network depth, which helps developing spatio-temporal representations without expensive 3D convolutions.</p><p>Simonyan and Zisserman <ref type="bibr" target="#b5">[6]</ref>. Their method consists of two different CNNs trained on a single RGB image frame and a stack of optical flow images followed by a late fusion of the prediction scores. The image stream encodes the appearance information while the optical flow stream encodes the motion information. Several works followed this approach to find a suitable fusion of the two streams <ref type="bibr" target="#b6">[7]</ref> and exploring residual connections between them <ref type="bibr" target="#b7">[8]</ref>. The downside of this approach is its reliance on externally computed optical flow which is computationally intensive. In order to address the aforementioned problem, researchers have explored techniques for extracting spatio-temporal features from the RGB frames itself. Karpathy et al. <ref type="bibr" target="#b8">[9]</ref> examined several fusion approaches at various levels of the CNN hierarchy. They found that a slow fusion approach where the features from adjacent frames are fused at multiple hierarchical levels of the CNN results in the best performance. Their fusion approach stacks the convolutional features from adjacent frames and perform temporal convolutions for extracting the temporal features. Later, Tran et al. <ref type="bibr" target="#b9">[10]</ref> developed 3DCNN with 3D convolutional layers so that spatio-temporal features can be extracted from a set of multiple RGB frames. Their approach showed that 3D convolutions are capable of extracting spatio-temporal features from small video segments consisting of a small number of frames. Several approaches have been later proposed for exploiting the parameters learned by a 2DCNN for image recognition <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Carreira and Zisserman <ref type="bibr" target="#b10">[11]</ref> developed a 3DCNN by inflating the 2D filters to 3D. Qiu et al. <ref type="bibr" target="#b11">[12]</ref> and Tran et al. <ref type="bibr" target="#b12">[13]</ref> proposed to factorize the 3D convolutions to a 2D convolution for spatial encoding followed by a 1D convolution for temporal encoding. Wang et al.developed an architecture based on 3DCNNs which decouples the spatial and temporal encoding. Their approach performs a linear encoding on individual frame features and a multiplicative encoding between a set of features from multiple frames. Such approaches using 3DCNNs have two major drawbacks, firstly, the massive increase in the number of trainable parameters and secondly, they extract spatio-temporal features from just a small sample of adjacent frames. The first problem makes such approaches difficult to be trained on smaller datasets and the second problem makes them incapable of extracting long range spatio-temporal features from videos.</p><p>In order to extract long-range spatio-temporal features, several techniques that perform sparse sampling of the video frames, as opposed to the dense sampling used in 3DCNNs, have been proposed. Such approaches use a 2DCNN for extracting the frame level features followed by a late fusion using Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, pooling techniques such as average pooling or max pooling <ref type="bibr" target="#b0">[1]</ref>, Fully Connected (FC) layers <ref type="bibr" target="#b15">[16]</ref>, or 3D convolutions <ref type="bibr" target="#b16">[17]</ref>, among others. Such late fusion based approaches ignore the information extracted at the different layers of a CNN. Several approaches have been proposed to address this drawback by fusing the features from consecutive frames at different layers of hierarchy of a CNN <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Ng and Davis <ref type="bibr" target="#b1">[2]</ref> propose to use an extra CNN that accepts the difference of feature maps from adjacent frames at different layers of a separate CNN. The final prediction is done by average pooling of the scores obtained from the two networks. Sun et al.improves this approach by applying a Sobel filter on the features in addition to the temporal differencing operation. Lee et al. <ref type="bibr" target="#b18">[19]</ref> improves this method by forwarding the spatial and temporal features across a single network. This is achieved by applying a set of fixed filters followed by differencing of the features from adjacent frames. Lin et al. <ref type="bibr" target="#b19">[20]</ref> proposes a plug and play module that shifts the channels in the feature tensor across the temporal dimension for transferring information across frames.</p><p>Our approach is related to <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b19">[20]</ref>. These approaches perform a fixed interaction between feature maps from adjacent frames whereas our approach learns a set of filters that can perform point wise difference or average operations across the feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HF-NET: HIERARCHICAL FEATURE AGGREGATION NETWORKS</head><p>In this section we describe our hierarchical feature aggregation for video representation learning architectures that utilize a CNN backbone for frame-level or spatio-temporal feature extraction. We then present details of our action recognition model used in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hierarchical Feature Aggregation</head><p>Deep architectures with CNN backbone by design do not account for correlations that may exist between frame-level or spatio-temporal features at the earlier layers of the CNN. Indeed, in a CNN backbone a feature F t,l for input t at layer l is computed from F t,l−1 independently of the features from inputs other than t, that is, F t,l = block(F t,l−1 ), where block represents a set of convolutional layers with non-linearities. This may limit performance by design when the input sequence exhibits strong temporal dependence such as video frames in action recognition. In such a setting, a feature aggregation in the early layers may better capture the temporal dependencies in the data sequence.</p><p>Our hierarchical feature aggregation is based on the assumption that adjacent features will benefit from interacting in order to produce a more abstract representation at each layer of the architecture. We want this interaction to be pairwise and feed-forward computable, that is, our backbone hierarchical aggregation scheme is realized with a re-designed F t,l = block2(F t,l−1 , F t−1,l−1 ).</p><p>In order to define block2 we consider two elements that are pervasive in the state-of-the-art architectures. First, most architectures perform feature pooling at the higher level, most often, average pooling. Second, many networks follow a two stream structure where optical flow is late fused from a separate branch. Some works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> already reported performance improvements even with frame differencing as a simplified version of optical flow. In our design of block2 both feature differencing and feature averaging are considered. We want the network to select or interpolate between these two modes, and maximise flexibility by learning to decide so locally in the feature tensor. This way, we provide the network with the capability to selectively route features through the hierarchy and build up discriminative representations integrating temporal context. That is, growing temporal receptive fields as we go up in the hierarchy.</p><p>We define block2 as:</p><formula xml:id="formula_0">F t,l = F t,l + G t,l F t+1,l − G t−1,l F t,l ,<label>(1)</label></formula><p>where F t,l = block(F t,l−1 ), ' * ' and ' ' represent convolution operation and Hadamard product, respectively, and G t acts as a gating tensor that chooses between averaging and differencing operations. <ref type="figure" target="#fig_0">Fig. 1b</ref> illustrates block2. Note that we subtract a gated G t−1,l F t,l from the backbone feature F t,l but we add it when computing F t−1,l . Thus, the total feature flow on the sequence is preserved. Gating G t is generated by performing a 3D convolution on the stacked features with a 2 × 3 × 3 kernel followed by tanh non-linearity:</p><formula xml:id="formula_1">G t,l = tanh(W l * [F t,l , F t+1,l ] + B l ).<label>(2)</label></formula><p>Since the range of tanh non-linearity is [-1, 1], the network is capable of selecting from both averaging and differencing operations. W l and B l represent the weights and the bias of the 3D convolution. We use a 2 × 3 × 3 kernel with 1 output channel in the 3D convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Layer Implementation</head><p>The proposed hierarchical aggregation across each layer can be realized in parallel, thereby allowing for a fast training and inference. Assuming that sequences of frames are applied as a batch to the network, the parallel implementation of hierarchical aggregation is realized as:</p><formula xml:id="formula_2">F + = block(F :,l−1 ),<label>(3)</label></formula><formula xml:id="formula_3">F -= shift_left(F + ),<label>(4)</label></formula><formula xml:id="formula_4">G -= tanh(conv3([F + , F -])),<label>(5)</label></formula><formula xml:id="formula_5">G + = shift_right(G -),<label>(6)</label></formula><formula xml:id="formula_6">F :,l = G -F -+ (1 − G + ) F + .<label>(7)</label></formula><p>At layer l of the backbone, we first feed the input sequence F :,l−1 through the corresponding convolutional block of the backbone to obtain a new sequence F + . We then left-shift and zero-pad to obtain Fand compute the gating tensor Gthrough a 3D convolution on the stacked [F + , F -]. This is followed by tanh to map the resulting tensor to values in the range <ref type="bibr">[-1, 1]</ref>. In order to obtain the sequence F :,l of output features, the Hadamard product of the gating tensor Gand the left shifted feature tensor Fis added to the input feature tensor F + while the Hadamard product of the right shifted gating tensor G + and the input feature tensor F + is subtracted.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hierarchical Feature Aggregation Networks</head><p>Hierarchical Feature Aggregation Network (HF-Net) is obtained by adding the Hierarchical Feature Aggregation Module, presented in Sec. III-A, at various levels of the backbone CNN used in the network. Our approach is generalizable to any CNN architecture (e.g, HF-TSN). In the experiments, we use Inception with Batch Normalization (BN) <ref type="bibr" target="#b20">[21]</ref> as our backbone CNN. We plug in our Hierarchical Feature Aggregation module after each Inception block in the CNN. At each iteration, a set of sparsely sampled frames K from a video is passed to the network. In addition to the common feed forward flow of information in conventional CNNs, the hierarchical feature aggregation modules enable a horizontal flow of information across the features at different levels of the CNN hierarchy. The output features corresponding to all the frames from the final layer of the CNN can then be pooled together for encoding the long range temporal features. Our architecture is highly flexible and can be combined with a number of temporal pooling techniques such as TSN <ref type="bibr" target="#b0">[1]</ref>, Temporal Relation Network (TRN) <ref type="bibr" target="#b15">[16]</ref>, Gated Recurrent Unit (GRU) <ref type="bibr" target="#b21">[22]</ref>, or 3DCNN <ref type="bibr" target="#b16">[17]</ref>. In Sec. IV-C, we evaluate the performance of our proposal on a number of existing approaches and show its flexibility and effectiveness in improving action recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>We evaluate the proposed hierarchical feature aggregation strategy on a number of existing models and standard action recognition datasets. We first briefly describe the considered datasets, evaluation metrics, and implementation details. Then, we perform an ablation analysis to quantitatively evaluate the impact of different HF properties and components. Finally, we compare the performance of various HF-Net architectures against state-of-the-art results on different public action recognition datasets. We show HF enhances recognition performance of all models where it is applied, with negligible additional complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and evaluation protocol</head><p>We evaluate our proposed hierarchical feature aggregation technique on three standard action recognition benchmarks, Something-v1 <ref type="bibr" target="#b2">[3]</ref>, EPIC-KITCHENS <ref type="bibr" target="#b22">[23]</ref> and HMDB51 <ref type="bibr" target="#b23">[24]</ref>. Something-v1 consists of 86K and 11K videos in the training and validation sets from 174 action classes. We report the performance on the validation set. EPIC-KITCHENS dataset comprises of egocentric videos with fine-grained activity labels. The labels are provided as verb and nouns and the dataset consists of 24K videos in the training set and 10K videos in the test set. We report the performance obtained on the test set. HMDB51 dataset consists of videos collected from Youtube and contains around 6000 video clips from 51 action categories. The dataset is provided with three standard train/test splits and the final recognition accuracy is reported as the average of the accuracy obtained on the three splits. Both Something-v1 and EPIC-KITCHENS datasets consists of crowd collected videos with actions involving objects. Something-v1 gives importance to the actions rather than the objects involved in the action while EPIC-KITCHENS gives relevance to both actions and objects. HMDB51 is a smaller dataset with less complex action categories of shorter temporal span, which can be identified with simple appearance cues. Some sample frames from the datasets are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In addition to the recognition accuracy, we also compare the complexity of the models in terms of number of parameters and Floating Point Operations (FLOPs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>As explained in Sec. III-C, we choose BNInception as the backbone. The proposed HF module is added after each Inception block of the CNN. The entire network, including the BN layers, is trained for 60 epochs using Stochastic Gradient Descent (SGD) optimization algorithm with a batch size of 32. The learning rate is fixed as 0.001 and is reduced by a factor of 0.1 after 25 and 40 epochs. Dropout at a rate of 0.5 is applied before the final classification layer to avoid overfitting. Random scaling and cropping, as proposed in <ref type="bibr" target="#b0">[1]</ref> is used as data augmentation. During inference, only the center crop of the frames are used. In all experiments, we use 16 frames as the input to the model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Analysis</head><p>In this section, we report the ablation analysis performed on the validation set of Something-v1 dataset. We compare the performance improvement by adding the proposed HF module on the CNN backbone of a standard action recognition technique. We choose TSN <ref type="bibr" target="#b0">[1]</ref> as the baseline since it is one of the standard techniques for video action recognition. TSN divides each video into a pre-defined number of segments and applies one frame from each of the segments as the input to the network. The average of the output from each of the frames is used for computing the prediction scores. Thus, TSN fails to encode the temporal relations between video frames and hence acts as a suitable baseline for showing the capability of the proposed hierarchical feature aggregation approach in extracting spatio-temporal features.</p><p>Tab. I shows the result of the ablation study conducted. We first evaluated the performance of the model when a single module is added after the final inception block of the backbone. We obtained an improvement of 1.6%. A further gain of 14.78% is obtained by adding 5 modules after each of the final 5 inception blocks. Finally, we add 10 modules after each of the inception blocks which resulted in an improvement of 24.2% over the TSN baseline. We also evaluated the performance of the model when two independent 3D convolutional layers are used to compute the gating, thereby breaking the conservative flow of features. This increases the number of parameters and complexity slightly, albeit the performance of the network is reduced thereby proving that feature flow conservation is useful for spatio-temporal feature extraction in HF-Nets.</p><p>In <ref type="figure" target="#fig_2">Fig. 3a</ref>, we show the top 10 action classes that improved the most by adding HF module to the backbone CNN of TSN. From the figure, it can be seen that the network has enhanced its ability to distinguish between action classes that are similar in appearance, such as Unfolding something and Folding something, Dropping something next to something and Showing something next to something, etc. The t-SNE plots of the features from the final layer of the CNN corresponding to these 10 action classes are shown in <ref type="figure" target="#fig_2">Fig. 3b and 3c</ref>. It can be seen that the features from HF-TSN show a lower intra-class and higher inter-class variability compared to those from TSN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. State-of-the-Art Comparison</head><p>In order to compare methods at the same conditions, we compare only with those models using raw RGB frames as input. However, note that the proposed approach is extendable to optical flow images as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Backbone Pre-training Accuracy (%) Something-v1 HMDB51 TDN <ref type="bibr" target="#b1">[2]</ref> ResNet-50 ImageNet -55.5 ECO <ref type="bibr" target="#b16">[17]</ref> BNInception+3D ResNet-18 ImageNet+Kinetics 41.4 68.2 MFNet-C <ref type="bibr" target="#b18">[19]</ref> ResNet-101 Scratch 43.92 -ARTNet <ref type="bibr" target="#b24">[25]</ref> 3D ResNet-18 Kinetics -70.9 I3D <ref type="bibr" target="#b10">[11]</ref> 3D ResNet-50 ImageNet+Kinetics 41.6 74.8 C3D <ref type="bibr" target="#b9">[10]</ref> 3D ResNet-18 Sports-1M -62.1 3D-ResNeXt-101 <ref type="bibr" target="#b25">[26]</ref> ResNeXt-101 Kinetics -70.2 R(2+1)D <ref type="bibr" target="#b12">[13]</ref> 3D ResNet-34 ImageNet+Kinetics -74.5 Non-local I3D <ref type="bibr" target="#b26">[27]</ref> 3D ResNet-50 ImageNet+Kinetics 44.4 -Non-local I3D+GCN <ref type="bibr" target="#b26">[27]</ref> 3D ResNet-50+GCN ImageNet+Kinetics 46.1 -TSM <ref type="bibr" target="#b19">[20]</ref> ResNet-50 ImageNet+Kinetics 44.8 73.2 S3D-G <ref type="bibr" target="#b27">[28]</ref> Inception-V1 ImageNet 48.   Something-v1 and HMDB51: Tab.II compares the proposed approach with state-of-the-art techniques on Something-v1 and HMDB51 datasets. For Something-v1 dataset, in addition to TSN, we also evaluate the performance of the proposed approach on TRN. In both methods, by adding the proposed HF module to the backbone, a large improvement in the performance is observed. From the table, one can see that the proposed approach results in comparable performance to other approaches that use a bigger backbone CNN such as ResNet-50 or I3D. It should also be noted that other methods which achieve superior performance use strong pre-training using Kinetics dataset. Importantly, the HF augmented models achieve this improved performance at a fraction of the number of parameters and FLOPs compared to other methods, allowing for faster inference and smaller memory footprint, e.g. rendering them to be deployed in mobile devices. <ref type="figure" target="#fig_3">Fig. 4a</ref> illustrates the accuracy vs complexity of state-of-the-art techniques. From the figure, it can be seen that HF-Net results in a large boost in the recognition accuracy over the baseline models. For HMDB51, we augment three baselines, TSN <ref type="bibr" target="#b0">[1]</ref>, TRN <ref type="bibr" target="#b15">[16]</ref> and ECOLite <ref type="bibr" target="#b16">[17]</ref>, with HF module and observe a gain of more than 2% on recognition accuracy over all the three baselines. As mentioned previously, HMDB51 consists of actions with shorter temporal duration. As a result, 3D CNNs that perform dense sampling of frames resulted in the best performance on this dataset <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b10">[11]</ref> due to the ability of 3D convolution layers in extracting spatio-temporal features from a short temporal window. ECOLite consists of a smaller number of 3D convolution layers on top of a 2D CNN backbone and thus is a middle ground between TSN and 3D CNNs. Even though ECOLite uses a more powerful (3D convolution) consensus layer than TSN (average pooling) and TRN (fully connected) the spatio-temporal features provided by HF aggregation are found to be beneficial. From the plot showing accuracy against complexity comparison of state-of-the-art approaches shown in <ref type="figure" target="#fig_3">Fig. 4b</ref>, one can see that HF-ECOLite performs on par with the bigger 3D CNN models. EPIC-KITCHENS: HF-TSN model is compared against the baseline methods on Tab. III. Results on all other models are  provided by the dataset developers <ref type="bibr" target="#b22">[23]</ref>. The model is trained for verb, noun and action classification. We also apply the action prediction score as a bias to the verb and noun classifiers. We report the scores on the test set obtained from the evaluation server. It can be seen that HF-TSN obtained a gain of 12% and 8% for verb classification on S1 and S2 settings over TSN (RGB) baseline. In fact, HF-TSN from RGB surpasses the performance of the baselines that use both RGB and optical flow, showing its capacity for extracting highly discriminative short and long term spatio-temporal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We presented a hierarchical aggregation scheme for video understanding architectures with CNN backbone that is lightweight and effective. In HF-Nets, adjacent feature branches interact between feature differencing and averaging as they compile higher level representations, thereby providing cheap spatio-temporal features for competitive performance. We plugged HF on top of different baseline models (TSN, TRN, ECO) and evaluated on three public action recognition datasets, obtaining consistent performance improvements. We improve action recognition accuracy of TSN on video clips of complex humanobject relationships <ref type="bibr" target="#b2">[3]</ref> by more than 24% while introducing only about 1% additional trainable parameters and 1% FLOPs of computation overhead in inference. Since HF-Net scheme can be plugged into any deep video architecture with CNN backbone, our future work includes the evaluation of additional baselines and two-stream solutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>HF-Net: We introduce hierarchical feature aggregation in video deep architectures with CNN backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Samples from the datasets used for evaluating the performance of HF-Nets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>(a) Improved classes over TSN (b) TSN t-SNE plot (c) HF-TSN t-SNE plot (a) Action classes in Something-v1 dataset with the highest improvement over TSN by adding HF on the backbone. Y-axis labels are in the format true label (HF-TSN)/predicted label (TSN). X-axis shows the number of corrected samples for each class. (b) and (c) show the t-SNE plots of the output layer features preceding the FC layer. The 10 improved classes shown in (a) are visualized in the t-SNE plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Recognition accuracy (%) of various state-of-the-art techniques on (a) Something-v1 and (b) HMDB51, plotted against the computational complexity in terms of GFLOPs. Color of the data point indicates the number of parameters (M, in millions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Ablation analysis conducted on the validation set of Something-v1 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Putting something similar to other things that are already on thetable/Taking one of many similar things on the table Moving something and something closer to each other/Moving something and something away from each other Moving something closer to something/Moving something away from something Dropping something in front of something/Showing something behind something Tearing something into two pieces/Tearing something just a little bit Putting something in front of something/Removing something, revealing something behind Taking one of many similar things on the table/Putting something similar to other things that are already on the table</figDesc><table><row><cell>25</cell><cell>20</cell><cell>15</cell><cell>10</cell><cell>5</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dropping something next to something/Showing something next to something</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Folding something/Unfolding something</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unfolding something/Folding something</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="2">: Comparison of proposed method with state-of-the-art techniques on the validation set of something-v1 and HMDB51</cell></row><row><cell>datasets.</cell><cell></cell></row><row><cell>(a) Something-v1 dataset</cell><cell>(b) HMDB51 dataset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Comparison of recognition accuracies with state-of-the-art on EPIC-KITCHENS dataset. HF-TSN is trained for verb, noun and action classification.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal difference networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The&quot; Something Something&quot; Video Database for Learning and Evaluating Visual Common Sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ECO: Efficient Convolutional Network for Online Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: a fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Temporal Shift Module for Efficient Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08383</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal Reasoning in Videos using Convolutional Gated Recurrent Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HMDB51: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing in Science and Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
