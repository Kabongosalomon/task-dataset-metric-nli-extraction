<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Effective Loss Function for Generating 3D Models from Single 2D Image without Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-30">30 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Technical Sciences</orgName>
								<address>
									<postCode>21125</postCode>
									<settlement>Novi Sad</settlement>
									<country key="RS">Serbia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB3 0FD</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Effective Loss Function for Generating 3D Models from Single 2D Image without Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-30">30 Apr 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>3D Reconstruction · Single-View 3D Reconstruction</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Differentiable rendering is a very successful technique that applies to a Single-View 3D Reconstruction. Current renderers use losses based on pixels between a rendered image of some 3D reconstructed object and ground-truth images from given matched viewpoints to optimise parameters of the 3D shape. These models require a rendering step, along with visibility handling and evaluation of the shading model. The main goal of this paper is to demonstrate that we can avoid these steps and still get reconstruction results as other state-of-the-art models that are equal or even better than existing category-specific reconstruction methods. First, we use the same CNN architecture for the prediction of a point cloud shape and pose prediction like the one used by Insafutdinov &amp; Dosovitskiy. Secondly, we propose the novel effective loss function that evaluates how well the projections of reconstructed 3D point clouds cover the groundtruth object's silhouette. Then we use Poisson Surface Reconstruction to transform the reconstructed point cloud into a 3D mesh. Finally, we perform a GAN-based texture mapping on a particular 3D mesh and produce a textured 3D mesh from a single 2D image. We evaluate our method on different datasets (including ShapeNet, CUB-200-2011, and Pascal3D+) and achieve state-of-the-art results, outperforming all the other supervised and unsupervised methods and 3D representations, all in terms of performance, accuracy, and training time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the main problems in 3D Computer Graphics and Vision is the ability of a model to learn 3D structure representation and reconstruction <ref type="bibr" target="#b8">[9]</ref>. Supervised 3D Deep Learning is highly efficient in direct learning from 3D data representations <ref type="bibr" target="#b0">[1]</ref>, such as meshes, voxels, and point clouds. They require a large amount of 3D data for the training process, and also, their representation is sometimes Work performed while the author was Research Intern Apprentice under the supervision of professor Pietro Liò.</p><p>complex for the task of direct learning. These factors lead to the abandonment of this approach because of its inefficient performance and time consumption. Unsupervised 3D structural learning learns 3D structure without 3D supervision and represents a promising approach.</p><p>Differentiable rendering is a novel field that allows the gradients of 3D objects to be calculated and propagated through images <ref type="bibr" target="#b10">[11]</ref>. It also reduces the requirement of 3D data collection and annotation, while enabling a higher success rate in various applications. Their ability to create a bond between 3D and 2D representations, by computing gradients of 2D loss functions with the respect to 3D structure, makes them a key component in unsupervised 3D structure learning. These loss functions are based on differences between RGB pixel values <ref type="bibr">[13]</ref>. By rendering the predicted 3D structure from a specific viewpoint and then evaluating the loss function based on pixel-wise loss between rendered and ground-truth image, model parameters are optimised to reconstruct the desired 3D structure.</p><p>However, these evaluation techniques are very time-consuming. They don't contribute at all to an accurate 3D structure reconstruction. Here, we propose a novel idea for fast 3D structure reconstruction (in the form of a point cloud silhouette) and then we convert it to a 3D mesh and transfer the object's texture from a 2D image onto the reconstructed 3D object. Hence, unlike in loss functions that are based on pixels, our approach has an effective loss function that arises exclusively from the 2D projections of 3D points, without interpolation based on pixels, shading and visibility handling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D representations</head><p>Previous works <ref type="bibr">[23,</ref><ref type="bibr">29]</ref> have concentrated on mesh reconstruction by using the full 3D supervision approach. The main problem with these approaches, besides inefficiency, is the usage of ground-truth 3D meshes, and they are mostly available in a limited number of datasets. Some approaches <ref type="bibr">[24,</ref><ref type="bibr">25]</ref> solved this problem by using 2D supervision from multiple-scene images based on voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Differentiable rendering</head><p>Prediction of 3D models from single images while achieving high-quality visual results is possible by using the differentiable renderer. A differentiable rendering framework allows gradients to be analytically (or approximately) computed for all pixels in an image. Famous frameworks include: RenderNet <ref type="bibr">[19]</ref> and OpenDR [17].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Unsupervised learning of shape and pose with differentiable point clouds</head><p>The work that inspired us addresses the learning of an accurate 3D shape and camera pose from a collection of unlabeled category-specific images <ref type="bibr" target="#b7">[8]</ref>. It uses a specific convolutional neural network architecture to predict both model's shape and the pose from a single image. However, it is still time-consuming since it uses differentiable point cloud projection.</p><p>3 Proposed method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Intuitive overview</head><p>In order to overcome the problems of structural 3D learning, unsupervised methods introduced different differentiable renderers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">16,</ref><ref type="bibr">17,</ref><ref type="bibr">19</ref>] to first render the reconstructed 3D shape into 2D images from different view-angles and then portray them as what got obtained through complete supervision. After this, we can calculate the pixel-wise losses between those 2D images from different view-angles and real (ground-truth) images from the dataset. Since the renderer is differentiable, the loss between these images back-propagates through the network to train it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point cloud representation</head><p>Pose of camera Effective loss function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted projection</head><p>Ground truth image Projection <ref type="figure">Fig. 1</ref>. Our method removes the rendering process and requires only 2D projections of 3D point clouds. During the generation of 3D shapes using multiple silhouette images (from different viewing angles), 2D projections of all points on the shape should uniformly cover the silhouette from each viewing angle. We implement this using two key ideas (that together form effective loss function). (1) For 3D shapes formed by 3D points, their projections for each view should locate within the silhouette. (2) All projections for each silhouette should distribute uniformly. We achieve this by maximising the loss between each of the pairs of these 2D projections. P -Point cloud representation, c -Pose of camera.</p><p>To evaluate the pixel-wise loss, previous differentiable renderers rendered the images by taking into account some form of interpolation <ref type="bibr" target="#b2">[3]</ref> of the reconstructed 3D structure over each pixel, such as rasterisation and visibility handling. We train a network that learns to generate a 3D point cloud based on a single image using the images from a dataset (from different view-angles) as supervision which is opposed to those that use ground-truth point clouds as supervision.</p><p>Current methods render based on differentiable renderers that render images of the reconstructed 3D shape and actual images and then minimise the pixelwise loss to optimise the reconstructed 3D shape.</p><p>Total effective loss informs us how well the projected points cover the objective silhouette. The process includes two terms, one that forces all the projections into the silhouette where the projections initialise randomly, and the other term moves the projections such that the distance between every two of them is the maximum possible, which allows the projections to cover the silhouette uniformly. Starting from some point cloud (randomly initialised), we can force all the projections in the silhouette using the first term, and then using the second term, we can uniformly distribute projections to cover the whole silhouette. After completing the process shown in <ref type="figure">Figure 1</ref>, we generate 3D point cloud for a desired image. After this, we apply Poisson Surface Reconstruction [12] to generate 3D mesh from given 3D point cloud and then we use GAN for texture mapping on a particular 3D mesh and produce a textured 3D mesh based on the input image texture, which is shown in the <ref type="figure" target="#fig_0">Figure 2</ref>. Main paper deals with implementation details of an Effective Loss Function E L which is our novelty, and compares the results with other approaches. More details and case study is available in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation details</head><p>Our goal is to learn the structure of 3D point clouds (P ) formed by N points n j only from G t ground-truth images of the silhouette S i , where j ∈ [1, N ] and i ∈ [1, G t ]. Current differentiable renderers rely on point clouds (P ) rendering into raster images S i from i-th viewing angle, which are used to produce a loss by comparing S i and S i pixel by pixel. These steps are not necessary to get a precise solution.</p><p>Let the projection of the point n j in view i be p i j . The error evaluates how well the sets of projected points p i j | j ∈ [1, N ] cover the silhouette of the object. So, the loss is composed of two parts.</p><p>If we have a predicted 3D point cloud and a binary image of the silhouette, the loss calculates as follows: First, we project the points n j and get projections p j (we write abbreviated without i, this is p i j ) on the images of the silhouette S i , where the pixel value of the projection p j is denoted by π i . The first term penalises points outside of the foreground by calculating the difference 1 − π i , assuming that the foreground in the binary silhouette image has a value of 1. Minimising this loss will force all projections into the foreground. Additionally,  <ref type="figure">Fig. 3</ref>. The left and right grids represent two ground-truth silhouette images Si. nj point projects onto an image, and its projection is pj. Left: For 3D shapes formed by 3D points, their projections for each view should locate within the silhouette, where the whole white grid is a silhouette, and its pixel values are 1 (red square). So, we are minimising differences between pixel values of projections and 1 for every projection. Right: Besides that, we must not only minimise the first term loss but also the second term loss which maximises the distance between two different point projections from a 3D point cloud: pj and p j .</p><p>the second term adjusts the spatial distribution of the projected points. It forces the pairs of projections in the foreground to be as far apart from each other as possible (right grid shown in the <ref type="figure">Figure 3)</ref>. Thus, such a system arranges the 3D locations of the points n j through their projections p j by simultaneously optimising these two losses.</p><p>The first term is calculated as the difference between 1 and the pixel value π i of each projection p i j on the silhouette image S i . We make use of bilinear interpolation to calculate the value π i using the binary pixel values of the nearest pixels around p i j . All projections are forced to the foreground for all silhouette images by minimising the following L 1 norm:</p><formula xml:id="formula_0">L 1 (π i ) = 1 − π i 1<label>(1)</label></formula><p>However, it is impossible to force all projections into the foreground by minimising this L1 norm. If we optimise point cloud according to some silhouette image (a) and start from some randomly initialised points (b), then we will get inadequate point projections if we use L1 norm, as shown in the <ref type="figure">Figure 4</ref>. There are two reasons why this problem occurs. One reason is the fact that L1 norm is non-differentiable. Even if we only look at the difference, the second reason is that we only examine the pixel intensity based on the difference between 1 and the interpolated pixel value π i based on the four closest binary pixel values. This prevents training if the projections p i j are too far from the foreground. Our goal is to produce non-zero gradients anywhere in the background part, while the pixel values in the foreground part do not require a modification. We will denote these processed silhouette images as S G i , to distinguish between the original silhouette image S i and the processed image. For each pixel x on the background of the silhouette image S i , we write:</p><formula xml:id="formula_1">S G i (x) = 1,x ∈ F 1 − d(x, ∂F),x ∈F (2) Fig. 4.</formula><p>We are given a silhouette image (a) and randomly initialized projections (b).</p><p>Then we cannot force projections into the foreground (c) because standard first term loss has a local minimum problem, which results in a non-uniform disposition of projections (blue dots). This problem is solved by smoothing the original silhouette to obtain pixel values of projections and calculate the difference.</p><p>where F = {x | π i (x) = 1} is the foreground, whileF = {x | π i (x) = 0} is a background, and ∂F is the foreground's boundary. d(x, ∂F) represents the L 2 distance between x and his closest ∂F, which is normalised by the resolution of the S i . Normalisation is also performed on the processed pixel values in the background for them to lie in the interval (0, 1). Min-max normalisation is used for</p><formula xml:id="formula_2">this sub-task: S G i (F) = minmax S G i (F) .</formula><p>Finally, the modified first term loss function is: According to <ref type="figure" target="#fig_2">Figure 5</ref>, using only the first term loss leads to non-uniform point projections in the foreground. To accurately represent the 3D shape and cover the silhouette, we will use a second term loss. Through this loss, we will model the spatial relationship between every two pairs of projections. That loss should force projections inside the foreground. They should be as far away from each other as possible.</p><formula xml:id="formula_3">M 1 (π i ) = 1 − π G i 1 (3)</formula><p>To solve this problem, we propose a second-term loss function that increases the distance between projection pairs that are deeper within the foreground and reduces it for projection pairs around the foreground's boundary. It skips projections within the background.</p><p>For every projection pair p i j and p i j , the L2 distance is calculated by the formula:</p><formula xml:id="formula_4">d p i j , p i j = p i j − p i j 2 ,<label>(4)</label></formula><p>which we then normalise according to the resolution of the silhouette image. This approach tends to maximise the distance d p i j , p i j . We use the Gaussian function <ref type="bibr">[14]</ref> to obtain a loss based on the invariance of the structure which decreases with increasing the distance. So, we can essentially minimise the loss of invariance along with the modified first term loss M 1 .</p><p>For each projection p i j , loss based on the invariance of the structure models its spatial relationship with all other projections p i j :</p><formula xml:id="formula_5">L 2 p i j , p i j = w i j N j =1 w i j · exp −d p i j p i j θ + µ i j ,<label>(5)</label></formula><p>where w i j and w i j are weights corresponding to the projections p i j and p i j respectively, θ &gt; 0 is the decay parameter, µ i j &gt; 0 is the boundary bias for the projection p i j . w i j expresses to what level the projection p i j merges with the background. If that weight is set to zero, the projection p i j is such that the invariance of the structure is completely removed so that the modified first term loss M 1 immediately forces p i j within the foreground. The decay (merge) parameter controls the merge interval (invariance of the structure intensity) of a given background 3D model. The projection boundary bias µ i j for projection p i j controls the distance to the foreground's boundary where the invariance over that projection reduces.</p><p>Weight w i j is calculated using bilinear interpolation based on the closest binary pixel values in the silhouette image S i , as shown in the <ref type="figure">Figure 3</ref>. We use multi-scale gradients [21] to compute µ i j . Binary pixel values are extracted from adjacent points located at the vertices of the squares in the grid (around the projection p i j ) - <ref type="figure">Figure 3</ref>. We perform interpolations over them, and we take the mean value of all of these interpolations to calculate µ i j . This approach progressively reduces invariance of the structure as p i j approaches the foreground's boundary.</p><p>Finally, the effective loss function E L is calculated through simultaneous minimisation of the modified first term loss function M 1 and the second term loss function L 2 based on the invariance of the structure. The total error E L is obtained by the following formula (α and β are used for balancing the losses, we average over points N and views G t ):</p><formula xml:id="formula_6">E L = Gt i=1 N j=1 αM 1 (π i ) + βL 2 p i j , p i j G t · N<label>(6)</label></formula><p>After this process, we have a 3D point cloud which is then transformed to a 3D mesh using Poisson Surface Reconstruction <ref type="bibr">[12]</ref>. We use GAN [30] for texture mapping on a particular 3D mesh and produce a textured 3D mesh based on the input image texture, which is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The generator generates displacement maps and textures, and the discriminator discriminates between real/fake displacement maps and textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section, we succinctly report the results, primarily through comparison with other approaches. More details and case study is available in Appendix A.</p><p>The quantitative results using Chamfer's distance [22] are shown in <ref type="table" target="#tab_0">Table  1</ref>. Our point cloud output (Ours) outperforms its voxel equivalent (Ours-V) in all cases. Chamfer's distance improves with the increase of resolution. We also outperform the previous best method that used rendering <ref type="bibr" target="#b7">[8]</ref> and DRC method [24]. Our results outperform state-of-the-art differentiable renderers in the Volumetric IoU metric [20] while simultaneously being less time-consuming during the training phase, as shown in <ref type="table" target="#tab_1">Table 2</ref>. For cars, our outcome is better than renderers based on voxels but very similar to renderers based on meshes because meshes represent a superior initial 3D representation for large areas of flat surfaces <ref type="bibr" target="#b9">[10]</ref> (such as cars).  Also, FID scores on Mesh (produced from 3D point cloud), Texture (extracted by a GAN) and Both (final output -textured 3D mesh) produced state-of-the art results, which can be seen in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Datasets, metrics &amp; code</head><p>Datasets We used the following datasets: ShapeNet <ref type="bibr">[</ref> Fréchet Inception Distance (FID) is widely used as an evaluation metric <ref type="bibr" target="#b6">[7]</ref> (not only for 2D GANs but also for our task). FID scores will evaluate 2D projections of generated point clouds to meshes. 3D mesh and textures are evaluated separately in this process.</p><p>Code Implementation, data and trained models are available at: https://github.com/NikolaZubic/2dimageto3dmodel</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Possible extensions and limitations</head><p>Our work can be used as part of more complex software that deals with video games, animation, or any aspect where it is necessary to have base 3D mod-els which can be additionally polished with more sculpting. The work can be extended by taking even more account of the smooth characteristics of the functions. Our work is the first one to tackle the challenging problem of Single-View 3D Reconstruction without Rendering. Results are impressive, but this task is far from being fully solved. Our model struggles to predict camera poses that are rare in the training dataset. Also, it captures the major shape characteristics of each instance but ignores some details. For example, legs of zebras, cows and horses are not separated <ref type="figure" target="#fig_0">(Figure 12</ref> and <ref type="figure" target="#fig_7">Figure 13</ref>). <ref type="figure">Fig. 6</ref>. We use real-world 2D bird images as input for generating a 3D model. In the first column is the input where we have images of the real birds, in the second column, there is a generated 3D mesh (obtained from 3D point cloud after Poisson Surface Reconstruction [12]), and in the next four columns, there is a predicted 3D model visible in 4 poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed a 3D reconstruction based on a single image, a method for learning the pose and shape of 3D objects given only their 2D projections, using the initial point cloud representation and then converting that representation to a 3D mesh. Mesh is textured using GANs to produce the final output. Extensive experiments have shown that point clouds compare well with the voxel-based representation, such as performance and accuracy. The proposed framework learns to predict shape, texture, and pose from single images, without rendering step, based solely on 2D projections of 3D point clouds and their coverage of the ground-truth silhouette. While rendering requires exhaustive computation, our key finding is that it does not endow accuracy in 3D structure learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>We would like to thank the anonymous reviewers for reviewing the paper before final submission and providing helpful and detailed comments. The network we used for the 3D reconstruction (in a point cloud representation) based on a single image is composed of a 2D encoder and a 3D point cloud decoder. The 2D encoder represents a 7-layer CNN. The first layer consists of a 5 × 5 kernel with 16 channels and a stride of 2. Each of the remaining layers has three kernels and comes in pairs, where the given layer in pairs has a stride of 2, while the second one has a stride of 1. The number of channels increases by a factor of 2 after each layer with a stride. These convolutional layers are followed by two fully connected layers whose dimensions are 1024. The 3D point cloud decoder has one fully connected layer whose dimensions are 1024, and it then predicts the point cloud representation. The point cloud that is consisted of N points gets predicted as a vector whose dimensions are 3N (point coordinates). We have chosen this architecture because it achieved state-of-the-art results for the problem of Single-View 3D Reconstruction, but the process of differentiable rendering was unnecessary, and we obtained a more precise solution without it. This architecture represents an optimal solution because it can firmly reconstruct real-world data, despite the absence of accurate ground-truth camera poses. Also, it can be used as a basis to learn colors and textures, but that would require explicit reasoning about lighting and shading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Appendix -Metrics details</head><p>The following section will explain all the details about metrics used for comparison with other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Chamfer's distance</head><p>Numerical evaluation is performed by using Chamfer's distance [22] between predicted and real (ground-truth) point clouds:</p><formula xml:id="formula_7">d Chamf er (P 1 , P 2 ) = 1 |P 1 | r∈P1 min s∈P2 r − s 2 + 1 |P 2 | s∈P2 min r∈P1 s − r 2 ,<label>(7)</label></formula><p>where P 1 is the predicted point cloud and P 2 is the ground-truth point cloud, r is a point on P 1 and s is a point on P 2 . |P 1 | and |P 2 | represent the number of points for point clouds P 1 and P 2 . The first sum evaluates the precision of the predicted point cloud by computing how far on average is the closest groundtruth point from a predicted point. The second sum measures the coverage of the ground-truth by the predicted point cloud: how far is on average the closest predicted point from a ground-truth point <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Volumetric IoU</head><p>When comparing with voxel-based results, we discretise the 3D space, where the predicted or the ground-truth point clouds are located, into a 3D voxel grid, where a voxel is set to 1 if it contains a point. Volumetric IoU [20] comparison is used by comparing the 3D grid voxelised from the predicted point cloud with the one voxelised from the ground-truth point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Texture evaluation</head><p>Fréchet Inception Distance (FID) is widely used as an evaluation metric <ref type="bibr" target="#b6">[7]</ref> (not only for 2D GANs but also for our task). FID scores will evaluate 2D projections of generated point clouds to meshes. 3D mesh and textures are evaluated separately in this process.</p><p>In addition to the full FID, we report the Texture FID, where we used meshes estimated using our method, and the Mesh FID, where we replaced generated textures with pseudo-ground-truth ones. While we mostly rely on the Full FID to discuss the results, the individual ones represent a convenient tool to analyze how the model responds to different hyperparameters. Generated samples render at 299 × 299, and ground-truth images scale to this resolution. We provide visualizations that give more insights into the conceptual differences between these types of FID metrics, which can be shown in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Both (Full FID)</head><p>Texture FID Mesh FID Image selected from the dataset Both (Full FID) Texture FID Mesh FID Image selected from the dataset <ref type="figure">Fig. 7</ref>. Images on which we computed the FID scores. We generated them from the viewpoint corresponding to the randomly selected image from the training set. In the Mesh FID case, we output the mesh using the pseudo-ground-truth texture from the real-world image. In the Texture FID, the actual mesh gets textured using the generated texture from 2D GAN. In the Full FID and Mesh FID case of the top-left vehicle, we can observe that the silhouette of the mesh looks fine but straight lines and stripes present unsteady structural effect caused by the underlying mesh, while in the Texture FID (which does not use generated meshes) the lines appear more straight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Appendix -Experiments and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Datasets details</head><p>We carried out experiments that involve 3D shapes in three categories from the ShapeNet <ref type="bibr" target="#b1">[2]</ref> dataset, including chairs, airplanes, and cars. They are commonly used for evaluation by others, and if the method is working well on them, then we can say that it will also generalize on the other shapes. We followed the same train/test splitting process as in <ref type="bibr" target="#b7">[8]</ref>, and employed the five rendered views from each 3D shape and the ground-truth point clouds by <ref type="bibr" target="#b7">[8]</ref>. More precisely, we had three different resolutions for rendered views (32 2 , 64 2 , and 128 2 ), all corresponding to the same set of ground-truth point clouds with different numbers of points, as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>For the CUB-200-2011 birds [26] dataset, we used the train/test split of <ref type="bibr" target="#b9">[10]</ref>, which consists of ≈6k training images and ≈5.7k test images. Each image has an annotated class label (out of 200 classes). We evaluated FID on test images using poses (where applicable) from the training set, although we concluded that the FID is almost identical between the two sets.</p><p>As for the Pascal3D+ [31] dataset, we used the same split as <ref type="bibr" target="#b9">[10]</ref> to train our model. The 2D GAN is trained only on the ImageNet subset (4.7k usable images) since we noticed that the images in the Pascal3D+ dataset were too small for practical purposes. The test split of <ref type="bibr" target="#b9">[10]</ref> does not contain any ImageNet images, so we evaluated FID scores on training images, motivated by the previous observation of the CUB-200-2011 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Experiments details</head><p>We used the same architecture as described in Appendix section A, which was introduced by <ref type="bibr" target="#b7">[8]</ref>, but we replaced the differentiable rendering module with our Effective Loss Function E L . Their approach uses structural learning of the 3D point clouds by employing pairs of RGB images. For each pair, the network first outputs a point cloud representation from the first RGB image and then renders the predicted point cloud from the view angle of the second image. Their differentiable rendering module generates a rendered silhouette image, and the neural network was trained by minimizing the pixel-wise error between the rendered silhouette image and the silhouette of the second input image. Our approach removes differentiable rendering and exploits the projected positions of generated point clouds to create the loss E L required during the training. At the test time, the trained network with loss E L outputs a 3D point cloud from a single RGB image.</p><p>Firstly, we compared our results with rendering-based approaches in terms of Chamfer's distance. All compared renderers produce silhouettes of the predicted shapes to compute their loss concerning the ground-truth silhouettes. We carried out the comparison by training the networks using silhouette images at three different resolutions, as already mentioned. We reported quantitative comparison in <ref type="table" target="#tab_0">Table 1</ref>. Our results outperformed all compared methods under all classes at all three resolutions. Our approach showed an obvious advantage over voxel-based differentiable renderers, where we recovered more geometry details in a more memory-efficient manner. In addition, by omitting the rendering process, our approach achieves higher accuracies of the reconstructed point clouds. These results further demonstrate that our method is robust to changes in image resolutions and the number of points.</p><p>Secondly, we compared our results with rendering-based methods for other 3D representations (such as meshes and voxel grids) in terms of IoU. The comparison includes Perspective Transform Nets <ref type="bibr" target="#b10">[11]</ref>, Neural Mesh Renderer <ref type="bibr" target="#b10">[11]</ref>, Soft Rasterizer [16], and Interpolation-based Differentiable Renderer <ref type="bibr" target="#b2">[3]</ref>. The first two methods are voxel-based, while the other two are mesh-based. To produce our IoU, we voxelized the point clouds predicted from images with a resolution of 128 2 in <ref type="table" target="#tab_0">Table 1</ref> into voxel grids with a resolution 32 3 to compare to the same ground-truth as other ones. Quantitative comparison in <ref type="table" target="#tab_1">Table 2</ref> shows that we significantly outperform the state-of-the-art differentiable renderers in terms of mean IoU, where we achieved the best results under airplane and chair classes. Mesh-based approaches are limited to a fixed (usually spherical) mesh topology. This leads to inaccuracies when representing more complex surfaces, such as chairs, which often display non-spherical topology.</p><p>Finally, we compared the results with the latest 3D supervised methods in <ref type="table" target="#tab_1">Table 2</ref>. In the first experiment, we conducted comparison with Multiview aggregation for learning category-specific shape reconstruction <ref type="bibr" target="#b10">[11]</ref>. Results were reported by using their evaluation code. Following the same setting, we used point clouds reconstructed from input images with a resolution of 64 2 in Table 1, scaled each predicted point cloud such that the diagonal of its bounding box is 1, and resampled a ground-truth point cloud to 8000 points if there are more than 8000 points. <ref type="table" target="#tab_4">Table 4</ref> shows that our results significantly outperform Multiview aggregation under all three classes. Cars Airplanes Chairs MV aggregation 0.3331 0.2795 0.4637</p><p>Ours 0.0342 0.0414 0.0437 <ref type="figure" target="#fig_4">Figure 8</ref> shows a few generated, textured meshes rendered from the multiple views in Blender <ref type="bibr" target="#b4">[5]</ref>, and also their corresponding textures. Results on the CUB-200-2011 dataset have high resolution, but the back of the cars in the Pascal3D+ dataset has some irregularities. After further analysis, we found that the dataset is very imbalanced, with only 20% of the images showing the back of the car and the majority of them showed the frontal part. So, this issue could be solved by using more training data. Each object has been rendered from three views (in Blender), and the top row represents the texture learned by GAN. <ref type="figure">Fig. 9</ref>. All generated 3D models can be visualised in the Blender tool <ref type="bibr" target="#b4">[5]</ref> and viewed in real-time from an user-free angle. Left: 3D mesh without texture. Right: Textured 3D mesh which represents the final output of our model based on a single (in this case: car) image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Appendix -Ablation study and efficiency</head><p>We carried out ablation studies to justify our claims in terms of the effectiveness of each element of our method under airplanes at a resolution of 2000 points in <ref type="table" target="#tab_0">Table 1</ref>. In <ref type="table" target="#tab_6">Table 6</ref>, we report results with only some losses, fewer views (like G t = 3 and G t = 2), and without weights and biases. This study shows that our loss E L cannot learn the structure of shapes using only M 1 or L 2 loss, also not with standard L 1 loss because of the local minimum issue and non-differentiability. The second term loss and its hyperparameters (indicator weights and boundary bias) contribute to the reconstruction accuracy and efficiency of optimization. Parameters α and β contribute to the conflict and trade-off between modified first term loss and second term loss based on structure invariance. Using fewer views than our G t = 4 views in training degenerates the structure learning performance.</p><p>As for efficiency, we compared our model's training time with state-of-theart differentiable renderers for 3D shapes, as shown in <ref type="table" target="#tab_2">Table 3</ref>. The voxel-based method (DRC) has a weakness in terms of a vast computational burden due to the cubic complexity of voxel grids, which limits it to work only in low resolutions such as 32 3 and 64 3 with a slow convergence rate. Although the point cloud-based method by Insafutdinov &amp; Dosovitskiy <ref type="bibr" target="#b7">[8]</ref> does not require 3D convolutional layers as DRC, the rendering procedure still requires intensive computation with discrete 3D grids. So, this method requires more time (6 × 10 5 mini-batch iterations) during training than our method (1 × 10 5 mini-batch iterations).</p><p>We used parameters learned in different steps during training to reconstruct a shape from a corresponding image in a test set. Additionally, by using an image from test rather than the training set we demonstrated the generalization ability learned in optimization, which strongly justifies our effectiveness. Also, it is shown that our model adapts well to real-world images. More examples can be seen in the last section of Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Appendix -Training details</head><p>We evaluated our loss using this network with ground-truth camera poses during projections. Additionally, we used RGB images with three different resolutions to train and to evaluate the generated point clouds in three resolutions that include 2000, 8000, and 16000 points. The network was trained using the Adam optimizer with a batch size of 16 rendered images (4 views of 4 shapes), where we iterated over 1 × 10 5 batches in each experiment. <ref type="figure">Fig. 10</ref>. Total unsupervised loss decreases through time (more steps of training) which is an indicator that our model is learning the desired objective if we assume that objective is correctly set up.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Generated 3D point cloud is transformed into 3D mesh and then textured.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>With modified first term loss function, it is possible to force all projections of randomly initialised points (b) from a smoothed silhouette image (a) into the foreground (c) by minimising the<ref type="bibr" target="#b2">(3)</ref>. Blue dots represent the projections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>12. Kazhdan, M., Bolitho, M., Hoppe, H.: Poisson surface reconstruction. In: Proceedings of the Fourth Eurographics Symposium on Geometry Processing. p. 61-70. SGP '06, Eurographics Association, Goslar, DEU (2006) 13. Kumar, T., Verma, K.: A theory based on conversion of rgb image to gray image. International Journal of Computer Applications 7(2), 7-10 (2010) 14. Li, Z., Shafiei, M., Ramamoorthi, R., Sunkavalli, K., Chandraker, M.: Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from a single image (2019) 15. Liu, J., Lu, H.: Imnet: A learning based detector for index modulation aided mimoofdm systems (2019) 16. Liu, S., Chen, W., Li, T., Li, H.: Soft rasterizer: Differentiable rendering for unsupervised single-view mesh reconstruction (2019) 17. Loper, M.M., Black, M.J.: Opendr: An approximate differentiable renderer. In: European Conference on Computer Vision. pp. 154-169. Springer (2014) 18. Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks: Learning 3d reconstruction in function space (2019) 19. Nguyen-Phuoc, T., Li, C., Balaban, S., Yang, Y.L.: Rendernet: A deep convolutional network for differentiable rendering from 3d shapes (2019) 20. Niemeyer, M., Mescheder, L., Oechsle, M., Geiger, A.: Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision (2020/doi.org/https://doi.org/10.1016/j.procs.2016.05.258, https://www. sciencedirect.com/science/article/pii/S1877050916306081, international Conference on Computational Modelling and Security (CMS 2016) 22. Sun, X., Wu, J., Zhang, X., Zhang, Z., Zhang, C., Xue, T., Tenenbaum, J.B., Freeman, W.T.: Pix3d: Dataset and methods for single-image 3d shape modeling (2018) 23. Tatarchenko, M., Dosovitskiy, A., Brox, T.: Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs (2017) 24. Tulsiani, S., Efros, A.A., Malik, J.: Multi-view consistency as supervisory signal for learning shape and pose prediction (2018) 25. Tulsiani, S., Zhou, T., Efros, A.A., Malik, J.: Multi-view supervision for single-view reconstruction via differentiable ray consistency (2017) 26. Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.J.: The caltech-ucsd birds-200-2011 dataset (2011) 27. Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., Jiang, Y.G.: Pixel2mesh: Generating 3d mesh models from single rgb images (2018) 28. Wang, W., Ceylan, D., Mech, R., Neumann, U.: 3dn: 3d deformation network (2019) 29. Wu, J., Wang, Y., Xue, T., Sun, X., Freeman, W.T., Tenenbaum, J.B.: Marrnet: 3d shape reconstruction via 2.5d sketches (2017) 30. Xian, W., Sangkloy, P., Agrawal, V., Raj, A., Lu, J., Fang, C., Yu, F., Hays, J.: Texturegan: Controlling deep image synthesis with texture patches (2018) 31. Xiang, Y., Mottaghi, R., Savarese, S.: Beyond pascal: A benchmark for 3d object detection in the wild. In: IEEE Winter Conference on Applications of Computer Vision (WACV) (2014) 32. Xu, Q., Wang, W., Ceylan, D., Mech, R., Neumann, U.: Disn: Deep implicit surface network for high-quality single-view 3d reconstruction (2019)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative results on Pascal3D+ (left) and CUB-200-2011 (right) dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11 .</head><label>11</label><figDesc>Ensemble of pose regressors for each example where an additional pose is student's prediction. For every view we can produce point clouds and get projections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FFig. 12 .</head><label>12</label><figDesc>More qualitative results on classes: horses &amp; cows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 .</head><label>13</label><figDesc>More qualitative results on classes: penguins &amp; zebras.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on shape prediction with known camera pose (on ShapeNet dataset). We report the Chamfer's distance between normalised point clouds, multiplied by 100 and use three categories: Airplanes, Cars and Chairs. Our point cloud output outperforms all other methods in terms of Chamfer's distance. Lower value is better; bold = best.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Resolution 32</cell><cell cols="3">Resolution 64 Resolution 128</cell></row><row><cell></cell><cell cols="6">DRC [25] DPC [8] Ours-V Ours DPC [8] Ours DPC [8] Ours</cell></row><row><cell>Airplane</cell><cell>8.35</cell><cell>4.52</cell><cell>4.49 3.99</cell><cell>3.50 3.15</cell><cell>2.84</cell><cell>2.63</cell></row><row><cell>Car</cell><cell>4.35</cell><cell>4.22</cell><cell>3.75 3.79</cell><cell>2.98 2.86</cell><cell>2.42</cell><cell>2.37</cell></row><row><cell>Chair</cell><cell>8.01</cell><cell>5.10</cell><cell>5.34 4.64</cell><cell>4.15 3.99</cell><cell>3.62</cell><cell>3.46</cell></row><row><cell>Mean</cell><cell>6.90</cell><cell>4.61</cell><cell>4.53 4.14</cell><cell>3.55 3.33</cell><cell>2.96</cell><cell>2.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative Volumetric IoU [20] comparison with differentiable renderers for different 3D representations and supervised methods (on ShapeNet dataset). We use three categories: Airplanes, Cars and Chairs. Bigger value is better; bold = best.</figDesc><table><row><cell></cell><cell cols="3">Unsupervised learning</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Supervised learning</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">SoftRas [16] DIB-R [3] Ours</cell><cell cols="2">P2M [27] IN [15]</cell><cell cols="2">RN [4] AN [6]</cell><cell cols="2">DSN [32] 3DN [28]</cell><cell cols="2">ON [18] Ours</cell></row><row><cell>Airplane</cell><cell>58.4</cell><cell>57.0</cell><cell>62.4</cell><cell>51.5</cell><cell>55.4</cell><cell>42.6</cell><cell>39.2</cell><cell>57.5</cell><cell>54.3</cell><cell>57.1</cell><cell>75.3</cell></row><row><cell>Car</cell><cell>77.1</cell><cell>78.8</cell><cell>75.6</cell><cell>50.1</cell><cell>74.5</cell><cell>66.1</cell><cell>22.0</cell><cell>74.3</cell><cell>59.4</cell><cell>73.7</cell><cell>75.1</cell></row><row><cell>Chair</cell><cell>49.7</cell><cell>52.7</cell><cell>58.3</cell><cell>40.2</cell><cell>52.2</cell><cell>43.9</cell><cell>25.7</cell><cell>54.3</cell><cell>34.4</cell><cell>50.1</cell><cell>57.8</cell></row><row><cell>Mean</cell><cell>61.7</cell><cell>62.8</cell><cell>65.43</cell><cell>47.3</cell><cell>60.7</cell><cell>50.9</cell><cell>29.0</cell><cell>62.0</cell><cell>49.4</cell><cell cols="2">60.3 64.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Training time efficiency in hours.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>32 2 image</cell><cell>64 2 image</cell><cell>128 2 image</cell></row><row><cell></cell><cell cols="2">3D representations Rendering</cell><cell cols="3">2000 points/ 8000 points/ 16000 points/</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">32 3 voxels 64 3 voxels 128 3 voxels</cell></row><row><cell>DRC [25]</cell><cell>Voxels</cell><cell>Yes</cell><cell>≈14h</cell><cell>≈60h</cell><cell>≈216h</cell></row><row><cell>DPC [8]</cell><cell>Point clouds</cell><cell>Yes</cell><cell>≈14h</cell><cell>≈24h</cell><cell>≈72h</cell></row><row><cell>Ours</cell><cell>Point clouds</cell><cell>No</cell><cell>≈6.5h</cell><cell>≈11h</cell><cell>≈34.5h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Chamfer's distance comparison with the latest supervised method Multiview aggregation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>FID scores on Mesh (produced from 3D point cloud), Texture (extracted by a GAN) and Both (final output -textured 3D mesh) grouped by dataset, texture resolution, and condition, in truncated case. Lower is better; bold = best in that dataset.</figDesc><table><row><cell>FID (truncated σ)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Ablation studies in terms of Chamfer's distance (CD).</figDesc><table><row><cell>M 1</cell><cell cols="6">L 2 Pixel+L 2 M 1 + no w i j M 1 + no µ i j G t = 2 G t = 3 G t = 4</cell></row><row><cell cols="2">CD 19.50 139.10 24.59</cell><cell>4.58</cell><cell>4.41</cell><cell>4.79</cell><cell>4.54</cell><cell>4.01</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Appendix -Network architecture details</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E R</forename><surname>Shabayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cherenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ottersten</surname></persName>
		</author>
		<title level="m">A survey on deep learning advances on different 3d data representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Shapenet: An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to predict 3d objects with an interpolation-based differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/f5ac21cd0ef1b88e9848571aeb53551a-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9609" to="9619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Blender Foundation, Stichting Blender Foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">O</forename><surname>Community</surname></persName>
		</author>
		<ptr target="http://www.blender.org" />
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
	<note>Blender -a 3D modelling and rendering package</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Atlasnet: A papiermâché approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised learning of shape and pose with differentiable point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno>abs/1810.09381</idno>
		<ptr target="http://arxiv.org/abs/1810.09381" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3d structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/file/1d94108e907bb8311d8802b48fd54b4a-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4996" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning category-specific mesh reconstruction from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<title level="m">Differentiable rendering: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
