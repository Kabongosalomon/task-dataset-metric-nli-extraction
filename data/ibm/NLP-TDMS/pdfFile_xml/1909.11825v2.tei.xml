<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNSUPERVISED DOMAIN ADAPTATION THROUGH SELF-SUPERVISION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<email>yusun@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
							<email>etzeng@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<email>trevor@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
							<email>efros@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UNSUPERVISED DOMAIN ADAPTATION THROUGH SELF-SUPERVISION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses unsupervised domain adaptation, the setting where labeled training data is available on a source domain, but the goal is to have good performance on a target domain with only unlabeled data. Like much of previous work, we seek to align the learned representations of the source and target domains while preserving discriminability. The way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously. Each self-supervised task brings the two domains closer together along the direction relevant to that task. Training this jointly with the main task classifier on the source domain is shown to successfully generalize to the unlabeled target domain. The presented objective is straightforward to implement and easy to optimize. We achieve state-of-the-art results on four out of seven standard benchmarks, and competitive results on segmentation adaptation. We also demonstrate that our method composes well with another popular pixel-level adaptation method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visual distribution shifts are fundamental to our constantly evolving world. We humans face them all the time, e.g. when we navigate a foreign city, read text in a new font, or recognize objects in an environment we have never encountered before. These real-world challenges to the human visual perception have direct parallels in computer vision. Formally, a distribution shift happens when a model is trained on data from one distribution (source), but the goal is to make good predictions on some other distribution (target) that shares the label space with the source. Often computational models struggle even for pairs of distributions that humans find intuitively similar.</p><p>Our paper studies the setting of unsupervised domain adaptation, with labeled data in the source domain, but only unlabeled data in the target domain. The general philosophy of the field is to induce alignment of the source and target domains through some transformation. In the context of deep learning, a convolutional neural network maps images to learned representations in some feature space, so inducing alignment is done by making the distribution shifts small between the source and target in this shared feature space <ref type="bibr" target="#b12">(Csurka, 2017;</ref><ref type="bibr" target="#b57">Wang &amp; Deng, 2018;</ref><ref type="bibr" target="#b23">Gopalan et al., 2011)</ref>. If, in addition, such representations preserve discriminability on the source domain, then we can learn a good classifier on the source, which now generalizes to the target under the reasonable assumption that the representations of the two domains have the same ground truth.</p><p>Most existing approaches implement this philosophy of alignment by minimizing a measurement of distributional discrepancy in the feature space (details in section 2), often some form of maximum mean discrepancy (MMD) e.g. <ref type="bibr" target="#b38">Long et al. (2017)</ref>, or a learned discriminator of the source and target as an approximation to the total variation distance e.g. <ref type="bibr" target="#b19">Ganin et al. (2016)</ref>. Both measurements lead to the formulation of the training objective as a minimax optimization problem a.k.a. adversarial learning, which is known to be very difficult to solve. Unless carefully balanced, the push and pull in opposite directions can often cause wild fluctuations in the discrepancy loss and lead to sudden divergence (details in section 2). Therefore, we propose to avoid minimax optimization altogether through a very different approach.</p><p>Our main idea is to achieve alignment between the source and target domains by training a model on the same task in both domains simultaneously. Indeed, if we had labels in both domains, we could simply use our original classification task for this. However, since we lack labels in the target (c) adding more tasks <ref type="figure">Figure 1</ref>: We propose a method for unsupervised domain adaptation that uses self-supervision to align the learned representations of two domains in a shared feature space. Here we visualize how these representations might be aligned in the feature space. a) Without our method, the source domain is far away from the target domain, and a source classifier cannot generalize to the target. b) Training a shared representation to support one self-supervised task on both domains can align the source and target along one direction. c) Using multiple self-supervised tasks can further align the domains along multiple directions. Now the source and target are close in this shared feature space, and the source classifier can hope to generalize to the target.</p><p>domain, we propose to use a self-supervised auxiliary task, which creates its own labels directly from the data (see section 3). In fact, we can use multiple self-supervised tasks, each one aligning the two domains along a direction of variation relevant to that task. Jointly training all the self-supervised tasks on both domains together with the original task on the source domain produces well-aligned representations as shown in <ref type="figure">Figure 1</ref>.</p><p>Like all of deep learning, we can only empirically verify that at least in our experiments, the model does not overfit by internally creating a different decision boundary for each domain along different dimensions, which would then yield bad results. Recent research suggests that stochastic gradient descent is indeed unlikely to find such overfitting solutions with a costly decision boundary of high complexity (implicit regularization), even though the models have enough capacity <ref type="bibr" target="#b60">(Zhang et al., 2016a;</ref><ref type="bibr" target="#b42">Neyshabur et al., 2017b;</ref><ref type="bibr" target="#b2">Arora et al., 2018)</ref>.</p><p>The key contribution of our work is to draw a connection between unsupervised domain adaptation and self-supervised learning. While we do not propose any fundamentally new self-supervised tasks, we offer insights in section 3 on how to select the right ones for adaptation, and propose in section 4 a novel training algorithm on those tasks, using batches of samples from both domains. Additionally, we demonstrate that domain alignment could be achieved with a simple and stable algorithm, without the need for adversarial learning. In section 5, we report state-of-the-art results on several standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section we provide a brief overview of the two fields that our work would like to bridge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">UNSUPERVISED DOMAIN ADAPTATION</head><p>Methods for unsupervised domain adaptation in computer vision can be divided into three broad classes. The dominant class, which our work belongs to, aims to induce alignment between the source and the target domains in some feature space. This has been done by optimizing for some measurement of distributional discrepancy. One popular measurement is the maximum mean discrepancy (MMD) -the distance between the mean of the two domains in some reproducing kernel Hilbert space, where the kernel is chosen to maximize the distance <ref type="bibr" target="#b3">(Bousmalis et al., 2016;</ref><ref type="bibr" target="#b37">Long et al., 2015;</ref>. Another way to obtain a measurement of discrepancy is to train an adversarial discriminator that distinguishes between the two domains <ref type="bibr" target="#b18">(Ganin &amp; Lempitsky, 2014;</ref><ref type="bibr" target="#b19">Ganin et al., 2016;</ref>. However, both MMD and adversarial training are formulated as minimax optimization problems, which are widely known, both in theory and practice, to be very difficult <ref type="bibr" target="#b16">(Fedus et al., 2017;</ref><ref type="bibr" target="#b15">Duchi et al., 2016;</ref><ref type="bibr" target="#b36">Liang, 2017;</ref><ref type="bibr" target="#b30">Jin et al., 2019)</ref>. Since the optimization landscape is much more complex than in standard supervised learning, training often does not converge or converges to a bad local minimum <ref type="bibr" target="#b22">(Goodfellow, 2016;</ref><ref type="bibr" target="#b39">Nagarajan &amp; Kolter, 2017;</ref><ref type="bibr" target="#b35">Li et al., 2017)</ref>, and requires carefully balancing the two sets of parameters (for minimization and maximization) so one does not dominate the other <ref type="bibr" target="#b49">(Salimans et al., 2016;</ref><ref type="bibr" target="#b41">Neyshabur et al., 2017a)</ref>.</p><p>To make minimax optimization easier, researchers have proposed numerous modifications to the loss function, network design, and training procedure <ref type="bibr" target="#b24">Gulrajani et al., 2017;</ref><ref type="bibr" target="#b31">Karras et al., 2017;</ref><ref type="bibr" target="#b11">Courty et al., 2017;</ref><ref type="bibr" target="#b53">Sun &amp; Saenko, 2016;</ref><ref type="bibr" target="#b51">Shu et al., 2018;</ref><ref type="bibr" target="#b50">Sener et al., 2016)</ref>. Over the years, these modifications have yielded practical improvements on many standard benchmarks, but have also made the state-of-the-art algorithms very complicated. Often practitioners are not sure which tricks are necessary for which applications, and implementing these tricks can be bug-prone and frustrating. To make matters worse, since there is no labeled target data available for a validation set, practitioners have no way to perform hyper-parameter tuning or early stopping.</p><p>The second class of methods directly transforms the source images to resemble the target images with generative models <ref type="bibr" target="#b54">(Taigman et al., 2016;</ref><ref type="bibr" target="#b4">Bousmalis et al., 2017)</ref>. While similar to the first class in the philosophy of alignment, these methods operate on image pixels directly instead of an intermediate representation space, and therefore can benefit from an additional round of adaptation in some representation space. In subsection 5.2 we demonstrate that composing our method with a popular pixel-level method yields stronger performance than either alone.</p><p>The third class of methods uses a model trained on the labeled source data to estimate labels on the target data, then trains on some of those estimated pseudo-labels (e.g. the most confident ones), therefore bootstrapping through the unlabeled target data. Sometimes called self-ensembling <ref type="bibr" target="#b17">(French et al., 2017)</ref>, this technique is borrowed from semi-supervised learning, where it is called co-training <ref type="bibr" target="#b48">(Saito et al., 2017;</ref><ref type="bibr" target="#b63">Zou et al., 2018;</ref><ref type="bibr" target="#b8">Chen et al., 2018;</ref>. In contrast, our method uses joint training (of the main and self-supervised tasks), different from co-training in every aspect except the name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SELF-SUPERVISED FEATURE LEARNING</head><p>The emerging field of self-supervised learning uses the machinery of supervised learning on problems where external supervision is not available. The idea is to use data itself as supervision for auxiliary (also called "pretext") tasks that learn deep feature representations which will hopefully be informative for downstream "real" tasks. Many such auxiliary tasks have been proposed in the literature, including colorization (predicting the chrominance channels of an image given its luminance) <ref type="bibr" target="#b61">(Zhang et al., 2016b;</ref><ref type="bibr" target="#b33">Larsson et al., 2017;</ref><ref type="bibr" target="#b62">Zhang et al., 2017)</ref>, image inpainting <ref type="bibr" target="#b46">Pathak et al. (2016)</ref>, spatial context prediction <ref type="bibr" target="#b14">(Doersch et al., 2015)</ref>, solving jigsaw puzzles <ref type="bibr" target="#b43">(Noroozi &amp; Favaro, 2016)</ref>, image rotation prediction <ref type="bibr" target="#b21">(Gidaris et al., 2018)</ref>, predicting audio from video <ref type="bibr" target="#b45">(Owens et al., 2016)</ref>, contrastive predictive coding (Oord et al., 2018), etc. Researchers have also experimented with self-supervision on videos <ref type="bibr" target="#b58">(Wang &amp; Gupta, 2015;</ref><ref type="bibr" target="#b59">Wang et al., 2019)</ref>, and combining multiple self-supervised tasks together <ref type="bibr" target="#b13">(Doersch &amp; Zisserman, 2017)</ref>.</p><p>Typically, self-supervision is used as a pre-training step on unlabeled data (e.g. the ImageNet training set without labels) to initialize a deep learning model, followed by fine-tuning on a labeled training set (e.g. PASCAL VOC) and evaluating on the corresponding test set. Instead, in this paper, we train the self-supervised tasks together with the main supervised task, encouraging a consistent representation that both aligns the two domains and does well on the main task 1 .</p><p>Recently, self-supervision has also been used for other problem settings, such as improving robustness <ref type="bibr" target="#b27">(Hendrycks et al., 2019)</ref>, domain generalization (Carlucci et al., 2019) and few-short learning <ref type="bibr" target="#b52">(Su et al., 2019)</ref>. The most relevant paper to us is <ref type="bibr" target="#b20">Ghifary et al. (2016)</ref>, which uses self-supervision for unsupervised domain adaptation, but not through alignment. Their algorithm trains a denoising autoencoder <ref type="bibr" target="#b56">Vincent et al. (2008)</ref> only on the target data, together with the main classifier only on the labeled source data. They argue theoretically that this is better than training the autoencoder on both domains together. However, their theory is based on the critical assumption that the domains are already aligned, which is rarely true in practice. Consequently, their empirical results are much weaker than ours, as discussed in section 5. Please see Appendix A for more detailed comparisons with these works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGNING SELF-SUPERVISED TASKS FOR ADAPTATION</head><p>Task Images and self-supervised labels Rotation 0°90°180°270°L ocation (0, 0) (1, 0) (0, 1) (1, 1) The design of auxiliary self-supervised tasks is an exciting area of research in itself, with many successful examples listed in section 2. However, not all of them are suitable for unsupervised domain adaptation. In order to induce alignment between the source and target, the labels created by selfsupervision should not require capturing information on the very factors where the domains are meaninglessly different, i.e. the factors of variation that we are trying to eliminate through adaptation.</p><p>A particularly unsuitable tasks are these that try to predict pixels of the original image, as image inpainting <ref type="bibr" target="#b46">(Pathak et al., 2016)</ref>, colorization <ref type="bibr" target="#b61">(Zhang et al., 2016b;</ref><ref type="bibr" target="#b33">Larsson et al., 2017;</ref><ref type="bibr" target="#b62">Zhang et al., 2017)</ref> or denoising autoencoder <ref type="bibr" target="#b56">(Vincent et al., 2008)</ref>. The success of pixel-wise reconstruction depends strongly on brightness information, or other factors of variation in overall appearance (e.g. sunny vs. coudy) that are typically irrelevant to high-level visual concepts. Thus, instead of inducing alignment, learning a pixel reconstruction task would instead serve to further separate the domains. We have experimented with using the colorization task and the denoising autoencoder for our training algorithm, and found their performance little better than the source only baseline, sometimes even worse! 2</p><p>In general, classification tasks that predict structural labels seems better suited for our purpose than reconstruction tasks that predict pixels. Therefore, we have settled on three classification-based self-supervised tasks that combine simplicity with high performance:</p><p>Rotation Prediction: <ref type="bibr" target="#b21">(Gidaris et al., 2018)</ref> An input image is rotated in 90-degree increments i.e. 0°, 90°, 180°, and 270°; the task is to predict the angle of rotation as a four-way classification problem.</p><p>Flip Prediction: An input image is randomly flipped vertically; the task is to predict whether the image is flipped or not 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch Location Prediction:</head><p>Patches are randomly cropped out of an input image; the task is to predict where the patches come from 4 .</p><p>Consider a trivial illustrative example where the source and target are exactly the same except that the target pixels are all scaled down by a constant factor (e.g. daylight to dusk transition). All three of the aforementioned forms of self-supervision are suitable for this example, because pixel scaling i.e. brightness is "orthogonal" to the prediction of rotation, flip and location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>Our training algorithm is simple once a set of K self-supervised tasks are selected. We already have the loss function on the main prediction task, denoted L 0 , that we do not have target labels for. Each self-supervised task corresponds to a loss function L k for k = 1...K. So altogether, our optimization problem is set up as a combination of these K + 1 loss functions, as is done for standard multi-task learning (see <ref type="figure" target="#fig_1">Figure 2</ref>). Implementation details are included in Appendix B.</p><p>Each loss function corresponds to a different "head" h k for k = 0...K, which produces predictions in the respective label space. All the task-specific heads (including h 0 for the actual prediction task) share a common feature extractor φ. Altogether, the parameters of φ and h k , k = 0, ..k are the learned variables i.e. the free parameters of our optimization problem.</p><p>In our paper, φ is a deep convolutional neural network, and every h k is simply a linear layer i.e. multiplication by a matrix of size output space dimension × feature space dimension. If kth task is classification in nature, then h k also has a softmax or sigmoid (for multi-class or binary) following the linear layer. The output space dimension is only four for rotation and location classification, and two for flip and location regression. Depending on the network architecture used, the feature space dimension ranges between 64 and 512. The point is to make every h k low capacity, so the heads are forced to share high-level features, as is desirable for inducing alignment. A linear map from the highest-level features to the output is the smallest possible head and performs well empirically.</p><p>Let S = {(x i , y i ), i = 1...m} contain the labeled source data, and T = {(x i ), i = 1...n} contain the unlabeled target data. L 0 for the main prediction task takes in the labeled source data, and produces the following term in our objective:</p><formula xml:id="formula_0">L 0 (S; φ, h 0 ) = (x,y)∈S L 0 (h 0 (φ(x)), y).</formula><p>Each self-supervised task F k for k = 1...K modifies the input samples with some transformation f k and creates labelsỹ. Denote F k (S) = {(f k (x i ),ỹ i ), i = 1...m} as the self-supervised samples generated from the source samples (with the original labels discarded), and F k (T ) = {(f k (x i ),ỹ i ), i = 1...n} from the target. Then the loss L k of each task k = 1...K produces the following term:</p><formula xml:id="formula_1">L k (S, T ; φ, h k ) = (f k (x),ỹ)∈F (S) L k (h k (φ(f k (x))),ỹ) + (f k (x),ỹ)∈F (T ) L k (h k (φ(f k (x))),ỹ). (1)</formula><p>Note that L k (S, T ; φ, h k ) for k = 1...K, unlike L 0 (S; φ, h 0 ), take in both the source and target data; as we emphasize for many times throughout the paper, this is critical for inducing alignment. Altogether, our optimization problem can be formalized as in multi-task learning 5 :  <ref type="figure">Figure 3</ref>: Results on MNIST→MNIST-M (left) and CIFAR-10→STL-10 (right). Test error converges smoothly on the source and target domains for the main task as well as the self-supervised task. This kind of smooth convergence is often seen in supervised learning, but rarely in adversarial learning. The centroid distance (linear MMD) between the feature distributions of the two domains converges alongside, even though it is never explicitly optimized for.</p><formula xml:id="formula_2">min. φ,h k ,k=1...K L 0 (S; φ, h 0 ) + K k=1 L k (S, T ; φ, h k ).<label>(2)</label></formula><p>At test-time, we discard the self-supervised heads and use h 0 (φ(x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A HEURISTIC FOR HYPER-PARAMETER TUNING AND EARLY STOPPING</head><p>As previously mentioned, in the unsupervised domain adaptation setting, there is no target label available and therefore no target validation set, so typical strategies for hyper-parameter tuning and early stopping that require a validation set cannot be applied. This problem remains underappreciated; in fact, it is often unclear how previous works select their hyper-parameters or detemine when training is finished, both of which can be important factors that impact performance, especially for complex algorithms using adversarial learning. In this subsection we describe a simple heuristic 6 , merely as rule-of-thumb to make things work instead of a technical innovation. Like almost all of deep learning, there is no statistical guarantee on this heuristic, but it is shown to be practically effective in our experiments (see <ref type="figure">Figure 3</ref>). We only hope that it serves as the first guess of a solution towards this underappreciated problem.</p><p>The main idea is that, because our method never explicitly optimizes for any measurement of distributional discrepancy, these measurements can instead be used for hyper-parameter tuning and early stopping. Since it would be counterproductive to introduce additional parameters in order to perform hyper-parameter tuning, we simply use the distance between the mean of the source and target samples in the learned representation space, as produced by φ. Formally, this can be expressed as</p><formula xml:id="formula_3">D(S , T ; φ) = 1 m x∈S φ(x) − 1 n x∈T φ(x) 2 ,<label>(3)</label></formula><p>where S and T are unlabeled source and target validation sets 7 .</p><p>Our heuristic combines D(S , T ; φ) and the main task error on the (labeled) source validation set.</p><p>Denote v = (v 1 , ..., v T ) and w = (w 1 , ..., w T ) the measurement vectors of those two quantities respectively over T epochs. The final measurement vector is u = v/ min(v) + w/ min(w) i.e. a normalized sum of the two vectors; the epoch at which we perform early stopping is then simply arg min t∈{1...T } u t . Intuitively, this heuristic roughly corresponds to our goal of inducing alignment while preserving discriminability.</p><p>Source MNIST MNIST SVHN MNIST USPS CIFAR-10 STL-10 Target MNIST-M SVHN MNIST USPS MNIST STL-10 CIFAR-10 DANN <ref type="bibr" target="#b19">(Ganin et al., 2016)</ref> 81.5 35.7 73.6 ----DRCN <ref type="bibr" target="#b20">(Ghifary et al., 2016)</ref> -40.1 82.0 --66.4 58.7 DSN <ref type="bibr" target="#b3">(Bousmalis et al., 2016)</ref> 83.2 -82.7 ----kNN-Ad <ref type="bibr" target="#b50">(Sener et al., 2016)</ref> 86.7 40.3 78.8 ----PixelDA <ref type="bibr" target="#b4">(Bousmalis et al., 2017)</ref> 98.2 ------ATT <ref type="bibr" target="#b48">(Saito et al., 2017)</ref> 94  <ref type="table">Table 2</ref>: Test accuracy (%) on standard domain adaptation benchmarks. Our results are organized according to the self-supervised task(s) used: R for rotation, L for location, and F for flip. We achieve state-of-the-art accuracy on four out of the seven benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SEVEN BENCHMARKS FOR OBJECT RECOGNITION</head><p>The seven benchmarks are based on the six datasets described in Appendix D, each with a predefined training set / test set split, and labels are available on both splits. Previous works (cited in <ref type="table">Table 2)</ref> have created those seven benchmarks by picking pairs of datasets with the same label space, treating one as the source and the other as the target, and training on the training set of the source with labels revealed and of the target with labels hidden. Following the standard setup of the field, labels on the target test set should only be used for evaluation, not for hyper-parameter tuning or early stopping; therefore we apply the heuristic described in subsection 4.1.</p><p>For the two natural scene benchmarks, we use all three tasks described in section 3: rotation, location and flip prediction. For the five benchmarks on digits we do not use location because it yields trivial solutions that do not encourage the learning of semantic concepts. Given the image of a digit cropped into the four quadrants, location prediction can be trivially solved by looking at the four corners where a white stroke determines the category. Adding flip to the digits does not hurt performance, but does not improve significantly either, so we do not report those results separately.</p><p>As shown in <ref type="table">Table 2</ref>, despite the simplicity of our method, we achieve state-of-the-art accuracy on four out of the seven benchmarks. In addition, we show the source only results from our closest competitor (VADA and DIRT-T), and note that our source only results are in fact lower than theirs on those very benchmarks that we perform the best on; this indicates that our success is indeed due to effectiveness in adaptation instead of the base architecture.</p><p>Our method fails on the pair of benchmarks with SVHN, on which rotation yields trivial solutions. Because SVHN digits are cropped from house numbers with multiple digits, majority of the images have parts of the adjacent digits on the side. The main task head needs to look at the center, but the rotation head learns to look at the periphery and cheat. This failure case shows that the success of our method is tied to how well the self-supervised task fits the application. Practioners should use their domain knowledge evaluate how well the task fits, instead of blithely apply it to everything.</p><p>Also seen in <ref type="table">Table 2</ref> is that our method excels at object recognition in natural scenes, especially with all three tasks together. For STL-10→CIFAR-10, our base model is much worse than that of VADA and DIRT-T, but still beats all the baselines. Adding location and flip gives an improvement of 9%, Our code is available at https://github.com/yueatsprograms/uda release for object recognition and https://github.com/erictzeng/ssa-segmentation-release for segmentation.  <ref type="table">Table 3</ref>: Test accuracy (%) on GTA5 → Cityscapes. Our method significantly improves over source only, and also over CyCADA when combined. This indicates that additional self-supervision using our training algorithm further aligns the domains.</p><p>on top of the 9% already over source only. This is not surprising since those tasks were originally developed for ImageNet pre-training i.e. object recognition in natural scenes -our method is very successful when the task fits the application well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">BENCHMARK FOR SEMANTIC SEGMENTATION</head><p>To experiment with our method in more diverse applications, we also evaluate on a challenging simulation-to-real benchmark for semantic segmentation -GTA5 → Cityscapes. <ref type="bibr">GTA5 (Richter et al., 2016)</ref> contains 24,966 video frames taken from the computer game, where dense segmentation labels are automatically given by the game engine. Cityscapes <ref type="bibr" target="#b10">(Cordts et al., 2016)</ref> contains 5,000 video frames taken from real-world dash-cams. The main task is to classify every pixel in an image as one of the 19 classes shared across both datasets, and accuracy is measured by intersection over union (IoU). The best possible results are given as the oracle, when the labels on Cityscapes are available for training, so typical supervised learning methods are applicable.</p><p>Our results are shown in <ref type="table">Table 3</ref>, and implementation details in Appendix F. Our self-supervised tasks were designed for classification, where the label on an image depends on its global content, while in segmentation the labels tend to be highly local. Nevertheless, with very little modification, we see significant improvements over the source only baseline.</p><p>We also experiment with combining our method with another popular unsupervised domain adaptation method -CyCADA , designed specifically for segmentation. Surprisingly, when operating on top of images produced by this already very strong baseline, our method further improves performance. This demonstrates that pixel-level adaptation methods might still benefit from an additional round of adaptation by inducing alignment through self-supervision. We emphasize that these results are obtained with a very simple instantiation of our method, as a start towards the development of self-supervised tasks more suitable for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>We hope that this work encourages future researchers in unsupervised domain adaptation to consider the study of self-supervision as an alternative to adversarial learning, and researchers in selfsupervision to consider designing tasks and evaluating them in our problem setting. Most selfsupervised tasks today were originally designed for pre-training and evaluated in terms of accuracy gains on a downstream recognition, localization or detection tasks. It will be interesting to see if new self-supervised tasks can arise from the motivation of adaptation, for which alignment is the key objective. Moreover, domain experts could perhaps incorporate their dataset specific knowledge into the design of a self-supervised task specifically for their application.</p><p>One additional advantage of our method, not considered in this paper, is that it might be particularly amenable to very small target sample size, when those other methods based on adversarial learning cannot accurately estimate the target distribution. We leave this topic for the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL DISCUSSION ON SPECIFIC PAPERS RELATED TO OURS</head><p>In this section we discuss the four papers mentioned in section 2 that are related to ours, but different in training algorithm, philosophy or problem setting.</p><p>Deep Reconstruction-Classification Networks <ref type="bibr" target="#b20">(Ghifary et al., 2016)</ref>. This method works in unsupervised domain adaptation, the same problem setting as ours. It learns a denoising autoencoder on the target, together with the main classifier on the source. However, they use only the target domain for reconstruction, claiming both empirically and theoretically that it is better than using both domains together. In contrast, we use both the source and target for self-supervision. These two algorithmic differences really reflect our fundamental difference in philosophy. They take the target data as analogous to the unlabeled (source) data in semi-supervised learning, so any form of self-supervision suitable for semi-supervised learning is good enough; their theoretical analysis is also directly borrowed from that of semi-supervised learning, which concludes that it is necessary and sufficient to only use the target data for self-supervision. We take data from both domains for self-supervision in order to align their feature distributions; also, both conceptually and empirically, we cannot use reconstruction tasks because they are unsuitable for inducing alignment (see section 3).</p><p>Empirical experiments further support our arguments in three ways: 1) If we were to only use the target for our self-supervision tasks, we would perform barely better than the source only baseline, even on MNIST→MNIST-M, the easiest benchmark where we would otherwise observe a huge improvement. 2) If we were to use the reconstruction task i.e. a denoising autoencoder, we would again perform barely better than the source only baseline, and sometimes even worse, as described in section 3. 3) As shown in <ref type="table">Table 2</ref>, our results are much better than theirs on all of the benchmarks by a large margin. This shows that implementing the philosophy of alignment, developed for unsupervised domain adaptation, is much superior in our problem setting to blithely borrowing from semi-supervised learning.</p><p>Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty <ref type="bibr" target="#b27">(Hendrycks et al., 2019)</ref>. This paper studies the setting of robustness, where there is no sample provided, even unlabeled, from the target domain. Their method jointly trains for the main task and the self-supervised task only on the source domain, really because there is no target data of any kind. Because their problem setting is very challenging (with so little information provided on the target), their primary evaluation is on a dataset of CIFAR-10 images with the addition of 15 types of common corruptions (e.g. impulse noise, JPEG compression, snow weather, motion blur and pixelation), simulated by an algorithm. No idea on unsupervised domain adaptation is mentioned. <ref type="bibr" target="#b5">(Carlucci et al., 2019)</ref>. This paper studies two setting. The first is the robustness setting, exactly the same as in <ref type="bibr" target="#b27">Hendrycks et al. (2019)</ref>, except that evaluation is done on MNIST→MNIST-M and MNIST→SVHN. Their baseline is also a method from the robustness community that trains on adversarial examples and uses no target data. Again, because their problem setting is very challenging, the accuracy is low for both their proposed method and the baseline. The second setting is called domain generalization, which is very similar to meta-learning. The goal is to perform well simultaenously on multiple distributions, all labeled. Evaluation is done using the mean accuracy on all the domains. Beside the name, there is little similarity between the setting of unsupervised domain adaptation and domain generalization, which has no unsupervised component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Generalization by Solving Jigsaw Puzzles</head><p>Boosting Supervision with Self-Supervision for Few-shot Learning <ref type="bibr" target="#b52">(Su et al., 2019)</ref>. As evident from the title, this paper studies the setting of few-shot learning, where the goal is to perform well on a very small dataset. Again, there is no unsupervised component and little connection to our setting.</p><p>Most of our results have already been produced when <ref type="bibr" target="#b5">Carlucci et al. (2019)</ref> and <ref type="bibr" target="#b52">Su et al. (2019)</ref> were presented at a conference. <ref type="bibr" target="#b27">Hendrycks et al. (2019)</ref> was submitted to NeurIPS 2019 and should be considered concurrent work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL ALGORITHMIC DETAILS</head><p>In practice and for our experiments, the source and target datasets are often imbalanced in size. If we were to blithely solve for the objective in Equation 2, the domain with a larger dataset would carry more weight for every L k (S, T ; φ, h k ), k = 1...K because we are summing over all samples in both datasets. We would like to keep the two sums inside L k roughly balanced such that in terms of features produced by φ, neither of the two domains would dominate the other. This is easy to achieve in our implementation through balanced batches. When we need to sample a batch for task k, we simply sample half the batch from the source, and another half from the target, then put them together. In the end, our implementation requires little change on top of an existing supervised learning codebase. Each self-supervised task is defined as a module, and adding a new one only amounts to defining the structural modifications and the loss function.</p><p>We optimize the objective in Equation 2 with stochastic gradient descent (SGD). One can simply take a joint step on Equation 2, which covers all the losses for k = 0 and k = 1...K. However, the implementation would then have to store the gradients with respect to all K + 1 losses together. For memory efficiency, our implementation loops over k = 1...K; for each self-supervised task k, it samples a batch of combined source and target data (without their original labels), structurally modifies the images by f k , creates new labels according to the modification, and obtains a loss for h k and φ on this batch. So a gradient step is taken for each k = 1...K, before finally a batch of original source images and labels are sampled for a gradient step on h 0 and φ. In terms of test accuracy, these two implementations make little difference (usually less than 1%), and the choice simply comes down to a time-memory trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL DISCUSSION ON THE MEAN DISTANCE</head><p>In this section we answer some potential questions about our selection rule, which uses the mean distance, from the perspective of a possibly confused reader.</p><p>Q: The motivation of the paper is that methods based on minimax optimization, such as those using kernel-MMD, are difficult to optimize. Since you are using the mean distance (MMD under the linear kernel) for hyper-parameter tuning and early stopping, which is a form of model selection, how are you different from those other methods and why is optimization easy for you?</p><p>A: Our method is fundamentally different from those other methods based on MMD, and therefore more amenable to optimization in the following two ways: 1) Even though we use the mean distance, which is a form of MMD, we never pose a minimax optimization problem. The model parameters minimize the loss functions of the tasks, which we hope makes the mean distance small (see <ref type="figure">Figure 3</ref>). Model selection using subsection 4.1 also minimizes the mean distance. In our method, all the loss functions, for model parameters and hyper-parameters, work towards the same goal of inducing alignment, so optimization is easier. 2) Even though hyper-parameter tuning and early stopping are a forms of model selection just like training, there is a qualitative difference in the degrees of freedom involved. For subsection 4.1, when performing early stopping for example, optimization amounts to a grid search over the epochs after training is finished, and there is only one degree of freedom.</p><p>Q: Why is the mean distance suitable for model selection when it comes to hyper-parameter tuning and early stopping, but not regular training?</p><p>A: Again, we agree that hyper-parameter tuning and early stopping are a forms of model selection. However, unlike model parameters ranging in the hundred of thousands, there are at most a few hyper-parameters and only one parameter for early stopping. Model parameters can easily overfit to the mean distance while those few degrees of freedom we use cannot. This is precisely also the reason why previous works using MMD resort to minimax optimization. Since they use MMD for training, they must also give MMD enough degrees of freedom to work towards the opposite direction of optimization as the model parameters, so that it is not as easy to overfit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DETAILS OF THE SIX DATASETS USED FOR OBJECT RECOGNITION</head><p>1) <ref type="bibr">MNIST (LeCun et al., 1998)</ref>: greyscale images of handwritten digits 0 to 9; 60,000 samples in the training set and 10,000 in the test set.</p><p>2) MNIST-M <ref type="bibr" target="#b19">(Ganin et al., 2016)</ref>: constructed by blending MNIST digits with random color patches from the BSDS500 dataset <ref type="bibr" target="#b0">Arbelaez et al. (2011)</ref>; same training / test set size as MNIST.</p><p>3) SVHN <ref type="bibr" target="#b40">(Netzer et al., 2011)</ref>: colored images of cropped out house numbers from Google Street View; the task is to classify the digit at the center; 73,257 samples in the training set, 26,032 in the test set and 531,131 easier samples for additional training. 4) USPS: greyscale images of handwritten digits only slightly different from MNIST; 7291 samples in the training set and 2007 in the test set. 5) CIFAR-10 <ref type="bibr" target="#b32">(Krizhevsky &amp; Hinton, 2009</ref>): colored images of 10 classes of centered natural scene objects; 50,000 samples in the training set and 10,000 in the test set. 6) STL-10 : colored images of objects only slightly different from CIFAR-10; 5000 samples in the training set and 8000 in the test set.</p><p>Because CIFAR-10 and STL-10 differ in one class category, we follow common practice <ref type="bibr" target="#b51">(Shu et al., 2018;</ref><ref type="bibr" target="#b17">French et al., 2017;</ref><ref type="bibr" target="#b20">Ghifary et al., 2016)</ref> and delete the offending categories, so each of these two datasets actually only has 9 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E IMPLEMENTATION DETAILS ON THE OBJECT RECOGNITION BENCHMARKS</head><p>We use a 26-layer pre-activation ResNet <ref type="bibr" target="#b26">(He et al., 2016)</ref> as our test-time model h 0 (φ(x)), where h 0 is the last linear layer that makes the predictions, and φ is everything before that. For unsupervised domain adaptation, there is no consensus on what base architecture to use among the previous works.</p><p>Our choice is simply base on the widespread adoption of the ResNet architecture and the ease of implementation. In <ref type="table">Table 2</ref> we provide the source only results using our base architecture, and the ones from <ref type="bibr" target="#b51">Shu et al. (2018)</ref>, our closest competitor. Our source only results are in fact worse than theirs, indicating that our improvements are indeed made through adaptation.</p><p>At training time, the self-supervised heads h k , k = 1...K are simply linear layers connected to the end of φ as discussed in section 4. There is no other modification on the standard ResNet. For all experiments on the object recognition benchmarks, we optimize our model with SGD using weight decay 5e-4 and momentum 0.9, with a batch size of 128. We use an initial learning rate of 0.1 and a two milestone schedule, where the learning rate drops by a factor of 10 at each milestone. All these optimization hyper-parameters are taken directly from the standard literature <ref type="bibr" target="#b26">(He et al., 2016;</ref><ref type="bibr" target="#b29">Huang et al., 2016;</ref><ref type="bibr" target="#b25">Guo et al., 2017)</ref> without any modification for our problem setting. We select the total number of epochs and the two milestones based on convergence of the source classifier h 0 and unsupervised classifiers h 1 , ..., h K . Early stopping is done using the selection heuristic discussed in subsection 4.1. For fair comparison with our baselines, we do not perform data augmentation, following previous works <ref type="bibr" target="#b50">Sener et al., 2016;</ref><ref type="bibr" target="#b20">Ghifary et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F IMPLEMENTATION DETAILS ON GTA5 → CITYSCAPES</head><p>For our experiments, we initialize our model from the DeepLab-v3 architecture <ref type="bibr" target="#b6">(Chen et al., 2017)</ref>, pre-trained on ImageNet, as commonly done in the field. Each self-supervised head consists of a global average pooling layer on the pre-logit layer, followed by a single linear layer. To take advantage of the large size of the natural scene images, we use the continuous i.e. regression version of location prediction. The self-supervised head is trained on the square loss, to regress the coordinates (in two dimensions) that the patch is cropped from. Natural for the regression version, instead of cropping from the quadrants like for the classification version on the small datasets, we instead crop out 400×400 patches taken at random from the segmentation scenes. We optimize our model with SGD using a learning rate of 0.007 for 15,000 iterations, with a batch size of 48. Once again, we use the selection heuristic in subsection 4.1 for early-stopping.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; S &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; S &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H I 9 D L v k 6 V 7 o g J c f + 1 h b m c M 3 R v 9 0 = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R I R d F l w 0 2 W F v q A N Z T K d t E M n M 2 H m R i i h n + H G h S J u / R p 3 / o 2 T N g t t P T B w O O d e 5 t w T J o I b 9 L x v p 7 S 1 v b O 7 V 9 6 v H B w e H Z 9 U T 8 + 6 R q W a s g 5 V Q u l + S A w T X L I O c h S s n 2 h G 4 l C w X j h 7 y P 3 e E 9 O G K 9 n G e c K C m E w k j z g l a K X B M C Y 4 p U R k 7 c W o W v P q 3 h L u J v E L U o M C r V H 1 a z h W N I 2 Z R C q I M Q P f S z D I i E Z O B V t U h q l h C a E z M m E D S y W J m Q m y Z e S F e 2 W V s R s p b Z 9 E d 6 n + 3 s h I b M w 8 D u 1 k H t G s e 7 n 4 n z d I M b o P M i 6 T F J m k q 4 + i V L i o 3 P x + d 8 w 1 o y j m l h C q u c 3 q 0 i n R h K J t q W J L 8 N d P 3 i T d m 7 r v 1 f 3 H 2 1 q j W d R R h g u 4 h G v w 4 Q 4 a 0 I Q W d I C C g m d 4 h T c H n R f n 3 f l Y j Z a c Y u c c / s D 5 / A G O + 5 F w &lt; / l a t e x i t &gt; S &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w O L Z B U A I 8 V 3 s 6 b y K z R K w u B 0 x I c A = " &gt; A A A B 8 n i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L g p s u K 9 g F t K J P p p B 0 6 m Y S Z G 6 G E f o Y b F 4 q 4 9 W v c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m d 7 n f f e L a i F g 9 4 i z h f k T H S o S C U b R S f x B R n D A q s 4 f 5 s F p z 6 + 4 C Z J 1 4 B a l B g d a w + j U Y x S y N u E I m q T F 9 z 0 3 Q z 6 h G w S S f V w a p 4 Q l l U z r m f U s V j b j x s 0 X k O b m w y o i E s b Z P I V m o v z c y G h k z i w I 7 m U c 0 q 1 4 u / u f 1 U w x v / U y o J E W u 2 P K j M J U E Y 5 L f T 0 Z C c 4 Z y Z g l l W t i s h E 2 o p g x t S x V b g r d 6 8 j r p X N U 9 t + 7 d X 9 c a z a K O M p z B O V y C B z f Q g C a 0 o A 0 M Y n i G V 3 h z 0 H l x 3 p 2 P 5 W j J K X Z O 4 Q + c z x + N d p F v &lt; / l a t e x i t &gt; Source classifier Self-supervised classifier #1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our method jointly trains a supervised head on labeled source data and self-supervised heads on unbaled data from both domains. The heads use high-level features from a shared encoder, which learns to align the feature distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Transformations and labels on a</cell></row><row><cell>sample input image created by two of the</cell></row><row><cell>self-supervised tasks used in our work.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that it is possible to apply the standard pre-training followed by fine-tuning regime to unsupervised domain adaptation, doing self-supervised pre-training on the target domain (or both the source and the target) and then fine-tuning on the source. However, this gives almost no benefit over no-adaptation baseline, and is far from being competitive.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">It is interesting to note that works borrowing from semi-supervised learning, e.g.<ref type="bibr" target="#b20">Ghifary et al. (2016)</ref>, use denoising autoencoder for their training algorithm and obtain performance better than source only (but still not competitive with ours). As explained in Appendix A, this difference is due to the difference in training algorithms -we use self-supervision on both domains while they only use on the target. This further reflects the difference in philosophy, as we use target data for alignment whereas they use it simply as extra data as in semi-supervised learning.3  We do not use horizontal flips, which is a common data augmentation technique, since it is typically desirable for natural scene features to be invariant to horizontal flips.4  For large images (e.g. segmentation), the crop comes from a continuous set of coordinates, and the task is a regression problem in two dimensions, trained with the square loss. For small images (e.g. object recognition), the crop comes from one of the four quadrants, and the task is a four-way classification problem; this distinction exists only for ease of implementation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We have experimented with trade-off hyper-parameters λi, i = 1...K for the loss terms inside the sum and found them unnecessary. Since a labeled target validation set might not be available, it can be beneficial to reduce the number of hyper-parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Appendix C contains some additional explanation of why this heuristic cannot be used for regular training. The short answer is given by the famous Goodhardt's law in economics and dynamical systems: "When a measurement becomes an objective, it ceases to be a good measurement." 7 D above is also known as the discrepancy under the linear kernel from the perspective of kernel MMD. Appendix C contains some additional explanation of why the mean distance is suitable for our heuristic, even though it is a specific form of MMD, which we claim to be difficult to optimize. The short answer is that it does not require minimax optimization.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Stronger generalization bounds for deep nets via a compression approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05296</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3722" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.05587" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2456" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial deep averaging networks for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="557" to="570" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal transport for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Flamary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1853" to="1865" />
		</imprint>
	</monogr>
	<note>Devis Tuia, and Alain Rakotomamonjy</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05374</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local minimax complexity of stochastic convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3423" to="3431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Many paths to equilibrium: Gans do not need to decrease a divergence at every step</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08446</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Self-ensembling for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05208</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">Nips 2016 tutorial: Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.12340</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Minmax optimization: Stable limit points of gradient descent ascent are locally optimal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praneeth</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00618</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On the limitations of first-order approximation in gan dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09884</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">How well can generative adversarial networks learn densities: A nonparametric view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyuan</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08244</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<title level="m">Learning transferable features with deep adaptation networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gradient descent gan optimization is locally stable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishnavh</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5585" to="5595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Stabilizing gan training with multiple random projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07831</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Geometry of optimization and implicit regularization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03071</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning transferrable representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2110" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08735</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Boosting supervision with self-supervision for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07079</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycleconsistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
