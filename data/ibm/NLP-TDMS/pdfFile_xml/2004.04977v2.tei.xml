<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ntavelis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Robotics and Machine Learning</orgName>
								<orgName type="institution">CSEM SA</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Romero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iason</forename><surname>Kastanis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Robotics and Machine Learning</orgName>
								<orgName type="institution">CSEM SA</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">PSI</orgName>
								<orgName type="institution">ESAT</orgName>
								<address>
									<settlement>Leuven</settlement>
									<region>KU</region>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative Adversarial Networks</term>
					<term>Interactive Image Edit- ing</term>
					<term>Image Synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>input labe <ref type="figure">Fig. 1</ref>. We assess SESAME on three tasks (a) image editing with free form semantic drawings (first row) (b) semantic layout driven semantic editing (second row) (c) layout to image generation with SESAME discriminator (third row)</p><p>Abstract. Recent advances in image generation gave rise to powerful tools for semantic image editing. However, existing approaches can either operate on a single image or require an abundance of additional information. They are not capable of handling the complete set of editing operations, that is addition, manipulation or removal of semantic concepts. To address these limitations, we propose SESAME, a novel generator-discriminator pair for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects. In our setup, the user provides the semantic labels of the areas to be edited and the generator synthesizes the corresponding pixels. In contrast to previous methods that employ a discriminator that trivially concatenates semantics and image as an input, the SESAME discriminator is composed of two input streams that arXiv:2004.04977v2 [cs.CV] 8 Oct 2020 2 E. Ntavelis et al.</p><p>independently process the image and its semantics, using the latter to manipulate the results of the former. We evaluate our model on a diverse set of datasets and report state-of-the-art performance on two tasks: (a) image manipulation and (b) image generation conditioned on semantic labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>input labe <ref type="figure">Fig. 1</ref>. We assess SESAME on three tasks (a) image editing with free form semantic drawings (first row) (b) semantic layout driven semantic editing (second row) (c) layout to image generation with SESAME discriminator (third row)</p><p>Abstract. Recent advances in image generation gave rise to powerful tools for semantic image editing. However, existing approaches can either operate on a single image or require an abundance of additional information. They are not capable of handling the complete set of editing operations, that is addition, manipulation or removal of semantic concepts. To address these limitations, we propose SESAME, a novel generator-discriminator pair for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects. In our setup, the user provides the semantic labels of the areas to be edited and the generator synthesizes the corresponding pixels. In contrast to previous methods that employ a discriminator that trivially concatenates semantics and image as an input, the SESAME discriminator is composed of two input streams that independently process the image and its semantics, using the latter to manipulate the results of the former. We evaluate our model on a diverse set of datasets and report state-of-the-art performance on two tasks: (a) image manipulation and (b) image generation conditioned on semantic labels.</p><p>Keywords: Generative Adversarial Networks, Interactive Image Editing, Image Synthesis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image editing is a challenging task that has received increasing attention in the media, movies and social networks. Since the early 90s, tools like Gimp <ref type="bibr" target="#b36">[38]</ref> and Photoshop <ref type="bibr">[36]</ref> have been extensively utilized for this task. Yet, both require high level expertise and are labour intensive. Generative Adversarial Networks (GANs) <ref type="bibr" target="#b9">[10]</ref> provide a learning-based alternative able to assist non-experts to express their creativity when retouching photographs. GANs have been able to produce results of high photo-realistic quality <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Despite their success in image synthesis, their applicability on image editing is still not fully explored. Being able to manipulate images is a crucial task for many applications such as autonomous driving <ref type="bibr" target="#b14">[15]</ref> and industrial imaging <ref type="bibr" target="#b6">[7]</ref>, where data augmentation boosts the generalization capabilities of neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Image manipulation has been used in the literature to refer to various tasks. In this paper, we follow the formulation of Bau et al . <ref type="bibr" target="#b2">[3]</ref>, and define the task of semantic image editing as the process of adding, altering, and removing instances of certain classes or semantic concepts in a scene. Examples of such manipulations include but are not limited to: removing a car from a road scene, changing the size of the eyes of a person, adding clouds in the sky, etc. We use the term semantic concepts to refer to various class labels that can not be identified as objects, e.g. mountains, grass, etc.</p><p>Training neural networks for visual editing is not a trivial task. It requires a high level of understanding of the scene, the objects, and their interconnections <ref type="bibr" target="#b43">[45]</ref>. Any region of an image added or removed should look realistic and should also fit harmoniously with the rest of the scene. In contrast to image generation, the co-existence of real and fake pixels makes the fake pixels more detectable, as the network cannot take the "easy route" of generating simple textures and shapes or even omit a whole class of objects <ref type="bibr" target="#b3">[4]</ref>. Moreover, the lack of natural image datasets, where a scene is captured with and without an object, makes it difficult to train such models in a supervised manner.</p><p>One way to circumvent this problem is by inpainting the regions of an image we seek to edit. Following this scheme, we mask out and remove all the pixels we want to manipulate. Recent works <ref type="bibr">[55,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b15">16]</ref> improve upon this approach by incorporating sketch and color inputs to further guide the generation of the missing areas and thus provide higher level control. However, inpainting can only tackle some aspects of semantic editing. To address this limitation, Hong et al . <ref type="bibr" target="#b11">[12]</ref> manipulate the semantic layout of an image and subsequently, they utilize for inpainting the image. Yet, this approach requires access to the full semantic information of the image, which is costly to acquire.</p><p>To this end, we propose SESAME, a novel semantic editing architecture based on adversarial learning, able to manipulate images based on a semantic input. In particular, our method is able to edit images with pixel-level guidance of semantic labels, permitting full control over the output. We propose using the semantics only for regions to be edited, which is more cost-efficient and produces better results in certain scenarios. Moreover, we introduce a new approach for semantics-conditioned discrimination, by utilizing two independent streams to process the input image and the corresponding semantics. We use the output of the semantics stream to adjust the output of the image stream. We employ visual results along with quantitative analysis and a human study to validate the performance and flexibility of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generative Adversarial Networks <ref type="bibr" target="#b9">[10]</ref> have completely revolutionized a great variety of computer vision tasks such as image generation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>, super resolution <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b26">27]</ref>, image attribute manipulation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">40]</ref> and image editing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Initially, GANs were only capable of generating samples drawn from a random distribution <ref type="bibr" target="#b9">[10]</ref>, but soon multiple models emerged able to perform conditional image synthesis <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref>. These approaches condition the generation on various types of information. For example, <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr">57,</ref><ref type="bibr" target="#b4">5]</ref> synthesize images characterized by a single label. In a different setting, <ref type="bibr" target="#b37">[39,</ref><ref type="bibr">58,</ref><ref type="bibr">59,</ref><ref type="bibr">52]</ref> employ a text to image pipeline to create an image based on a high-level description. Recently, many methods utilize information of a scene graph <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref> and sketches with color <ref type="bibr" target="#b40">[42]</ref> to represent where objects should be positioned on the output image. A more fine-grained approach aims to translate semantic maps, which carry pixel-wise information, to realistic looking images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22]</ref>. For all the aforementioned models, the user can control the output image by altering the conditional information. Nonetheless, they are not suitable for manipulating an existing image, as they do not consider an image as an input.</p><p>In user-guided semantic image editing the user is able to semantically edit an image by adding, manipulating, or removing semantic concepts <ref type="bibr" target="#b2">[3]</ref>. Both GANPaint <ref type="bibr" target="#b2">[3]</ref> and SinGAN <ref type="bibr" target="#b41">[43]</ref> are able to perform such operations: GAN-Paint <ref type="bibr" target="#b2">[3]</ref> by manipulating the neuron activations and SinGAN <ref type="bibr" target="#b41">[43]</ref> by learning the internal batch statistics of an image. However, both are trained on a single image and require retraining in order to be applied to another, while our model is able to handle the manipulation of multiple images without retraining.</p><p>Another simple type of editing is inpainting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">56,</ref><ref type="bibr" target="#b24">25]</ref>, where the user masks a region of the image for removal and the network fills it accordingly to the image context. In its classic form the user does not have control over the generated pixel. To address this, other research works guide the generation of the missing areas using edges <ref type="bibr">[55,</ref><ref type="bibr" target="#b31">32]</ref> and/or color <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b15">16]</ref> information.</p><p>Recently, semantic aware approaches for inpainting address object addition and removal. Shetty et al . <ref type="bibr" target="#b42">[44]</ref> proposes a two-stage architecture to facilitate removal operations, with an auxiliary network predicting the objects' masks during training; at inference users provide them. Note that their model cannot handle object generation. Works in the object synthesis task are utilizing semantic layout information, a fine-grained guidance over the manipulation of an image. Yet, a subset of them is limited by generating objects from a single class <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b49">51]</ref> or placing prior fixed objects on the semantics plane <ref type="bibr" target="#b22">[23]</ref>. Hong et al . <ref type="bibr" target="#b11">[12]</ref> are able to handle both addition and removal, but require full semantic information of the scene to produce even the smallest change to an image. In contrast, our method requires only the semantics of the region to be edited.</p><p>The core of the majority of the aforementioned works rely on adjusting the generator for the task of image editing. Most recent models use a PatchGAN variant <ref type="bibr" target="#b13">[14]</ref> which is able to discriminate on the high frequencies of the image. This is a desired attribute as conventional losses like Mean Squared Error and Mean Absolute Error can only convey information about the lower frequencies to the generator. PatchGAN can also be used for conditional generation of images on semantic maps, similar to our case study. Previous works targeting a similar problem concatenate the semantic information to the image and use it as an input to the discriminator. However, conventional conditional generation literature suggests that concatenation is not the optimal approach for conditional discrimination <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref>. To address this, Liu et al . <ref type="bibr" target="#b25">[26]</ref> propose a feature pyramid semantics-embedding (FPSE) discriminator using an Encoder-Decoder architecture. Each upsampling layer outputs two per-patch score maps, one trying to measure the realness and one to gauge the semantic matching with the labels; the later is derived after a patch-wise inner product operation with the downsampled semantic embeddings. Rather than incorporating a semantics loss, we use semantics to guide the image discrimination. Our model incorporates conditional information by processing it separately from the image input. In a later stage of the network, the two processed streams are merged to produce the final output of the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SESAME</head><p>In this work we describe a deep learning pipeline for semantically editing images, using conditional Generative Adversarial Networks (cGANs). Given an image I real and a semantic guideline of the regions that should be altered by the network, denoted by M sem , we want to produce a realistic output I out . The real pixels values corresponding to M sem are removed from the input image. The generated pixels in their place should be both true to the semantics dictated by the mask and coherent with the rest of the pixels of I real . In order to achieve this, our network is trained end-to-end in an adversarial manner. The generator is a Encoder-Decoder architecture, with dilated convolutions [53] and SPADE <ref type="bibr" target="#b34">[35]</ref> layers and the discriminator is a two stream patch discriminator. <ref type="figure">Fig. 2</ref>. The SESAME Generator aims to generate the pixels designated by the semantic mask so they are both (1) true to their label and (2) fit naturally to the rest of the picture. It is an encoder-decoder architecture with dilated convolutions to increase the receptive field as well as SPADE layers in the decoder to guide in-class generation SESAME Generator. Semantically editing a scene is an Image to Image translation problem. We want to transform an image where we substituted some of the RGB pixels with an one-hot semantics vector. From the generator's output, only the pixels on the masked out regions are retained, while the rest are retrieved from the original image:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Generator</head><formula xml:id="formula_0">I gen = G(I m , M, M sem ),<label>(1)</label></formula><formula xml:id="formula_1">I out = I gen · M + I real · (1 − M ).<label>(2)</label></formula><p>This architecture has two goals: generated pixels should 1) be coherent with their real neighboring ones as well as 2) be true to the semantic input. To achieve these goals we adapt our generator from the network proposed by Johnson et al . <ref type="bibr" target="#b16">[17]</ref> to fill the gaps: two downsampling layers, a semantic core made of multiple residual layers and two upsampling ones.</p><p>We conceptually divide our architecture into an encoder and a decoder part. The first extracts the contextual information of the pixels we want to synthesize. The seconds combines the semantic information using Spatially Adaptive De-Normalization <ref type="bibr" target="#b34">[35]</ref> blocks to every layer. As the area to be edited can span over a large region, we would like the receptive field of our network to be relatively large. Thus, we use dilated convolutions in the last and first layers of the encoder and the decoder, respectively. A scheme of our SESAME generator can be seen in the <ref type="figure">Fig. 2</ref>, and for further details refer to the supplementary materials. SESAME Discriminator. Layout to image editing can be seen as a subtask of label to image translation. Inspired by the Pix2Pix <ref type="bibr" target="#b13">[14]</ref>, more recent approaches <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b34">35]</ref> employ a variation of the PatchGAN discriminator. The Markovian discriminator, as it is also called, was a paradigm shift that made Convolutional Layer Sum Global Pooling OR <ref type="figure">Fig. 3</ref>. The SESAME discriminator, in contrast to the commonly used PatchGAN, is handling the RGB Image and its Semantics independently. Before the last convolutional layer the two streams, DRGB and DSem, are merged. The semantics stream is reduced via a Sum Global Pooling operation to a 2D matrix of spatial dimensions equal to the number of output patches. The feature vector of DRGB at each path is scaled by DSem and a residual is added to product the discriminator focus on the higher frequencies by limiting the attention of the discriminator into local patches, producing a different fake/real prediction value for each of them. The subsequent methods added a multiscale discrimination approach, the Feature Matching-Loss <ref type="bibr" target="#b45">[47]</ref> and the use of Spectral Normalization <ref type="bibr" target="#b29">[30]</ref> instead of Instance Normalization <ref type="bibr" target="#b34">[35]</ref>, which stabilized training and further improved the quality of the generated samples. However, the way that the conditional information was provided to the discriminator remained unchanged.</p><p>Label to image generation is a sub-task of conditional image generation. In this more general category of methods, the discriminator has evolved from the cGAN's input concatenation <ref type="bibr" target="#b28">[29]</ref>, to concatenating the class information with a hidden layer <ref type="bibr" target="#b37">[39]</ref>, and lastly, to take the form of the projection discriminator <ref type="bibr" target="#b30">[31]</ref>. In the latter approach, the inner product of the embedding of the conditional information and a feature extracted from the hidden layers of the discriminator are summed with the output of the discriminator to produce the final prediction. Each step of the conditional discriminator evolution improved the results over the naive concatenation at its input <ref type="bibr" target="#b30">[31]</ref>. On all aforementioned methods the discriminator produces, nonetheless, a scalar output for the whole image.</p><p>We aim to design a discriminator for label to image generation that combines the aforementioned attributes. On the one hand, it should preserve the ability of PatchGAN to discriminate on high-frequencies. On the other hand, we want to enforce the semantic information guidance on the discriminator's decision. If the pixels of the whole image shared semantic class, the projection discriminator would be easily extended to PatchGAN. In contrast, our case is characterized by fine-grained per pixel semantics: each output patch encompasses a variety of classes and different compositions of them.</p><p>Our proposed SESAME discriminator is comprised by two independent streams that handle the RGB and Semantic Labels inputs. As <ref type="figure">Fig. 3</ref> depicts, the two streams have identical architectures. Before the information is merged a Sum Global Pooling operation is applied to the output of the Semantic Stream. The output of the semantic stream is used to scale each output coming from the RGB stream. The resulted feature map is passed as input to a last 3 × 3 convolutional layer, which produces the final output. The process can be written as follows:</p><formula xml:id="formula_2">D(I, Sem) = Conv 3×3 ( D RGB (I out ) · (1 + channels D sem (Sem)) ),<label>(3)</label></formula><p>where the D RGB is the output of the RGB stream and D sem of the semantic stream before the Global Sum Pooling. We also integrate the changes made to PatchGAN by Pix2PixHD <ref type="bibr" target="#b45">[47]</ref> and SPADE <ref type="bibr" target="#b34">[35]</ref>. We use a multiscale discrimination scheme with squared patches and two different edge-sizes of 70 and 140 pixels, in order to provide also discrimination at a coarser level and Spectral Normalization. The input to the semantic stream is the same for both fake and real images discrimination, so we only need to calculate D sem once. Moreover, it makes sense to apply the Feature Matching Loss only to the Feature Maps produced by the RGB stream.</p><p>Training Losses. We train the Generator in an adversarial manner using the following losses: Perceptual Loss <ref type="bibr" target="#b16">[17]</ref>, Feature Matching Loss <ref type="bibr" target="#b39">[41]</ref>, and Hinge Loss <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b29">30]</ref> as the Adversarial Loss. Early experiments with Style Loss <ref type="bibr" target="#b16">[17]</ref> did not improve the results. Accordingly:</p><formula xml:id="formula_3">L G = λ percept · L perc + λ f eat · L F M − E z∼p(z) [D k (I out , M, M sem ))],<label>(4)</label></formula><p>Where each λ represents the relative importance of each loss component.</p><p>For the discriminator at each scale, the Hinge Loss takes the following form: <ref type="formula">(5)</ref> which is then combined to form the full discrimination loss,</p><formula xml:id="formula_4">L D k = E z∼q data (x) [min(0, −1 + D k (I real , M, M sem ))]+ E z∼p(z) [min(1, −1 − D k (I real , M, M sem ))],</formula><formula xml:id="formula_5">L D = L D1 + L D2 .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In Section 3 we described how the SESAME Generator can be used to semantic edit images for addition, manipulation, and removal, and how we designed the SESAME discriminator to tackle both image editing and generation. To elucidate the merits of our approach, we conducted a series of different experiments: <ref type="figure">Fig. 4</ref>. Visual results of addition on Cityscapes: (a) input image (b) edited semantics (c) SESAME (d)Hong et al . <ref type="bibr" target="#b11">[12]</ref>. Note that Hong et al . <ref type="bibr" target="#b11">[12]</ref> require the whole semantics while we use only the semantics of the box -In order to quantify the performance of our network we follow the data preparation and evaluation steps of Hong et al . <ref type="bibr" target="#b11">[12]</ref>, for generating and removing objects based on a given semantic layout.</p><formula xml:id="formula_6">(a) (b) (d) (c) (a) (b) (d) (c)</formula><p>-We train our model to permit free form semantic input from users to manipulate scenes and show qualitative results. -We train our SESAME discriminator along with SPADE Generator for Label to Image Generation.</p><p>Implementation Details. For training we are using the Two Time-Scale Update Rule <ref type="bibr" target="#b10">[11]</ref> to determine the scale between the learning rate of the generator and the discriminators, with lr gen = 0.0001 and lr disc = 0.0004. We train for 200 epochs. After 100 we start to linearly decay the learning rates to 0. For our generator losses we multiplied the Feature Matching Loss and Perceptual loss by a factor of 10 before adding them to the adversarial loss. We use the Adam optimizer <ref type="bibr" target="#b20">[21]</ref> with coefficient values of b 1 = 0 and b 2 = 0.999, similar to <ref type="bibr" target="#b34">[35]</ref>.</p><p>Datasets. In line with the literature we conduct experiments on:</p><p>-Cityscapes <ref type="bibr" target="#b7">[8]</ref>. The dataset contains 3,000 street-level view images of 50 different cities in Europe for the training set and 500 images for the validation set. The images are accompanied by fine-grained information of the per-pixel semantics and instance segmentation with original resolution of the images is 2048×1024 pixels. For addition and removal, we downsample to 1024×512 pixels before patches of 256 × 256 pixels are extracted. Following Hong et al . <ref type="bibr" target="#b11">[12]</ref>, we choose 10 of the 30 available semantics classes as foreground objects, e.g. pedestians, cars, bicycles, etc.. For generation we resize the image to 512 × 256 pixels.  <ref type="bibr" target="#b11">[12]</ref>. In this setting we use the full semantic information to guide the editing -ADE20K [60,61] ADE20K has over 20,000 images together with their detailed semantics for 150 different semantic classes. In addition, 2,000 more images are offered for validation. The whole dataset is used for the generation task. For manipulation, following Hong et al . <ref type="bibr" target="#b11">[12]</ref>, we experiment on a subset of the ADE20K dataset comprised of bedroom scenes. Similarly to Cityscapes, 31 objects are chosen as foreground objects. In total we consider 49 semantic categories for training and evaluation. -Flickr-Landscapes Datasets <ref type="bibr" target="#b34">[35]</ref> Similar to SPADE <ref type="bibr" target="#b34">[35]</ref>, we first scrapped 200,000 images from flickr with only landscape constraint. As our main purpose is to show image editing over significant areas within a landscape, we use a DeepLab-v2 <ref type="bibr" target="#b5">[6]</ref> network trained on COCO-Stuff in order to extract images that contain at least 80% pixels of clouds, mountains, water, grass, etc. After post-processing, our curated dataset consists of 7367 training and 500 validation images with their corresponding segmentation for 17 different semantic classes.</p><formula xml:id="formula_7">(a) (b) (d) (c) (a) (b) (d) (c)</formula><p>Data Preprocessing. Free-form semantic editing is not trivial to achieve. The model can easily overfit on mask shapes used during training. In order to train our free-form semantic editing experiments, we randomly draw a box mask in conjunction with random strokes [55] with 70% chance, otherwise we drop all the pixels belonging to a semantic class of the training image. For layout driven editing, we extend the data pre-processing scheme introduced by Hong et al . <ref type="bibr" target="#b11">[12]</ref>. A rectangular area is removed from the input image and we try to inpaint it using the semantic labels. To train the addition operation, they extract the boundary boxes based on the instances of the foreground classes. For the removal subtask, they randomly choose and remove blocks to train the network to inpaint background classes. While this makes sense for a  <ref type="figure">Fig. 6</ref>. Free-form image manipulation. The user can select a semantic brush and paint over the image to adjust as they see fit dataset like ADE20k where the foreground objects can be found anywhere in the pictures, in Cityscapes the foreground objects placement follow certain distributions <ref type="bibr" target="#b22">[23]</ref>. Thus, we only extract a randomly chosen rectangular area if it contains at least a pixel of ground, road, sidewalk and parking.</p><p>Quantitative Results. For measuring the performance of our network, we combine the evaluation approach of previous methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>. To assess the perceptual quality of our synthesis we use the Frechet Inception Distance (FID) <ref type="bibr" target="#b10">[11]</ref>. We compare the mean Intersection over Union (mIoU), and the pixel-accuracy loss between the ground truth semantic layout and the inferred one. We infer the semantic labels of the generated images using pretrained semantic segmentation networks [53,54,60,61]. In order to maintain consistency, we chose the same object for validating all our experiments. Additionally, in our comparison with Hong et al . <ref type="bibr" target="#b11">[12]</ref>, we compute the Structural Similarity Index (SSIM) <ref type="bibr" target="#b48">[50]</ref> between each I real , I out pair, taking into account only the generated pixels. Naturally, as in the editing task only a small percentage of image pixels are changed we expect better results than those in the Generation experiments, but also better methods yield larger performance gains when tackling the latter.</p><p>Our Baselines. For our semantic image editing baseline we are using the work of Hong et al . <ref type="bibr" target="#b11">[12]</ref>. They introduced a hierarchical model to tackle the task of image editing. On the first stage, they inpaint the semantic classes of an image with a missing region. Then they combine the predicted output with the ground truth and after concatenating the real image with the missing pixels, they use their second stage model to fill the image. Similar to their work, we focus on the mask to image generation task and compare our model against their image generator trained on the ground truth labels. Their approach consists of an encoder and a decoder. The encoder has two input streams where the image and the semantics are processed separately and are then fused based on the mask of the object location. The result of the fusion is then passed to an image decoder which produces the end result. The generator is trained in conjunction with a PatchGAN discriminator. We use different architectures and largely decrease the number of parameters for the generator and have a larger discriminator as shown in <ref type="table">Table 4</ref>. However, during inference time only the generator is used. Reduced number of parameters for the generator is clearly beneficial during execution. For Image Generation we compare against SPADE <ref type="bibr" target="#b34">[35]</ref> and CC-FPSE <ref type="bibr" target="#b25">[26]</ref>.</p><p>Addition and Removal of Objects To demonstrate the ability of our network to perform well both on the addition and the removal part we compare on both tasks separately. The computed metrics for these cases can be found in <ref type="table">Tables 1  and 2</ref>, respectively.</p><p>In the visual results we can observe that objects look sharper and their features are more distinctive. Furthermore, as Figures 4 and 5 illustrate, our method generates different patterns for different clothes, and cars in which the windows are not mixed with the rest of the car. Besides our better numerical results, our user study further illustrates the superiority of our approach. In the case of removal, artifacts of the BBox are commonly left in picture by the method of Hong et al . <ref type="bibr" target="#b11">[12]</ref>, whilst in our case this effect is difficult to notice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Labels to Image Generation</head><p>The SESAME Discriminator is designed to tackle the shortcomings of the naive concatenation of an image and its semantics label when generating images. We measure the performance on Labels to Image Generation against SPADE <ref type="bibr" target="#b34">[35]</ref> using the same generator and against CC-FPSE of Liu et al . <ref type="bibr" target="#b25">[26]</ref> . Please refer to SM for the differences in our approaches. The results for Cityscapes and ADE20k datasets can be found on <ref type="table">Table 4</ref> and <ref type="figure" target="#fig_2">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Free-Form Semantic Image Editing</head><p>The user selects a brush of a semantic class and paints over the image. The pixels that are painted over are removed from the image and SESAME is filling the gaps based on the painted semantic guidance. Examples of hand painted masks and corresponding results can be seen on <ref type="figure">Fig. 11</ref>. Additionally, the context is very important for the label we want to add: a patch of grass cannot be drawn in the middle of the sky. More results can be found in the supplementary materials.</p><p>Ablation Study SESAME incorporates a Generation/Discrimination pair able to edit a scene by only considering the Semantics of regions in the image that the user seeks to edit. In order to showcase the benefits of our approach we ablate the performance of our architecture by varying (a) the generator architecture, (b) the discriminator architecture for both image manipulation and generation and (c) the available semantics only for manipulation, by utilizing either the Full semantic layout or the semantics of the rectangular region we want to edit, which we refer to as BBox Semantics. As we observe in <ref type="table">Table 1</ref> and <ref type="table">Table 4</ref> in almost all cases using the SESAME discriminator improves the performance compared to the commonly used Patch-GAN. Our generator is producing better visual quality results(mIoU, FID) but is lagging in fidelity(SSIM) when compared with the one from Hong et al . <ref type="bibr" target="#b11">[12]</ref>. The partial BBox semantics improve the performance in the case of Cityscapes dataset but full semantics work better for ADE20k. We observe the same effect when using full semantics for SESAME.</p><p>In another series of experiments, we substituted our Semantics merging operation. Instead of applying Sum Global Pooling, we experiment with 1) concatenating the two streams of information and 2) calculating their element-wise product resulted in lower FID score compared to the proposed approach, 11.96 and 12.02 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Study</head><p>We employed Amazon Mechanical Turk 4 for the two experiments of our user study. For each of them we took 100 samples from our validation set and asked 20 Turkers: Which among the images looks more photorealistic?</p><p>The first experiment presented the Turkers with three options: our method with access to only the BBox information and both ours and Hong et al . <ref type="bibr" target="#b11">[12]</ref>  <ref type="table">Table 5</ref>. User Study Results: Which image is the most photorealistic? The first study invited the users to choose between Hong et al . <ref type="bibr" target="#b11">[12]</ref>with full semantics information and ours with full and bbox semantics, respectively. The second study invited to choose between the results produced by our SESAME and the PatchGAN discriminator, for different availabilities of semantics model using the Full Semantics. As shown in <ref type="table">Table 5</ref> the results of our SESAME approach were clearly preferred by the users over the results of our baseline. Moreover, in agreement with our quantitative analysis, the proposed scaling scheme in our discriminator benefits from less irrelevant semantic information.</p><p>Another group of settings compared the results when the PatchGAN is used instead of our SESAME discriminator. The results consistently show that the independent processing of the semantic information leads to better perceptual quality of the results; they are picked more often by the human subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we introduce SESAME a novel method for semantic image editing covering the complete spectrum of adding, manipulating, and erasing operations. Our generator is capable of manipulating an image by only conditioning on the semantics of the regions the user seeks to edit, namely without requiring the information about the full layout. Our discriminator processes the semantic and image information in separate streams and overcomes the limitations of the concatenating approach inherent in PatchGAN. SESAME produces state-of-theart results on the tasks of (a) semantic image manipulation and (b) layout to image generation and permits the user to edit an image by intuitively painting over it. As a future research direction, we plan to extend this work on image generation conditioned on other types of information, e.g. scene graphs could also benefit from our two-stream discriminator. We refer to supplementary material for more details and to OpenSESAME for the code and the pretrained models.</p><p>52. Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X.: Attngan: Fine-grained text to image generation with attentional generative adversarial networks. arXiv preprint arXiv:1711.10485 <ref type="formula" target="#formula_0">(2017)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>Model Architectures The detailed architectures of the SESAME generator and discriminator are depicted in <ref type="table">Tables 6 and 7</ref> respectively. Please note that for the discriminator we use this architecture twice, once for each scale.</p><p>To further justify the architectural choices of our generator we compare against Pix2PixHD++ <ref type="bibr" target="#b45">[47]</ref>, with SPADE layers on the decoder part. We use both our SESAME discriminator and the commonly used PatchGAN.</p><p>The performance assessment is reported in <ref type="table" target="#tab_3">Table 8</ref>. The use of our discriminator over PatchGAN improved the results in almost all cases. However, we observe that while our generator-discriminator combination performs the best, the second combination is without any of our networks. We argue that our proposed method works better together as the large receptive field provided by the dilated convolutions in our generator synergizes well with the highly focused gradient flow coming from our discriminator. <ref type="table">Table 6</ref>. SESAME generator architecture. We depict the number of Filters, the Kernel size, the Stride and the Dilation factor Replacement of objects Apart from adding and removing objects, SESAME can also be used to replace an instance of an object, given that we know its class and its outline. SESAME can be utilized in this manner for dataset augmentation. We conduct experiments on replacing objects in street scenes of Cityscapes and we ablate on the usage of our SESAME discriminator against the Patch-GAN. In <ref type="table" target="#tab_4">Table 9</ref> we measure the FID score of the image results, the SSIM of the generated regions and we also devoted a part of our user study, described in <ref type="table">Table 7</ref>. SESAME discriminator architecture per scale, We depict the number of Filters, the Kernel size, the Stride and the Dilation factor  Section 4 of the main paper, to test which of the two discriminators produces the most photo-realistic results. Visual results can be found in <ref type="figure">Figure 8</ref>. Visual Results We show more edited and generated images produced by our method:</p><p>- <ref type="figure">Figure 8</ref> contains visual results of our ablation analysis on various access levels of semantics and different discriminators. - <ref type="figure">Figure 9</ref> contains visual results for removing objects under different configurations.</p><p>- <ref type="figure">Figure 10</ref> shows results for editing ADE20k-Bedroom scenes[61,60].</p><p>- <ref type="figure">Figure 11</ref> showcases examples of free-from semantic editing.</p><p>-On <ref type="figure">Figures 12 and 13</ref> we can observe layout to image generation results for Cityscapes and ADE20k.</p><p>Label to Image Generation: Comparison with CC-FPSE: Similarly to SESAME, Liu et al . <ref type="bibr" target="#b25">[26]</ref> developed an approach to tackle image generation conditioned on semantic layouts. They propose a generator architecture that learns to predict convolutional kernel weights conditioned on the semantic input. Moreover, they propose a feature pyramid semantics-embedding (FPSE) discriminator using an Encoder-Decoder architecture. Each upsampling layer outputs two per-patch score maps, one trying to measure the realness and one to gauge the semantic matching with the labels; the later is derived after a patch-wise inner product operation with the down-sampled semantic embeddings. Their FPSE discriminator, while it also addresses the shortcomings of previous models, follows a different approach to our SESAME discriminator. Although they similarly aim to short-circuit the guidance of the semantic labels to the discrimination, they choose to do so by embedding the patch with a 1 × 1 convolution and down-sampling via average pooling the semantic layout to match the size of their image processing pipeline. As we explained in the main paper, the receptive field of a patch may contain a multitude of different semantic classes with a variety of compositions. Trivially down-sampling the semantic label can result into loss of information. Thus, we proposed a dedicated part of the discriminator to derive a meaningful representation for such an intricate semantic patch. Moreover, our model independently processes the semantic information for each scale. We experimented with their discriminator architecture along our SESAME generator but we were unable to achieve similar performance to our full method: SSIM 0.371, FID: 12.49, mIoU 64.24% and accuracy 85.93%. <ref type="figure">Fig. 8</ref>. Visual results for addition on Cityscapes <ref type="bibr" target="#b7">[8]</ref>. We ablate on: (a) using the Full context, using only the labels of the rectangular areas to be edited and only replacing an object given its mask (b) generations due to training with the PatchGAN Discriminator and SESAME. Finally, we show the results produced by the method of Hong et al . <ref type="bibr" target="#b11">[12]</ref>, using the full semantics information masked labels generated input masked labels generated <ref type="figure">Fig. 9</ref>. Visual results for removal on Cityscapes <ref type="bibr" target="#b7">[8]</ref>. We ablate on: (a) using the Full context and using only the labels of the rectangular areas to be edited (b) generations due to training with the PatchGAN Discriminator and SESAME. In the first row we show the results produced by the method of Hong et al . <ref type="bibr" target="#b11">[12]</ref>, using the full semantics information Label Masked Hong et al. SESAME <ref type="figure">Fig. 10</ref>. Visual results for editing Bedroom scenes from ADE20K dataset. Here we are using the Full semantic information to alter the gray area Mountain Sea Trees Ground Clouds Sky Grass <ref type="figure">Fig. 11</ref>. Examples of free form editing using semantic brushes. Note that snow is a different semantic label from mountain and we can observe when draw with the mountain brush the model learned to differentiate this from snow. The model fails to correctly depict a semantic concept out of context or with an unexpected shape real label SPADE SESAME <ref type="figure">Fig. 12</ref>. Cityscapes <ref type="bibr" target="#b7">[8]</ref>: Visual results for image generation conditioned on Semantic Labels. We showcase the results using the generator from SPADE <ref type="bibr" target="#b34">[35]</ref> with the PatchGAN Discriminator(SPADE) and ours(SESAME) real label SPADE SESAME real label SPADE SESAME <ref type="figure">Fig. 13</ref>. Ade20k[61,60]: Visual results for image generation conditioned on Semantic Labels. We showcase the results using the generator from SPADE <ref type="bibr" target="#b34">[35]</ref> with the Patch-GAN Discriminator(SPADE) and ours(SESAME)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5 .</head><label>5</label><figDesc>Visual results of addition on ADE20k: (a) input image (b) edited semantics (c) SESAME (d) Hong et al .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 .</head><label>7</label><figDesc>Label to Image Generation results. For each triplet of images we are showing the semantic layout input (left), generation using PatchGAN (center) and SESAME (right) discriminator on top of SPADE generator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>53. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In: Proceedings of the International Conference on Learning Representations (ICLR) (2016) 54. Yu, F., Koltun, V., Funkhouser, T.: Dilated residual networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017) 55. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Free-form image inpainting with gated convolution. arXiv preprint arXiv:1806.03589 (2018) 56. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Generative image inpainting with contextual attention. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5505-5514 (2018) 57. Zhang, H., Goodfellow, I.J., Metaxas, D.N., Odena, A.: Self-attention generative adversarial networks. arXiv preprint arXiv:1805.08318 (2018) 58. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., Metaxas, D.: Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In: Proceedings of the International Conference Computer Vision (ICCV) (2017) 59. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., Metaxas, D.N.: Stack-gan++: Realistic image synthesis with stacked generative adversarial networks. arXiv preprint arXiv:1710.10916 (2017) 60. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Semantic understanding of scenes through the ade20k dataset. arXiv preprint arXiv:1608.05442 (2016) 61. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>256, K = 3, S = 1, D = 2 SPADE LeakyReLU(0.02) ResBlock F = 256, K = 3, S = 1, D = 1 SPADE LeakyReLU(0.02) Nearest Neighbour Upsampling ×2 --ResBlock F = 128, K = 3, S = 1, D = 1 SPADE LeakyReLU(0.02) Nearest Neighbour Upsampling ×2 --ResBlock F = 64, K = 3, S = 1, D = 1 SPADE LeakyReLU(0.02) ConvBlock F = 3, K = 3, S = 1, D = 1 -TanH</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>128, K = 4, S = 2, D = 1 SpectralInstance LeakyReLU(0.02) ConvBlock F = 256, K = 4, S = 2, D = 1 SpectralInstance LeakyReLU(0.02) ConvBlock F = 512, K = 4, S = 1, D = 1 SpectralInstance LeakyReLU(0.02) Semantics Stream ConvBlock F = 64, K = 4, S = 2, D = 1 -LeakyReLU(0.02) ConvBlock F = 128, K = 4, S = 2, D = 1 SpectralInstance LeakyReLU(0.02) ConvBlock F = 256, K = 4, S = 2, D = 1 SpectralInstance LeakyReLU(0.02) ConvBlock F = 512, K = 4, S = 1, D = 1 SpectralInstance LeakyReLU(0.02) Sum Global Pooling --Common Head ConvBlock F = 1, K = 4, S = 1, D = 1 --</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Addition Results for Cityscapes and ADE20k dataset. We ablate on the Generator and Discriminator architecture as well as the semantic availability. For the SSIM, accuracy and mIoU higher is better, while for FID, lower is better. Removal results for Cityscapes and ADE20k datasets. For the SSIM, accuracy and mIoU higher is better, while for FID, lower is better.</figDesc><table><row><cell></cell><cell></cell><cell>Cityscapes</cell><cell>ADE20k</cell></row><row><cell>Generator</cell><cell>Disc</cell><cell cols="2">Labels SSIM accu mIoU FID SSIM accu mIoU FID</cell></row><row><cell cols="4">Hong et al . [12] PatchGAN Full 0.377 83.8 60.7 12.11 0.205 92.2 34.6 28.48</cell></row><row><cell cols="4">Hong et al . [12] PatchGAN BBox 0.379 85.9 63.4 11.50 0.183 92.7 35.3 28.36</cell></row><row><cell cols="4">Hong et al . [12] SESAME Full 0.396 86.0 64.0 11.76 0.192 91.3 34.1 28.55</cell></row><row><cell>SESAME</cell><cell cols="3">PatchGAN BBox 0.375 86.0 64.5 11.13 0.193 92.2 35.7 28.16</cell></row><row><cell>SESAME</cell><cell cols="3">SESAME BBox 0.410 86.0 65.3 11.03 0.209 93.3 37.1 26.66</cell></row><row><cell></cell><cell></cell><cell>Cityscapes</cell><cell>ADE20k</cell></row><row><cell>Method</cell><cell cols="3">SSIM↑ accu↑ mIoU↑ FID↓ SSIM↑ accu↑ mIoU↑ FID↓</cell></row><row><cell cols="4">Hong et al . [12] 0.584 83.9% 65.3% 10.34 0.456 91.7% 40.0% 24.98</cell></row><row><cell>SESAME</cell><cell cols="3">0.797 85.0% 67.6% 7.43 0.491 92.3% 41.6% 23.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison in number of parameters</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Parameters in millions</cell></row><row><cell></cell><cell>Method</cell><cell cols="2">Generator Discriminator</cell></row><row><cell></cell><cell cols="2">Hong et al . [12] 190m</cell><cell>5.6m</cell></row><row><cell></cell><cell>SESAME</cell><cell>20.5m</cell><cell>11.1m</cell></row><row><cell cols="4">Table 4. Layout to image generation results. For mIoU and accu, higher is better,</cell></row><row><cell cols="2">while for FID, lower is better</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Cityscapes</cell><cell>ADE20k</cell></row><row><cell cols="4">Generator Discriminator mIoU accu FID mIoU accu FID</cell></row><row><cell cols="4">Pix2PixHD PatchGAN 58.3 81.4 95.0 20.3 69.2 81.8</cell></row><row><cell cols="2">Pix2PixHD SESAME</cell><cell cols="2">59.6 81.1 55.4 49.0 85.5 36.8</cell></row><row><cell>SPADE</cell><cell cols="3">PatchGAN 62.3 81.9 71.8 38.5 79.9 33.9</cell></row><row><cell>CC</cell><cell>FPSE</cell><cell cols="2">65.5 82.3 54.3 43.7 82.9 31.7</cell></row><row><cell>SPADE</cell><cell cols="3">SESAME 66.0 82.5 54.2 49.0 85.5 31.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 8 .</head><label>8</label><figDesc>We ablate on the semantics availability, the generator architecture and the discriminator architecture for adding objects on street scenes from Cityscapes w.r.t.</figDesc><table><row><cell>FID score (lower is better)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Discriminator</cell><cell></cell></row><row><cell></cell><cell cols="2">Full semantics</cell><cell cols="2">BBox semantics</cell></row><row><cell>Generator</cell><cell cols="4">PatchGAN SESAME PatchGAN SESAME</cell></row><row><cell>SPADEPix2PixHD</cell><cell>11.92</cell><cell>12.74</cell><cell>12.32</cell><cell>12.66</cell></row><row><cell>SESAME</cell><cell>11.95</cell><cell>11.64</cell><cell>11.13</cell><cell>11.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 9 .</head><label>9</label><figDesc>Object Replacement -Cityscapes. We show the performance of our SESAME model with our discriminator and the PatchGAN<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref> discriminator as well as the percentage of user answers to the question: Which image looks more photo-realistic?</figDesc><table><row><cell cols="3">Discriminator SSIM↑ FID↓ User Preference(%)</cell></row><row><cell>PatchGAN</cell><cell>0.390 10.63</cell><cell>45.03</cell></row><row><cell>SESAME</cell><cell>0.433 9.3</cell><cell>54.97</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.mturk.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was partly supported by CSEM, ETH Zurich Fund (OK) and by Huawei, Amazon AWS and Nvidia GPU grants. We are grateful to Despoina Paschalidou, Siavash Bigdeli and Danda Pani Paudel for fruitful discussions. We also thank Gene Kogan for providing guidance on how to prepare the Flickr Landscapes Dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04340</idno>
		<title level="m">Data augmentation generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Specifying object attributes and relations in interactive scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Computer Vision (ICCV)</title>
		<meeting>the International Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic photo manipulation with a generative image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Seeing what a gan cannot generate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Computer Vision (ICCV)</title>
		<meeting>the International Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cognex: Visionpro vidi: Deep learning-based software for industrial image analysis</title>
		<idno>accessed: 2019-03-05</idno>
		<ptr target="https://www.cognex.com/products/machine-vision/vision-software/visionpro-vidi" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gan-based synthetic medical image augmentation for increased cnn performance in liver lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frid-Adar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Diamant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., Weinberger, K.Q.</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning hierarchical semantic image manipulation through structured representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2713" to="2723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Computer vision for autonomous vehicles: Problems, datasets and state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05519</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sc-fegan: Face editing generative adversarial network with user&apos;s sketch and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Computer Vision (ICCV)</title>
		<meeting>the International Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11922</idno>
		<title level="m">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context-aware synthesis and placement of object instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cesa-Bianchi, N., Garnett, R.</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10393" to="10403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<title level="m">Geometric gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to predict layout-to-image conditional convolutions for semantic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritsche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Joon</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference Computer Vision (ICCV), Advances in Image Manipulation Workshop</title>
		<meeting>the International Conference Computer Vision (ICCV), Advances in Image Manipulation Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Aim 2019 challenge on real-world image super-resolution: Methods and results</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mode seeking generative adversarial networks for diverse image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1429" to="1437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Z</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ebrahimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00212</idno>
		<title level="m">Edgeconnect: Generative image inpainting with adversarial edge learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional image synthesis with auxiliary classifier gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pedestrian-synthesisgan: Generating pedestrian data in real scene and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02047</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faceshop: Deep sketch-based face image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Portenier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szabó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bigdeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Program</surname></persName>
		</author>
		<idno>2.10.18</idno>
	</analytic>
	<monogr>
		<title level="j">The GIMP Development Team</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generative adversarial text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Smit: Stochastic multi-label image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Computer Vision (ICCV), Workshops</title>
		<meeting>the International Conference Computer Vision (ICCV), Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D.D., Sugiyama, M., Luxburg, U.V., Guyon, I., Garnett, R.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scribbler: Controlling deep image synthesis with sketch and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Computer Vision (ICCV)</title>
		<meeting>the International Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adversarial scene editing: Automatic object removal from weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Graumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7716" to="7726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Not using the car to see the sidewalk: Quantifying and controlling the effects of context in classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical implicit models and likelihoodfree variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5523" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semi-supervised pedestrian instance synthesis and detection with mutual reinforcement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azzam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Computer Vision (ICCV)</title>
		<meeting>the International Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
