<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ensemble of MRR and NDCG models for Visual Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
							<email>idansc@cs.technion.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Technion NetApp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ensemble of MRR and NDCG models for Visual Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single humanderived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., 'yeah' and 'yes'). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as 'I don't know. Crafting a model that excels on both MRR and NDCG metrics is challenging (Murahari et al., 2020). Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a twostep non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won the recent Visual Dialog 2020 challenge. Source code is available at https: //github.com/idansc/mrr-ndcg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction <ref type="bibr">Das et al. (2017)</ref> introduced the task of Visual Dialog, which requires an agent to converse about visual input. Evaluating visually aware conversation should examine both linguistic properties and visual reasoning. Analysis of generative metrics for dialog often shows no correlation with human judgments <ref type="bibr">(Liu et al., 2016)</ref>. Hence, to evaluate the correctness of the candidate answers, a retrieval approach is preferred. Two metrics are standard, Question: what is the nightstand made of ? 1. can't tell it's covered in cloth 2. it appears to be a large red pillow that may be leather 5. not sure 6. can't tell 7. some kind of metal , it's out of focus 8. Wood ... 99. 0 100.I can't see a baggage cart MRR NDCG <ref type="figure">Figure 1</ref>: A visual dialog interaction. The question asks, "what is the nightstand made of ?". We show our final ranking, created by the ensemble of an MRR/NDCG models' rankings. The MRR/NDCG models are trained to optimize the MRR/NDCG metric. The MRR metric measures the number of retrievals to retrieve the human-derived answer. Hence, the MRR model favors human-like and detailed answers. On the other hand, the NDCG metric measures the rank of all the correct candidates based on dense annotation, which are often general and uncertain. Our ensemble approach seeks a minimal candidate set that is likely to contain the human-derived answer. The remaining candidates are ranked according to the NDCG model. MRR and NDCG. The MRR metric focuses on a single human-derived ground-truth answer. Despite preferring the more human-like answer, the metric ignores many correct candidate answers. Differently, the NDCG considers the rank of all the correct answers. The metric relies on dense annotation, where three annotators were asked to mark all the correct candidate answers. However, the candidate answers are generated plausible answers. The analysis shows that the NDCG metric favors uncertain, generally correct answers, such as "not sure" <ref type="bibr">(Murahari et al., 2020;</ref><ref type="bibr">Qi et al., 2020)</ref>.</p><p>Prior work in visual dialog focused on a single metric. Ideally, an AI agent should answer humanlike and detailed reply (the MRR metric) and be able to validate the correctness of any answer (the NDCG metric). However, crafting a model that excels in both metrics is challenging <ref type="bibr">(Murahari et al., 2020)</ref>. To this end, we propose principals to ensemble the rankings of strong MRR and NDCG models. Our approach is to find a minimal set that is likely arXiv:2104.07511v1 [cs.AI] 15 Apr 2021 to hold the human-derived answer. This permits ranking the rest of the candidates according to the NDCG model. Our approach won the recent Visual Dialog 2020 challenge and achieved strong performance on both the MRR and the NDCG metrics simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual conversation evaluation: Early attempts to marry conversation with vision used street scene images, and binary questions <ref type="bibr">(Geman et al., 2015)</ref>. While binary answers are easy to verify, such an approach is limiting for an AI agent. On the other hand, analysis of generative metrics for dialog often show no correlation with human judgements <ref type="bibr">(Liu et al., 2016)</ref>. Intuitively, metrics like BLEU-scores rely on corresponding words with the ground-truth answer and often miss synonyms or the subjective nature. More importantly, generative metrics are geared toward textual assessment rather than visual reasoning, which results in models mainly relying on textual cues <ref type="bibr">(Schwartz et al., 2019a)</ref>. <ref type="bibr">Malinowski and Fritz (2014)</ref> suggest Wu-Palmer similarity metric that calculates similarity based on the depth of two words based on the WordNet taxonomy <ref type="bibr">(Miller, 1995)</ref>. A different approach suggested in the VQA dataset focus only on brief, mostly 1-word answers <ref type="bibr">(Antol et al., 2015)</ref>. In this setup, the task turns into popular answers classification, alleviating many text-generation challenges. Notably, VQA requires 3 out of 10 annotators to agree on the answer, which is robust to inter-person variation. Still, accuracy ignores the reasoning process. <ref type="bibr">Hudson and Manning (2019)</ref> propose GQA, which extends the accuracy metric and uses a scene graph for both question generation and evaluation. Following, <ref type="bibr">Das et al. (2017)</ref> propose the VisDial dataset for the visual dialog task, which formulates multiple image-language interactions via a dialog. <ref type="bibr">Concurrently, de Vries et al. (2017)</ref> propose Guess-What, a goal-driven dialog dataset for object identification. Different from VQA and goal-driven dialogs, the VisDial answers are detailed and more human-like. For instance, in <ref type="figure">Fig. 1</ref>, the answer is "Can't tell...cloth", while a VQA answer would be "cloth". Therefore, metrics that require exact matching are no longer suitable. Instead, each question is accompanied with 100 candidate answers. Consequently, the metric has been shifted from accuracy to retrieval-based metrics, e.g., MRR and NDCG. Prior works focus on optimizing a single metric <ref type="bibr">(Guo et al., 2019;</ref><ref type="bibr">Jiang et al., 2020;</ref><ref type="bibr">Hu et al., 2017;</ref><ref type="bibr">Gan et al., 2019)</ref>. Differently, <ref type="bibr">Murahari et al. (2020)</ref> attempt to optimize both metrics with a joint loss. Still, a dedicated single metric model is superior. Instead, we propose principals to ensemble two dedicated models, one for NDCG and one for MRR. Our approach allows most of the MRR and NDCG to be preserved simultaneously. Visual dialog models: Various approaches were proposed to solve the Visual Dialog task. Most of them focus on dialog history reasoning per interaction. <ref type="bibr">Serban et al. (2017)</ref>   <ref type="bibr">(FGA)</ref>, that lets all entities (e.g., question-words, image-regions, answer-candidate, and caption-words) interact to infer an attention map for each modality. An ensemble of five FGA models achieves the state-of-the-art MRR performance. However, FGA optimizes using the sparse annotations, i.e., the human-derived answer. <ref type="bibr">Murahari et al. (2020)</ref> recently propose Large-Scale(LS) model, which pre-trains on related vision-language datasets, e.g., Conceptual Captions and Visual Question Answering <ref type="bibr">(Sharma et al., 2018;</ref><ref type="bibr">Antol et al., 2015)</ref>. <ref type="bibr">Concurrently, Wang et al. (2020)</ref> leverage the pretrained BERT language models. Both methods mentioned above finetune using the dense annotation (i.e., human assessment of all the candidates), resulting in a substantial improvement on the NDCG metric. Importantly, Murahari et al. find that finetuning a model for NDCG hurts MRR performance. This work demonstrates that re-ranking MRR model (e.g., FGA) and NDCG model (e.g., LS) with simple principles keeps most MRR and NDCG performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Two-step Ranks Ensemble</head><p>The MRR metric depends on a single humanderived answer. Hence, given that this answer is ranked highly, the remaining candidates can be ranked according to the NDCG model. In the following, we describe two steps: (i) the MRR step responsible for preserving the human-derived rank high, and (ii) the NDCG step responsible for ranking the remaining candidates based on the NDCG model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>We are given a set of dialog questions</p><formula xml:id="formula_0">{(q, C q ) i } d i=1 ,</formula><p>where d is the dataset size, q is a dialog question, and C q = {c q,j } 100 j=1 are the corresponding candidates. The MRR metric, i.e., the inverse harmonic mean of rank, is defined as:</p><formula xml:id="formula_1">MRR = 1 d d i=1 1 r mrr ,<label>(1)</label></formula><p>where r mrr is the rank of the human response. The DCG, i.e., discounted cumulative gain over the K correct answers, is defined as:</p><formula xml:id="formula_2">DCG K = K i=1 s i log 2 (i + 1) ,<label>(2)</label></formula><p>where s i is a binary score, representing the fraction of annotators that marked the candidate at position as correct. We normalize by the ideal DCG K score (IDCG K ), i.e., NDCG K = DCG K IDCG K . We denote the set of MRR models as M = {M 1 , . . . , M nm } where n m is the number of MRR models. Each MRR model is built by altering the initial conditions. We denote the NDCG model as N . We define an operator T(M, n, q) that returns the model M 's top n responses given a question q. Next, we describe the MRR step that aims to keep the MRR score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MRR step</head><p>The purpose of the MRR step is to find a minimal candidate set C MRR,q that is likely to contain the human-derived answer given a question q. We build this set as a union of three sets, as follows:</p><formula xml:id="formula_3">C MRR,q = T q ∪ N q ∪ H q ,<label>(3)</label></formula><p>where T q is a set of first ranked candidates according to MRR models, N q is a set of high ranked candidates by both MRR and NDCG models, H q is a set of high-certainty candidates agreed by all the MRR models. All sets are conditioned by the question q. In the following, we formally define those sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">High-certainty answers</head><p>One of the most significant signals to be the humanderived answer is being a top MRR-model's answer. However, in many subjective questions, the MRR model is not certain. We found that in those cases, the top answers often varies between different MRR models. Thus, to verify the top candidate's certainty, we require an agreement of MRRmodels. Let q be a dialog question, we define the high-certainty set as follows: <ref type="formula">(4)</ref> where ρ h ∈ R is an hyperparameter. Intuitively, a low ρ h results in higher certainty. We Next, we add the MRR-models' answer at first retrieval.</p><formula xml:id="formula_4">H q = {c| (∀M ∈ M; c ∈ T(M, ρ h , q))},</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Top answers</head><p>The MRR metric prioritizes the first-ranked answer (See Eq. <ref type="formula" target="#formula_1">(1)</ref>). This property suits the nature of dialog models that reply with a single response. Consequently, we keep the first responses of the MRR models. Let q be a dialog question, the top-answers set is defined as:</p><formula xml:id="formula_5">T q = {c| (∃M ∈ M; c ∈ T(M, ρ t , q))},<label>(5)</label></formula><p>where ρ t ∈ R is an hyperparameter. We note that ρ t should be low to maintain candidates' certainty.</p><p>In the next step, we consider top NDCG candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">NDCG-agreement answers</head><p>When the NDCG model and the MRR model agree that a candidate is likely to be correct, it implies that both the NDCG and MRR metrics gain by ranking this candidate high. Thus, we want to rank it high. We note that the MRR set is ranked first, so we include these candidates in the MRR set. Let q be a dialog question, the ndcg-agreement set is defined as:</p><formula xml:id="formula_6">N q = {c|∃M ∈ M; c ∈ T(N, ρ n n , q) ∩ T(M, ρ n m , q)},<label>(6)</label></formula><p>where ρ n n , ρ n m ∈ R are hyperparameters that indicate relevancy to NDCG and MRR, respectively. I.e., as ρ nn increases, we may include more relevant candidates according to the NDCG model.</p><p>Up until this stage we have built a minimal set C MRR,q that is likely to hold the human-derived answer. In the following we describe how we rank this set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">MRR ranking</head><p>Let r M i ,c,q denote the rank according to M i ∈ M of candidate c for a question q. We compute the MRR rank of candidate c ∈ C MRR,q via geometric mean:</p><formula xml:id="formula_7">r MRR,c,q = nm i=1 r M i ,c,q .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NDCG step</head><p>In this step, we rank the remaining candidates C NDCG,q = C q \ C MRR,q . We assume the correct MRR answer is in C MRR . Thus, we rank the remaining candidates, according to the NDCG model via geometric mean: r NDCG,c,q = (r N,c,q ) p · r M,c,q ,  where M ∈ M is the most accurate MRR model, and p ∈ R is a calibration hyperparameter which controls the trade-off between MRR and NDCG.</p><p>To conclude, let q be a dialog question and C q the corresponding candidates. We first find C MRR,q , and rank the set according to r MRR,c,q . We then rank the remaining candidates, according to r NDCG,c,q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We show our results on the VisDial v1.0 dataset, where 123,287 images are used for training, 2,000 images for validation, and 8,000 images for testing <ref type="bibr">(Das et al., 2017)</ref>. Each image is associated with ten questions, and each question has 100 corresponding answer candidates. We use two MRR models (i.e., n m = 2), FGA (Schwartz et al., 2019b) and an ensemble of LS (Murahari et al., 2020) with FGA. We use LS(CE) as the NDCG model. We set ρ h = 3, ρ t = 1, ρ n n = 5, ρ n m = 10, and p = 3. These hyperparameters were chosen by evaluating the validation set using a random search. Comparison to state-of-the-art: In Tab. 1 we compare our method to naïve ensembles and previous baselines. We first ensemble the LS's output with the FGA's output. By combining them, we achieve the new MRR state-of-the-art (71.24% vs. 69.37%). Next, we attempt to build an ensemble of MRR and NDCG models. We do so by adding the MRR ensemble scores (denoted by S M ) to LS(CE) scores (denoted by S N ), as follows:</p><formula xml:id="formula_8">α · S M + (1 − α)S N ,</formula><p>where α ∈ R calibrates the trade-off between MRR and NDCG performance. We show in <ref type="figure" target="#fig_0">Fig. 2</ref> an analysis of different α values on the validation set. In Tab. 1, we report results for α = 0.8. Our two-step method outperforms the MRR (70.41% vs. 68.78%) and NDCG (72.16% vs. 69.22%) metrics, despite lacking the output scores and only requiring rankings.</p><p>We also compare our approach to previous baselines. Most methods use the sparse annotations, i.e., the human-derived answer, while MRealBDAI and LS(CE) finetune using the dense annotations. Finetuning with the dense annotations tremendously boosts the NDCG performance but loses MRR performance. The MRR performance decline can be attributed to NDCG being biased toward uncertain answers. We also note that LS leverages largescale image-text corpora. LS(CE+NSP) optimizes both the dense and sparse annotations but still suffers from a performance drop on MRR (63.87% vs. 67.50%) with low NDCG gain (68.08% vs. 63.87%). Unlike the method mentioned above, our method re-rank the candidates based on two distinct models, with two distinct steps, to keep the humanderived answer high. In doing so, we achieve the good MRR performance (70.41%), yet notably with limited NDCG drop (72.25% vs. 75.35%). This property comes in handy in the recent Visual Dialog Challenge, where the winners were picked based on both the NDCG and MRR evaluation metrics. Our method performs well on both metrics simultaneously and won the challenge. Ablation analysis: The MRR candidate set consists of different subsets. In Tab. 2 we show the influence of each of subset independently on the retrieval metrics. Further, omitting a subset harms the performance, i.e. each component is essential to preserve both the MRR and NDCG metrics. We also report the average size of the MRR-candidate set, and the validation performance of the MRR model (i.e., 5xFGA) and the NDCG model (i.e., LS(CE)). In addition we provide the results of the MRR ensemble, and the naïve NDCG and MRR ensemble for α = 0.8. In <ref type="figure" target="#fig_1">Fig. 3</ref>, we examine how the NDCG and MRR metrics are affected by modifying one hyperparameter while maintaining the others. On the first figure from the left, we alter ρ c . The higher ρ c , we require higher agreement between the MRR models, resulting in higher certainty for elements in the MRR set. Because the MRR models are responsible for the MRR set ranking, an MRR set that is too large hurts the NDCG metric. For the same reason, in the second image from the left, increasing ρ t , significantly harms the NDCG performance. In the third figure from the left, we show that considering more candidates that both NDCG and MRR models agree upon (i.e., increasing ρ n n ) helps both metrics' performance. However, adding too many candidates harms the NDCG metric. In the fourth image from the left, we show that the performance remains stable when ρ n m is larger than three. Last, on the fifth image from the left, we show the effect of changing p, which calibrates the trade-off between MRR and NDCG during the NDCG ranking step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We describe a non-parametric method to ensemble the candidate ranks of two strong MRR and NDCG models into a single ranking that excels on both NDCG and MRR. Intuitively, we use the MRRmodel for non-ambiguous questions with certain answers. The dense-annotations cue is more applicable in ambiguous questions than the sparse annotations. Thus, in the case of low certainty, our method relies almost entirely on the NDCG model. We hope the proposed principles can guide the community towards a parametric model that can employ answers' semantics to measure certainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Qualitative Analysis</head><p>In the following, we show 200 randomly picked visual dialog samples from test-std. For each sample, we provide the ranked MRR candidate set and the next 10 NDCG candidates. The analysis reveals the answers' ambiguity and that the MRR candidate set mostly consists of the human-derived response. To further detail our approach, we illustrate the candidates within each MRR candidate subset by highlighting the candidates with different colors. We colorize the high-certainty candidates (H) with orange, the NDCG-agreement candidates (N ) with purple, and the top-answers subset (T ) with red. Note, if a candidate belongs to more than one set, we sketch the colors in the following order: orange→red→purple. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Performance of a naïve score ensemble of MRR model and NDCG model on the VisDialv1.0 val set. We calibrate the importance of each model with a scalar α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>MRR and NDCG scores for different hyperparameter values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance on VisDial v1.0 test-std. (*) denotes the use of external knowledge. † indeicates ensemble model and ‡ indicates fine-tuning on the dense annotations. Shown are the MRR and NDCG metric, the mean rank of the human-derived answer, and the recall at a certain number of retrievals.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">MRR↑ R@1↑ R@5↑ R@10↑ Mean↓ NDCG↑</cell></row><row><cell cols="4">NMN (Kottur et al., 2018)</cell><cell></cell><cell cols="2">58.80 44.15 76.88</cell><cell>86.88</cell><cell>4.81</cell><cell>58.10</cell></row><row><cell cols="3">NN (Zheng et al., 2019)</cell><cell></cell><cell></cell><cell cols="2">61.37 47.33 77.98</cell><cell>87.83</cell><cell>4.57</cell><cell>52.82</cell></row><row><cell cols="4">CorefNMN (Kottur et al., 2018)</cell><cell></cell><cell cols="2">61.50 47.55 78.10</cell><cell>88.80</cell><cell>4.40</cell><cell>54.70</cell></row><row><cell cols="3">RvA (Niu et al., 2019)</cell><cell></cell><cell></cell><cell cols="2">63.03 49.03 80.40</cell><cell>89.83</cell><cell>4.18</cell><cell>55.59</cell></row><row><cell cols="4">HACAN (Yang et al., 2019)</cell><cell></cell><cell cols="2">64.22 50.88 80.63</cell><cell>89.45</cell><cell>4.20</cell><cell>57.17</cell></row><row><cell cols="4">MReal -BDAI ‡ (Qi et al., 2020)</cell><cell></cell><cell cols="2">52.62 40.03 68.85</cell><cell>79.15</cell><cell>6.76</cell><cell>74.02</cell></row><row><cell cols="4">ReDAN (Gan et al., 2019)</cell><cell></cell><cell cols="2">53.13 41.38 66.07</cell><cell>74.50</cell><cell>8.91</cell><cell>61.86</cell></row><row><cell cols="4">ReDAN+  † (Gan et al., 2019)</cell><cell></cell><cell cols="2">53.74 42.45 64.68</cell><cell>75.68</cell><cell>6.64</cell><cell>64.47</cell></row><row><cell cols="4">DualVD (Jiang et al., 2020)</cell><cell></cell><cell cols="2">63.23 49.25 80.23</cell><cell>89.70</cell><cell>4.11</cell><cell>56.32</cell></row><row><cell cols="3">DL-61 (Guo et al., 2019)</cell><cell></cell><cell></cell><cell cols="2">62.20 47.90 80.43</cell><cell>89.95</cell><cell>4.17</cell><cell>57.32</cell></row><row><cell cols="3">DL-61  † (Guo et al., 2019)</cell><cell></cell><cell></cell><cell cols="2">63.42 49.30 80.77</cell><cell>90.68</cell><cell>3.97</cell><cell>57.88</cell></row><row><cell cols="3">DAN (Kang et al., 2019)</cell><cell></cell><cell></cell><cell cols="2">63.20 49.63 79.75</cell><cell>89.35</cell><cell>4.30</cell><cell>57.59</cell></row><row><cell cols="3">DAN  † (Kang et al., 2019)</cell><cell></cell><cell></cell><cell cols="2">64.92 51.28 81.60</cell><cell>90.88</cell><cell>3.92</cell><cell>59.36</cell></row><row><cell cols="4">FGA (Schwartz et al., 2019b)</cell><cell></cell><cell cols="3">63.75 49.58 80.975 88.55</cell><cell>4.51</cell><cell>52.12</cell></row><row><cell cols="4">5×FGA  † (Schwartz et al., 2019b)</cell><cell></cell><cell cols="2">69.37 55.65 86.73</cell><cell>94.05</cell><cell>3.14</cell><cell>57.29</cell></row><row><cell cols="4">LS(CE)* ‡ (Murahari et al., 2020)</cell><cell></cell><cell cols="2">50.74 37.95 64.13</cell><cell>80.00</cell><cell>6.28</cell><cell>74.47</cell></row><row><cell cols="7">LS(CE+NSP)* ‡ (Murahari et al., 2020) 63.87 50.78 79.53</cell><cell>89.60</cell><cell>4.28</cell><cell>68.08</cell></row><row><cell cols="4">LS* (Murahari et al., 2020)</cell><cell></cell><cell cols="2">67.50 53.85 84.68</cell><cell>93.25</cell><cell>3.32</cell><cell>63.87</cell></row><row><cell cols="4">VD-BERT* † ‡ (Wang et al., 2020)</cell><cell></cell><cell cols="2">51.17 38.90 62.82</cell><cell>77.98</cell><cell>6.69</cell><cell>75.35</cell></row><row><cell cols="2">5xFGA + LS* †</cell><cell></cell><cell></cell><cell></cell><cell cols="2">71.24 58.28 87.55</cell><cell>94.45</cell><cell>2.96</cell><cell>64.04</cell></row><row><cell cols="4">5xFGA + LS + LS(CE)* † ‡</cell><cell></cell><cell cols="2">68.78 55.72 85.02</cell><cell>93.55</cell><cell>3.26</cell><cell>69.22</cell></row><row><cell cols="2">Two-Step* † ‡</cell><cell></cell><cell></cell><cell></cell><cell cols="2">70.41 58.18 83.85</cell><cell>90.83</cell><cell>3.66</cell><cell>72.16</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Visual Dialog Challenge 2020 Leaderboard</cell><cell></cell><cell></cell></row><row><cell>LS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">68.79 55.20 86.15</cell><cell>93.88</cell><cell>3.12</cell><cell>63.34</cell></row><row><cell cols="2">VD-BERT</cell><cell></cell><cell></cell><cell></cell><cell cols="2">51,84 39.91 63.45</cell><cell>78.56</cell><cell>6.57</cell><cell>75.92</cell></row><row><cell cols="3">MReaL Lab (3 rd Place)</cell><cell></cell><cell></cell><cell cols="2">64.12 50.81 80.03</cell><cell>90.92</cell><cell>3.83</cell><cell>75.70</cell></row><row><cell cols="3">SES-100M (2 nd Place)</cell><cell></cell><cell></cell><cell cols="2">63.84 55.62 72.20</cell><cell>83.70</cell><cell>5.84</cell><cell>75.86</cell></row><row><cell cols="2">Ours (1 st Place)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">70.42 58.59 82.85</cell><cell>88.84</cell><cell>3.96</cell><cell>73.35</cell></row><row><cell>H</cell><cell>T</cell><cell>N</cell><cell>MRR↑</cell><cell>R@1↑</cell><cell>R@5↑</cell><cell>R@10↑</cell><cell>Mean↓</cell><cell>NDCG↑</cell><cell>|CMRR|</cell></row><row><cell></cell><cell></cell><cell></cell><cell>70.83</cell><cell>58.87</cell><cell>84.32</cell><cell>90.67</cell><cell>3.67</cell><cell>74.32</cell><cell>2.34</cell></row><row><cell></cell><cell></cell><cell></cell><cell>68.63</cell><cell>59.12</cell><cell>79.08</cell><cell>88.53</cell><cell>4.15</cell><cell>74.31</cell><cell>1.12</cell></row><row><cell></cell><cell></cell><cell></cell><cell>61.75</cell><cell>51.74</cell><cell>70.35</cell><cell>85.88</cell><cell>4.94</cell><cell>74.69</cell><cell>3.97</cell></row><row><cell></cell><cell></cell><cell></cell><cell>69.21</cell><cell>59.17</cell><cell>78.68</cell><cell>88.53</cell><cell>4.11</cell><cell>74.33</cell><cell>4.27</cell></row><row><cell></cell><cell></cell><cell></cell><cell>71.15</cell><cell>59.11</cell><cell>84.38</cell><cell>90.67</cell><cell>3.65</cell><cell>73.29</cell><cell>4.87</cell></row><row><cell></cell><cell></cell><cell></cell><cell>71.06</cell><cell>59.15</cell><cell>84.49</cell><cell>90.78</cell><cell>3.64</cell><cell>72.98</cell><cell>2.39</cell></row><row><cell></cell><cell></cell><cell></cell><cell>71.26</cell><cell>59.18</cell><cell>84.62</cell><cell>90.78</cell><cell>3.62</cell><cell>73.23</cell><cell>4.91</cell></row><row><cell cols="2">LS(CE)</cell><cell></cell><cell>52.21</cell><cell>39.92</cell><cell>65.04</cell><cell>80.62</cell><cell>6.16</cell><cell>75.24</cell><cell>-</cell></row><row><cell>LS</cell><cell></cell><cell></cell><cell>69.00</cell><cell>55.80</cell><cell>85.36</cell><cell>93.13</cell><cell>3.35</cell><cell>64.89</cell><cell>-</cell></row><row><cell cols="2">5xFGA</cell><cell></cell><cell>69.38</cell><cell>56.17</cell><cell>86.15</cell><cell>92.95</cell><cell>3.32</cell><cell>58.68</cell><cell>-</cell></row><row><cell cols="2">5xFGA + LS</cell><cell></cell><cell>72.25</cell><cell>59.20</cell><cell>88.55</cell><cell>94.52</cell><cell>2.84</cell><cell>65.34</cell><cell>-</cell></row><row><cell cols="3">5xFGA + LS + LS(CE)</cell><cell>69.14</cell><cell>56.79</cell><cell>84.24</cell><cell>92.37</cell><cell>3.43</cell><cell>73.78</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>MRR candidates set ablation analysis. Performance reported on the VisDialv1.0 val set.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Yftah Ziser, Itai Gat, Alexander Schwing and Tamir Hazan for useful discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering. In ICCV.</p><p>Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In CVPR.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Question:anything else interesting about the photo ?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">not visibly , but i &apos;d say probably so 10. yes , it is Question:are there any farmers near them ? MRR candidate set 1</title>
		<imprint/>
	</monogr>
	<note>there are no people visible 2. no , there are n&apos;t any people in the image 3. no , there is not likely Question:what is in background ? MRR candidate set</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
