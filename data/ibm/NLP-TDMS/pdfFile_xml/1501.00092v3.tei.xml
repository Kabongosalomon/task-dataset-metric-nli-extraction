<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Super-Resolution Using Deep Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Kaiming</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
						</author>
						<title level="a" type="main">Image Super-Resolution Using Deep Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Super-resolution</term>
					<term>deep convolutional neural networks</term>
					<term>sparse coding !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve tradeoffs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality. 1. The implementation is available at http://mmlab.ie.cuhk.edu.hk/ projects/SRCNN.html. 2. Numerical evaluations by using different metrics such as the Peak Signal-to-Noise Ratio (PSNR), structure similarity index (SSIM) [43], multi-scale SSIM [44], information fidelity criterion [38], when the ground truth images are available. arXiv:1501.00092v3 [cs.CV] 31 Jul 2015 SRCNN SC Bicubic Bicubic / 24.04 dB SC / 25.58 dB SRCNN / 27.95 dB Original / PSNR Bicubic / 24.04 dB SC / 25.58 dB SRCNN / 27.95 dB Original / PSNR SRCNN SC Bicubic Bicubic / 24.04 dB SC / 25.58 dB SRCNN / 27.95 dB Original / PSNR Number of backprops Average test PSNR (dB)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Single image super-resolution (SR) <ref type="bibr" target="#b19">[20]</ref>, which aims at recovering a high-resolution image from a single lowresolution image, is a classical problem in computer vision. This problem is inherently ill-posed since a multiplicity of solutions exist for any given low-resolution pixel. In other words, it is an underdetermined inverse problem, of which solution is not unique. Such a problem is typically mitigated by constraining the solution space by strong prior information. To learn the prior, recent state-of-the-art methods mostly adopt the example-based <ref type="bibr" target="#b45">[46]</ref> strategy. These methods either exploit internal similarities of the same image <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b46">[47]</ref>, or learn mapping functions from external low-and high-resolution exemplar pairs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. The external example-based methods can be formulated for generic image super-resolution, or can be designed to suit domain specific tasks, i.e., face hallucination <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b49">[50]</ref>, according to the training samples provided.</p><p>The sparse-coding-based method <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> is one of the representative external example-based SR methods. This method involves several steps in its solution pipeline. First, overlapping patches are densely cropped from the input image and pre-processed (e.g.,subtracting mean and normalization). These patches are then encoded by a low-resolution dictionary. The sparse coefficients are passed into a high-resolution dictionary for reconstructing high-resolution patches. The overlapping re-constructed patches are aggregated (e.g., by weighted averaging) to produce the final output. This pipeline is shared by most external example-based methods, which pay particular attention to learning and optimizing the dictionaries <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> or building efficient mapping functions <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b46">[47]</ref>. However, the rest of the steps in the pipeline have been rarely optimized or considered in an unified optimization framework.</p><p>In this paper, we show that the aforementioned pipeline is equivalent to a deep convolutional neural network <ref type="bibr" target="#b26">[27]</ref> (more details in Section 3.2). Motivated by this fact, we consider a convolutional neural network that directly learns an end-to-end mapping between low-and high-resolution images. Our method differs fundamentally from existing external example-based approaches, in that ours does not explicitly learn the dictionaries <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> or manifolds <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref> for modeling the patch space. These are implicitly achieved via hidden layers. Furthermore, the patch extraction and aggregation are also formulated as convolutional layers, so are involved in the optimization. In our method, the entire SR pipeline is fully obtained through learning, with little pre/postprocessing.</p><p>We name the proposed model Super-Resolution Convolutional Neural Network (SRCNN) <ref type="bibr" target="#b0">1</ref> . The proposed SRCNN has several appealing properties. First, its structure is intentionally designed with simplicity in mind, and yet provides superior accuracy 2 compared with state-of-the-art example-based methods. <ref type="figure">Figure 1</ref> shows a comparison on an example. Second, with moderate  <ref type="figure">Fig. 1</ref>. The proposed Super-Resolution Convolutional Neural Network (SRCNN) surpasses the bicubic baseline with just a few training iterations, and outperforms the sparse-coding-based method (SC) <ref type="bibr" target="#b49">[50]</ref> with moderate training. The performance may be further improved with more training iterations. More details are provided in Section 4.4.1 (the Set5 dataset with an upscaling factor 3). The proposed method provides visually appealing reconstructed image.</p><p>numbers of filters and layers, our method achieves fast speed for practical on-line usage even on a CPU. Our method is faster than a number of example-based methods, because it is fully feed-forward and does not need to solve any optimization problem on usage. Third, experiments show that the restoration quality of the network can be further improved when (i) larger and more diverse datasets are available, and/or (ii) a larger and deeper model is used. On the contrary, larger datasets/models can present challenges for existing example-based methods. Furthermore, the proposed network can cope with three channels of color images simultaneously to achieve improved super-resolution performance.</p><p>Overall, the contributions of this study are mainly in three aspects: 1) We present a fully convolutional neural network for image super-resolution. The network directly learns an end-to-end mapping between lowand high-resolution images, with little pre/postprocessing beyond the optimization. 2) We establish a relationship between our deep-learning-based SR method and the traditional sparse-coding-based SR methods. This relationship provides a guidance for the design of the network structure.</p><p>3) We demonstrate that deep learning is useful in the classical computer vision problem of superresolution, and can achieve good quality and speed. A preliminary version of this work was presented earlier <ref type="bibr" target="#b10">[11]</ref>. The present work adds to the initial version in significant ways. Firstly, we improve the SRCNN by introducing larger filter size in the non-linear mapping layer, and explore deeper structures by adding nonlinear mapping layers. Secondly, we extend the SRCNN to process three color channels (either in YCbCr or RGB color space) simultaneously. Experimentally, we demonstrate that performance can be improved in comparison to the single-channel network. Thirdly, considerable new analyses and intuitive explanations are added to the initial results. We also extend the original experiments from Set5 <ref type="bibr" target="#b1">[2]</ref> and Set14 <ref type="bibr" target="#b50">[51]</ref> test images to BSD200 <ref type="bibr" target="#b31">[32]</ref> (200 test images). In addition, we compare with a number of recently published methods and confirm that our model still outperforms existing approaches using different evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Super-Resolution</head><p>According to the image priors, single-image super resolution algorithms can be categorized into four typesprediction models, edge based methods, image statistical methods and patch based (or example-based) methods. These methods have been thoroughly investigated and evaluated in Yang et al.'s work <ref type="bibr" target="#b45">[46]</ref>. Among them, the example-based methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b46">[47]</ref> achieve the state-of-the-art performance.</p><p>The internal example-based methods exploit the selfsimilarity property and generate exemplar patches from the input image. It is first proposed in Glasner's work <ref type="bibr" target="#b15">[16]</ref>, and several improved variants <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b44">[45]</ref> are proposed to accelerate the implementation. The external example-based methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> learn a mapping between low/highresolution patches from external datasets. These studies vary on how to learn a compact dictionary or manifold space to relate low/high-resolution patches, and on how representation schemes can be conducted in such spaces. In the pioneer work of Freeman et al. <ref type="bibr" target="#b13">[14]</ref>, the dictionaries are directly presented as low/high-resolution patch pairs, and the nearest neighbour (NN) of the input patch is found in the low-resolution space, with its corresponding high-resolution patch used for reconstruction. Chang et al. <ref type="bibr" target="#b3">[4]</ref> introduce a manifold embedding technique as an alternative to the NN strategy. In Yang et al.'s work <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, the above NN correspondence advances to a more sophisticated sparse coding formulation. Other mapping functions such as kernel regression <ref type="bibr" target="#b24">[25]</ref>, simple function <ref type="bibr" target="#b46">[47]</ref>, random forest <ref type="bibr" target="#b36">[37]</ref> and anchored neighborhood regression <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> are proposed to further improve the mapping accuracy and speed. The sparsecoding-based method and its several improvements <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b47">[48]</ref> are among the state-of-the-art SR methods nowadays. In these methods, the patches are the focus of the optimization; the patch extraction and aggregation steps are considered as pre/post-processing and handled separately.</p><p>The majority of SR algorithms <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> focus on gray-scale or single-channel image super-resolution. For color images, the aforementioned methods first transform the problem to a different color space (YCbCr or YUV), and SR is applied only on the luminance channel. There are also works attempting to super-resolve all channels simultaneously. For example, Kim and Kwon <ref type="bibr" target="#b24">[25]</ref> and Dai et al. <ref type="bibr" target="#b6">[7]</ref> apply their model to each RGB channel and combined them to produce the final results. However, none of them has analyzed the SR performance of different channels, and the necessity of recovering all three channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolutional Neural Networks</head><p>Convolutional neural networks (CNN) date back decades <ref type="bibr" target="#b26">[27]</ref> and deep CNNs have recently shown an explosive popularity partially due to its success in image classification <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>. They have also been successfully applied to other computer vision fields, such as object detection <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b51">[52]</ref>, face recognition <ref type="bibr" target="#b38">[39]</ref>, and pedestrian detection <ref type="bibr" target="#b34">[35]</ref>. Several factors are of central importance in this progress: (i) the efficient training implementation on modern powerful GPUs <ref type="bibr" target="#b25">[26]</ref>, (ii) the proposal of the Rectified Linear Unit (ReLU) <ref type="bibr" target="#b32">[33]</ref> which makes convergence much faster while still presents good quality <ref type="bibr" target="#b25">[26]</ref>, and (iii) the easy access to an abundance of data (like ImageNet <ref type="bibr" target="#b8">[9]</ref>) for training larger models. Our method also benefits from these progresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Learning for Image Restoration</head><p>There have been a few studies of using deep learning techniques for image restoration. The multi-layer perceptron (MLP), whose all layers are fully-connected (in contrast to convolutional), is applied for natural image denoising <ref type="bibr" target="#b2">[3]</ref> and post-deblurring denoising <ref type="bibr" target="#b35">[36]</ref>. More closely related to our work, the convolutional neural network is applied for natural image denoising <ref type="bibr" target="#b21">[22]</ref> and removing noisy patterns (dirt/rain) <ref type="bibr" target="#b11">[12]</ref>. These restoration problems are more or less denoising-driven. Cui et al. <ref type="bibr" target="#b4">[5]</ref> propose to embed auto-encoder networks in their superresolution pipeline under the notion internal examplebased approach <ref type="bibr" target="#b15">[16]</ref>. The deep model is not specifically designed to be an end-to-end solution, since each layer of the cascade requires independent optimization of the self-similarity search process and the auto-encoder. On the contrary, the proposed SRCNN optimizes an end-toend mapping. Further, the SRCNN is faster at speed. It is not only a quantitatively superior method, but also a practically useful one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONVOLUTIONAL NEURAL NETWORKS FOR SUPER-RESOLUTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>Consider a single low-resolution image, we first upscale it to the desired size using bicubic interpolation, which is the only pre-processing we perform 3 . Let us denote the interpolated image as Y. Our goal is to recover from Y an image F (Y) that is as similar as possible to the ground truth high-resolution image X. For the ease of presentation, we still call Y a "low-resolution" image, although it has the same size as X. We wish to learn a mapping F , which conceptually consists of three operations:</p><p>1) Patch extraction and representation: this operation extracts (overlapping) patches from the lowresolution image Y and represents each patch as a high-dimensional vector. These vectors comprise a set of feature maps, of which the number equals to the dimensionality of the vectors. 2) Non-linear mapping: this operation nonlinearly maps each high-dimensional vector onto another high-dimensional vector. Each mapped vector is conceptually the representation of a high-resolution patch. These vectors comprise another set of feature maps. 3) Reconstruction: this operation aggregates the above high-resolution patch-wise representations to generate the final high-resolution image. This image is expected to be similar to the ground truth X. We will show that all these operations form a convolutional neural network. An overview of the network is depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. Next we detail our definition of each operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Patch extraction and representation</head><p>A popular strategy in image restoration (e.g., <ref type="bibr" target="#b0">[1]</ref>) is to densely extract patches and then represent them by a set of pre-trained bases such as PCA, DCT, Haar, etc. This is equivalent to convolving the image by a set of filters, each of which is a basis. In our formulation, we involve the optimization of these bases into the optimization of the network. Formally, our first layer is expressed as an operation F 1 :</p><formula xml:id="formula_0">F 1 (Y) = max (0, W 1 * Y + B 1 ) ,<label>(1)</label></formula><p>where W 1 and B 1 represent the filters and biases respectively, and ' * ' denotes the convolution operation.</p><p>Here, W 1 corresponds to n 1 filters of support c × f 1 × f 1 , where c is the number of channels in the input image, f 1 is the spatial size of a filter. Intuitively, W 1 applies n 1 convolutions on the image, and each convolution has <ref type="bibr" target="#b2">3</ref>. Bicubic interpolation is also a convolutional operation, so it can be formulated as a convolutional layer. However, the output size of this layer is larger than the input size, so there is a fractional stride. To take advantage of the popular well-optimized implementations such as cuda-convnet <ref type="bibr" target="#b25">[26]</ref>, we exclude this "layer" from learning. </p><formula xml:id="formula_1">a kernel size c × f 1 × f 1 .</formula><p>The output is composed of n 1 feature maps. B 1 is an n 1 -dimensional vector, whose each element is associated with a filter. We apply the Rectified Linear Unit (ReLU, max(0, x)) [33] on the filter responses 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Non-linear mapping</head><p>The first layer extracts an n 1 -dimensional feature for each patch. In the second operation, we map each of these n 1 -dimensional vectors into an n 2 -dimensional one. This is equivalent to applying n 2 filters which have a trivial spatial support 1 × 1. This interpretation is only valid for 1 × 1 filters. But it is easy to generalize to larger filters like 3 × 3 or 5 × 5. In that case, the non-linear mapping is not on a patch of the input image; instead, it is on a 3 × 3 or 5 × 5 "patch" of the feature map. The operation of the second layer is:</p><formula xml:id="formula_2">F 2 (Y) = max (0, W 2 * F 1 (Y) + B 2 ) .<label>(2)</label></formula><p>Here W 2 contains n 2 filters of size n 1 × f 2 × f 2 , and B 2 is n 2 -dimensional. Each of the output n 2 -dimensional vectors is conceptually a representation of a high-resolution patch that will be used for reconstruction. It is possible to add more convolutional layers to increase the non-linearity. But this can increase the complexity of the model (n 2 × f 2 × f 2 × n 2 parameters for one layer), and thus demands more training time. We will explore deeper structures by introducing additional non-linear mapping layers in Section 4.3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Reconstruction</head><p>In the traditional methods, the predicted overlapping high-resolution patches are often averaged to produce the final full image. The averaging can be considered as a pre-defined filter on a set of feature maps (where each position is the "flattened" vector form of a highresolution patch). Motivated by this, we define a convolutional layer to produce the final high-resolution image:</p><formula xml:id="formula_3">F (Y) = W 3 * F 2 (Y) + B 3 .<label>(3)</label></formula><p>4. The ReLU can be equivalently considered as a part of the second operation (Non-linear mapping), and the first operation (Patch extraction and representation) becomes purely linear convolution.</p><p>Here W 3 corresponds to c filters of a size n 2 × f 3 × f 3 , and B 3 is a c-dimensional vector.</p><p>If the representations of the high-resolution patches are in the image domain (i.e.,we can simply reshape each representation to form the patch), we expect that the filters act like an averaging filter; if the representations of the high-resolution patches are in some other domains (e.g.,coefficients in terms of some bases), we expect that W 3 behaves like first projecting the coefficients onto the image domain and then averaging. In either way, W 3 is a set of linear filters.</p><p>Interestingly, although the above three operations are motivated by different intuitions, they all lead to the same form as a convolutional layer. We put all three operations together and form a convolutional neural network ( <ref type="figure" target="#fig_0">Figure 2</ref>). In this model, all the filtering weights and biases are to be optimized. Despite the succinctness of the overall structure, our SRCNN model is carefully developed by drawing extensive experience resulted from significant progresses in super-resolution <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. We detail the relationship in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relationship to Sparse-Coding-Based Methods</head><p>We show that the sparse-coding-based SR methods <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> can be viewed as a convolutional neural network. <ref type="figure" target="#fig_1">Figure 3</ref> shows an illustration.</p><p>In the sparse-coding-based methods, let us consider that an f 1 × f 1 low-resolution patch is extracted from the input image. Then the sparse coding solver, like Feature-Sign <ref type="bibr" target="#b28">[29]</ref>, will first project the patch onto a (lowresolution) dictionary. If the dictionary size is n 1 , this is equivalent to applying n 1 linear filters (f 1 × f 1 ) on the input image (the mean subtraction is also a linear operation so can be absorbed). This is illustrated as the left part of <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>The sparse coding solver will then iteratively process the n 1 coefficients. The outputs of this solver are n 2 coefficients, and usually n 2 = n 1 in the case of sparse coding. These n 2 coefficients are the representation of the high-resolution patch. In this sense, the sparse coding solver behaves as a special case of a non-linear mapping operator, whose spatial support is 1 × 1. See the middle part of <ref type="figure" target="#fig_1">Figure 3</ref>. However, the sparse coding solver is not feed-forward, i.e.,it is an iterative algorithm. On the contrary, our non-linear operator is fully feed-forward and can be computed efficiently. If we set f 2 = 1, then our non-linear operator can be considered as a pixel-wise fully-connected layer. It is worth noting that "the sparse coding solver" in SRCNN refers to the first two layers, but not just the second layer or the activation function (ReLU). Thus the nonlinear operation in SRCNN is also well optimized through the learning process. The above n 2 coefficients (after sparse coding) are then projected onto another (high-resolution) dictionary to produce a high-resolution patch. The overlapping high-resolution patches are then averaged. As discussed above, this is equivalent to linear convolutions on the n 2 feature maps. If the high-resolution patches used for reconstruction are of size f 3 × f 3 , then the linear filters have an equivalent spatial support of size f 3 × f 3 . See the right part of <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>The above discussion shows that the sparse-codingbased SR method can be viewed as a kind of convolutional neural network (with a different non-linear mapping). But not all operations have been considered in the optimization in the sparse-coding-based SR methods. On the contrary, in our convolutional neural network, the low-resolution dictionary, high-resolution dictionary, non-linear mapping, together with mean subtraction and averaging, are all involved in the filters to be optimized. So our method optimizes an end-to-end mapping that consists of all operations.</p><p>The above analogy can also help us to design hyperparameters. For example, we can set the filter size of the last layer to be smaller than that of the first layer, and thus we rely more on the central part of the highresolution patch (to the extreme, if f 3 = 1, we are using the center pixel with no averaging). We can also set n 2 &lt; n 1 because it is expected to be sparser. A typical and basic setting is f 1 = 9, f 2 = 1, f 3 = 5, n 1 = 64, and n 2 = 32 (we evaluate more settings in the experiment section). On the whole, the estimation of a high resolution pixel utilizes the information of (9 + 5 − 1) 2 = 169 pixels. Clearly, the information exploited for reconstruction is comparatively larger than that used in existing external example-based approaches, e.g., using (5+5−1) 2 = 81 pixels 5 <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b49">[50]</ref>. This is one of the reasons why the SRCNN gives superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Learning the end-to-end mapping function F requires the estimation of network parameters Θ =</p><formula xml:id="formula_4">{W 1 , W 2 , W 3 , B 1 , B 2 , B 3 }.</formula><p>This is achieved through minimizing the loss between the reconstructed images F (Y; Θ) and the corresponding ground truth highresolution images X. Given a set of high-resolution images {X i } and their corresponding low-resolution images {Y i }, we use Mean Squared Error (MSE) as the loss function:</p><formula xml:id="formula_5">L(Θ) = 1 n n i=1 ||F (Y i ; Θ) − X i || 2 ,<label>(4)</label></formula><p>where n is the number of training samples. Using MSE as the loss function favors a high PSNR. The PSNR is a widely-used metric for quantitatively evaluating image restoration quality, and is at least partially related to the perceptual quality. It is worth noticing that the convolutional neural networks do not preclude the usage of other kinds of loss functions, if only the loss functions are derivable. If a better perceptually motivated metric is given during training, it is flexible for the network to adapt to that metric. On the contrary, such a flexibility is in general difficult to achieve for traditional "handcrafted" methods. Despite that the proposed model is trained favoring a high PSNR, we still observe satisfactory performance when the model is evaluated using alternative evaluation metrics, e.g., SSIM, MSSIM (see Section 4.4.1). The loss is minimized using stochastic gradient descent with the standard backpropagation <ref type="bibr" target="#b27">[28]</ref>. In particular, the weight matrices are updated as</p><formula xml:id="formula_6">∆ i+1 = 0.9 · ∆ i − η · ∂L ∂W i , W i+1 = W i + ∆ i+1 ,<label>(5)</label></formula><p>5. The patches are overlapped with 4 pixels at each direction.</p><p>where ∈ {1, 2, 3} and i are the indices of layers and iterations, η is the learning rate, and ∂L ∂W i is the derivative. The filter weights of each layer are initialized by drawing randomly from a Gaussian distribution with zero mean and standard deviation 0.001 (and 0 for biases). The learning rate is 10 −4 for the first two layers, and 10 −5 for the last layer. We empirically find that a smaller learning rate in the last layer is important for the network to converge (similar to the denoising case <ref type="bibr" target="#b21">[22]</ref>).</p><p>In the training phase, the ground truth images {X i } are prepared as f sub ×f sub ×c-pixel sub-images randomly cropped from the training images. By "sub-images" we mean these samples are treated as small "images" rather than "patches", in the sense that "patches" are overlapping and require some averaging as post-processing but "sub-images" need not. To synthesize the low-resolution samples {Y i }, we blur a sub-image by a Gaussian kernel, sub-sample it by the upscaling factor, and upscale it by the same factor via bicubic interpolation.</p><p>To avoid border effects during training, all the convolutional layers have no padding, and the network produces a smaller output (</p><formula xml:id="formula_7">(f sub − f 1 − f 2 − f 3 + 3) 2 × c).</formula><p>The MSE loss function is evaluated only by the difference between the central pixels of X i and the network output. Although we use a fixed image size in training, the convolutional neural network can be applied on images of arbitrary sizes during testing. We implement our model using the cuda-convnet package <ref type="bibr" target="#b25">[26]</ref>. We have also tried the Caffe package <ref type="bibr" target="#b23">[24]</ref> and observed similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We first investigate the impact of using different datasets on the model performance. Next, we examine the filters learned by our approach. We then explore different architecture designs of the network, and study the relations between super-resolution performance and factors like depth, number of filters, and filter sizes. Subsequently, we compare our method with recent state-ofthe-arts both quantitatively and qualitatively. Following <ref type="bibr" target="#b41">[42]</ref>, super-resolution is only applied on the luminance channel (Y channel in YCbCr color space) in Sections 4.1-4.4, so c = 1 in the first/last layer, and performance (e.g., PSNR and SSIM) is evaluated on the Y channel. At last, we extend the network to cope with color images and evaluate the performance on different channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Data</head><p>As shown in the literature, deep learning generally benefits from big data training. For comparison, we use a relatively small training set <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b49">[50]</ref> that consists of 91 images, and a large training set that consists of 395,909 images from the ILSVRC 2013 ImageNet detection training partition. The size of training sub-images is f sub = 33. Thus the 91-image dataset can be decomposed into 24,800 sub-images, which are extracted from original images with a stride of 14. Whereas the ImageNet provides over 5 million sub-images even using a stride of 33. We use the basic network settings, i.e., f 1 = 9, f 2 = 1, f 3 = 5, n 1 = 64, and n 2 = 32. We use the Set5 <ref type="bibr" target="#b1">[2]</ref> as the validation set. We observe a similar trend even if we use the larger Set14 set <ref type="bibr" target="#b50">[51]</ref>. The upscaling factor is 3. We use the sparse-coding-based method <ref type="bibr" target="#b49">[50]</ref> as our baseline, which achieves an average PSNR value of 31.42 dB.</p><p>The test convergence curves of using different training sets are shown in <ref type="figure">Figure 4</ref>. The training time on Ima-geNet is about the same as on the 91-image dataset since the number of backpropagations is the same. As can be observed, with the same number of backpropagations (i.e.,8 × 10 8 ), the SRCNN+ImageNet achieves 32.52 dB, higher than 32.39 dB yielded by that trained on 91 images. The results positively indicate that SRCNN performance may be further boosted using a larger training set, but the effect of big data is not as impressive as that shown in high-level vision problems <ref type="bibr" target="#b25">[26]</ref>. This is mainly because that the 91 images have already captured sufficient variability of natural images. On the other hand, our SRCNN is a relatively small network (8,032 parameters), which could not overfit the 91 images (24,800 samples). Nevertheless, we adopt the ImageNet, which contains more diverse data, as the default training set in the following experiments. <ref type="figure">Figure 5</ref> shows examples of learned first-layer filters trained on the ImageNet by an upscaling factor 3. Please refer to our published implementation for upscaling factors 2 and 4. Interestingly, each learned filter has its specific functionality. For instance, the filters g and h are like Laplacian/Gaussian filters, the filters ae are like edge detectors at different directions, and the filter f is like a texture extractor. Example feature maps of different layers are shown in <ref type="figure" target="#fig_3">figure 6</ref>. Obviously, feature maps of the first layer contain different structures (e.g., edges at different directions), while that of the second layer are mainly different on intensities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learned Filters for Super-Resolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model and Performance Trade-offs</head><p>Based on the basic network settings (i.e., f 1 = 9, f 2 = 1, f 3 = 5, n 1 = 64, and n 2 = 32), we will progressively modify some of these parameters to investigate the best trade-off between performance and speed, and study the relations between performance and parameters.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Filter number</head><p>In general, the performance would improve if we increase the network width <ref type="bibr" target="#b5">6</ref> , i.e., adding more filters, at the cost of running time. Specifically, based on our network default settings of n 1 = 64 and n 2 = 32, we conduct two experiments: (i) one is with a larger network with n 1 = 128 and n 2 = 64, and (ii) the other is with a smaller network with n 1 = 32 and n 2 = 16. Similar to Section 4.1, we also train the two models on ImageNet and test on Set5 with an upscaling factor 3. The results observed at 8 × 10 8 backpropagations are shown in <ref type="table" target="#tab_2">Table 1</ref>. It is clear that superior performance could be achieved by increasing the width. However, if a fast restoration speed is desired, a small network width is preferred, which could still achieve better performance than the sparsecoding-based method (31.42 dB). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Filter size</head><p>In this section, we examine the network sensitivity to different filter sizes. In previous experiments, we set filter size f 1 = 9, f 2 = 1 and f 3 = 5, and the network could be denoted as 9-1-5. First, to be consistent with sparse-coding-based methods, we fix the filter size of the second layer to be f 2 = 1, and enlarge the filter size of other layers to f 1 = 11 and f 3 = 7 <ref type="figure" target="#fig_4">(11-1-7)</ref>. All the other <ref type="bibr" target="#b5">6</ref>. We use 'width' to term the number of filters in a layer, following <ref type="bibr" target="#b16">[17]</ref>. The term 'width' may have other meanings in the literature. settings remain the same with Section 4.1. The results with an upscaling factor 3 on Set5 are 32.57 dB, which is slightly higher than the 32.52 dB reported in Section 4.1. This indicates that a reasonably larger filter size could grasp richer structural information, which in turn lead to better results. Then we further examine networks with a larger filter size of the second layer. Specifically, we fix the filter size f 1 = 9, f 3 = 5, and enlarge the filter size of the second layer to be (i) f 2 = 3 (9-3-5) and (ii) f 2 = 5 (9-5-5). Convergence curves in <ref type="figure" target="#fig_4">Figure 7</ref> show that using a larger filter size could significantly improve the performance. Specifically, the average PSNR values achieved by 9-3-5 and 9-5-5 on Set5 with 8 × 10 8 backpropagations are 32.66 dB and 32.75 dB, respectively. The results suggest that utilizing neighborhood information in the mapping stage is beneficial.</p><p>However, the deployment speed will also decrease with a larger filter size. For example, the number of parameters of 9-1-5, 9-3-5, and 9-5-5 is 8,032, 24,416, and 57,184 respectively. The complexity of 9-5-5 is almost twice of 9-3-5, but the performance improvement is marginal. Therefore, the choice of the network scale should always be a trade-off between performance and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Number of layers</head><p>Recent study by He and Sun <ref type="bibr" target="#b16">[17]</ref> suggests that CNN could benefit from increasing the depth of network moderately. Here, we try deeper structures by adding another non-linear mapping layer, which has n 22 = 16 filters with size f 22 = 1. We conduct three controlled experiments, i.e., 9-1-1-5, 9-3-1-5, 9-5-1-5, which add an additional layer on 9-1-5, 9-3-5, and 9-5-5, respectively. The initialization scheme and learning rate of the additional layer are the same as the second layer. <ref type="figure" target="#fig_1">From  Figures 13(a)</ref>, 13(b) and 8(c), we can observe that the four-layer networks converge slower than the three-layer network. Nevertheless, given enough training time, the deeper networks will finally catch up and converge to the three-layer ones.</p><p>The effectiveness of deeper structures for super resolution is found not as apparent as that shown in image classification <ref type="bibr" target="#b16">[17]</ref>. Furthermore, we find that deeper networks do not always result in better performance. Specifically, if we add an additional layer with n 22 = 32 filters on 9-1-5 network, then the performance degrades and fails to surpass the three-layer network (see <ref type="figure">Figure 9(a)</ref>). If we go deeper by adding two non-linear mapping layers with n 22 = 32 and n 23 = 16 filters on 9-1-5, then we have to set a smaller learning rate to ensure convergence, but we still do not observe superior performance after a week of training (see <ref type="figure">Figure 9</ref>(a)). We also tried to enlarge the filter size of the additional layer to f 22 = 3, and explore two deep structures -9-3-3-5 and 9-3-3-3. However, from the convergence curves shown in <ref type="figure">Figure 9</ref>(b), these two networks do not show better results than the 9-3-1-5 network.</p><p>All these experiments indicate that it is not "the deeper the better" in this deep model for super-resolution. It may be caused by the difficulty of training. Our CNN network contains no pooling layer or full-connected layer, thus it is sensitive to the initialization parameters and learning rate. When we go deeper (e.g., 4 or 5 layers), we find it hard to set appropriate learning rates that guarantee convergence. Even it converges, the network may fall into a bad local minimum, and the learned filters are of less diversity even given enough training time. This phenomenon is also observed in <ref type="bibr" target="#b15">[16]</ref>, where improper increase of depth leads to accuracy saturation or degradation for image classification. Why "deeper is not better" is still an open question, which requires investigations to better understand gradients and training dynamics in deep architectures. Therefore, we still adopt three-layer networks in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparisons to State-of-the-Arts</head><p>In this section, we show the quantitative and qualitative results of our method in comparison to state-of-the-art methods. We adopt the model with good performancespeed trade-off: a three-layer network with f 1 = 9, f 2 = 5, f 3 = 5, n 1 = 64, and n 2 = 32 trained on the ImageNet. For each upscaling factor ∈ {2, 3, 4}, we train a specific network for that factor 7 .</p><p>Comparisons. We compare our SRCNN with the stateof-the-art SR methods:</p><p>• SC -sparse coding-based method of Yang et al. <ref type="bibr" target="#b49">[50]</ref> • NE+LLE -neighbour embedding + locally linear embedding method <ref type="bibr" target="#b3">[4]</ref> • ANR -Anchored Neighbourhood Regression method <ref type="bibr" target="#b40">[41]</ref> • A+ -Adjusted Anchored Neighbourhood Regression method <ref type="bibr" target="#b41">[42]</ref>, and</p><p>• KK -the method described in <ref type="bibr" target="#b24">[25]</ref>, which achieves the best performance among external examplebased methods, according to the comprehensive evaluation conducted in Yang et al.'s work <ref type="bibr" target="#b45">[46]</ref> The implementations are all from the publicly available codes provided by the authors, and all images are downsampled using the same bicubic kernel. Test set. The Set5 [2] (5 images), Set14 [51] (14 images) and BSD200 <ref type="bibr" target="#b31">[32]</ref> (200 images) <ref type="bibr" target="#b7">8</ref> are used to evaluate the performance of upscaling factors 2, 3, and 4. Evaluation metrics. Apart from the widely used PSNR and SSIM <ref type="bibr" target="#b42">[43]</ref> indices, we also adopt another four evaluation matrices, namely information fidelity criterion (IFC) <ref type="bibr" target="#b37">[38]</ref>, noise quality measure (NQM) <ref type="bibr" target="#b7">[8]</ref>, weighted peak signal-to-noise ratio (WPSNR) and multiscale structure similarity index (MSSSIM) <ref type="bibr" target="#b43">[44]</ref>, which obtain high correlation with the human perceptual scores as reported in <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Quantitative and qualitative evaluation</head><p>As shown in <ref type="table" target="#tab_3">Tables 2, 3</ref> and 4, the proposed SRCNN yields the highest scores in most evaluation matrices <ref type="bibr">7.</ref> In the area of denoising <ref type="bibr" target="#b2">[3]</ref>, for each noise level a specific network is trained.</p><p>8. We use the same 200 images as in <ref type="bibr" target="#b45">[46]</ref>.  in all experiments <ref type="bibr" target="#b8">9</ref> . Note that our SRCNN results are based on the checkpoint of 8 × 10 8 backpropagations. Specifically, for the upscaling factor 3, the average gains on PSNR achieved by SRCNN are 0.15 dB, 0.17 dB, and 0.13 dB, higher than the next best approach, A+ <ref type="bibr" target="#b41">[42]</ref>, on the three datasets. When we take a look at other evaluation metrics, we observe that SC, to our surprise, gets even lower scores than the bicubic interpolation on IFC and NQM. It is clear that the results of SC are more visually pleasing than that of bicubic interpolation. This indicates that these two metrics may not truthfully reveal the image quality. Thus, regardless of these two metrics, SRCNN achieves the best performance among all methods and scaling factors.</p><p>It is worth pointing out that SRCNN surpasses the bicubic baseline at the very beginning of the learning stage (see <ref type="figure">Figure 1</ref>), and with moderate training, SR-CNN outperforms existing state-of-the-art methods (see <ref type="figure">Figure 4</ref>). Yet, the performance is far from converge. We conjecture that better results can be obtained given longer training time (see <ref type="figure" target="#fig_8">Figure 10</ref>). <ref type="figure" target="#fig_3">Figures 14, 15 and 16</ref> show the super-resolution results of different approaches by an upscaling factor 3. As can be observed, the SRCNN produces much sharper edges than other approaches without any obvious artifacts across the image.</p><p>In addition, we report to another recent deep learning method for image super-resolution (DNC) of Cui et al. <ref type="bibr" target="#b4">[5]</ref>. As they employ a different blur kernel (a Gaussian filter with a standard deviation of 0.55), we train a specific network (9-5-5) using the same blur kernel as DNC for fair quantitative comparison. The upscaling factor is 3 and the training set is the 91-image dataset. From the convergence curve shown in <ref type="figure" target="#fig_10">Figure 11</ref>, we observe that our SRCNN surpasses DNC with just 2.7 × 10 7 backprops, and a larger margin can be obtained given longer training time. This also demonstrates that the end-to-end learning is superior to DNC, even if that model is already "deep". <ref type="figure" target="#fig_0">Figure 12</ref> shows the running time comparisons of several state-of-the-art methods, along with their restoration performance on Set14. All baseline methods are obtained <ref type="bibr" target="#b8">9</ref>. The PSNR value of each image can be found in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Running time</head><p>from the corresponding authors' MATLAB+MEX implementation, whereas ours are in pure C++. We profile the running time of all the algorithms using the same machine (Intel CPU 3.10 GHz and 16 GB memory). Note that the processing time of our approach is highly linear to the test image resolution, since all images go through the same number of convolutions. Our method is always a trade-off between performance and speed. To show this, we train three networks for comparison, which are 9-1-5, 9-3-5, and 9-5-5. It is clear that the 9-1-5 network is the fastest, while it still achieves better performance than the next state-of-the-art A+. Other methods are several times or even orders of magnitude slower in comparison to 9-1-5 network. Note the speed gap is not mainly caused by the different MATLAB/C++ implementations; rather, the other methods need to solve complex optimization problems on usage (e.g., sparse coding or embedding), whereas our method is completely feed-forward. The 9-5-5 network achieves the best performance but at the cost of the running time. The test-time speed of our CNN can be further accelerated in many ways, e.g., approximating or simplifying the trained networks <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b30">[31]</ref>, with possible slight degradation in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiments on Color Channels</head><p>In previous experiments, we follow the conventional approach to super-resolve color images. Specifically, we first transform the color images into the YCbCr space. The SR algorithms are only applied on the Y channel, while the Cb , Cr channels are upscaled by bicubic interpolation. It is interesting to find out if super-resolution performance can be improved if we jointly consider all three channels in the process.</p><p>Our method is flexible to accept more channels without altering the learning mechanism and network design. In particular, it can readily deal with three channels simultaneously by setting the input channels to c = 3. In the following experiments, we explore different training strategies for color image super-resolution, and subsequently evaluate their performance on different channels. Implementation details. Training is performed on the 91-image dataset, and testing is conducted on the Set5 <ref type="bibr" target="#b1">[2]</ref>. The network settings are: c = 3, f 1 = 9, f 2 = 1, f 3 = 5, n 1 = 64, and n 2 = 32. As we have proved the     SRCNN(9-3-5) SRCNN <ref type="bibr">(9-5-5)</ref> --&gt; Faster Slower &lt;-- <ref type="figure" target="#fig_0">Fig. 12</ref>. The proposed SRCNN achieves the stateof-the-art super-resolution quality, whilst maintains high and competitive speed in comparison to existing external example-based methods. The chart is based on Set14 results summarized in <ref type="table" target="#tab_4">Table 3</ref>. The implementation of all three SRCNN networks are available on our project page. effectiveness of SRCNN on different scales, here we only evaluate the performance of upscaling factor 3.</p><p>Comparisons. We compare our method with the stateof-art color SR method -KK <ref type="bibr" target="#b24">[25]</ref>. We also try different learning strategies for comparison: • Y only: this is our baseline method, which is a single-channel (c = 1) network trained only on the luminance channel. The Cb, Cr channels are upscaled using bicubic interpolation.</p><p>• YCbCr: training is performed on the three channels of the YCbCr space.</p><p>• Y pre-train: first, to guarantee the performance on the Y channel, we only use the MSE of the Y channel as the loss to pre-train the network. Then we employ the MSE of all channels to fine-tune the parameters.</p><p>• CbCr pre-train: we use the MSE of the Cb, Cr channels as the loss to pre-train the network, then fine-tune the parameters on all channels.</p><p>• RGB: training is performed on the three channels of the RGB space.</p><p>The results are shown in <ref type="table" target="#tab_6">Table 5</ref>, where we have the following observations. (i) If we directly train on the YCbCr channels, the results are even worse than that of bicubic interpolation. The training falls into a bad local minimum, due to the inherently different characteristics of the Y and Cb, Cr channels. (ii) If we pre-train on the Y or Cb, Cr channels, the performance finally improves, but is still not better than "Y only" on the color image (see the last column of <ref type="table" target="#tab_6">Table 5</ref>, where PSNR is computed (a) First-layer filters -Cb channel (b) First-layer filters -Cr channel <ref type="figure" target="#fig_1">Fig. 13</ref>. Chrominance channels of the first-layer filters using the "Y pre-train" strategy.</p><p>in RGB color space). This suggests that the Cb, Cr channels could decrease the performance of the Y channel when training is performed in a unified network. (iii) We observe that the Cb, Cr channels have higher PSNR values for "Y pre-train" than for "CbCr pre-train". The reason lies on the differences between the Cb, Cr channels and the Y channel. Visually, the Cb, Cr channels are more blurry than the Y channel, thus are less affected by the downsampling process. When we pre-train on the Cb, Cr channels, there are only a few filters being activated. Then the training will soon fall into a bad local minimum during fine-tuning. On the other hand, if we pre-train on the Y channel, more filters will be activated, and the performance on Cb, Cr channels will be pushed much higher. <ref type="figure" target="#fig_1">Figure 13</ref> shows the Cb, Cr channels of the first-layer filters with "Y pre-train", of which the patterns largely differ from that shown in <ref type="figure">Figure 5</ref>. (iv) Training on the RGB channels achieves the best result on the color image. Different from the YCbCr channels, the RGB channels exhibit high crosscorrelation among each other. The proposed SRCNN is capable of leveraging such natural correspondences between the channels for reconstruction. Therefore, the model achieves comparable result on the Y channel as "Y only", and better results on Cb, Cr channels than bicubic interpolation. (v) In KK <ref type="bibr" target="#b24">[25]</ref>, super-resolution is applied on each RGB channel separately. When we transform its results to YCbCr space, the PSNR value of Y channel is similar as "Y only", but that of Cb, Cr channels are poorer than bicubic interpolation. The result suggests that the algorithm is biased to the Y channel. On the whole, our method trained on RGB channels achieves better performance than KK and the singlechannel network ("Y only"). It is also worth noting that the improvement compared with the single-channel network is not that significant (i.e., 0.07 dB). This indicates that the Cb, Cr channels barely help in improving the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have presented a novel deep learning approach for single image super-resolution (SR). We show that conventional sparse-coding-based SR methods can be reformulated into a deep convolutional neural network. The proposed approach, SRCNN, learns an end-to-end mapping between low-and high-resolution images, with little extra pre/post-processing beyond the optimization. With a lightweight structure, the SRCNN has achieved superior performance than the state-of-the-art methods. We conjecture that additional performance can be further gained by exploring more filters and different training strategies. Besides, the proposed structure, with its advantages of simplicity and robustness, could be applied to other low-level vision problems, such as image deblurring or simultaneous SR+denoising. One could also investigate a network to cope with different upscaling factors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Given a low-resolution image Y, the first convolutional layer of the SRCNN extracts a set of feature maps. The second layer maps these feature maps nonlinearly to high-resolution patch representations. The last layer combines the predictions within a spatial neighbourhood to produce the final high-resolution image F (Y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>An illustration of sparse-coding-based methods in the view of a convolutional neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Training with the much larger ImageNet dataset improves the performance over the use of 91 images. The figure shows the first-layer filters trained on ImageNet with an upscaling factor 3. The filters are organized based on their respective variances.InputFeature maps of the first layer Output Feature maps of the second layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Example feature maps of different layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>A larger filter size leads to better results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Comparisons between three-layer and four-layer networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 Fig. 9 .</head><label>39</label><figDesc>n 22 = 32) and 9-1-1-1-5 (n 22 = 32, n 23 = 16) Deeper structure does not always lead to better results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>The test convergence curve of SRCNN and results of other methods on the Set5 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>The test convergence curve of SRCNN and the result of DNC on the Set5 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>The results of using different filter numbers in SRCNN. Training is performed on ImageNet whilst the evaluation is conducted on the Set5 dataset.</figDesc><table><row><cell cols="2">n 1 = 128</cell><cell></cell><cell>n 1 = 64</cell><cell></cell><cell>n 1 = 32</cell></row><row><cell cols="2">n 2 = 64</cell><cell></cell><cell>n 2 = 32</cell><cell></cell><cell>n 2 = 16</cell></row><row><cell>PSNR</cell><cell cols="3">Time (sec) PSNR Time (sec)</cell><cell cols="2">PSNR Time (sec)</cell></row><row><cell>32.60</cell><cell>0.60</cell><cell>32.52</cell><cell>0.18</cell><cell>32.26</cell><cell>0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>The average results of PSNR (dB), SSIM, IFC, NQM, WPSNR (dB) and MSSIM on the Set5 dataset.</figDesc><table><row><cell cols="4">Eval. Mat Scale Bicubic SC [50]</cell><cell>NE+LLE [4]</cell><cell cols="2">KK [25] ANR [41]</cell><cell cols="2">A+ [41] SRCNN</cell></row><row><cell></cell><cell>2</cell><cell>33.66</cell><cell>-</cell><cell>35.77</cell><cell>36.20</cell><cell>35.83</cell><cell>36.54</cell><cell>36.66</cell></row><row><cell>PSNR</cell><cell>3</cell><cell>30.39</cell><cell>31.42</cell><cell>31.84</cell><cell>32.28</cell><cell>31.92</cell><cell>32.59</cell><cell>32.75</cell></row><row><cell></cell><cell>4</cell><cell>28.42</cell><cell>-</cell><cell>29.61</cell><cell>30.03</cell><cell>29.69</cell><cell>30.28</cell><cell>30.49</cell></row><row><cell></cell><cell>2</cell><cell>0.9299</cell><cell>-</cell><cell>0.9490</cell><cell>0.9511</cell><cell>0.9499</cell><cell>0.9544</cell><cell>0.9542</cell></row><row><cell>SSIM</cell><cell>3</cell><cell>0.8682</cell><cell>0.8821</cell><cell>0.8956</cell><cell>0.9033</cell><cell>0.8968</cell><cell>0.9088</cell><cell>0.9090</cell></row><row><cell></cell><cell>4</cell><cell>0.8104</cell><cell>-</cell><cell>0.8402</cell><cell>0.8541</cell><cell>0.8419</cell><cell>0.8603</cell><cell>0.8628</cell></row><row><cell></cell><cell>2</cell><cell>6.10</cell><cell>-</cell><cell>7.84</cell><cell>6.87</cell><cell>8.09</cell><cell>8.48</cell><cell>8.05</cell></row><row><cell>IFC</cell><cell>3</cell><cell>3.52</cell><cell>3.16</cell><cell>4.40</cell><cell>4.14</cell><cell>4.52</cell><cell>4.84</cell><cell>4.58</cell></row><row><cell></cell><cell>4</cell><cell>2.35</cell><cell>-</cell><cell>2.94</cell><cell>2.81</cell><cell>3.02</cell><cell>3.26</cell><cell>3.01</cell></row><row><cell></cell><cell>2</cell><cell>36.73</cell><cell>-</cell><cell>42.90</cell><cell>39.49</cell><cell>43.28</cell><cell>44.58</cell><cell>41.13</cell></row><row><cell>NQM</cell><cell>3</cell><cell>27.54</cell><cell>27.29</cell><cell>32.77</cell><cell>32.10</cell><cell>33.10</cell><cell>34.48</cell><cell>33.21</cell></row><row><cell></cell><cell>4</cell><cell>21.42</cell><cell>-</cell><cell>25.56</cell><cell>24.99</cell><cell>25.72</cell><cell>26.97</cell><cell>25.96</cell></row><row><cell></cell><cell>2</cell><cell>50.06</cell><cell>-</cell><cell>58.45</cell><cell>57.15</cell><cell>58.61</cell><cell>60.06</cell><cell>59.49</cell></row><row><cell>WPSNR</cell><cell>3</cell><cell>41.65</cell><cell>43.64</cell><cell>45.81</cell><cell>46.22</cell><cell>46.02</cell><cell>47.17</cell><cell>47.10</cell></row><row><cell></cell><cell>4</cell><cell>37.21</cell><cell>-</cell><cell>39.85</cell><cell>40.40</cell><cell>40.01</cell><cell>41.03</cell><cell>41.13</cell></row><row><cell></cell><cell>2</cell><cell>0.9915</cell><cell>-</cell><cell>0.9953</cell><cell>0.9953</cell><cell>0.9954</cell><cell>0.9960</cell><cell>0.9959</cell></row><row><cell>MSSSIM</cell><cell>3</cell><cell>0.9754</cell><cell>0.9797</cell><cell>0.9841</cell><cell>0.9853</cell><cell>0.9844</cell><cell>0.9867</cell><cell>0.9866</cell></row><row><cell></cell><cell>4</cell><cell>0.9516</cell><cell>-</cell><cell>0.9666</cell><cell>0.9695</cell><cell>0.9672</cell><cell>0.9720</cell><cell>0.9725</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3</head><label>3</label><figDesc>The average results of PSNR (dB), SSIM, IFC, NQM, WPSNR (dB) and MSSIM on the Set14 dataset.</figDesc><table><row><cell cols="4">Eval. Mat Scale Bicubic SC [50]</cell><cell>NE+LLE [4]</cell><cell cols="2">KK [25] ANR [41]</cell><cell cols="2">A+ [41] SRCNN</cell></row><row><cell></cell><cell>2</cell><cell>30.23</cell><cell>-</cell><cell>31.76</cell><cell>32.11</cell><cell>31.80</cell><cell>32.28</cell><cell>32.45</cell></row><row><cell>PSNR</cell><cell>3</cell><cell>27.54</cell><cell>28.31</cell><cell>28.60</cell><cell>28.94</cell><cell>28.65</cell><cell>29.13</cell><cell>29.30</cell></row><row><cell></cell><cell>4</cell><cell>26.00</cell><cell>-</cell><cell>26.81</cell><cell>27.14</cell><cell>26.85</cell><cell>27.32</cell><cell>27.50</cell></row><row><cell></cell><cell>2</cell><cell>0.8687</cell><cell>-</cell><cell>0.8993</cell><cell>0.9026</cell><cell>0.9004</cell><cell>0.9056</cell><cell>0.9067</cell></row><row><cell>SSIM</cell><cell>3</cell><cell>0.7736</cell><cell>0.7954</cell><cell>0.8076</cell><cell>0.8132</cell><cell>0.8093</cell><cell>0.8188</cell><cell>0.8215</cell></row><row><cell></cell><cell>4</cell><cell>0.7019</cell><cell>-</cell><cell>0.7331</cell><cell>0.7419</cell><cell>0.7352</cell><cell>0.7491</cell><cell>0.7513</cell></row><row><cell></cell><cell>2</cell><cell>6.09</cell><cell>-</cell><cell>7.59</cell><cell>6.83</cell><cell>7.81</cell><cell>8.11</cell><cell>7.76</cell></row><row><cell>IFC</cell><cell>3</cell><cell>3.41</cell><cell>2.98</cell><cell>4.14</cell><cell>3.83</cell><cell>4.23</cell><cell>4.45</cell><cell>4.26</cell></row><row><cell></cell><cell>4</cell><cell>2.23</cell><cell>-</cell><cell>2.71</cell><cell>2.57</cell><cell>2.78</cell><cell>2.94</cell><cell>2.74</cell></row><row><cell></cell><cell>2</cell><cell>40.98</cell><cell>-</cell><cell>41.34</cell><cell>38.86</cell><cell>41.79</cell><cell>42.61</cell><cell>38.95</cell></row><row><cell>NQM</cell><cell>3</cell><cell>33.15</cell><cell>29.06</cell><cell>37.12</cell><cell>35.23</cell><cell>37.22</cell><cell>38.24</cell><cell>35.25</cell></row><row><cell></cell><cell>4</cell><cell>26.15</cell><cell>-</cell><cell>31.17</cell><cell>29.18</cell><cell>31.27</cell><cell>32.31</cell><cell>30.46</cell></row><row><cell></cell><cell>2</cell><cell>47.64</cell><cell>-</cell><cell>54.47</cell><cell>53.85</cell><cell>54.57</cell><cell>55.62</cell><cell>55.39</cell></row><row><cell>WPSNR</cell><cell>3</cell><cell>39.72</cell><cell>41.66</cell><cell>43.22</cell><cell>43.56</cell><cell>43.36</cell><cell>44.25</cell><cell>44.32</cell></row><row><cell></cell><cell>4</cell><cell>35.71</cell><cell>-</cell><cell>37.75</cell><cell>38.26</cell><cell>37.85</cell><cell>38.72</cell><cell>38.87</cell></row><row><cell></cell><cell>2</cell><cell>0.9813</cell><cell>-</cell><cell>0.9886</cell><cell>0.9890</cell><cell>0.9888</cell><cell>0.9896</cell><cell>0.9897</cell></row><row><cell>MSSSIM</cell><cell>3</cell><cell>0.9512</cell><cell>0.9595</cell><cell>0.9643</cell><cell>0.9653</cell><cell>0.9647</cell><cell>0.9669</cell><cell>0.9675</cell></row><row><cell></cell><cell>4</cell><cell>0.9134</cell><cell>-</cell><cell>0.9317</cell><cell>0.9338</cell><cell>0.9326</cell><cell>0.9371</cell><cell>0.9376</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>The average results of PSNR (dB), SSIM, IFC, NQM, WPSNR (dB) and MSSIM on the BSD200 dataset.</figDesc><table><row><cell cols="4">Eval. Mat Scale Bicubic SC [50]</cell><cell>NE+LLE [4]</cell><cell cols="2">KK [25] ANR [41]</cell><cell cols="2">A+ [41] SRCNN</cell></row><row><cell></cell><cell>2</cell><cell>28.38</cell><cell>-</cell><cell>29.67</cell><cell>30.02</cell><cell>29.72</cell><cell>30.14</cell><cell>30.29</cell></row><row><cell>PSNR</cell><cell>3</cell><cell>25.94</cell><cell>26.54</cell><cell>26.67</cell><cell>26.89</cell><cell>26.72</cell><cell>27.05</cell><cell>27.18</cell></row><row><cell></cell><cell>4</cell><cell>24.65</cell><cell>-</cell><cell>25.21</cell><cell>25.38</cell><cell>25.25</cell><cell>25.51</cell><cell>25.60</cell></row><row><cell></cell><cell>2</cell><cell>0.8524</cell><cell>-</cell><cell>0.8886</cell><cell>0.8935</cell><cell>0.8900</cell><cell>0.8966</cell><cell>0.8977</cell></row><row><cell>SSIM</cell><cell>3</cell><cell>0.7469</cell><cell>0.7729</cell><cell>0.7823</cell><cell>0.7881</cell><cell>0.7843</cell><cell>0.7945</cell><cell>0.7971</cell></row><row><cell></cell><cell>4</cell><cell>0.6727</cell><cell>-</cell><cell>0.7037</cell><cell>0.7093</cell><cell>0.7060</cell><cell>0.7171</cell><cell>0.7184</cell></row><row><cell></cell><cell>2</cell><cell>5.30</cell><cell>-</cell><cell>7.10</cell><cell>6.33</cell><cell>7.28</cell><cell>7.51</cell><cell>7.21</cell></row><row><cell>IFC</cell><cell>3</cell><cell>3.05</cell><cell>2.77</cell><cell>3.82</cell><cell>3.52</cell><cell>3.91</cell><cell>4.07</cell><cell>3.91</cell></row><row><cell></cell><cell>4</cell><cell>1.95</cell><cell>-</cell><cell>2.45</cell><cell>2.24</cell><cell>2.51</cell><cell>2.62</cell><cell>2.45</cell></row><row><cell></cell><cell>2</cell><cell>36.84</cell><cell>-</cell><cell>41.52</cell><cell>38.54</cell><cell>41.72</cell><cell>42.37</cell><cell>39.66</cell></row><row><cell>NQM</cell><cell>3</cell><cell>28.45</cell><cell>28.22</cell><cell>34.65</cell><cell>33.45</cell><cell>34.81</cell><cell>35.58</cell><cell>34.72</cell></row><row><cell></cell><cell>4</cell><cell>21.72</cell><cell>-</cell><cell>25.15</cell><cell>24.87</cell><cell>25.27</cell><cell>26.01</cell><cell>25.65</cell></row><row><cell></cell><cell>2</cell><cell>46.15</cell><cell>-</cell><cell>52.56</cell><cell>52.21</cell><cell>52.69</cell><cell>53.56</cell><cell>53.58</cell></row><row><cell>WPSNR</cell><cell>3</cell><cell>38.60</cell><cell>40.48</cell><cell>41.39</cell><cell>41.62</cell><cell>41.53</cell><cell>42.19</cell><cell>42.29</cell></row><row><cell></cell><cell>4</cell><cell>34.86</cell><cell>-</cell><cell>36.52</cell><cell>36.80</cell><cell>36.64</cell><cell>37.18</cell><cell>37.24</cell></row><row><cell></cell><cell>2</cell><cell>0.9780</cell><cell>-</cell><cell>0.9869</cell><cell>0.9876</cell><cell>0.9872</cell><cell>0.9883</cell><cell>0.9883</cell></row><row><cell>MSSSIM</cell><cell>3</cell><cell>0.9426</cell><cell>0.9533</cell><cell>0.9575</cell><cell>0.9588</cell><cell>0.9581</cell><cell>0.9609</cell><cell>0.9614</cell></row><row><cell></cell><cell>4</cell><cell>0.9005</cell><cell>-</cell><cell>0.9203</cell><cell>0.9215</cell><cell>0.9214</cell><cell>0.9256</cell><cell>0.9261</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Average PSNR (dB) of different channels and training strategies on the Set5 dataset.</figDesc><table><row><cell>Training</cell><cell></cell><cell cols="3">PSNR of different channel(s)</cell></row><row><cell>Strategies</cell><cell>Y</cell><cell>Cb</cell><cell>Cr</cell><cell>RGB color image</cell></row><row><cell>Bicubic</cell><cell>30.39</cell><cell>45.44</cell><cell>45.42</cell><cell>34.57</cell></row><row><cell>Y only</cell><cell>32.39</cell><cell>45.44</cell><cell>45.42</cell><cell>36.37</cell></row><row><cell>YCbCr</cell><cell>29.25</cell><cell>43.30</cell><cell>43.49</cell><cell>33.47</cell></row><row><cell>Y pre-train</cell><cell>32.19</cell><cell>46.49</cell><cell>46.45</cell><cell>36.32</cell></row><row><cell cols="2">CbCr pre-train 32.14</cell><cell>46.38</cell><cell>45.84</cell><cell>36.25</cell></row><row><cell>RGB</cell><cell>32.33</cell><cell>46.18</cell><cell>46.20</cell><cell>36.44</cell></row><row><cell>KK</cell><cell>32.37</cell><cell>44.35</cell><cell>44.22</cell><cell>36.32</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L A</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with BM3D?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep network cascade for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jointly optimized regressors for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Softcuts: a soft edge smoothness prior for color image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="969" to="981" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image quality assessment based on a degradation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Damera-Venkata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="636" to="650" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Restoring an image taken through a window covered with dirt or rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="633" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Example-based superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning lowlevel vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">T</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1710</idno>
		<title level="m">Convolutional neural networks at constrained time cost</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving resolution by image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical Models and Image Processing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image transformation based on learning dictionaries across image spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="367" to="380" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face hallucination: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="134" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simplifying convnets for fast learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.3505</idno>
		<title level="m">Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2056" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A machine learning approach for non-blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1067" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3791" to="3799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An information fidelity criterion for image quality assessment using natural scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2117" to="2128" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<title level="m">Scalable, highquality object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference Record of the Thirty-Seventh Asilomar Conference on Signals</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploiting self-similarities for single frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="497" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Single-image super-resolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast image super-resolution based on in-place example regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1059" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Part-based R-CNNs for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Bicubick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K24</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Originalk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kpsnr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anrk/K25</surname></persName>
		</author>
		<idno>90KdB SRCNNK/K27.95KdB</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sck/K25</surname></persName>
		</author>
		<idno>58KdB NE+LLEK/K25.75KdB</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<title level="m">Fig. 14. The &quot;butterfly&quot; image from Set5 with an upscaling factor 3. Bicubic+/+23.71+dB</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Original+</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>+psnr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anr+/+25</surname></persName>
		</author>
		<idno>03+dB SRCNN+/+27.04+dB</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<title level="m">SC+/+24.98+dB NE+LLE+/+24.94+dB</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<title level="m">Fig. 15. The &quot;ppt3&quot; image from Set14 with an upscaling factor 3. BicubicL/L26.63LdB</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Originall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lpsnr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anrl/L28</surname></persName>
		</author>
		<idno>43LdB SRCNNL/L29.29LdB</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scl/L27</surname></persName>
		</author>
		<idno>95LdB NE+LLEL/L28.31LdB</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">16. The &quot;zebra&quot; image from Set14 with an upscaling factor 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
