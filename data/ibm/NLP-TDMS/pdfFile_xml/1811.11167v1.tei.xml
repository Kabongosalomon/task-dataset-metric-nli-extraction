<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Integrated Object Detection and Tracking with Tracklet-Conditioned Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">Xizhou</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
							<email>jifdai@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Integrated Object Detection and Tracking with Tracklet-Conditioned Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate detection and tracking of objects is vital for effective video understanding. In previous work, the two tasks have been combined in a way that tracking is based heavily on detection, but the detection benefits marginally from the tracking. To increase synergy, we propose to more tightly integrate the tasks by conditioning the object detection in the current frame on tracklets computed in prior frames. With this approach, the object detection results not only have high detection responses, but also improved coherence with the existing tracklets. This greater coherence leads to estimated object trajectories that are smoother and more stable than the jittered paths obtained without tracklet-conditioned detection. Over extensive experiments, this approach is shown to achieve state-of-the-art performance in terms of both detection and tracking accuracy, as well as noticeable improvements in tracking stability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detection and tracking of moving objects is an essential element of many video understanding tasks, such as visual surveillance, autonomous navigation, and video captioning. Different from the more commonly addressed problem of object detection in still images, the additional temporal dimension in the video case introduces challenges that arise from scene dynamics. As an object moves, its appearance can vary due to occlusions, pose changes, and illumination differences. Imaging-related degradations such as motion blur and video defocus may affect object appearance as well. These factors collectively complicate the task of discovering objects and following their trajectories in a scene.</p><p>A common practice among existing methods for object detection and tracking is to detect objects in each frame * Equal contribution. †This work is done when Dazhi Cheng and Xizhou Zhu are interns at Microsoft Research Asia. independently and then link the detected objects across frames to form tracklets <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>. Applying detection and tracking in this sequential manner is of appealing simplicity. But unlike how detection assists tracking in this approach, there are no means for tracking to aid detection. Some methods attempt to address this issue by using tracklets to propagate detection bounding boxes from previous frames to the current frame, and then add these boxes to those produced by the detector <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10]</ref>. However, with this late integration of tracking into the detection process, the tracking has no effect on the object detector itself. Rather, tracking exerts its influence only after the object detector has computed its bounding box results.</p><p>The disjoint design can be partially attributed to the relatively independent development of video object detection and multi-object tracking techniques. In the research of video object detection, the focus is on improving the perframe object detection accuracy, while employing off-theshelf trackers for post-processing <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b70">71]</ref>. Meanwhile, for research on multi-object tracking, the detection results are usually assumed to be given by external object detectors applied on individual frames <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b3">4]</ref>. Such decoupling simplifies research for each task, but misses the benefit of integrating detection and tracking.</p><p>In this paper, we present an approach in which detection and tracking are more closely intertwined through an early integration of the two tasks. Instead of simply aggregating two sets of bounding boxes that are estimated separately by the detector and tracker, a single set of boxes is generated jointly by the two processes by conditioning the outputs of the object detector on the tracklets computed over the prior frames. In this way, the resulting detection boxes are both consistent with the tracklets and have high detection responses, instead of often having just one or the other in late integration techniques.</p><p>This advantage is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, which shows an example of detection boxes obtained with late integration Sequential detection and tracking with late integration ID: 1 Algorithm 1 Online sequential detection and tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integrated detection and tracking</head><formula xml:id="formula_0">input: video frames {I t } T t=0 B 0 := DetectOnImage(I 0 ) initialize the tracklets D 0 from B 0 for t = 1 to T do B t := DetectOnImage(I t ) B t := P ropagateBox(D t−1 ) Optional B t := [B t , B t ] Optional B t := N M S(B t ) D t := AssociateT racklet(D t−1 , B t ) B t := RescoreBox(D t )</formula><p>Optional end for output: all tracklets D T and all boxes {B t } T t=0 as done in <ref type="bibr" target="#b24">[25]</ref>, and with early integration via our trackletconditioned detection. Due in part to the aforementioned challenges of object detection in video, the boxes that have the highest detection scores without consideration of tracking may lie at various locations that deviate from the corresponding tracklet. Including boxes from late integration provides additional candidates, but they may not coincide closely with the actual object location due to errors in optical flow. With our tracklet-conditioned detection, temporal cues compiled over multiple frames can robustly guide the detector in a way that can compensate for variabilities in the detection of moving objects.</p><p>A natural outcome of tracklet-conditioned detection is increased stability in tracking. Besides generating detection boxes that more closely adhere to the target object, the conditioning also results in smoother trajectories where the detection boxes overlap the moving object in a consistent manner, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> and detailed in Sec. 2.5. This property is beneficial for applications such as live compositing of virtual makeup on faces, where a lack of stability would produce unwanted jittering of the makeup relative to the face.</p><p>We show how tracklet-based conditioning can be applied within a modern two-stage detector, employing it in both region proposal generation and classification. Through comprehensive evaluation on the Image VID <ref type="bibr" target="#b46">[47]</ref> and MOT <ref type="bibr" target="#b29">[30]</ref> datasets, it is shown that this provides state-of-the-art performance on both object detection and tracking. Noticeable gains in tracking stability are achieved as well. The code for this technique will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Integrated Object Detection and Tracking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Background</head><p>Given a video of multiple frames I t , t = 0, . . . , T , our goal is to detect and to track all the object instances within it up to time t, which we denote as D t . D t = {&lt; d t j , c t j &gt; }, j = 1, . . . , m, where d t j denotes the j-th tracklet, and c t j denotes its corresponding category. For a tracklet d t , it is composed of a set of bounding boxes detected on individual frames up to time t,</p><formula xml:id="formula_1">as d t = [b t k k ], where b t k k is the k-th bounding box in d t at frame t k , where t k ≤ t.</formula><p>A scheme widely adopted in previous work <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref> is sequential detection and tracking, outlined in Algorithm 1. Here we describe an online variant of the algorithm. Given a new video frame I t , an object detector for individual images is first applied to produce per-frame detection results B t := DetectOnImage(I t ), where B t denotes a set of bounding boxes together with their corresponding category scores. Non-maximum suppression is then applied to remove redundant bounding boxes, result-ing in B t := N M S(B t ). Then the tracking algorithm associates the existing tracklets D t−1 to the detection results B t , producing tracklets up to frame I t as D t := AssociateT racklet(D t−1 , B t ). Finally, the algorithm outputs all the tracklets D T up to time T .</p><p>To improve performance, two optional techniques are widely used as adds-on to better exploit tracklet information: (1) Box propagation, where detected boxes in the existing tracklets D t−1 are propagated to the current frame I t , usually with the aid of flow information, to get boxes B t := P ropagateBox(D t−1 ). The propagated boxes are concatenated with the per-image detected boxes as</p><formula xml:id="formula_2">B t := [B t , B t ].</formula><p>The concatenated boxes undergo non-maximum suppression and are associated to the existing tracklets. This technique can be helpful when bounding boxes are not reliably detected in the new frame I t . (2) Box rescoring, a post-processing step to obtain more accurate classification scores for the detected boxes. For a bounding box newly associated to a tracklet, its score is set to the average score of all the bounding boxes that compose the tracklet. Here we denote this operation as B t := RescoreBox(D t ).</p><p>Including box propagation and/or box rescoring leads to better integration of detection and tracking. However, these techniques allow tracking to impact detection at only a late stage, after the per-image detection boxes are fixed. As a result, the detector cannot take full advantage of the tracking information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Tracklet-Conditioned Detection Formulation</head><p>We aim at improving per-frame detection results through early integration of object detection and tracking. Our goal is for detection to exploit not only the image appearance of the current frame, but also information from tracklets recovered in the previous frames. We refer to this approach as tracklet-conditioned detection.</p><p>The problem can be formulated as: Given a set of candidate boxes {b t i } 1 on frame I t , where b t i specifies the 4-D coordinates of the i-th box, together with the tracklets {d t−1 j } m j=1 up to frame I t−1 , classify each box to different categories (including background) by estimating the score P (c|b t i , {d t−1 j }). Based on the intuition that a candidate box should more likely take labels consistent with tracklets it is more likely to be associated with, the score is further decomposed to be conditioned on each tracklet, as</p><formula xml:id="formula_3">P (c|b t i , {d t−1 j }) = m j=0 w(b t i , d t−1 j )P (c|b t i , d t−1 j ),<label>(1)</label></formula><p>where w(b t i , d t−1 j ) specifies the association weight between box b t i and tracklet d t−1 j . To account for newly detected objects that do not appear in existing tracklets, we include a null tracklet d t−1 0 , as detailed at the end of this subsection. The score P (c|b t i , d t−1 j ) is estimated based on both the appearance of the current frame and information from previous tracklets, as</p><formula xml:id="formula_4">P (c|b t i , d t−1 j ) ∝ exp(log P det (c|b t i ) + α log P tr (c|d t−1 j )),<label>(2)</label></formula><p>where P det (c|b t i ) is predicted by the per-image object detector on I t , P tr (c|d t−1 j ) is the classification probability for tracklet d t−1 j , and the hyper-parameter α balances the two log-likelihood terms (α = 1 by default).</p><formula xml:id="formula_5">P (c|b t i , d t−1 j ) is normalized over all the categories, by C c=0 P (c|b t i , d t−1 j ) = 1,</formula><p>where C denotes the number of foreground object categories, plus one for the background (c = 0). P tr (c|d t−1 j ) is defined on the classification scores of all the bounding boxes assigned to tracklet d t−1 j , in a running average fashion. Suppose tracklet d t j (j &gt; 0) is composed of box b t k and tracklet d t−1 j , then P tr (c|d t j ) is computed as</p><formula xml:id="formula_6">P tr (c|d t j ) = P (c|b t k , {d t−1 j }) + βP tr (c|d t−1 j )len(d t−1 j ) 1 + βlen(d t−1 j ) (3)</formula><p>where β is an exponential decay parameter (β = 0.99 by default), and len(d t−1 j ) denotes the trajectory length of d t−1 j . The association weight w(b t i , d t−1 j ) is defined based on the intuition that box b t i is more likely to be associated to a tracklet that is visually similar:</p><formula xml:id="formula_7">w(b t i , d t−1 j ) := exp(γ cos(E(b t i ), E(d t−1 j ))) j &gt; 0,<label>(4)</label></formula><p>where E(b t i ) and E(d t−1 j ) are embedding features (128-D in our work) that encode the visual appearance of box b t i and tracklet d t−1 j respectively, which are generated as described in Section 2.3. The cosine similarity between the embedding features is calculated and modulated by hyperparameter γ (set to 8 in this paper) to be the log-likelihood of the association weight.</p><p>It is worth noting that new objects may appear in a video frame, and these objects will not be associated with any existing tracklets. To handle these cases, a null tracklet d t−1 0 is introduced. For every candidate box, its association weight with d t−1 0 is set to a constant, as</p><formula xml:id="formula_8">w(b t i , d t−1 0 ) := exp(R),<label>(5)</label></formula><p>where R = 0.3 in this paper. The association weights defined in Eq. (4) and Eq. (5) are further normalized over all the tracklets, as</p><formula xml:id="formula_9">w(b t i , d t−1 j ) := w(b t i , d t−1 j ) m k=0 w(b t i , d t−1 k ) .<label>(6)</label></formula><p>Thus, when a candidate box has low association weights with all the existing tracklets, its normalized association weight with the null tracklet will be high. For the null tracklet, its classification probability is set to a uniform distribution over all the categories, as</p><formula xml:id="formula_10">P tr (c|d t 0 ) = 1 C + 1 .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Tracklet-Conditioned Two-stage Detectors</head><p>The proposed tracklet-conditioned detection algorithm can be readily applied in state-of-the-art object detectors. In this paper, we incorporate it into the two-stage Faster R-CNN <ref type="bibr" target="#b45">[46]</ref> + ResNet-101 <ref type="bibr" target="#b19">[20]</ref> detector, with OHEM <ref type="bibr" target="#b49">[50]</ref>. For this baseline, following the practice in <ref type="bibr" target="#b11">[12]</ref>, all the convolutional layers in ResNet-101 are applied on the whole input image. The effective stride in the conv5 blocks is reduced from 32 to 16 pixels to increase feature map resolution. The RPN <ref type="bibr" target="#b45">[46]</ref> head is added on top of the conv4 features of ResNet-101. The Fast R-CNN <ref type="bibr" target="#b17">[18]</ref> head is added on top of the conv5 features, and is composed of RoIpooling and two fully-connected (fc) layers of 1024-D, followed by the classification and the bounding box regression branches.</p><p>The tracklet-conditioned two-stage detector is exhibited in <ref type="figure" target="#fig_2">Figure 2</ref>. The tracklet conditioning in the second stage is relatively straightforward, with the equations in Section 2.2 applied on sparse region proposals. P det (c|b t i ) is predicted by the classification branch of the Fast R-CNN detection head. The box embedding E s2 (b t i ) (of the second stage) is computed by attaching a branch (consisting of a fullyconnected layer) to the Fast R-CNN head, sibling to the classification and bounding box regression branches. The tracklet embedding E s2 (d t j ) (j &gt; 0) is updated based on the embedding features of the boxes associated to it, as We further apply the tracklet-conditioned detection in the first stage, to make use of tracklet information for improving region proposal quality. Compared to the application in the second stage, the key differences are that the candidate boxes are dense anchor boxes, and only two categories are involved, namely foreground and background. Given an anchor box b t i , its foreground probability P (fg|b t i , {d t−1 j }) is estimated by Eq. (1), with P det (fg|b t i ) predicted by the RPN classification branch and P tr (fg|d t−1 j ) Algorithm 2 Online integrated detection and tracking.</p><formula xml:id="formula_11">E s2 (d t j ) = ηE s2 (b t k ) + (1 − η)E s2 (d t−1 j ) if t &gt; 0, E s2 (b 0 k ) otherwise,<label>(8)</label></formula><formula xml:id="formula_12">input: video frames {I t } T t=0 B 0 := DetectOnImage(I 0 ) initialize the tracklets D 0 from B 0 for t = 1 to T do B t := T rackletCondDetect(I t , D t−1 ) B t := N M S(B t ) D t := AssociateT racklet(D t−1 , B t ) D t := RescoreT racklet(D t ) end for output: all tracklets D T and all boxes {B t } T t=0 computed as P tr (fg|d t−1 j ) = C c=1 P tr (c|d t−1 j ),<label>(9)</label></formula><p>which is the summation of the probability P tr (c|d t−1 j ) over all the foreground categories (c &gt; 0).</p><p>To derive the association weights for the first stage, two additional branches are added for producing the embedding features. The embedding features E anchor (b t i ) for the dense anchor boxes are computed following the design in <ref type="bibr" target="#b34">[35]</ref>, via a sibling branch (consisting of a 1 × 1 convolution) added to the RPN classification branch. Supposing there are K anchors at each location and the embedding features are 128-D, the output of the embedding branch is of dimension 128 × K. The tracklet embedding features E s1 (d t j ) of the first stage are computed in a manner similar to those of the second stage. An additional branch is added to the Fast R-CNN head to produce E s1 (b t i ). After the RoIpooling layer, two additional fc layers of 1024-D are added (sibling to the existing two fc layers), followed by one fc layer to produce E s1 (b t i ). Here, we tried different network designs for producing E s1 (b t i ), as shown in <ref type="table">Table 2</ref>. We find that adding two fc layers to reduce correlation between the embedding features of the two stages is beneficial for accuracy.</p><formula xml:id="formula_13">Given E s1 (b t i ), E s1 (d t j )</formula><p>is obtained by applying Eq. (8) (replacing the subscript s2 by s1 in the equation). Finally, the anchor box embedding features E anchor (b t i ) are compared to the tracklet embedding features E s1 (d t−1 j ) by Eq. (4) to obtain the association weights for the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Training and Inference</head><p>Inference. Algorithm 2 presents the inference procedure for our integrated object detection and tracking with tracklet-conditioned detection. Given the input video frames {I t } T t=0 , the per-image object detector is applied on the first frame I 0 to produce detection boxes, B 0 := DetectOnImage(I 0 ). With these boxes, the tracklets D 0 are initialized (one tracklet per box). Then for each subsequent frame I t , tracklet-conditioned detection is applied Training. The network is trained to better detect objects based on image content and to better associate them across frames. Due to memory constraints, the forward pass in SGD training cannot be kept identical to that in inference.</p><p>In each mini-batch, two consecutive frames from the same video, I t−1 and I t , are randomly sampled. In the forward pass, bounding boxes are detected on I t−1 based on image content only, as B t−1 := DetectOnImage(I t−1 ). The detected boxes are matched to the ground-truth annotations B gt t−1 and B gt t , on I t−1 and I t respectively, to inject object detection loss and tracking loss.</p><p>The object detection loss is defined on B t−1 and B gt t−1 in the same way as in conventional two-stage object detectors <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b10">11]</ref>. It is composed of a foreground / background Softmax cross-entropy loss for region proposal scoring, an L1 regression loss for regressing proposal boxes, a (C + 1)way Softmax cross-entropy loss for detection scoring, and an L1 regression loss for regressing detected boxes.</p><p>The tracking loss is defined on B t−1 and the B gt</p><formula xml:id="formula_14">t associ- ated to B gt t−1 . For a detected box b t−1 ∈ B t−1 , it is assigned to the ground-truth box b gt t−1 ∈ B gt t−1 having the highest IoU overlap with it. Let b gt</formula><p>t be the ground-truth bounding box on the next frame that corresponds to the same object as b gt t−1 . The tracking loss on b t−1 is then defined as</p><formula xml:id="formula_15">L track box (b t−1 , b gt t−1 , b gt t ) = (1 − cos(E(b t−1 ), E(b gt t ))) 2 if IoU(b t−1 , b gt t−1 ) ≥ 0.5 max(0, cos(E(b t−1 ), E(b gt t ))) 2 otherwise<label>(</label></formula><p>10) which encourages the cosine similarity cos(E(b t−1 ), E(b gt t )) to be close to 1 if b t−1 captures the same object as b gt t (IoU(b t−1 , b gt t−1 ) ≥ 0.5), and to be no more than 0 otherwise. The overall tracking loss is the summation of the loss values on all the detected boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Discussion</head><p>Accuracy and robustness In the proposed approach, detection is enhanced by accounting for temporal information when determining the classification probabilities of the bounding boxes. In previous techniques, these probabilities P det (c|b t i ) are obtained from the per-image object detector based solely on the appearance of frame I t . Object appearance variations and visual degradations in I t can lead to significant distortions in the predicted probabilities of its detection boxes B t . To counteract these complications, our method takes advantage of the tracklets D t−1 from the previous frame, which model the visual appearance E(d t−1 j ) and classification probabilities P tr (c|d t−1 j ) of each object. As these tracklet attributes are computed over the full existing trajectory of an object, they provide a representation that is relatively robust to the appearance changes that may occur during object motion, while placing greater weight on more recent frames. The classification probabilities of a bounding box are influenced by the tracklets most similar to it, as determined from association weights. By taking advantage of tracklet information in this way, the classification scores of bounding boxes are more robustly obtained, leading to more accurate final boxes.</p><p>By comparison, late integration techniques typically incorporate temporal information by adding bounding boxes propagated from preceding frames by optical flow. These boxes are aggregated with the detector's bounding boxes just prior to NMS. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, this is less ideal because the resulting boxes either have distorted classification probabilities (boxes from the detector) or rely on optical flow (boxes from tracklets) which can be inaccurate especially over the course of multiple frames or when there is background movement. Furthermore, since propagated boxes inherit the high classification scores of their corresponding tracklets, they may suppress more accurate boxes from the detector in the NMS. The difference in performance is examined in Sec. 4.2.</p><p>Stability Another key advantage of the proposed approach is the improvement of box localization stability across frames, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Unstable localization is a commonly observed problem in video object detection and tracking, and such instability can be attributed to appearance change in different frames. For example, in <ref type="figure" target="#fig_0">Figure 1</ref> k , respectively. Due to slight appearance changes on frame I t , P det (c|b t i ) is slightly higher than P det (c|b t k ), and thus b t i is kept after NMS. As a result, there is jitter between box b t−1 k and box b t i because of the sudden shift in box position relative to the actual object. If the jitter is large enough, the embedding features E(b t i ) can be quite different from those of the tracklet E(d t−1 j ) that bounding box b t−1 k is associated with. Thus a mismatch occurs even though frames I t−1 and I t are of high visual quality. Although box b t k could have been well associated with tracklet d t−1 j to generate a stable trajectory, it was already suppressed by NMS, thus becoming unavailable for consideration in tracklet association.</p><p>Tracklet-conditioned detection can effectively remedy this issue. The candidate boxes on a new frame would be scored not only based on the per-frame appearance, but also based on their association weights with the existing tracklets. In the example of <ref type="figure" target="#fig_0">Figure 1</ref>, when scoring candidate boxes b t i and b t k , the association weight w(b t k , d t−1 j ) is high, and tracklet d t−1 j would cast a large vote on box b t k . Thus the tracklet-conditioned score P (c|b t k , {d t−1 j }) would be higher than those of the other boxes, and box b t k would be kept after NMS, generating a stable trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Implementation Details</head><p>In the procedure AssociateT racklet, we employ a modified version of maximum bipartite graph matching, an algorithm widely used in multi-object tracking systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b59">60]</ref>. Given tracklets D t−1 and bounding boxes B t , a bipartite graph is generated in which nodes corresponding to d t−1 j ∈ D t−1 and b t i ∈ B t are on the two sides of the graph, respectively. An edge is added between d t−1 j and b t i if there is overlap between the last box in d t−1 j and b t i , with their connection weight set to cos(E(b t i ), E(d t−1 j )). There are no edges between non-overlapping tracklets and bounding boxes, so they will not be associated. To account for newly detected boxes which are not associated with any existing tracklet, a pseudo tracklet d pseudo i is initialized for each such bounding box b t i . The nodes of the pseudo tracklets are added on the side of the existing tracklets in the bipartite graph. An edge is added between pseudo tracklet d pseudo i and its corresponding bounding box b t i , with the connection weight set to 0. If the cosine similarity values between b t i and existing tracklets are all low (less than 0), b t i is likely to be associated with d pseudo i and a new tracklet is formed. Finally, the standard Hungarian maximum matching algorithm <ref type="bibr" target="#b27">[28]</ref> is applied on the constructed bipartite graph to associate the bounding boxes to the tracklets. For the procedure P ropagateBox, we follow the implementation in <ref type="bibr" target="#b24">[25]</ref>, but replace the OpenCV flow estimator <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">38]</ref> with the more recent FlowNet v2 <ref type="bibr" target="#b22">[23]</ref> for more accurate correspondence estimation between frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Object Detection in Images Current leading object detectors are built on deep Convolutional Neural Networks (CNNs). They can be mainly divided into two families, namely, region-based two-stage detectors (e.g., R-CNN <ref type="bibr" target="#b18">[19]</ref>, Fast(er) R-CNN <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">46]</ref>, and R-FCN <ref type="bibr" target="#b10">[11]</ref>) and one-stage detectors that directly predict boxes (e.g., YOLO <ref type="bibr" target="#b44">[45]</ref>, SSD <ref type="bibr" target="#b36">[37]</ref>, and CornerNets <ref type="bibr" target="#b28">[29]</ref>).</p><p>We build our approach on Faster R-CNN with ResNet-101 and OHEM, which is a state-of-the-art object detector.</p><p>Multiple Object Tracking (MOT) Research on MOT primarily follow the "sequential detection and tracking" paradigm, often under the setting where detection results are given by an external object detector and the focus is on correctly associating the detection boxes across frames. Various approaches to the association problem have been proposed, including but not limited to min-cost flow <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b32">33]</ref>, energy models <ref type="bibr" target="#b42">[43]</ref>, Markov decision processes <ref type="bibr" target="#b57">[58]</ref>, node labeling <ref type="bibr" target="#b33">[34]</ref>, graph matching <ref type="bibr" target="#b0">[1]</ref>, and Graph Cut <ref type="bibr">[63,</ref><ref type="bibr" target="#b51">52]</ref>. In addition, recent works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b48">49]</ref> explore utilizing deep networks to better solve the association problem. In <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b2">3]</ref>, the authors seek to refine the detection and tracking results by various optimization formulations. Improved accuracies are reported, but the refinement is performed at a late stage on the results produced by off-the-shelf object detectors and trackers.</p><p>In contrast to the previous research on multi-object tracking, we advocate a new paradigm of "integrated object detection and tracking", which aims to improve detection by considering tracking information and in turn further enhance tracking performance. The integration is at an early stage within the object detector. In this paper, tracking is performed simply by maximum bipartite graph matching, and we note that advances in MOT can benefit our method.</p><p>Single Object Tracking In this classic vision problem, a single object is annotated in the first frame of an input video and the tracking algorithm aims to follow the specified object throughout subsequent frames. The major challenge lies in distinguishing the object from background clutter and occluding objects. To address these issues, recent works <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13]</ref> leverage the strong representation power of deep networks, via either Siamese networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref> or correlation filters <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref> based on network features. Such trackers are usually employed in MOT to provide the raw association weights of possible tracklet-box pairs. In this paper, we utilize a simple Siamese-network-based tracker to obtain the association weights, while also noting the benefits our method can reap from improvements in single object tracking.</p><p>Video Object Detection Research on video object detection gained renewed interest with the introduction of the ImageNet VID benchmark <ref type="bibr" target="#b46">[47]</ref>, which evaluates detection performance on individual frames. Numerous algorithms <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b16">17]</ref> and systems <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b14">15]</ref> have been developed on it, with the main focus of improving perframe object detection results by exploiting temporal information. In large, these works can be classified into boxlevel methods and feature-level techniques.</p><p>Box-level methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b16">17]</ref> primarily follow the "sequential detection and tracking" approach. Bounding boxes are detected based on features from individual frames, and then are associated and rescored across frames. Prior to <ref type="bibr" target="#b16">[17]</ref>, box-level techniques associate boxes across frames by employing external tracking modules. In <ref type="bibr" target="#b16">[17]</ref>, for the first time, object detection and tracking modules share backbone features and are trained end-to-end. The network architecture design in our work follows <ref type="bibr" target="#b16">[17]</ref> in sharing features. However, our inference procedure diverges from <ref type="bibr" target="#b16">[17]</ref>, whose tracking module associates the detected boxes on individual frames at a late stage, like other "sequential detection and tracking" techniques.</p><p>Feature-level techniques <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b68">69]</ref> enhance the quality of per-frame features by integrating temporal information, via flow-guided feature propagation from previous frames. This early exploitation of temporal cues leads to improved detection accuracy. We found that this technique can work in tandem with ours, by utilizing it to obtain the per-image detection scores in our method. Our experiments demonstrate the complementarity of these two approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>We evaluate our method on two popular datasets. The first is ImageNet VID <ref type="bibr" target="#b46">[47]</ref>, a large-scale video set where the object instances are fully annotated. Following the protocol in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25]</ref>, we train our model on the union of the ImageNet VID and ImageNet DET training sets, and test our method on the ImageNet VID validation set. Evaluation is based on the ImageNet VID competition metrics. For detection, it is the mean average precision (mAP det ) score under a boxlevel IoU threshold of 0.5. For tracking, it is the mAP track score, under a box-level IoU threshold of 0.5 and temporallevel thresholds of [0.25, 0.5, 0.75]. All the ablations in this paper are performed on ImageNet VID.</p><p>The other dataset is 2D MOT 2015 <ref type="bibr" target="#b29">[30]</ref> object instances. On this benchmark, submission entries are split into public / private tracks, depending on whether they use the provided set of detection boxes or their own detector. As our work proposes a new detector, we compare it to entries in the private track. Due to the limited training samples, a common practice is to finetune the model on MOT train after training on large-scale external datasets <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b1">2]</ref>. We note that since our approach integrates detection and tracking, we cannot train our detector on datasets consisting of cropped image patches (for person re-ID) as done in some sequential detection and tracking methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b1">2]</ref>. So instead, we train our network on the COCO <ref type="bibr" target="#b35">[36]</ref> training and validation sets for object detection only, and finetune the whole network on the MOT training set for integrated detection and tracking. The standard evaluation metric of this dataset is MOTA, which combines false positives (FP) and false negatives (FN) for object detection, together with ID switch (IDSw) for tracking. The hyper-parameters in training and inference on both datasets are presented in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>To examine the impact of the key components in our integrated object detection and tracking, we perform ablations in an online setting. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. The baseline method excludes tracklet-conditioned detection in Algorithm 2, which is equivalent to a basic version of sequential detection and tracking where box propagation and rescoring are removed in Algorithm 1. This baseline obtains mAP det and mAP track scores of 74.6% and 65.2%, respectively. Applying tracklet-conditioned object detection on either the first or second stages of the object detector leads to improvements in mAP det and mAP track of 0.8% and 1.9%, and 2.2% and 1.6%, respectively. With trackletconditioned detection on both stages, mAP det and mAP track become 78.1% and 67.9%, respectively, which are improvements of 3.5% and 2.7% over the baseline.</p><p>In the sequential counterpart with late integration, applying box propagation improves the mAP det and mAP track scores to 75.2% and 65.9%, respectively. Additionally applying online box rescoring as a postprocess improves the mAP det score to 75.8%. The mAP track score remains the  <ref type="table">Table 2</ref>. Ablation study of hyper-parameter settings and choices for producing the box embedding features in the proposed approach.</p><p>same, because tracklets are not changed by box rescoring.</p><p>To sum up, our full version of integrated detection and tracking outperforms that of sequential detection and tracking with late integration by 2.3% and 2.0% in mAP det and mAP track , respectively. For a more detailed look at our algorithm's performance, following <ref type="bibr" target="#b70">[71]</ref>, we break down the results into different motion speeds, based on whether the ground-truth tracklet is slow (the mean IoU overlap between boxes in consecutive frames is more than 0.8), medium (0.6≤mean IoU≤0.8), or fast (mean IoU&lt;0.6). As shown in <ref type="table" target="#tab_0">Table 1</ref>, the gain in mAP track over the sequential baseline and late integration grows larger with medium and faster object motion. For these more challenging cases, object detection benefits even more from tracklet information, which in turn leads to improved tracking performance.</p><p>We additionally ablate choices for producing the box embedding features used in determining the association weights. The results, displayed in <ref type="table">Table 2</ref>, show that adding a separate 2fc head to produce the first stage embedding features leads to better performance. <ref type="table">Table 2</ref> also shows ablations over hyper-parameter values. The performance was found to be relatively stable with respect to these values, and the best combination was chosen as the default setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Tracklet Stability</head><p>We also analyze the tracklet stability of different approaches. In <ref type="bibr" target="#b65">[66]</ref>, the stability of detection boxes in videos was first studied, using proposed metrics that account for temporal stability (fragment error) and spatial stability (box center and aspect ratio errors). Here, we employ slight modifications of these metrics that instead measure tracklet stability. Details on these metrics are given in the Appendix. <ref type="table">Table 3</ref> compares the difference in stability of our approach to those of sequential detection and tracking with late integration. Our approach reduces the fragment, center and aspect ratio errors by a relative 4%, 6% and 7%, respectively. Improvements in stability are found to be more obvious for objects with fast motion. These numerical results verify the discussion in Section 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Stronger Baselines and Comparison</head><p>to State-of-the-art Approaches</p><p>In <ref type="table">Table 4</ref>, our approach is compared to sequential detection and tracking with late integration on stronger baselines. The network features are enhanced by applying combina-  <ref type="table">Table 3</ref>. Tracking stability change from "sequential detection and tracking with late integration" to "integrated detection and tracking". 'Frag', 'center', and 'aspect' denote fragment, box center, and aspect ratio errors, respectively. The error numbers are shown in the format of "sequential with late  <ref type="table">Table 4</ref>. Improvement on stronger baselines with FGFA <ref type="bibr" target="#b70">[71]</ref>, and Deformable ConvNets v2 (DCNv2) <ref type="bibr" target="#b69">[70]</ref>. The scores are reported in the format of "sequential with late integration→integrated". method inference backbone mAP det mAP track NUIST <ref type="bibr" target="#b23">[24]</ref> off  <ref type="table">Table 5</ref>. Comparison to state-of-the-art systems on the ImageNet VID validation set. In the paper of D&amp;T <ref type="bibr" target="#b16">[17]</ref>, the mAP track score is not reported, so we reproduced the approach and report the results.</p><p>tions of FGFA <ref type="bibr" target="#b70">[71]</ref> and Deformable ConvNets v2 <ref type="bibr" target="#b69">[70]</ref>. On these high baselines, the integrated detection and tracking approach still outperforms the sequential counterpart by a clear margin in both detection and tracking.</p><p>We further compare the proposed approach implemented on the highest baseline to the state-of-the-art methods at the system level. <ref type="table">Table 5</ref> and <ref type="table">Table 6</ref> present the results. We note that due to system complexity and missing implementation details, direct and fair comparison among different works is difficult. Our system of "integrated detection and tracking" achieves accuracy that is very competitive with all the other systems. And we note that the idea of early integration and tracklet-conditioned detection should be appli- cable to these other detection and tracking systems as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Both object detection and tracking are fundamental tasks in video understanding that are closely coupled by nature. However, in the previous approaches, the object detection and tracking modules are applied in a sequential manner, and are optionally integrated at a late stage. In this paper, we propose the first approach to tightly integrate the tasks by conditioning object detection on the current frame by tracklets from the previous frames. The object detection results are not only more accurate, but also more coherent with the existing tracklets, which further improves tracking results. Extensive experiments on the ImageNet VID and the 2D MOT 2015 benchmarks demonstrate the effectiveness of the proposed approach. The idea of early integration and tracklet-conditioned detection can also be applied to other video understanding tasks which involve both recognition and temporal association, such as jointly estimating and tracking human pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Experimental Setting Details</head><p>ImageNet VID dataset <ref type="bibr" target="#b46">[47]</ref> This dataset is a commonly used large-scale benchmark for video object detection and tracking. The training, validation, and test sets contain 3862, 555, and 937 video snippets, respectively. The frame rate is 25 or 30 fps for most snippets. All the object instances are fully annotated with bounding boxes and instance IDs, providing a good benchmark for joint object detection and tracking. There are 30 object categories, which are a subset of the categories in the ImageNet DET dataset.</p><p>Following the protocol in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b70">71]</ref>, in all our experiments, the models are trained on the union of the ImageNet VID training set and the ImageNet DET training set (only the same 30 category labels are used), and are evaluated on the ImageNet VID validation set. In both training and inference, the input images are resized to a shorter side of 600 pixels. In RPN, the anchors are of 3 aspect ratios {1:2, 1:1, 2:1} and 4 scales {64 2 , 128 2 , 256 2 , 512 2 }. 300 region proposals are generated for each frame at an NMS threshold of 0.7 IoU. SGD training is performed, with one image at each mini-batch. 120k iterations are performed on 4 GPUs, with each GPU holding one mini-batch. The learning rates are 10 −3 and 10 −4 in the first 80k and last 40k iterations, respectively. In each mini-batch, images are sampled from ImageNet DET and ImageNet VID at a 1:1 ratio. The weight decay and the momentum parameters are set to 0.0001 and 0.9, respectively. In inference, detection boxes are generated at an NMS threshold of 0.3 IoU.</p><p>2D MOT 2015 <ref type="bibr" target="#b29">[30]</ref> This dataset is a widely used benchmark for multiple object tracking. It contains a total of 22 videos collected under varying scenes, devices and angles. Only the pedestrians are annotated. These videos are divided into 11 training videos and 11 test videos. The training videos have 5500 frames, 500 tracklets, and 39905 boxes. The test videos have 5783 frames, 721 tracklets, and 61440 boxes. The average number of boxes for each frame is 7.3 and 10.6 in the training and test set, respectively. The frame rates of this dataset varies greatly, ranging from 2.5 fps to 30 fps. This dataset is very challenging for pedestrian detection and tracking, due to occlusions, high annotation density, high diversity of scenarios, etc.</p><p>In both training and inference, the input images are resized to a shorter side of 800 pixels. Anchors of 3 aspect ratios {1:2, 1:1, 2:1} and 5 scales {32 2 , 64 2 , 128 2 , 256 2 , 512 2 } are utilized in RPN. 512 and 2000 region proposals are generated on each frame during training and inference at an NMS threshold of 0.7, respectively. In SGD training on COCO for object detection, 120k iterations are performed on 8 GPUs with 2 images per GPU. The learning rate is initialized to 0.02 and is divided by 10 at the 75k and 100k iterations. In finetuning on 2D MOT 2015 for integrated detection and tracking, 110k iterations are performed on 4 GPUs, with each GPU holding one image. The learning rates are 10 −3 and 10 −4 in the first 70k and last 40k iterations, respectively. The weight decay and the momentum parameters are set to 0.0001 and 0.9, respectively. In inference, detection boxes are generated at an NMS threshold of 0.5 IoU. We also utilize common practices developed in previous works <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b57">58]</ref> to better fit the MOTA metric: (1) To reduce FP error, detection boxes with confidence score less than 0.95 are removed, and tracklets with length less than 10 frames are removed; (2) To reduce IDSw error, in online processing, previous tracklets not associated with boxes for 10 consecutive frames are not allowed to be associated with any new boxes in the upcoming frames (but the tracklets are kept in the final results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. Tracklet Stability Metric</head><p>In <ref type="bibr" target="#b65">[66]</ref>, the authors first examined the problem of detection and tracking stability. Three metrics are proposed for evaluating stability, namely, fragment error, center position error, and scale and aspect ratio error. The metrics are applied on the per-frame detection boxes, produced by video object detection algorithms. For stability evaluation, the detection boxes are assigned to pseudo tracklets, aided by the oracle ground-truth annotations. For each groundtruth tracklet, a pseudo tracklet is formed approximately by picking the detection box with the highest overlap with respect to the corresponding ground-truth at each frame 2 . The stability errors are averaged over all the pseudo tracklets. It is not specified in <ref type="bibr" target="#b65">[66]</ref> how to extend their approach to tracklets produced by detection and tracking algorithms.</p><p>Here, we extend <ref type="bibr" target="#b65">[66]</ref> for evaluating the stability of detection and tracking algorithms in a straightforward way. Similar to the approach in <ref type="bibr" target="#b65">[66]</ref>, we seek to find a "best-match" tracklet for each ground-truth tracklet. All the recognized tracklets are first classified into positive and negative tracklets, according to the box IoU and temporal IoU thresholds in the mAP track metric. A positive tracklet is assigned to the ground-truth tracklet with the highest temporal IoU. For each ground-truth tracklet, the tracklet with the highest classification score among all its assigned tracklets is picked as its "best-match". The resulting stability errors are the averaged errors over all the "best-match" tracklets (generated at various box and temporal IoU thresholds as done for mAP track ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visualized detection and tracking results by the previous late integration (top row) and the proposed early integration (bottom row) approaches. Shown are scored bounding boxes prior to non-maximum suppression (NMS), where the boxes are colored according to the corresponding category scores. The highest scored bounding box at each image is kept after NMS as the detection result, and is associated to existing tracklets (or initiates a new tracklet if association fails). The tracklet ID is indicated at the top-left corner of the detection box. More accurate and stable results are generated by the proposed approach of integrating detection and tracking early.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where b t k denotes the detection box associated to tracklet d t j at time t, and η is the update weight parameter (η = 0.8 by default). The box embedding features E s2 (b t i ) are compared to the tracklet embedding features E s2 (d t−1 j ) by Eq. (4) to obtain the association weights for the second stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Tracklet-conditioned two-stage detectors. and followed by non-maximum suppression, as B t := T rackletCondDetect(I t , D t−1 ) and B t := N M S(B t ).As done in Algorithm 1, the detected bounding boxes B t are associated with the existing tracklets D t−1 by D t := AssociateT racklet(D t−1 , B t ). Then the obtained tracklets D t are rescored as D t := RescoreT racklet(D t ), by applying Eq. 3. Finally, the algorithm outputs all the tracklets D T and all the boxes {B t } T t=0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>, suppose b t− 1 i</head><label>1</label><figDesc>and b t−1 k are two candidate boxes properly covering the object 'squirrel' on frame I t−1 , but with slight shifts respectively. For both boxes, the corresponding perframe recognition scores P det (c|b t−1 i ) and P det (c|b t−1 k ) are high. But after NMS, only one of b t−1 i and b t−1 k would be kept. Suppose P det (c|b t−1 k ) is slightly higher, so box b t−1 k is kept and associated with the existing tracklet to form d t−1 j . On frame I t , suppose b t i and b t k are the highest overlapped candidate boxes with b t−1 i and b t−1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>det 78.4 78.1 76.7 78.1 78.1 78.1 78.0 78.1 78.1 76.9 78.1 78.3 78.1 78.1 78.1 77.2 77.4 78.1 mAP track 66.7 67.9 67.9 67.5 67.9 67.4 66.8 67.9 67.4 67.5 67.9 67.8 67.4 67.9 67.7 66.5 67.2 67.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>motion split frag (×10 −3 ) center (×10 −3 ) aspect (×10 −3 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>integration relative change − −−−−−− →integrated".DCNv2 FGFA mAP det mAP track 75.8 → 78.1 65.9 → 67.9 79.4 → 82.0 68.4 → 70.8 81.5 → 83.5 70.2 → 72.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>+P ropagateBox 75.<ref type="bibr" target="#b1">2</ref> 65.9 80.3 53.3 38.9 ++RescoreBox 75.8 65.9 80.3 53.3 38.9 integrated first stage only 75.4 67.1 79.9 55.2 37.9 second stage only 76.8 66.8 80.4 58.0 36.9 both stages 78.1 67.9 80.9 58.1 41.9Ablation of key components of our integrated detection and tracking, and of sequential detection and tracking with late integration, on the ImageNet VID validation set.</figDesc><table><row><cell>, consisting of</cell></row><row><cell>11 training videos and 11 test videos with fully annotated</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>method inference pre-trained MOTA FP FN IDSw H1 SJTUZTE [62] off-line unknown 56.6</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>7198 18926 533</cell></row><row><cell>RAR15 [16]</cell><cell>on-line</cell><cell>D+T</cell><cell>56.5 9386 16921 428</cell></row><row><cell>TRID [40]</cell><cell>off-line</cell><cell>D+T</cell><cell>55.7 6273 20611 351</cell></row><row><cell cols="4">NOMTwSDP [9] off-line unknown 55.5 5594 21322 427</cell></row><row><cell cols="2">AP HWDPL [7] on-line</cell><cell>D+T</cell><cell>53.0 5159 22984 708</cell></row><row><cell cols="2">CDA DDAL [2] on-line</cell><cell>D+T</cell><cell>51.3 7110 22271 544</cell></row><row><cell cols="2">MDP SubCNN [48] on-line</cell><cell>D</cell><cell>47.5 8632 22969 628</cell></row><row><cell>DMT [27]</cell><cell>off-line</cell><cell>D</cell><cell>44.5 8088 25336 684</cell></row><row><cell>Ours</cell><cell>on-line</cell><cell>D</cell><cell>56.1 5717 20460 788</cell></row><row><cell cols="4">Table 6. Comparison to state-of-the-art systems on the 2D MOT15</cell></row><row><cell cols="4">test set. 'D' and 'T' indicate pre-training for the object detection</cell></row><row><cell cols="3">task and the tracking task, respectively.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The candidate boxes can be either dense sliding windows / anchor boxes in the first stage of two-stage object detectors, or sparse region proposals in the second stage.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The implementation in<ref type="bibr" target="#b65">[66]</ref> performs maximum bipartite graph matching with the box IoUs as the weights of the bipartite graph.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Confidence-based data association and discriminative deep appearance learning for robust online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simultaneous object detection, tracking, and event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cognitive Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multiple object tracking using k-shortest paths optimization. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opencv. Dr. Dobbs journal of software tools</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<ptr target="http://image-net.org/challenges/talks_2017/Imagenet2017VID.pdf" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent autoregressive networks for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust object tracking by hierarchical association of detection responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient object detection from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y R F Q M Q L J D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Shuai</surname></persName>
		</author>
		<ptr target="http://image-net.org/challenges/talks/2016/Imagenet%202016%20VID.pptx" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-object tracking with neural gating using bilinear lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cdt: Cooperative detection and tracking for tracing multiple objects in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The hungarian method for the assignment problem. Naval research logistics quarterly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">Motchallenge 2015: Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-class multi-object tracking using changing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdenee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coupled detection and trajectory estimation for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Followme: Efficient online min-cost flow tracking with bounded memory and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint graph decomposition &amp; node labeling: Problem, algorithms, applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pathtrack: Fast trajectory annotation with path supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint tracking and segmentation of multiple targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Online multi-target tracking using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Continuous energy minimization for multitarget tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hedged deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015. 2</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep network flow for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-object tracking with quadruplet convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Subgraph decomposition for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Improving context modeling for video object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D M L X J W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<ptr target="http://image-net.org/challenges/talks_2017/ilsvrc2017_short" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Improving context modeling for video object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="http://http://image-net.org/challenges/talks_2017/ilsvrc2017_short" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Coupling detection and data association for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thangali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Ilsvrc2016 object detection from video: Team nuist</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<ptr target="http://image-net.org/challenges/talks/2016/Imagenet_202016%20VID.pptx" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Poi: Multiple object tracking with high performance detection and appearance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Integrated detection and tracking for multiple moving objects using data-driven mcmc data association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L Y C J F H S J D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="https://motchallenge.net/tracker/H1_SJTUZTE" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>2d mot 2015 leader board</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Gmcp-tracker: Global multi-object tracking using generalized minimum clique graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>ECCV. 2012. 6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Crafting gbd-net for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for visual object tracking in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08936</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06467</idno>
		<title level="m">On the stability of video detection and tracking</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Towards high performance video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv Tech Report</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
