<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAURER, BRUHN: PROFLOW: LEARNING TO PREDICT OPTICAL FLOW ProFlow: Learning to Predict Optical Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maurer</surname></persName>
							<email>maurer@vis.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Visualization and Interactive Systems</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr√©s</forename><surname>Bruhn</surname></persName>
							<email>bruhn@vis.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Visualization and Interactive Systems</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MAURER, BRUHN: PROFLOW: LEARNING TO PREDICT OPTICAL FLOW ProFlow: Learning to Predict Optical Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal coherence is a valuable source of information in the context of optical flow estimation. However, finding a suitable motion model to leverage this information is a non-trivial task. In this paper we propose an unsupervised online learning approach based on a convolutional neural network (CNN) that estimates such a motion model individually for each frame. By relating forward and backward motion these learned models not only allow to infer valuable motion information based on the backward flow, they also help to improve the performance at occlusions, where a reliable prediction is particularly useful. Moreover, our learned models are spatially variant and hence allow to estimate non-rigid motion per construction. This, in turns, allows to overcome the major limitation of recent rigidity-based approaches that seek to improve the estimation by incorporating additional stereo/SfM constraints. Experiments demonstrate the usefulness of our new approach. They not only show a consistent improvement of up to 27% for all major benchmarks (KITTI 2012, KITTI 2015, MPI Sintel) compared to a baseline without prediction, they also show top results for the MPI Sintel benchmark -the one of the three benchmarks that contains the largest amount of non-rigid motion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating the apparent motion from a given image sequence is one of the fundamental problems in computer vision. Typically, one is thereby interested in computing the inter-frame displacement field between consecutive frames, the so-called optical flow. In order to solve this task, many methods rely solely on two frames; see e.g. the recent methods in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35]</ref>. While they allow to obtain good results in most cases, they do not allow an actual reasoning in the context of occlusions. Leveraging information from additional frames could help to overcome their limitation. This, however, requires to formulate explicit motion models that relate the sought displacement vector field to motion estimates from the past.</p><p>While simple models based on a temporally constant flow can be a valid choice in case of sufficiently small motion <ref type="bibr" target="#b15">[16]</ref>, more complex models are required in general scenarios with fast and non-rigidly moving objects. Unfortunately, as observed in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>, finding such models is a highly non-trivial task. Thus, recent multi-frame approaches resort to the scenario of mostly rigid scenes in order to still be able to use temporal information <ref type="bibr" target="#b33">[34]</ref>. Assuming a moving camera and multiple independently moving objects, they exploit temporal information only in rigid parts of the scene -by solving a multi-frame stereo/SfM problem <ref type="bibr" target="#b27">[28]</ref> there. Although the latter strategy can be very beneficial w.r.t. occlusions, it typically c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:1806.00800v1 [cs.CV] 3 Jun 2018 requires a sufficient amount of ego-motion. Moreover, the benefit of temporal information is limited to the rigid parts of the scene. Hence, to exploit the full potential of such information, it would be desirable to come up with a strategy that (i) does not rely on a moving camera and that (ii) allows to leverage temporal information also in non-rigid parts of the scene.</p><p>Contributions. In this paper, we tackle both problems. Instead of assuming a moving camera that comes with rigidity constraints, we propose a novel optical flow method that learns suitable motion models based on a convolutional neural network (CNN). In this context, our contributions are fourfold: (i) In contrast to other approaches that train a network before the estimation, our approach learns the models online, i.e. during the estimation. (ii) Moreover, instead of relying on potentially unsuitable data sets with ground truth, our models are trained using initial flow estimates of the actual sequence. Such an unsupervised training offers the advantage that appropriate models can be learned for each sequence. (iii) Thirdly, our approach not only learns one model per sequence but one model for each frame of every sequence. Evidently, this results in a high degree of adaptability when it comes to a change of the scene content. (iv) Finally, the learned models are spatially variant, i.e. location dependent. This in turn addresses the problem of independently moving objects. Having learned such dedicated motion models eventually enables us to predict the forward flow from the backward flow. Thus it becomes possible to improve the estimation at locations where the forward flow is not available, e.g. in occluded regions. Experiments make the benefits of our novel method explicit. They not only show consistent improvements compared to a baseline approach without prediction but also very good results for all major benchmarks in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>In the following we discuss related work in the field of optical flow estimation. Thereby we first focus on multi-frame methods and then comment on learning-based approaches.</p><p>Multi-Frame Approaches. In order to improve the quality and the robustness of the estimation, multi-frame approaches typically rely on some kind of motion model that describes how objects/pixels are expected to move over time. In this context, recent approaches go far beyond a simple constant velocity model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref> by using constraints based on constant acceleration <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>, parametrized trajectories <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27]</ref> or a moving camera <ref type="bibr" target="#b33">[34]</ref>. Moreover, to avoid a significant deterioration of the results in case the model turns out to be inappropriate, they typically allow deviations from the model either by formulating it as a soft constraint <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32]</ref> or by restricting the estimation to locations where the assumed model is most likely to hold <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>. Compared to most of the aforementioned methods, our methods differs in two ways: On the one hand, our approach does not use hand-crafted or geometric/rigid motion models but learns spatially varying mappings from the backward to the forward flow. On the other hand, our approach uses the learned motion models as a hard constraint, i.e. without any filtering and at all location where the backward flow provides additional information, e.g. at occlusions. The only two methods that are close in spirit to our method are the approaches <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b9">[10]</ref> that learn temporal basis functions for long term trajectories via PCA from pre-computed tracks and flow fields, respectively. However, in contrast to these approaches that focus on a robust long term motion representation to perform dense tracking and non-rigid video registration, respectively, our method aims at the classical short-term optical flow setting and, hence, only our approach is able to provide state-of-the-art results for standard optical flow benchmarks.</p><p>Learning Approaches. Regarding learning approaches for optical flow estimation, one can basically distinguish two types of methods: pure learning-based methods and partially learning-based methods. Pure learning-based methods aim at learning an end-to-end relation between the input images and the corresponding flow field, typically via one or multiple stacked CNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref>. While the overall learning process is quite time-consuming and typically requires a large amount of training data, the learned models allow to compute high quality flow estimates in real-time <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref>. Recently, also unsupervised learning approaches have been considered to tackle the lack of realistic training data; see e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. They either replace the ground truth by a proxy ground truth computed with recent optical flow methods <ref type="bibr" target="#b36">[37]</ref> or they propose a loss function that does not depend on the ground truth, i.e. by using an image-based registration error <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> or some kind of smoothness constraint <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>. Partially learning-based approaches on the other hand, are hybrid methods: They seek to combine the advantages of two worlds. While relying on a transparent global energy minimization framework, they make use of machine learning techniques to replace some difficult task during the modeling or the estimation. These tasks includes descriptor learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>, instance level segmentation <ref type="bibr" target="#b2">[3]</ref>, rigidity estimation <ref type="bibr" target="#b33">[34]</ref>, and semantic scene segmentation <ref type="bibr" target="#b28">[29]</ref>. Although our approach is partially-learning based, since it embeds a CNN into a traditional optical flow pipeline <ref type="bibr" target="#b25">[26]</ref>, it is completely different from all aforementioned learning-based approaches. Not only that the learning step solves a different problem, i.e. it predicts a forward flow from a backward flow, also the training itself is completely different. It uses an unsupervised online approach that relies on initial flow estimates to train the network individually for each frame of the sequence at runtime instead of training the network once for an entire task based on previously collected set of training data. In this respect, our approach is also intrinsically different from the unsupervised methods listed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>Let us start by giving a brief overview over the proposed method; see <ref type="figure" target="#fig_1">Fig. 1</ref>. Please note that, in contrast to classical two frame approaches, our method considers image triplets, i.e. the frames at times t‚àí1, t, and t+1. This allows to compute the optical flow from the reference frame t not only to the subsequent frame t +1 (forward flow) but also to the previous frame t‚àí1 (backward flow). After we have estimated both flows fields with a conventional optical  flow approach, we perform outlier filtering via a bi-directional consistency check. While this requires the additional computation of flow fields using the reversed frame order, it allows us to identify possibly occluded image regions. Based on locations where both the forward and the backward flow are available after filtering, we then learn a model that allows to predict the forward flow from the backward flow. To this end, we train a CNN such that it performs a regression from small backward flow patches to forward flow vectors. Using the trained network, we then predict a new forward flow field from the filtered backward flow. This provides additional information at those locations where only the backward flow is given, e.g. at occlusions. Finally, the new and the initial forward flow field are combined such that predictions are used if no initial forward flow is available. As a last step, we inpaint the combined flow field to obtain dense results and refine it to improve its accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Initial Flow Estimation / Baseline</head><p>In a first step, we compute the initial forward and backward flow field, i.e. the flow fields from frame t to frame t +1 and from frame t to frame t ‚àí1, respectively. To this end, we consider a baseline approach that follows the four steps of the large displacement optical flow pipeline by Revaud et al. <ref type="bibr" target="#b25">[26]</ref>: matching, outlier filtering, inpainting and variational refinement. However, instead of using the original components, we make use of recent progress in the field. For the matching we employ the coarse-to-fine PatchMatch approach (CPM) of Hu et al. <ref type="bibr" target="#b11">[12]</ref>, for the inpainting of the matches we use the robust interpolation technique (RIC) of Hu et al. <ref type="bibr" target="#b12">[13]</ref> and for the final refinement we apply the order-adaptive illumination-aware refinement (OIR) of Maurer et al. <ref type="bibr" target="#b18">[19]</ref>. Only the outlier filtering applied to the initial matches in terms of a bi-directional consistency check remains unchanged. Please note that this check requires to compute matches in the reverse direction as well, i.e. from frame t+1 to frame t and from frame t‚àí1 to frame t, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Outlier Filtering</head><p>After we have computed the initial forward and backward flow field with our baseline approach, we apply another outlier filtering step. Analogously to the bi-directional consistency check that is part of our baseline, this requires to compute flow fields in the reverse direction, i.e. from frame t +1 to frame t and from frame t ‚àí1 to frame t, respectively. Once again, our baseline is used for this task. Finally, only those flow vectors are considered valid in the forward and backward flow field which are consistent with the corresponding vectors in the reverse direction. This allows to eliminate many outliers, in particular in occluded regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning a Motion Model</head><p>Having the filtered forward and backward flow field at hand, let us now discuss how the underlying motion model is learned. The goal of this step is to derive the relation between the backward flow and the forward flow which enables us to use the backward flow for predicting the forward flow at locations where the forward flow is not available. Since motion patterns typically vary across different scenes and frames, we do not use a network that has been trained in advance on a huge data base with ground truth [4, 15, 31, 35], but we apply an unsupervised learning approach that trains a CNN during the optical flow estimation -and that individually for each frame of the sequence. As shown in <ref type="bibr" target="#b8">[9]</ref> in the context of predicting surface normals for multi-view stereo, such unsupervised learning techniques can be highly beneficial for densifying initially sparse results.</p><p>Training Data Extraction. The training data required for the learning process is extracted from the initially computed flow fields after outlier filtering. Thereby, all locations where both the forward and the backward flow surpassed the outlier filtering serve as potential training samples. These potential samples are sampled equidistantly, using a grid spacing of 10 pixels, to finally obtain a reasonably sized and reasonably diverse training set. Thereby, the input of each training sample consists of stacked 7 √ó 7 patches composed of (i) the backward flow components u bw and v bw , (ii) a validity flag {0, 1} indicating if the location surpassed outlier filtering step and (iii) the x-and y-component of the pixel location (normalized to</p><formula xml:id="formula_0">[‚àí1, 1] √ó [‚àí1, 1]).</formula><p>The corresponding output is given by the stacked forward flow components u fw and v fw . The whole process is illustrated <ref type="figure" target="#fig_2">Fig. 2 (left)</ref>. Please note that the training data is extracted automatically per image triplet during the estimation and does not rely on any kind of ground truth information nor manually labeled training data. CNN-based Regression. With the extracted training samples we now train a motion model in terms of a CNN which allows us to predict the forward flow solely based on the backward flow. The input of the network consists of stacked 7 √ó 7 patches including information on the backward flow, the validity and the location as described in the previous section. The output of the network is the predicted forward flow for the center location of the input patch. By considering not only the backward flows in the input patch but also the corresponding image coordinates, the network is enabled to learn a location dependent model. This aspect is particularly important, since motion patterns may locally vary due to independently moving objects, non-rigid deformations as well as perspective effects.</p><p>Let us now detail on the architecture and the training process of our regression network. As loss function we minimize the absolute difference of the predicted flow vector and the actual forward flow vector. Thereby, we kept the network architecture simple, since it has to be trained online for each frame of the sequence: it consists of 2 convolutional layers each with 16 kernels of window size 3 √ó 3 and a fully connected layer with a 2-vector output, which represents the desired predicted forward flow vector; see <ref type="figure" target="#fig_2">Fig. 2 (right)</ref>. As nonlinearities we employed ReLUs <ref type="bibr" target="#b22">[23]</ref>. The network is implemented in TensorFlow <ref type="bibr" target="#b0">[1]</ref> and trained using the ADAM optimizer <ref type="bibr" target="#b17">[18]</ref> with an exponential learning rate decay. The initial learning rate is set to 0.01 and decays every 200 steps with a base of 0.8. Using the described network and learning scheme 4000 steps where sufficient to train the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Combination and Final Estimation</head><p>After learning the motion model in terms of a CNN, we can use it to predict a new forward flow based on the filtered backward flow. The predicted flow vectors can then be employed to augment the initial forward flow at those locations where no flow vectors are present; see <ref type="figure" target="#fig_4">Fig. 3</ref>. Since the combined flow field is not dense -at some locations neither forward nor backward flow vectors are available -we finally perform inpainting and refinement with the same techniques as in our baseline; i.e. RIC <ref type="bibr" target="#b12">[13]</ref> and OIR <ref type="bibr" target="#b18">[19]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To investigate the benefit of our new optical flow approach, which we named ProFlow ("predict optical flow"), we consider the training data sets as well as the test data sets of the three most popular optical flow benchmarks: the KITTI 2012 benchmark <ref type="bibr" target="#b10">[11]</ref>, the KITTI 2015 benchmark <ref type="bibr" target="#b21">[22]</ref> and the MPI Sintel benchmark <ref type="bibr" target="#b5">[6]</ref>.</p><p>Baseline Performance. Since our baseline approach for computing the initial flow fields is not based on a single approach but on a combination of recently published techniques, we first evaluate and compare its performance with those of the original methods. The outcome is listed in Tab. 1 (top). While the results already show some improvements compared to CPM-Flow <ref type="bibr" target="#b11">[12]</ref>, RIC-Flow <ref type="bibr" target="#b12">[13]</ref> and CPM+OIR <ref type="bibr" target="#b18">[19]</ref>, only the DF+OIR <ref type="bibr" target="#b18">[19]</ref> approach performs slightly better. In the latter case the more advanced DiscreteFlow matches <ref type="bibr" target="#b20">[21]</ref> are used that are, however, computationally much more expensive than our CPM-matches <ref type="bibr" target="#b11">[12]</ref>.</p><p>Learned vs. Constant Model. In our second experiment, we compare our learned motion model with the constant motion model that is frequently used in the literature; see e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref>. This model assumes the forward flow w fw and the backward flow w bw to be simply related via w fw = ‚àíw bw . As already mentioned, this model can be a reasonable approximation in case of slowly moving objects <ref type="bibr" target="#b15">[16]</ref>, but it typically does not hold for fast or complex motion scenarios <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Moreover, due to the projection involved in the optical flow, such a constant motion model does not represent an actual constant 3D motion unless the motion is parallel to the image plane. For our comparison we computed the results for the training data sets of all three benchmarks using our approach as well as a modified version, where we omitted the model learning part and directly applied the constant motion model. In Tab. 1 (bottom) we listed the outcome of both approaches. As one can see, using the constant model for predicting the optical flow does not work well for all benchmarks and even leads to a deterioration compared to the baseline. Our approach, in contrast, learns an appropriate motion model and consistently achieves improvements ranging from 8 to 27 percent. This observation is confirmed by the visual comparison in <ref type="figure" target="#fig_5">Figs. 4 and 5</ref> that show the three input frames, the computed flow field and a bad pixel visualization for a sequence of the KITTI 2015 and the MPI Sintel benchmark, respectively. While <ref type="figure">Fig. 4</ref> makes the quan- <ref type="table">Table 1</ref>: Results for the training data sets of the KITTI 2012 benchmark <ref type="bibr" target="#b10">[11]</ref>, the KITTI 2015 benchmark <ref type="bibr" target="#b21">[22]</ref> and the MPI Sintel benchmark <ref type="bibr" target="#b5">[6]</ref> (clean render path) in terms of the average endpoint error (AEE) and the percentage of bad pixels (BP) with a 3px threshold. titative gains for the KITTI benchmark explicit, <ref type="figure" target="#fig_5">Fig. 5</ref> shows that our approach also allows to obtain improvements in non-occluded areas, such as for the head of the dragon.</p><p>Only Prediction. To further investigate the quality of the predicted flow fields, we performed a third experiment, where we skipped the combination step and only used the predicted flow to compute the final flow estimate. Thereby we computed the final estimate in two ways: once by solely inpainting the predicted flow field, i.e. without refinement, and once with the entire pipeline, i.e. with inpainting and refinement. The outcome is listed in Tab. 1 (middle). As one can see, in case of the KITTI 2012 and the KITTI 2015 benchmark, the pure prediction variant even outperforms our baseline. This not only confirms the high quality and reliability of our learned motion models but also reveals that due to the dominating forward motion in the benchmark many occlusions appear at the image boundaries and hence can be resolved by considering information from the preceding frame. The more challenging MPI Sintel benchmark, in contrast, does not contain such a regular motion. Nevertheless, also in this case the learned prediction is still able to achieve reasonable results. For the sake of completeness, we also computed predictions based on the constant motion model. However, as one can see, it does not allow to achieve nearly as good results as the learned approach.</p><p>Comparison to the Literature. In our final experiment we compare the performance of our novel optical flow approach to other methods from the literature. To this end, we submitted results for the test data sets to all three benchmarks. The results are shown in Tab. 2, where we have listed the ten best performing non-anonymous optical flow methods for each benchmark. In case of KITTI 2012 our approach ranks eighth w.r.t. the bad pixel error accounting only for pixels in non-occluded areas (Out-Noc). Since our method aims at improving the estimation in occluded areas, however, the bad pixel measure considering all pixels (Out-All) is more informative. Here, our approach ranks second. In case of the more challenging KITTI 2015 benchmark we also rank eighth. However, on the most challenging and diverse benchmark, the MPI Sintel benchmark, we rank first in the final and second in the clean render path. In particular, in the significantly more challenging final render path, we not only obtain the best result, bus also obtain the lowest error in occluded areas (unmatched) -even outperforming recent multi-frame methods such as MR-Flow <ref type="bibr">[</ref>     <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> and of the MPI Sintel benchmark <ref type="bibr" target="#b5">[6]</ref>, excluding scene flow methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI 2012</head><p>Out-Noc Out-All Avg-Noc Avg-All constraints with a semantic rigidity segmentation. This shows that, in particular in difficult scenes with partially non-rigid motion, learned temporal models may be worthwhile strategy.</p><p>Runtime and Parameters. Running our approach on a desktop PC equipped with an Intel Core i7-7820X CPU @ 3.60GHz and an Nvidia GeForce GTX 1070 the runtime is approximately 112s for a flow field of size 1226√ó370. The overall runtime splits up into: 36s for the initial flow field estimation, 50s for the motion model learning and prediction, and another 26s for the final inpainting and refinement. Regarding the parameters of the used approaches (CPM, RIC, OIR), we used the default parameters as provided by the authors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>. In case of the matching (CPM) and inpainting (RIC) these parameters are the same for all benchmarks, only in case of the refinement (OIR) there is a set of parameters per benchmark.</p><p>Limitations. Finally, we also want to comment on the limitations of our approach. In case of large image regions that only contain poor or possibly no training samples, the validity of the learned motion model may not be able to generalize to the entire image domain. In <ref type="figure" target="#fig_6">Fig. 6</ref> such a scenario is depicted. Due to the missing training samples at the bottom corners of the image, the prediction cannot achieve a noticeable improvement in these areas. This problem, however, could be resolved by additionally using geometric constraints in terms of a rigid motion model. Hence, we believe that combining our learning based approach with such a model could even allow for further improvements -at least in case of rigid scenes with a vast amount of ego-motion, such as the KITTI 2012 and the KITTI 2015 benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We have presented a novel multi-frame optical flow approach that integrates flow predictions based on a CNN. To this end, we made use of an unsupervised learning approach that learns a motion model by estimating a spatially variant mapping from the backward to the forward flow. In contrast to existing approaches from the literature that train their network only once before the estimation based on a huge data set, our methods exploits flow estimates from the current image sequence to learn the model online, i.e. during the estimation. In this way, it becomes possible to learn motion models that are specifically tailored to the actual motion occurring in each frame. Experiments made the good performance of our method explicit. They not only show significant improvements compared to a baseline without prediction, they also show consistently good results in all major benchmarksincluding top results on the MPI Sintel benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Visual Results</head><p>To make the improvements of our approach more graspable, we depict a selection of sequences that contain a variety of different scenarios, i.a. such as non-rigid deformations, occlusions and out-of-frame motion. These can be found in <ref type="figure" target="#fig_7">Fig. 7 -9</ref> (KITTI 2015) and <ref type="figure" target="#fig_1">Fig. 10 -13</ref> (MPI Sintel). On the one hand, one can see from visualization of selected areas that predictions by the learned motion models (orange) are mainly used in occluded regions while the initial forward flow (turquois) is used elsewhere. On the other hand, one can observe from the baseline and the final flow field (our approach) that this typically leads to a noticeable improvement of the estimation quality.        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Visualization of Intermediate Results</head><p>In order to provide a better insight into the overall approach, we show intermediate results of the different steps for the two exemplary sequences depicted in the main paper. These intermediate results are presented in <ref type="figure" target="#fig_1">Fig. 14</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>CNN</head><label></label><figDesc>initial forward flow (t ‚Üí t + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Schematic overview over our optical flow approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Left: Training sample extraction. Right: Regression network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Illustration showing the combination step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Example for the MPI Sintel benchmark<ref type="bibr" target="#b5">[6]</ref> (market_5 #8). First row: Previous, reference and subsequent frame. Second and third row: Estimated flow field, bad pixel visualization. From left to right: Baseline, constant motion model and our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Limitations example (#0 KITTI 2015 benchmark [22]). First row: Overlayed reference and subsequent input frame, final flow estimate, bad pixel visualization. Second row: Filtered forward flow, filtered backward flow, possible training candidates (white).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of improvements for Sequence #199 of KITTI 2015<ref type="bibr" target="#b21">[22]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of improvements for Sequence #133 of KITTI 2015<ref type="bibr" target="#b21">[22]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of improvements for Sequence #186 of KITTI 2015<ref type="bibr" target="#b21">[22]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Visualization of improvements for the ambush_7 sequence (#9) of MPI Sintel<ref type="bibr" target="#b5">[6]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Visualization of improvements for the bamboo_2 sequence (#43) of MPI Sintel<ref type="bibr" target="#b5">[6]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Visualization of improvements for the ambush_5 sequence (#31) of MPI Sintel<ref type="bibr" target="#b5">[6]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Visualization of improvements for the cave_2 sequence (#13) of MPI Sintel<ref type="bibr" target="#b5">[6]</ref>.(i) Input Frames (ii) Initial Flow Estimation &amp; Outlier Filtering (iii) Model Learning &amp; Prediction (iv) Combination &amp; Final Estimation Overview showing intermediate results of our optical flow approach for the mar-ket_5 #8 sequence of the MPI Sintel benchmark [6].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>Overview showing intermediate results of our optical flow approach for the #149 sequence of the KITTI 2015 benchmark<ref type="bibr" target="#b21">[22]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>and Fig. 15 .</head><label>15</label><figDesc>They include: (i) the three input images with the reference frame at time t (ii) the initial forward and backward flow fields, the auxiliary flow fields (required to perform the bi-directional consistency check), as well as the filtered flow field (iii) the considered training samples, the predicted forward flow (computed using the backward flow and the learned motion model), and a bad pixel (BP) visualization of the predicted flow field (iv) the decisions within the combination step (turquois=forward, orange=prediction), the inpainted combined flow field and the final flow estimate (refined combination)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Top 10 non-anonymous optical flow methods on the test data of the KITTI 2012/2015</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank the German Research Foundation (DFG) for financial support within project B04 of SFB/Transregio 161. Moreover, we thank Lourdes Agapito for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/.Softwareavailablefromtensor-flow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised convolutional neural networks for motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Image Processing</title>
		<meeting>International Conference on Image essing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting semantic information and deep matching for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CNN-based patch matching for optical flow with thresholded Hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2710" to="2719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust dynamic motion estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="296" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H√§usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PatchBatch: A batch augmented loss for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gadot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4236" to="4245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Just look at the image: Viewpoint-specific surface normal prediction for improved multi-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5479" to="5487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A variational approach to video registration with subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="286" to="314" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient coarse-to-fine PatchMatch for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5704" to="5712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust interpolation of correspondences for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="481" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MirrorFlow: Exploiting symmetries in joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Slow flow: Exploiting high-speed cameras for accurate and diverse optical flow reference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G√ºney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3597" to="3607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optical flow with geometric occlusion estimation and fusion of multiple frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="364" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Order-adaptive and illumination-aware variational optical flow refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discrete optimization for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. German Conference on Pattern Recognition</title>
		<meeting>German Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="16" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optical flow using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2720" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dense Lagrangian motion estimation with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3D geometry from planar parallax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="929" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optical flow with semantic segmentation and localized layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling temporal coherence for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Volz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1116" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Anistropic Huber-L1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Trobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optical flow in mostly rigid scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6911" to="6920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5807" to="5815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshops European Conference on Computer Vision</title>
		<meeting>Workshops European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Guided optical flow learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshops Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Workshops Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
