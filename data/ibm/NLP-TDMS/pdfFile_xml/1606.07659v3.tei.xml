<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid Recommender System based on Autoencoders Romaric Gaudel</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
							<email>florian.strub@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Centrale Lille</orgName>
								<orgName type="institution" key="instit4">Inria Jérémie Mary</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Centrale Lille, Inria</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Centrale Lille, Inria</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hybrid Recommender System based on Autoencoders Romaric Gaudel</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proficient Recommender Systems heavily rely on Matrix Factorization (MF) techniques. MF aims at reconstructing a matrix of ratings from an incomplete and noisy initial matrix; this prediction is then used to build the actual recommendation. Simultaneously, Neural Networks (NN) met tremendous successes in the last decades but few attempts have been made to perform recommendation with autoencoders. In this paper, we gather the best practice from the literature to achieve this goal. We first highlight the link between these autoencoder based approaches and MF. Then, we refine the training approach of autoencoders to handle incomplete data. Second, we design an end-to-end system which handles external information. Finally, we empirically evaluate these approaches on the MovieLens and Douban dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recommendation systems advise users on which items (movies, music, books etc.) they are more likely to be interested in. A good recommendation system may dramatically increase the number of sales of a firm or retain customers. For instance, 80% of movies watched on Netflix come from the recommender system of the company <ref type="bibr" target="#b5">[Gomez-Uribe and Hunt, 2015]</ref>. Collaborative Filtering (CF) aims at recommending an item to a user by predicting how a user would rate this item. To do so, the feedback of one user on some items is combined with the feedback of all other users on all items to predict a new rating. For instance, if someone rated a few books, CF objective is to estimate the ratings he would have given to thousands of other books by using the ratings of all the other readers. The most successful approach in CF is to factorize an incomplete matrix of ratings <ref type="bibr" target="#b6">[Koren et al., 2009;</ref>. This approach simultaneously learns a representation of users and items that encapsulate the taste, genre, writing styles, etc. A well-known limit of this approach is the cold start setting: how to recommend an item to a user when few rating exists for either the user or the item?</p><p>While Deep Learning methods start to be used for several scenarios in recommendation system that goes from dialogue systems <ref type="bibr" target="#b12">[Wen et al., 2016]</ref> to temporal user-item interactions <ref type="bibr" target="#b4">[Dai et al., 2016]</ref> through heterogeneous classification applied to the recommendation setting <ref type="bibr" target="#b3">[Cheng et al., 2016]</ref>, few attempts have been done to use Neural Networks (NN) in CF. Deep Learning key successes have mainly been on fully observable data <ref type="bibr" target="#b8">[LeCun et al., 2015]</ref> while CF relies on input with missing values. This constraint has received less attention and remains a challenging problem for NN. Handling missing values for CF has first been tackled by <ref type="bibr" target="#b11">Salakhutdinov et al. [Salakhutdinov et al., 2007]</ref> for the Netflix challenge by using Restricted Boltzmann Machines. More recent works train autoencoders to perform CF tasks on explicit data <ref type="bibr" target="#b11">[Sedhain et al., 2015;</ref><ref type="bibr" target="#b11">Strub et al., 2016]</ref> and implicit data <ref type="bibr" target="#b13">[Zheng et al., 2016]</ref>. Other approaches rely on recurrent networks . Although those methods report excellent results, they ignore the cold start scenario and provide little insights.</p><p>The key contribution of this paper is to collect the best practices of autoencoder approaches in order to standardize the use of autoencoders for CF tasks. We first highlight autoencoders perform a factorization of the matrix of ratings. Then we contribute to an end-to-end autoencoder based approach in two points: (i) we introduce a proper training loss/process of autoencoders on incomplete data, (ii) we integrate the side information to autoencoders to alleviate the cold start problem. Compared to previous attempts in that direction <ref type="bibr" target="#b11">[Salakhutdinov et al., 2007;</ref><ref type="bibr" target="#b11">Sedhain et al., 2015;</ref><ref type="bibr" target="#b13">Wu et al., 2016]</ref>, our framework integrates both ratings and side information into a unique network. This joint model leads to a scalable and robust approach which beats stateof-the-art results in CF. Reusable source code is provided in Lua/Torch to reproduce the results.</p><p>The paper is organized as follows. First, Sec. 2 fixes the setting and highlights the link between autoencoders and matrix factorization in the context of CF. Then, Sec. 3 describes our model. Finally, Sec. 4 details several experimental results from our approach.  <ref type="bibr" target="#b6">[Koren et al., 2009]</ref>. Given N users and M items, we denote r ij the rating given by the i th user for the j th item. It entails an incomplete matrix of ratings R ∈ R N ×M . MF aims at finding a rank k matrix R ∈ R N ×M which matches known values of R and predicts unknown ones. Typically,</p><formula xml:id="formula_0">R = UV T with U ∈ R N ×k , V ∈ R M ×k and (U ,V) the solution of arg min U,V (i,j)∈K(R) (r ij − u T i v j ) 2 + λ( u i 2 F + v j 2 F ),</formula><p>where K(R) is the set of indices of known ratings of R, (u i , v j ) are column-vectors of the low rank rows of (U, V) and . F is the Frobenius norm. In the following, r i,. and r .,j will respectively be the i-th row and j-th column of R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Autoencoder based Collaborative Filtering</head><p>Recently, autoencoders have been used to handle CF problems <ref type="bibr" target="#b11">[Sedhain et al., 2015;</ref><ref type="bibr" target="#b11">Strub et al., 2016]</ref>. Autoencoders are NN popularized by Kramer <ref type="bibr" target="#b7">[Kramer, 1991]</ref>.They are unsupervised networks where the output of the network aims at reconstructing the input. The network is trained by backpropagating the squared error loss on the output. More specifically, when the network limits itself to one hidden layer, its output is</p><formula xml:id="formula_1">nn(x) def = σ(W 2 σ(W 1 x + b 1 ) + b 2 ), with x ∈ R N the input, W 1 ∈ R k×N and W 2 ∈ R N ×k the weight matrices, b 1 ∈ R k and b 2 ∈ R N</formula><p>the bias vectors, and σ(.) a non-linear transfer function.</p><p>In the context of CF, the autoencoder is fed with incomplete rows r i,. (resp. columns r .,j ) of R <ref type="bibr" target="#b11">[Sedhain et al., 2015;</ref><ref type="bibr" target="#b11">Strub et al., 2016]</ref>. It then outputs a vectorr i,. (resp.r .,j ) which predict the missing entries. Note that these approaches perform a non-linear low-rank approximation of R. Using MF notations and assuming that the network works on rows r i,. of R, we recover a predicted vectorr i,. of the form r i,. = σ (Vu i ):</p><formula xml:id="formula_2">r i,. = nn(r i,. ) = σ      [W 2 I N ] V σ(W 1 r i,. + b 1 ) b 2 u i      .</formula><p>The activation of the hidden units of the autoencoder u i iteratively builds the low rank matrix U. Besides, the final matrix of weights corresponds to the low rank matrix V. In the end, the output of the autoencoder performs a non linear matrix factorizationR = σ UV T . Identically, it is possible to iterate over the columns r .,j of R to computer .,j and get the final matrixR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Challenges</head><p>While the link between MF and autoencoders is straightforward, there is no guarantee that autoencoders can successfully perform matrix completion from incomplete data. Indeed, most of the prominent results with autoencoders such as word2vec <ref type="bibr" target="#b11">[Mikolov et al., 2013]</ref> only deal with complete vectors. The task with missing data is all the more difficult as the missing data have an impact on both input and target vectors. <ref type="bibr" target="#b11">Miranda et al. [Miranda et al., 2012]</ref> study the impact of missing values on autoencoders with industrial data. Yet, they only have 5% of missing values in her dataset, whereas CF tasks usually have more than 95% of missing values. Autorec <ref type="bibr" target="#b11">[Sedhain et al., 2015]</ref> handles the missing values by associating one autoencoder per sample, whose input size matches the number of known values. The corresponding weight matrices are shared among the autoencoders. Even if this approach is intellectually appealing, it faces technical limitations. First, sharing weights among networks prevent efficient computations (especially on GPU). Secondly, it prevents gradient optimization methods such as momentum, weight decay, etc. from being applied to missing data. In the rest of the paper, we introduce a new method based on a single autoencoder to fully regain the benefits of NN techniques. Although this architecture is mathematically equivalent to a mixture of autoencoders <ref type="bibr" target="#b11">[Sedhain et al., 2015]</ref>, it turns out to be a more flexible framework.</p><p>CF systems also face the cold start problem. The main solution is to supplant the lack of ratings by integrating side information. Some approaches <ref type="bibr" target="#b1">[Burke, 2002]</ref> mix CF with a second system based only on side information. Recent work tends to incorporate the side information into the matrix completion by modifying the training error <ref type="bibr" target="#b0">[Adams et al., 2010;</ref><ref type="bibr">Rendle, 2010;</ref><ref type="bibr" target="#b11">Porteous et al., 2010]</ref>. In this line of research, some papers incorporate NN embedding on side information. For instance, <ref type="bibr" target="#b12">[Wang et al., 2014]</ref> respectively auto-encode bag-of-words from movie plots, <ref type="bibr" target="#b9">[Li et al., 2015]</ref> auto-encode heterogeneous side information from users and items. Finally, <ref type="bibr" target="#b12">[Wang and Wang, 2014]</ref> uses convolutional networks on music samples. From the best of our knowledge, training a NN from end to end on both ratings and heterogeneous side information has never been done. In Sec. 3.2, we build such NN by integrating side information into our CF system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">End-to-End Collaborative Filtering with Autoencoders</head><p>Our approach builds upon an autoencoder to predict full vectorsr i,. /r .,j from incomplete vectors r i,. /r .,j . As in <ref type="bibr" target="#b11">[Salakhutdinov and Mnih, 2008;</ref><ref type="bibr" target="#b11">Sedhain et al., 2015;</ref><ref type="bibr" target="#b11">Strub et al., 2016]</ref>, we define two types of autoencoders: U-CFN is defined asr i,. = nn(r i,. ) and predicts the missing ratings given by the users; I-CFN is defined asr .,j = nn(r .,j ) and predicts the missing ratings given to the items. First, we design a training process to predict missing ratings from incomplete vectors. Secondly, we extend CF techniques using side information to autoencoders to improve predictions for users/items with few ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Handling Incomplete Data</head><p>MC tasks introduce two major difficulties for autoencoders.</p><p>The training process must handle incomplete input/target vectors. The other challenge is to predict missing ratings as opposed to the initial goal of autoencoders to rebuild the initial input. We handle those obstacles by two means. <ref type="figure">Figure 1</ref>: Training steps for autoencoders with incomplete data. The input is drawn from the matrix of ratings, unknown values are turned to zero, some ratings are masked (corruption) and a dense estimate is obtained. Before backpropagation, unknown ratings are turned to zero error, prediction errors are reweighted by α and reconstruction errors are reweighted by β.</p><p>First, we inhibit input and output nodes corresponding to missing data. For input nodes, the inhibition is obtained by setting their value to zero. To inhibit the back-propagated unknown errors, we use an empirical loss that disregards the loss of unknown values. No error is back-propagated for missing values, while the error is back-propagated for actual zero values.</p><p>Second, we shake the training data and design a specific loss function to enforce reconstruction of missing values. Indeed, basic autoencoders loss function only aims at reconstructing the initial input. Such approach misses the point that CF goal is to predict missing ratings. Worse, there is little interest to reconstruct already known ratings. To increase the generalization properties of autoencoders, we apply the Denoising AutoEncoder (DAE) approach <ref type="bibr">[Vincent et al., 2008]</ref> to the CF task we handle. The DAE approach corrupts the input vectors and lets the network denoise the outputs. The corresponding loss function reflects that objective and is based on two main hyperparameters: α and β. They balance whether the network would focus on denoising the input (α) or reconstructing the input (β). In our context, the data are corrupted by masking a small fraction of the known rating. This corruption simulates missing values in the training process to train the autoencoders to predict them. We also set α &gt; β to emphasize the prediction of missing ratings over the reconstruction of known ratings. In overall, the training loss with regularization is:</p><formula xml:id="formula_3">L α,β (x,x) = α   j∈K(x)∩C(x) (nn(x) j − x j ) 2   + β   j∈K(x)\C(x) (nn(x) j − x j ) 2   + λ W 2 F ,<label>(1)</label></formula><p>wherex is the corrupted input, C contains the indices of corrupted elements inx, K(x) contains the indices of known values of x, W is the flatten vector of weights of the network and λ is the regularization parameter. Note that the regularization is applied to the full matrix of weights as opposed to <ref type="bibr" target="#b11">[Sedhain et al., 2015]</ref>. The overall training process is described in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Integrating Side Information</head><p>CF only relies on the feedback of the users regarding a set of items. However, adding more information may help to increase the prediction accuracy and robustness. Furthermore, CF suffers from the cold start problem: when little information is available on an item, it will greatly lower the prediction accuracy. We now integrate side information to CFN to alleviate the cold start scenario.</p><p>The simplest approach to integrate side information to MF techniques is to append a user/item bias to the rating prediction <ref type="bibr" target="#b6">[Koren et al., 2009]</ref></p><formula xml:id="formula_4">:r ij = u T i v j + b u,i + b v,j + b , where b u,i , b v,j , b</formula><p>are respectively the user, item, and global bias. These biases are computed with hand-crafted engineering or CF technique <ref type="bibr" target="#b2">[Chen et al., 2012;</ref><ref type="bibr" target="#b11">Porteous et al., 2010;</ref><ref type="bibr">Rendle, 2010]</ref>. For instance, side information can directly be concatenated to the feature vector u i /v j of rank k. Therefore, the estimated rating is computed by:</p><formula xml:id="formula_5">r ij = {u i , x i } ⊗ {v j , y j } (2) def =u T [1:k],i v [1:k],j + x T i v [k+1:k+P ],j bu,i + u T [k+1:k+Q],i y j bv,j ,<label>(3)</label></formula><p>where x i ∈ R P and y j ∈ R Q encodes user/item side information. Unfortunately, those methods cannot be directly applied to NNs as autoencoders optimize U and V independently. However, it is possible to retrieve similar equations by (i) appending side information to incomplete input vectors, (ii) injecting side information to every layer of the autoencoders as in <ref type="figure">Fig. 2</ref>. By example, with U-CFN we get the following output:</p><formula xml:id="formula_6">nn({r i,. , x i }) = σ(V { u i σ(W 1 {r i,. , x i } + b 1 ), x i } + b 2 ) = σ(V [1:k] u i + V [k+1:k+P ] x i bu,i + b 2 [bv,1...b v,M ] T ),<label>(4)</label></formula><p>where <ref type="bibr">:k+P ]</ref> ∈ R N ×P are respectively the submatrices of V that contain the columns from 1 to k and k + 1 to k +P . Injecting side information to the last hidden layer as in Eq. 4 enables to partially retrieve the error function of classic <ref type="figure">Figure 2</ref>: Side information is wired to all the neuron.</p><formula xml:id="formula_7">V ∈ R (N ×k+P ) is a weight matrix, V [1:k] ∈ R N ×k , V [k+1</formula><p>hybrid systems described in Eq. 2. Secondly, injecting side information to the other intermediate hidden layers enhance the internal representation. Finally, appending side information to the input vectors supplants the absence of input data when no rating is available. The autoencoder fits CF standards while being trained end-to-end by backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we empirically evaluate CFN on two major CF datasets: MovieLens and Douban. We first describe the experimental settings before introducing the benchmark models. Finally, we provide an extensive analysis of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Experiments are conducted on MovieLens and Douban datasets as they are among the biggest CF dataset freely available at the time of writing. Besides, MovieLens contains side information on the items and it is a widespread benchmark while Douban provides side information on the users. The MovieLens-1M, MovieLens-10M and MovieLens-20M respectively provide 1/10/20 million discrete ratings from 6/72/138 thousands users on 4/10/27 thousands movies. Side information for MovieLens-1M is the age, sex and gender of the user and the movie category (action, thriller etc.). Side information for MovieLens-10/20M is a matrix of tags T where T ij is the occurrence of the j th tag for the i th movie and the movie category. No side information is provided for users. The Douban dataset <ref type="bibr" target="#b11">[Ma et al., 2011]</ref> provides 17 million discrete ratings from 129 thousand users on 58 thousands movies. Side information is the bi-directional user/friend relations for the user. The user/friend relations are treated like the matrix of tags from MovieLens. No side information is provided for items. Finally, we perform 90%-10% train-test sets as reported in <ref type="bibr" target="#b9">[Lee et al., 2013;</ref><ref type="bibr" target="#b10">Li et al., 2016;</ref><ref type="bibr" target="#b13">Zheng et al., 2016]</ref>.</p><p>Preprocessing For each dataset, we first centered the ratings to zero-mean by row (resp. by col) for U-CFN (resp I-CFN): denoting b i the mean of the i th user and b j the mean of the j th item, U-CFN and I-CFN respectively learn from r unbiased ij = r ij − b i and r unbiased ij = r ij − b j . Then all the ratings are linearly rescaled from -1 to 1 to fit the output range of the autoencoder transfer functions. Theses operations are finally reversed while evaluating the final matrix.</p><p>Side Information As the dimensionality of the side information is huge, we first perform a dimension reduction of the matrix of tags/friendships. To do so, we use a low rank matrix factorization in our experiments. For a different nature of data as texts or pictures (which are not available in our dataset), CFN can directly learn this embedding as <ref type="bibr" target="#b12">[Wang and Wang, 2014;</ref><ref type="bibr" target="#b12">Wang et al., 2014;</ref><ref type="bibr" target="#b9">Li et al., 2015]</ref>. Formally, we use the left part of a matrix factorization of the tag matrix T. From T = PDQ T with D the diagonal matrix of eigenvalues sorted in descending order, the movie tags are represented by Y = P J×K D 0.5 K ×K with K the number of kept eigenvectors. Binary representation such as the movie category is concatenated to Y.</p><p>Error Function The algorithms are compared based on their respective Root Mean Square Error (RMSE) on test data. Denoting R test the matrix of test ratings and R the full matrix returned by the learning algorithm, the RMSE is:</p><formula xml:id="formula_8">L( R, R test ) = 1 |K(R test )| (i,j)∈K(R test ) (r test ij −r ij ) 2 ,<label>(5)</label></formula><p>where |K(R test )| is the number of ratings in the testing dataset. Note that in the case of autoencoders R is computed by feeding the network with training data. As such,r ij stands for nn(r train i,.</p><p>) j for U-CFN, and nn(r train .,j ) i for I-CFN.</p><p>Training Settings We train a one-hidden layer autoencoders with hyperbolic tangent transfer functions. The layers have 600 hidden neurons. Weights are randomly initialized with a uniform law</p><formula xml:id="formula_9">W ij ∼ U [−1/ √ n, 1/ √ n].</formula><p>The latent dimension of the low rank matrix of tags/friendships is set to 50. Hyperparamenters were are fine-tuned by a genetic algorithm and the final learning rate, learning decay and weight decay are respectively set to 0.7, 0.3 and 0.5. α, β and masking ratio are set to 1, 0.5 and 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source code</head><p>In order to ensure easy reproducibility and reuse the experimental results, we provide the code in an outof-the-box tutorial in Torch. 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmark Models</head><p>We benchmark CFN with five matrix completion algorithms:</p><p>• ALS-WR (Alternating Least Squares with Weighted-λ-Regularization)  solves the low-rank MF problem by alternatively fixing U and V and solving the resulting linear regression problem. Experiments are run with the Apache Mahout Software 2 with a rank of 200; • SVDFeature <ref type="bibr" target="#b2">[Chen et al., 2012]</ref> learns a feature-based MF : side information is used to predict the bias term and to reweight the matrix factorization. We use a rank of 64 and tune other hyperparameters by random search; • BPMF (Bayesian Probabilistic Matrix Factorization) infers the matrix decomposition after a statistical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithms</head><p>MovieLens-1M MovieLens-10M MovieLens-20M Douban BPMF 0.8705 ± 4.3e-3 0.8213 ± 6.5e-4 0.8123 ± 3.5e-4 0.7133 ± 3.0e-4 ALS-WR 0.8433 ± 1.8e-3 0.7830 ± 1.9e-4 0.7746 ± 2.7e-4 0.7010 ± 3.2e-4 SVDFeature 0.8631 ± 2.5e-3 0.7907 ± 8.4e-4 0.7852 ± 5.4e-4 * LLORMA 0.8371 ± 2.4e-3 0.7949 ± 2.3e-4 0.7843 ± 3.2e-4 0.6968 ± 2.7e-4 I-Autorec 0.8305 ± 2.8e-3 0.7831 ± 2.4e-4 0.7742 ± 4.4e-4 0.6945 ± 3.1e-4 U-CFN 0.8574 ± 2.4e-3 0.7954 ± 7.4e-4 0.7856 ± 1.4e-4 0.7049 ± 2.2e-4 U-CFN++ 0.8572 ± 1.6e-3 N/A N/A 0.7050 ± 1.2e-4 I-CFN 0.8321 ± 2.5e-3 0.7767 ± 5.4e-4 0.7663 ± 2.9e-4 0.6911 ± 3.2e-4 I-CFN++ 0.8316 ± 1.9e-3 0.7754 ± 6.3e-4 0.7652 ± 2.3e-4 N/A <ref type="table">Table 1</ref>: RMSE on MovieLens-10M (90%/10%). The ++ suffix denotes when side information is added to CFN.</p><p>As a bayesian algorithm, the performances can be improved by the fine tuning of priors over the parameters Here, we use the recommendations of <ref type="bibr" target="#b11">[Salakhutdinov and Mnih, 2008]</ref> for priors and rank (set to 10). • LLORMA estimates the rating matrix as a weighted sum of low-rank matrices. Experiments are run with the Prea API 3 . We use a rank of 20, 30 anchor points which entail a global pseudo-rank of 600. Other hyperparameters are picked as recommended in <ref type="bibr" target="#b9">[Lee et al., 2013]</ref>; <ref type="bibr">et al., 2015]</ref> trains one autoencoder per item, sharing the weights between the different autoencoders. We use 600 hidden neurons with the training hyperparameters recommended by the author. In every scenario, we select the highest possible rank which does not lead to overfitting despite a strong regularization. For instance, increasing the rank of BPMF does not significantly increase the final RMSE, idem for SVDFeature. Similar benchmarks exist in the literature <ref type="bibr" target="#b9">[Lee et al., 2013;</ref><ref type="bibr" target="#b10">Li et al., 2016;</ref><ref type="bibr" target="#b13">Zheng et al., 2016]</ref>.</p><formula xml:id="formula_10">• I-Autorec [Sedhain</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">General Results</head><p>Comparison to state-of-the-art Tab. 1 summarizes the RMSE on MovieLens and Douban datasets. Confidence intervals correspond to a 95% range. I-CFNs have excellent performance for every dataset we run. It is competitive compared to the state-of-the-art CF algorithms and outperforms them for MovieLens-10M. To the best of our knowledge, the best result published for MovieLens-10M (without side information) has a final RMSE of 0.7682 <ref type="bibr" target="#b10">[Li et al., 2016]</ref>. Note that I-CFN outperforms U-CFN as shown in <ref type="figure" target="#fig_1">Fig. 5</ref>. It suggests that the structure of the items is stronger than the one on the users i.e. it is easier to guess tastes based on movies you liked than to find users similar to you. Yet, this behavior could be different on another dataset. Finally, I-CFN outperforms Autorec on big dataset while both methods rely on autoencoder. In addition to the DAE loss, CFN benefits from regularizing missing ratings during the training as described in Eq. 1. Thus, uncommon ratings are more regularized and they turn out to be less overfitted.</p><p>Impact of side information At first sight at Tab. 1, the use of side information has a limited impact on the RMSE. This statement has to be mitigated: as the repartition of known 3 http://prea.gatech.edu/ <ref type="figure">Figure 3</ref>: Impact of the denoising loss in the training process. α = 1 is kept constant while varying the reconstruction hyperparameter β and the masking ratio . entries in the dataset is not uniform, the estimates are biased towards users and items with a lot of ratings. For theses users and movies, the dataset already contains a lot of information, thus having some extra information will have a marginal effect. Users and items with few ratings should benefit more from some side information but the estimation bias hides them. In order to exhibit the utility of side information, we report in Tab. 2 the RMSE conditionally to the number of missing values for items. As expected, the fewer number of ratings for an item, the more important is the side information. This is very desirable for a real system: the effective use of side information to the new items is crucial to deal with the flow of new products. A more careful analysis of the RMSE improvement in this setting shows that the improvement is uniformly distributed over the users whatever their number of ratings. This corresponds to the fact that the available side information is only about items. Finally, we train I-CFN on MovieLens-10M (90%/10%) with either the movie genre or the matrix of tags. Individually picked, side information increases the global RMSE by 0.10% while concatenating them increases the final score by 0.14%. Therefore, I-CFN handles the heterogeneity of side information.</p><p>Impact of the loss The DAE loss positively impacts the final RMSE on big dataset as highlighted in <ref type="figure">Fig. 3</ref> when carefully balanced. Surprisingly, the autoencoder has already good generalization properties while only focusing on the reconstruction criterion (no masking). More importantly, the reconstruction criterion cannot be discarded (β = 0) to learn an efficient representation.    Impact of the non-linearity We removed the non-linearity from I-CFN to study its relative impact. For fairness, we kept α, β, the masking ratio and the number of hidden neurons constant. We fine-tune the learning rates and weight decay. For MovieLens-10M, we obtain a final RMSE of 0.8151 ± 1.4e-3 which is far worse than classic non-linear I-CFN.</p><p>Impact of the training ratio CFN remains very robust to a variation of data density as shown in <ref type="figure" target="#fig_0">Fig. 4</ref>. It is all the more impressive that hyperparameters are first optimized for movieLens-10M (90%/10%). Cold-start and warm-start scenario are also far more well-handled by NNs than more classic CF algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CFN Tractability</head><p>One major problem faced by CF algorithms is scalability as CF datasets contain hundred of thousands of users and items.  Recent advances in GPU computation managed to reduce the training time of NNs by several orders of magnitude. CFN (and ALS-WR) fully benefits from those advances and is trained within a few minutes as shown in Tab. 3. On the other side, CF gradient based methods such as SVDFeature <ref type="bibr" target="#b2">[Chen et al., 2012]</ref> cannot be easily parallelized or used on GPU as they are mainly iterative. Similarly, Autorec <ref type="bibr" target="#b11">[Sedhain et al., 2015]</ref> suffers from synchronization and memory fetching latencies because of the shared weights among autoencoders. Furthermore, CFN has the key advantage to provide excellent performance while being able to refine its prediction on the fly for new ratings. Thus, U-CFN/I-CFN does not need to be retrained for new items/users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we highlight the connections between autoencoders and matrix factorization for matrix completion. We pack some modern training techniques -as well than some code -able to defeat state of the art methods while remaining scalable. Moreover, we propose a systematic way to integrate side information without the need to combine two separate systems. To some extent, this work extends the construction of embeddings by neural networks to the collaborative filtering setting. A natural follow-up is to work with deeper architectures using batch normalization <ref type="bibr" target="#b11">[Sergey and Szegedy, 2015]</ref>, adaptive gradient methods (such as ADAM [Kingma and Ba, 2014]) or residual networks <ref type="bibr" target="#b5">[He et al., 2015]</ref>. Other extensions could use recurrent networks to grasp the sequential aspect of the collection of ratings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>RMSE vs training set ratio for MovieLens-10M. Training hyperparameters are kept constant across dataset. CFN and I-Autorec are very robust to a change in the density while SVDFeature must be refined each time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>RMSE by epoch for CFN for MovieLens-10M (90%/10%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>RMSE computed by clusters of items sorted by the number of ratings on MovieLens-10M (90%/10%). For instance, the first cluster contains the 20% of items with the lowest number of ratings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Training time and memory footprint for a 2-layers CFN on GTX 980 for 20 epochs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/fstrub95/Autoencoders_cf 2 http://mahout.apache.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to acknowledge the stimulating environment provided by SequeL research group, Inria and CRIStAL. This work was supported by French Ministry of Higher Education and Research, by CPER Nord-Pas de Calais / FEDER DATA Advanced data science and technologies 2015-2020, the Projet CHIST-ERA IGLU and by FUI Hermès. Experiments presented in this paper were carried out using Grid'5000 testbed, hosted by Inria and supported by CNRS, RENATER and several Universities as well as other organizations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating side information in probabilistic matrix factorization with gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hybrid recommender systems: Survey and experiments. User modeling and user-adapted interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">R</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="331" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Svdfeature: a toolkit for feature-based collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3619" to="3622" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recsys DLRS workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Recurrent Coevolutionary Feature Embedding Processes for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The netflix recommender system: Algorithms, business value, and innovation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gomez-Uribe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hunt ; K. He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun ; Diederik Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR<address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlinear principal component analysis using autoassociative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">M A</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIChE journal</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="243" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep collaborative filtering via marginalized denoising auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Proc. of CIKM</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-rank matrix approximation with stability</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recsys DLRS workshop</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="604" to="611" />
		</imprint>
	</monogr>
	<note>Proc. of ICML</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM Int. Conf. on Multimedia</title>
		<meeting>of the ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A networkbased end-to. end trainable task-oriented dialogue system</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collaborative denoising auto-encoders for top-n recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of WSDM</title>
		<meeting>of WSDM</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proc. of ICML</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale parallel collaborative filtering for the netflix prize</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAIM</title>
		<meeting>of AAIM</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
