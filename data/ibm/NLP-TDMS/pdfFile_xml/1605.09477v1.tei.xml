<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Autoregressive Approach to Collaborative Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
							<email>yin.zheng@hulu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkui</forename><surname>Ding</surname></persName>
							<email>wenkui.ding@hulu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">BANGSHENG@HULU.COM</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Hulu LLC</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Autoregressive Approach to Collaborative Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes CF-NADE, a neural autoregressive architecture for collaborative filtering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing parameters between different ratings. A factored version of CF-NADE is also proposed for better scalability. Furthermore, we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize CF-NADE, which shows superior performance. Finally, CF-NADE can be extended to a deep model, with only moderately increased computational complexity. Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Collaborative filtering (CF) is a class of methods for predicting a user's preference or rating of an item, based on his/her previous preferences or ratings and decisions made by similar users. CF lies at the core of most recommender systems and has attracted increasing attention along with the recent boom of e-commerce and social network systems. The premise of CF is that a person's preference does not change much over time. A good CF algorithm helps a user discover products or services that suit his/her taste efficiently.</p><p>Generally speaking, there is a dichotomy of CF methods: Memory-based CF and Model-based CF. Memorybased CF usually computes the similarities between users or items directly from the rating data, which are then used for recommendation. The explainatbility of the recommended results as well as the easy-to-implement nature of memory-based CF ensured its popularity in early recommender systems <ref type="bibr" target="#b20">(Resnick et al., 1994)</ref>. However, memorybased CF has faded out due to its poor performance on reallife large-scale and sparse data.</p><p>Distinct from memory-based CF, model-based CF learns a model from historical data and then uses the model to predict preferences of users. The models are usually developed with machine learning algorithms, such as Bayesian networks, clustering models and latent semantic models. Complex preference patterns can be recognized by these models, allowing model-based CF to perform better for preference prediction tasks. Among all these models, matrix factorization is most popular and successful, c.f. <ref type="bibr" target="#b11">(Koren et al., 2009;</ref><ref type="bibr" target="#b21">Salakhutdinov &amp; Mnih, 2008;</ref><ref type="bibr" target="#b17">Mackey et al., 2011;</ref><ref type="bibr" target="#b4">Gopalan et al., 2013)</ref>.</p><p>With the recent development of deep learning <ref type="bibr" target="#b12">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b26">Szegedy et al., 2014;</ref><ref type="bibr" target="#b8">He et al., 2015)</ref>, neural network based CF, a subclass of model-based CF, has gained enormous attention. A prominent example is RBMbased CF (RBM-CF) . RBM-CF is a two-layer undirected generative graph model which generalizes Restricted Boltzmann Machine (RBM) to modeling the distribution of tabular data, such as user's ratings of movies. RBM-CF has shown its power in Netflix prize challenge. However, RBM-CF suffers from inaccuracy and impractically long training time, since training RBM-CF is intractable and one has to rely on variational approximation or MCMC sampling. ement given the other elements to its left in the binary vector, where all conditionals share the same parameters. The probability of the binary vector can then be obtained by taking the product of these conditionals. Unlike RBM, NADE does not incorporate any latent variable where expensive inference is required, in constrast it can be optimized efficiently by backpropagation. NADE together with its variants achieved competitive results on many machine learning tasks <ref type="bibr" target="#b13">(Larochelle &amp; Lauly, 2012;</ref><ref type="bibr" target="#b28">Uria et al., 2013;</ref><ref type="bibr" target="#b34">Zheng et al., 2014b;</ref><ref type="bibr" target="#b29">Uria et al., 2014;</ref><ref type="bibr" target="#b33">Zheng et al., 2014a;</ref>.</p><p>In this paper, we propose a novel model-based CF approach named CF-NADE, inspired by RBM-CF and NADE models. Specifically, we will show how to adapt NADE to CF tasks and describe how to improve the performance of CF-NADE by encouraging the model to share parameters between different ratings. We also propose a factored version of CF-NADE to deal with large-scale dataset efficiently. As <ref type="bibr" target="#b27">Truyen et al. (2009)</ref> observed, preference usually has the ordinal nature: if the true rating of an item by a user is 3 stars in a 5-star scale, then predicting 4 stars is preferred to predicting 5 stars. We take this ordinal nature of preferences into consideration and propose an ordinal cost to optimize CF-NADE. Moreover, we propose a deep version of CF-NADE, which can be optimized efficiently. The performance of CF-NADE is tested on 3 real world benchmarks: MovieLens 1M, MovieLens 10M and Netflix dataset. Experimental results show that CF-NADE outperforms all previous state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>As mentioned previously, some of the most successful model-based CF methods are based on matrix factorization (MF) techniques, where a prevalent assumption is that the partially observed matrix is of low rank. In general, MF characterizes both users and items by vectors of latent factors, where the number of factors is much smaller than the number of users or items, and the correlation between user and item factor vectors are used for recommendation tasks. Specifically, <ref type="bibr" target="#b2">Billsus &amp; Pazzani (1998)</ref> proposed to apply Singular Value Decomposition (SVD) to CF tasks, which is an early work on MF-based CF. Bias MF <ref type="bibr" target="#b11">(Koren et al., 2009</ref>) is proposed to improve the performance of SVD by introducing systematic biases associated with users and items.  extended MF to a probabilistic linear model with Gaussian noise referred to as Probabilistic Matrix Factorization (PMF), and showed that PMF performed better than SVD. <ref type="bibr" target="#b21">Salakhutdinov &amp; Mnih (2008)</ref> proposed a Bayesian treatment of PMF, which can be trained efficiently by MCMC methods. Along this line, <ref type="bibr" target="#b15">Lawrence &amp; Urtasun (2009)</ref> proposed a non-linear PMF using Gaussian process latent variable models. There are other MF-based CF methods such as <ref type="bibr" target="#b19">(Rennie &amp; Srebro, 2005;</ref><ref type="bibr" target="#b17">Mackey et al., 2011)</ref>. Recently, Poisson Matrix Factorization <ref type="bibr" target="#b6">(Gopalan et al., 2014b;</ref><ref type="bibr">a;</ref> was proposed, replacing Gaussian assumption of PMF by Poisson distribution. <ref type="bibr" target="#b16">Lee et al. (2013)</ref> extended the low-rank assumption by embedding locality into MF models and proposed Local Low-Rank Matrix Approximation (LLORMA) method, which achieved impressive performance on several public benchmarks.</p><p>Another line of model-based CF is based on neural networks. With the tremendous success of deep learning <ref type="bibr" target="#b12">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b26">Szegedy et al., 2014;</ref><ref type="bibr" target="#b8">He et al., 2015)</ref>, neural networks have found profound applications in CF tasks. <ref type="bibr" target="#b22">Salakhutdinov et al. (2007)</ref> proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge <ref type="bibr" target="#b1">(Bennett &amp; Lanning, 2007)</ref>. Recently, <ref type="bibr" target="#b23">Sedhain et al. (2015)</ref> proposed AutoRec, an autoencoder-based CF model, which achieved the state-of-the-art performance on some benchmarks. RBM-CF  and AutoRec <ref type="bibr" target="#b23">(Sedhain et al., 2015)</ref> are common in that both of them build different models for different users, where all these models share the parameters. <ref type="bibr" target="#b27">Truyen et al. (2009)</ref> proposed to apply Boltzmann Machine (BM) on CF tasks, which extends RBM-CF by integrating the correlation between users and between items. <ref type="bibr" target="#b27">Truyen et al. (2009)</ref> also extended the standard BM model so as to exploit the ordinal nature of ratings. Recently, <ref type="bibr" target="#b3">Dziugaite &amp; Roy (2015)</ref> proposed Neural Network Matrix Factorization (NNMF), where the inner product between the vectors of users and items in MF is replaced by a feed-forward neural network. However, NNMF does not produce convincing results on benchmarks.</p><p>Our proposed method CF-NADE, can be generally categorized as a neural network based CF method. CF-NADE bears some similarities with NADE <ref type="bibr" target="#b14">(Larochelle &amp; Murray, 2011</ref>) in that both model vectors with neural autoregressive architectures. The crucial difference between CF-NADE and NADE is that CF-NADE is designed to deal with vectors of variable length, while NADE can only deal with binary vectors of fixed length. Though Doc-NADE <ref type="bibr" target="#b13">(Larochelle &amp; Lauly, 2012)</ref> does take inputs with various lengths, it is designed to model unordered sets of words where each element of the input corresponds to a word, while CF-NADE models user rating vectors, where each element corresponds to the rating to a specific item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">NADE for Collaborative Filtering</head><p>This section devotes to CF-NADE, a NADE-based model for CF tasks. Specifically, we describe the basic model of CF-NADE in Section 3.1, and propose to improve CF-NADE by sharing parameters between different ratings in Section 3.2. At last, a factored version of CF-NADE is described in Section 3.3 to deal with large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Model</head><p>Suppose that there are M items, N users, and the ratings are integers from 1 to K (K-star scale). One practical and prevalent assumption in CF literature is that a user usually rated D items 1 , where D M . To tackle sparsity, similar to RBM-CF , we use a different CF-NADE model for each user and all these models share the same parameters. Specifically, all models have the same number of hidden units, but a user-specific model only has D visible units if the user only rated D items. Thus, each CF-NADE has only one single training case, which is a vector of ratings that a user gave to his/her viewed items, but all the weights and biases of these CF-NADE's are tied.</p><p>In this paper, we denote the training case for user u as</p><formula xml:id="formula_0">r u = (r u mo 1 , r u mo 2 , . . . , r u mo D ),</formula><p>where o is a D-tuple in the set of permutations of (1, 2, . . . , D) which serves as an ordering of the rated items m i ∈ {1, 2, . . . , M }, and r u mo i ∈ {1, 2, . . . , K} denotes the rating that the user gave to item m oi . For simplicity, we will omit the index u of r u , and focus on a single user-specific CF-NADE in the rest of the paper.</p><p>CF-NADE models the probability of the rating vector r by the chain rule as:</p><formula xml:id="formula_1">p (r) = D i=1 p r mo i |r mo &lt;i<label>(1)</label></formula><p>where r mo &lt;i = (r mo 1 , r mo 2 , . . . , r mo i−1 ) denotes the first i − 1 elements of r indexed by o.</p><p>Similar to NADE <ref type="bibr" target="#b14">(Larochelle &amp; Murray, 2011)</ref>, CF-NADE models the conditionals in Equation 1 with neural networks. To compute the conditionals in Equation 1, CF-NADE first computes the hidden representation of dimension H given r mo &lt;i as follows:</p><formula xml:id="formula_2">h r mo &lt;i = g   c + j&lt;i W rm o j :,mo j   (2) where g(·) is the activation function, such as tanh(x) = exp(x)−exp(−x) exp(x)+exp(−x)</formula><p>, W k ∈ R H×M is the connection matrix associated with rating k, W k :,j ∈ R H is the j th column of W k and W k i,j is an interaction parameter between the i th hidden unit and item j with rating k, c ∈ R H is the bias term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">D might vary between different users</head><p>Then the conditionals in Equation 1 could be modeled as:</p><formula xml:id="formula_3">p r mo i = k|r mo &lt;i = exp s k mo i r mo &lt;i K k =1 exp s k mo i r mo &lt;i (3) where s k mo i (r mo &lt;i )</formula><p>is the score indicating the preference that the user gave rating k for item m oi given the previous ratings r mo &lt;i , and s k</p><formula xml:id="formula_4">mo i (r mo &lt;i ) is computed as, s k mo i r mo &lt;i = b k mo i + V k mo i ,: h r mo &lt;i<label>(4)</label></formula><p>where V k ∈ R M ×H and b k ∈ R M are the connection matrix and the bias term associated with rating k, respectively.</p><p>CF-NADE is optimized for minimum negative loglikelihood of p (r) (Equation <ref type="formula">(3))</ref>,</p><formula xml:id="formula_5">− log p (r) = − D i=1 log p r mo i |r mo &lt;i<label>(5)</label></formula><p>averaged over all the training cases. As in NADE, the ordering o in CF-NADE must be predefined and fixed during training for each user. Ideally, the ordering should follow the timestamps when the user gave the ratings. In practice, we find that a ordering that is randomly drawn from the set of permutations of (1, 2, . . . , D u ) for each user u yields good results. As <ref type="bibr" target="#b29">Uria et al. (2014)</ref> observed, we can think of the models trained with different orderings as different instantiations of CF-NADE for the same user. Section 5 will discuss how to (virtually) train a factorial number of CF-NADE's with different orderings simultaneously, which is the key to extend CF-NADE to a deep model efficiently.</p><p>Once the model is trained, given a user's past behavior r = (r mo 1 , r mo 2 , . . . , r mo D ), the user's rating of a new item m * can be predicted aŝ</p><formula xml:id="formula_6">r m * = E p(r m * =k|r) [k]<label>(6)</label></formula><p>where conditional p (r m * = k|r) are computed by Equation 3 along with the hidden representation h (r) and score s k m * (r) computed as</p><formula xml:id="formula_7">s k m * (r) = b k m * + V k m * ,: h (r) (7) h (r) = g   c + D j=1 W rm o j :,mo j   .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sharing Parameters Between Different Ratings</head><p>In Equations 2 and 4, the connection matrices W k , V k and the bias b k are different for different ratings k's. In other words, CF-NADE uses different parameters for different ratings. In practice, for a specific item, some ratings can be much more often observed than others. As a result, parameters associated with a rare rating might not be sufficiently optimized. To alleviate this problem, we propose to share parameters between different ratings of the same item.</p><p>Particularly, we propose to compute the hidden representation h(r mo &lt;i ) as follows:</p><formula xml:id="formula_8">h r mo &lt;i = g   c + j&lt;i rm o j k=1 W k :,mo j  <label>(9)</label></formula><p>Note that, given an item m oj rated r mo j by the user, h(r mo &lt;i ) depends on all the weights W k , ∀k ≤ r mo j . Thus, Equation 9 encourages a solution that W t is utilized by all the ratings k, ∀k ≥ t.</p><p>Similarly, the score s k</p><formula xml:id="formula_9">mo i (r mo &lt;i ) in Equation 3 is adjusted as s k mo i r mo &lt;i = j≤k b j mo i + V j mo i ,: h r mo &lt;i<label>(10)</label></formula><p>where V j and b j are shared by the rating k, where k ≥ j.</p><p>Sharing parameters between different ratings can again be understood as a kind of regularization, which encourages the model to use as many parameters as possible to explain the data. Experimental results in Section 6.2.1 confirm the advantage of this regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dealing with Large-Scale Datasets</head><p>One disadvantage of CF-NADE we have described so far is that the parameterization of W k ∈ R H×M and V k ∈ R M ×H , where k ranges from 1 to K, will result in too many free parameters, especially when dealing with massive datasets. For example, for the Netflix dataset <ref type="bibr" target="#b1">(Bennett &amp; Lanning, 2007)</ref>, when H = 500, the number of free parameters by W k and V k would be around 89 million 2 . Although severe overfitting can be avoided by proper weightdecay or dropout <ref type="bibr" target="#b25">(Srivastava et al., 2014)</ref>, learning such a huge network would still be problematic.</p><p>Inspired by RBM-CF  and Fix-ationNADE <ref type="bibr" target="#b33">(Zheng et al., 2014a)</ref>, we propose to address this problem by factorizing W k and V k into products of two lower-rank matrices. Particularly,</p><formula xml:id="formula_10">W k i,m = J j=1 B i,j A k j,m<label>(11)</label></formula><formula xml:id="formula_11">V k m,i = J j=1 P k m,j Q j,i<label>(12)</label></formula><p>where A k ∈ R J×M , P k ∈ R M ×J , B ∈ R H×J and Q ∈ R J×H are lower-rank matrices with J H and J M . For example, by setting J = 50, the number of free parameters for W and V decreases from 89 million to about 9 million. In our experiments, this factored version of CF-NADE will be applied on large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Traing CF-NADE with Ordinal Cost</head><p>CF-NADE can be trained by minimizing the negative loglikelihood based on conditionals defined by Equation 3. To go one step further, following <ref type="bibr" target="#b27">Truyen et al. (2009)</ref>, we take the ordinal nature of a user's preference into consideration. That is, if a user rated an item k, the preference of the user to the ratings from 1 to k should increase monotonically and the preference to the ratings from k to K should decrease monotonically. The basic CF-NADE treats different ratings as separate labels, leaving the ordinal information not captured. Here we describe how to equip CF-NADE with an ordinal cost.</p><p>Formally, suppose r mo i = k, the ranking of preferences over all the possible ratings under the ordinal assumption can be expressed as:</p><formula xml:id="formula_12">k k − 1 . . . 1 (13) k k + 1 . . . K<label>(14)</label></formula><p>where k k − 1 denotes the preference of rating k over k − 1, k ∈ {1, 2, . . . , K} 3 . Two rankings of ratings, y down and y up , can be induced by Equation 13 and 14:</p><p>y down = (k, k − 1, . . . , 1) (15) y up = (k, k + 1, . . . , K)</p><p>Note that maximizing the conditional p(r mo i = k|r mo &lt;i ) in Equation 3 only ensures that the probability of rating k is the largest among all possible ratings. To capture the ordinal nature induced by Equations 13 and 14, we propose to compute the conditional p(r mo i = k|r mo &lt;i ) as</p><formula xml:id="formula_14">p r mo i = k|r mo &lt;i = 1 j=k exp(s j mo i ) j t=1 exp(s t mo i ) K j=k exp(s j mo i ) K t=j exp(s t mo i )<label>(17)</label></formula><p>where s j mo i is a shorthand for the score s j mo i (r mo &lt;i ) introduced in Section 3.1, which indicates the preference to rating k of item m oi given the previous context r mo &lt;i .</p><p>Both two products in Equation 17 can be interpreted as the likelihood loss introduced in <ref type="bibr" target="#b31">(Xia et al., 2008)</ref> in the context of Listwise Learning To Rank problem. Actually, from the perspective of learning-to-rank, CF-NADE acts as a ranking function which produces rankings of ratings based on previous ratings, where s j mo i (r mo &lt;i ) corresponds to the score function in <ref type="bibr" target="#b31">(Xia et al., 2008)</ref> and the rankings, y down and y up , corresponds to true rankings that we would like CF-NADE to fit. Thus, the conditional computed by Equation 17 is actually the conditional distribution of the rankings y down and y up given previous i − 1 ratings. Put differently, the ranking loss in Equation 17 is defined on the ratings, while other learning-to-rank based CF methods, such as <ref type="bibr" target="#b24">(Shi et al., 2010)</ref>, are on items, which is the crucial difference.</p><p>For the rest of the paper, we denote the negative loglikelihood based on the conditionals computed by Equation 17 as ordinal cost C ord , and denote the negative loglikelihood based on Equation 3 as regular cost C reg . The final cost to optimize the model is then defined as</p><formula xml:id="formula_15">C hybrid = (1 − λ)C reg + λC ord<label>(18)</label></formula><p>where λ is the hyperparameter to determine the weight of C ord . The impact of the hyperparameter λ on the performance of CF-NADE is discussed in Section 6.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Extending CF-NADE to a Deep Model</head><p>So far we have described CF-NADE with single hidden layer. As suggested by the recent and impressive success of deep neural networks <ref type="bibr" target="#b12">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b26">Szegedy et al., 2014;</ref><ref type="bibr" target="#b8">He et al., 2015)</ref>, extending CF-NADE to a deep, multiple hidden layers architecture could allow us to have better performance. Recently, <ref type="bibr" target="#b29">Uria et al. (2014)</ref> proposed an efficient deep extension to original NADE <ref type="bibr" target="#b14">(Larochelle &amp; Murray, 2011)</ref> for binary vector observations, which inspires other related deep model <ref type="bibr" target="#b32">(Zheng et al., 2015)</ref>. Following <ref type="bibr" target="#b29">(Uria et al., 2014)</ref>, we propose a deep variant of CF-NADE.</p><p>As mentioned in Section 3.1, a different CF-NADE model is used for each user and the ordering o in r is stochastically sampled from the set of permutations of (1, 2, . . . , D).</p><p>Training CF-NADE on stochastically sampled orderings corresponds, in expectation, to minimizing the cost in Equation 18 over all possible orderings for each user. As noticed by <ref type="bibr" target="#b29">Uria et al. (2014)</ref> and <ref type="bibr" target="#b32">Zheng et al. (2015)</ref>, training over all possible orderings for CF-NADE implies that for any given context r mo &lt;i , the model performs equally well at predicting all the remaining items in r mo ≥i , since for each item there is an ordering such that it appears at position i. This is the key observation to extend CF-NADE to a deep model. Specifically, instead of sampling a complete ordering over all the D items, we instead sample a context r mo &lt;i and perform an update of the conditionals using that context.</p><p>The procedure is done as follows. Given a user who has rated D items, an ordering o is first sampled randomly from the set of permutations of (1, 2, . . . , D) for each update and a vector r = (r mo 1 , r mo 2 , . . . , r mo D ) is generated according to the ordering o. Then a split point i is randomly drawn from {1, 2, . . . , D} for each update. The split point i divides r into two parts: r mo &lt;i and r mo ≥i . According to the analysis above, in the new training procedure, r mo &lt;i is considered as the input of CF-NADE and the training objective is to maximize conditionals p(r mo j |r mo &lt;i ) for each element in r mo ≥i . The cost function with this procedure is</p><formula xml:id="formula_16">C = D D − i + 1 j≥i − log p r mo j |r mo &lt;i .<label>(19)</label></formula><p>By Equation 19, the model predicts the ratings of each items after the splitting position i in the randomly drawn ordering o as if it were actually at position i. The factors in front of the sum come from the fact that the total number of elements in the sum is D and that we are averaging over D − i + 1 possible choices for the item at position i, similar to <ref type="bibr" target="#b29">(Uria et al., 2014)</ref> and <ref type="bibr" target="#b32">(Zheng et al., 2015)</ref>. Derivation of <ref type="figure" target="#fig_0">Equation 19</ref> can be found in the supplementary materials.</p><p>In this procedure, a training update relies only on a single hidden representation h(r mo &lt;i ), more hidden layers can be added into CF-NADE with the computational complexity increased moderately. Particularly, suppose h (1) (r mo &lt;i ) is the hidden representation computed in Equation 2. Then new hidden layers can be added as in a regular deep feedforward neural network:</p><formula xml:id="formula_17">h (l) r mo &lt;i = g c (l) + W (l) h (l−1) r mo &lt;i<label>(20)</label></formula><p>for l = 2, . . . , L, where L is the total number of hidden layers. Then the conditionals p(r mo j = k|r mo &lt;i ), either in Equation <ref type="formula">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we test the performance of CF-NADE on 3 real-world benchmarks: MovieLens 1M, MovieLens 10M <ref type="bibr" target="#b7">(Harper &amp; Konstan, 2015)</ref> and Netflix dataset <ref type="bibr" target="#b1">(Bennett &amp; Lanning, 2007)</ref>, which contain 10 6 , 10 7 and 10 8 ratings, respectively. Following LLORMA <ref type="bibr" target="#b16">(Lee et al., 2013)</ref> and AutoRec <ref type="bibr" target="#b23">(Sedhain et al., 2015)</ref>, 10% of the ratings in each of these datasets are randomly selected as the test set, leaving the remaining 90% of the ratings as the training set. Among the ratings in the training set, 5% are used as validation set. We use a default rating of 3 for items without training observations. Prediction error is measured by Root Mean Squared Error (RMSE),</p><formula xml:id="formula_18">RMSE = S i=1 (r i −r i ) 2 S<label>(22)</label></formula><p>where r i is the i th true rating andr i is the predicted rating by the model, S is the total number of ratings in the testset. We report the average RMSE on test set over 5 different splits and compare CF-NADE with some strong baselines including LLORMA, AutoRec, and other competitive methods. Experimental results show that CF-NADE outperforms the state-of-the-art performance on these benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets Description</head><p>MovieLens 1M dataset contains around 1 million anonymous ratings of approximately 3900 movies by 6040 users, where each user rated at least 20 items. The ratings in MovieLens 1M dataset are made on a 5-star scale, with 1-star increments. MovieLen 10M dataset contains about 10 million ratings of 10681 movies by 71567 users. The users of MovieLens 10M dataset are randomly chosen and each user rated at least 20 movies. Unlike MovieLens 1M dataset, the ratings in MovieLens 10M are on a 5-star scale with half -star increments. Thus, the number of rating scales of MovieLens 10M is actually 10. In our experiments, we rescale the ratings in MovieLens 10M to 10-star scale with 1-star increments. Netflix dataset comes from the Netflix challenge prize 4 . It is massive compared to the previous two, which contains more than 100 million ratings of 17770 movies by 480189 users. The ratings of Netflix dataset are on 5-star scale, with 1-star increments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experiments on MovieLen 1M Dataset</head><p>In this section, we test the performance of CF-NADE on MovieLen 1M dataset. We first evaluate the performance of the ordinal cost described in Section 4 with/without sharing parameters between different ratings as described in Section 3.2. Then we compare several variants of CF-NADE with some strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.">THE PERFORMANCE OF THE ORDINAL COST</head><p>In this section, we evaluate the impact of the ordinal weight λ in Equation 21 on the performance of CF-NADE. As <ref type="bibr" target="#b23">Sedhain et al. (2015)</ref> mentioned, item-based CF outperforms user-based CF, therefore we use item-based CF-NADE (I-CF-NADE) in this section. Distinct from user-based CF-NADE (U-CF-NADE), which builds a different model for each user as we described previously, I-CF-NADE model builds a different CF-NADE model for each item. In other words, the only difference between U-CF-NADE and I-CF-NADE is that the roles of users and items are switched. Comparison between U-CF-NADE and I-CF-NADE can be found in Section 6.2.2.</p><p>The configuration of the experiments is as follows. We use a single hidden layer architecture and the number of hidden units is set to 500, same as AutoRec <ref type="bibr" target="#b23">(Sedhain et al., 2015)</ref> and LLORMA <ref type="bibr" target="#b16">(Lee et al., 2013)</ref>. Adam <ref type="bibr" target="#b10">(Kingma &amp; Ba, 2014)</ref> with default parameters (b 1 = 0.1,b 2 = 0.001 and = 10 −8 ) are utilized to optimize the cost function in <ref type="figure" target="#fig_0">Equation 19</ref>. The learning rate is set to 0.001 , the weight decay is set to 0.015 and we use the tanh activation function.</p><p>Let CF-NADE-S denote the variant of CF-NADE model where parameters are shared between different ratings, as described in Section 3.2. <ref type="figure" target="#fig_0">Figure 1</ref> shows the superior performance of CF-NADE and CF-NADE-S w.r.t different values of λ. Effectiveness of parameter sharing and ordinal cost can be justified by observing that: 1) CF-NADE-S always outperforms regular CF-NADE; 2) as the ordinal weight λ increases, test RMSE of both CF-NADE and CF-NADE-S decrease monotonically. Based on these observations, we will use CF-NADE-S and fix λ = 1 throughout the rest of the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.">COMPARING WITH STRONG BASELINES ON MOVIELENS 1M</head><p>In this comparison, we compare CF-NADE with other baselines on MoiveLens 1M dataset. During the comparison, the learning rate is chosen on the validation set by cross-validation among {0.001, 0.0005, 0.0002}, and the weight decay is chosen among {0.015, 0.02}. According to Section 6.2.1, the weight λ of ordinal cost is fixed to 1 and CF-NADE-S is adopted. The model is trained with Adam optimizer and tanh as activation function. <ref type="table" target="#tab_1">Table 1</ref> shows the performance of CF-NADE-S and baselines. The number of hidden units of CF-NADE is 500, same as AutoRec <ref type="bibr" target="#b23">(Sedhain et al., 2015)</ref> for a fair comparison. One can observe that I-CF-NADE-S outperforms U-CF-NADE-S by a large margin. I-CF-NADE-S with a single hidden layer achieves RMSE of 0.830, which is comparable with any strong baseline. Moreover, I-CF-NADE-S with 2 hidden layers achieves RMSE of 0.829. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the performance of I-CF-NADE-S w.r.t the number of hidden units. Increasing the number of hidden units is beneficial, but the return is diminishing. It can also be observed from <ref type="figure" target="#fig_1">Figure 2</ref> that deep CF-NADE models achieve better performance than the shallow ones, as expected.  <ref type="bibr" target="#b3">(Dziugaite &amp; Roy, 2015)</ref>. * : Taken from <ref type="bibr" target="#b23">(Sedhain et al., 2015)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Experiments on MovieLens 10M Dataset</head><p>As mentioned in Section 6.1, the MovieLens 10M is much bigger than the MovieLens 1M, so we opt to use the factored version of CF-NADE described in Section 3.3 and set J = 50. Even in this setting, I-CF-NADE with 71567 users and 10 rating scales will still bring about as many as 70 million free parameters, Hence, we only report the performance of U-CF-NADE in this experiment. Same as in Section 6.2.1, we train the model with Adam optimizer and using tanh as activation function. Other configurations are as follows: The number of hidden units to 500, the weight decay is 0.015 and λ is set to 1 and parameters are shared between ratings following Section 6.2.1. The base learning rate is 0.0005, and we double it for the parameters of the first layer. <ref type="table" target="#tab_2">Table 2</ref> shows the comparison between CF-NADE and other baselines on MovieLens 10M dataset. U-CF-NADE-S with a single hidden layer has already outperformed the baselines, which achieves RMSE of 0.772. The performance of U-CF-NADE-S can be slightly improved by adding another hidden layer. Noticeably, the test RMSE of U-AutoRec is much worse than I-AutoRec, whereas U-CF-NADE-S outperforms I-AutoRec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Experiments on Netflix Dataset</head><p>Our final set of experiments are on the massive Netflix dataset, which contains 10 8 ratings. Similar to Section 6.3, we use the factored version of U-CF-NADE with J = 50. The Netflix dataset is so big that we need not add a strong regularization to avoid overfitting and therefore set the weight decay to 0.001. Other configurations are the same as in Section 6.3.  <ref type="bibr" target="#b23">(Sedhain et al., 2015)</ref>. <ref type="table" target="#tab_3">Table 3</ref> compares the performance of U-CF-NADE with other baselines. We can see that U-CF-NADE-S with a single hidden layer achieves RMSE of 0.804, outperforming all baselines. Another observation from <ref type="table" target="#tab_3">Table 3</ref> is that using a deep CF-NADE architecture achieves a slight improvement over the shallow one, with a test RMSE of 0.803.  <ref type="bibr">2015)</ref> 0.823 U-CF-NADE-S (SINGLE LAYER) 0.804 U-CF-NADE-S (2 LAYERS) 0.803 †: Taken from <ref type="bibr" target="#b23">(Sedhain et al., 2015)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">The Complexity and Running Time of CF-NADE</head><p>We implement CF-NADE using Theano <ref type="bibr" target="#b0">(Bastien et al., 2012)</ref> and <ref type="bibr">Blocks (van Merriënboer et al., 2015)</ref>, and the code is available at https://github.com/Ian09/ CF-NADE. <ref type="table" target="#tab_4">Table 4</ref> shows the running time of one epoch 5 as well as the number of parameters used by CF-NADE. For MovieLens 1M dataset, we used the item-based CF-NADE and did not use the factorization method introduced by Sec 3.3, hence the number of parameters for Movie-Lens 1M is bigger than the other two. Running times in <ref type="table" target="#tab_4">Table 4</ref> include overheads such as transferring data from and to GPU memory for each update. Note that there is still room for faster implementations 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we propose CF-NADE, a feed-forward, autoregressive architecture for collaborative filtering tasks. CF-NADE is inspired by the seminal work of RBM-CF and the recent advancements of NADE. We propose to share parameters between different ratings to improve the performance. We also describe a factored version of CF-NADE, which reduces the number of parameters by factorizing a large matrix by a product of two lower-rank matrices, for better scalability. Moreover, we take the ordinal nature of preference into consideration and propose an ordinal cost to optimize CF-NADE. Finally, following recent advancements of deep learning, we extend CF-NADE to a deep model with moderate increase of computational complexity. Experimental results on three real-world benchmark datasets show that CF-NADE outperforms the state-of-theart methods on collaborative filtering tasks. all results of this work rely on explicit feedback, namely, ratings explicitly given by users. however, explicit feedback is not always available or as common as implicit feedback (watch, search, browse behaviors) in real-world recommender systems <ref type="bibr" target="#b9">(Hu et al., 2008)</ref>. Developing a version of CF-NADE tailored for implicit feedback is left for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The performance of CF-NADE and CF-NADE-S w.r.t ordinal weight λ on MovieLens 1M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The performance of I-CF-NADE w.r.t the number of hidden units on MovieLens 1M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>or Equation 17, can be computed from h (L) (r mo &lt;i ). To this end, the number of operations a CF-NADE takes for one input is O(KDH + H 2 L), as in regular multiple layers neural networks, whereD is the average number of ratings for a user and H is the number of hidden units for each layer. We denoteC reg andC ord as the cost functions of Equation 19 associated with conditionals computed by Equation 3 and Equation 17, respectively.</figDesc><table><row><cell cols="2">Finally, similar to Equation 18, we can also define a hybrid</cell></row><row><cell>costC hybrid asC</cell><cell></cell></row><row><cell>hybrid = (1 − λ)C reg + λC ord</cell><cell>(21)</cell></row></table><note>Note that CF-NADE with a single hidden layer can also be trained by Equation 19. In practice, Equation 19 can be im- plemented efficiently on GPUs, hence we use it throughout our experiments for either one hidden layer or multiple- layers architecture.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Test RMSE of different models on MovieLens 1M.</figDesc><table><row><cell>METHOD</cell><cell>TEST RMSE</cell></row><row><cell>PMF †</cell><cell>0.883</cell></row><row><cell>U-RBM *</cell><cell>0.881</cell></row><row><cell>U-AUTOREC (SEDHAIN ET AL., 2015)</cell><cell>0.874</cell></row><row><cell>LLORMA-GLOBAL (LEE ET AL., 2013)</cell><cell>0.865</cell></row><row><cell>I-RBM *</cell><cell>0.854</cell></row><row><cell>BIASMF *</cell><cell>0.845</cell></row><row><cell>NNMF (DZIUGAITE &amp; ROY, 2015)</cell><cell>0.843</cell></row><row><cell>LLORMA-LOCAL (LEE ET AL., 2013)</cell><cell>0.833</cell></row><row><cell>I-AUTOREC (SEDHAIN ET AL., 2015)</cell><cell>0.831</cell></row><row><cell>U-CF-NADE-S (SINGLE LAYER)</cell><cell>0.850</cell></row><row><cell>U-CF-NADE-S (2 LAYERS )</cell><cell>0.845</cell></row><row><cell>I-CF-NADE-S (SINGLE LAYER)</cell><cell>0.830</cell></row><row><cell>I-CF-NADE-S (2 LAYERS)</cell><cell>0.829</cell></row><row><cell>†: Taken from</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Test RMSE of different models on MovieLens 10M.</figDesc><table><row><cell>METHOD</cell><cell>TEST RMSE</cell></row><row><cell>U-AUTOREC (SEDHAIN ET AL., 2015)</cell><cell>0.867</cell></row><row><cell>I-RBM †</cell><cell>0.825</cell></row><row><cell>U-RBM †</cell><cell>0.823</cell></row><row><cell>LLORMA-GLOBAL (LEE ET AL., 2013)</cell><cell>0.822</cell></row><row><cell>BIASMF †</cell><cell>0.803</cell></row><row><cell>LLORMA-LOCAL (LEE ET AL., 2013)</cell><cell>0.782</cell></row><row><cell>I-AUTOREC (SEDHAIN ET AL., 2015)</cell><cell>0.782</cell></row><row><cell>U-CF-NADE-S (SINGLE LAYER)</cell><cell>0.772</cell></row><row><cell>U-CF-NADE-S (2 LAYERS)</cell><cell>0.771</cell></row><row><cell>†: Taken from</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Test RMSE of different models on Netflix dataset.</figDesc><table><row><cell>METHODS</cell><cell>TEST RMSE</cell></row><row><cell>LLORMA-GLOBAL (LEE ET AL., 2013)</cell><cell>0.874</cell></row><row><cell>U-RBM †</cell><cell>0.845</cell></row><row><cell>BIASMF †</cell><cell>0.844</cell></row><row><cell>LLORMA-LOCAL (LEE ET AL., 2013)</cell><cell>0.834</cell></row><row><cell>I-AUTOREC (SEDHAIN ET AL.,</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Complexity of CF-NADE on different benchmarks</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Layers #Params Train Time Test Time</cell></row><row><cell></cell><cell></cell><cell>(million)</cell><cell>(second)</cell><cell>(second)</cell></row><row><cell>ML 1M</cell><cell>1 2</cell><cell>30.2 30.48</cell><cell>3.09 3.11</cell><cell>0.65 0.68</cell></row><row><cell>ML 10M</cell><cell>1 2</cell><cell>10.78 10.98</cell><cell>134.76 135.62</cell><cell>31.72 32.73</cell></row><row><cell>Netflix</cell><cell>1 2</cell><cell>9.02 9.19</cell><cell>1057.81 1064.33</cell><cell>239.78 243.79</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Netflix dataset contains 17770 movies (M = 17770) and the ratings are 5-star scale (K = 5). Thus, the number of free parameters from W k and V k is 88850000 = 2×17770×5×500.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Equation 13is omitted if the true rating is 1; likewise, Equation 14 is omitted if the true rating is K.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The test set of Netflix prize challenge dataset is not available now. Following<ref type="bibr" target="#b16">Lee et al. (2013)</ref> and<ref type="bibr" target="#b23">Sedhain et al. (2015)</ref>, we split the available trainset of Netflix dataset into train, valid and test sets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Experiments are conducted on a single NVIDIA Titan X Card.6  In our implementation, samples are represented as M × K binary matrices, where M is the number of items and K is the number of rating scales. An entry (m, k) is assigned 1 only if the user gave a k-star to item m. Thus, we could use the tensordot operator in Theano and feed CF-NADE with a batch of samples.In the experiments, mini-batch size is set to 512. One disadvantage of this implementation is that some amount of computational time is spent on unrated items, which can be enormous especially when the data is sparse.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Hugo Larochelle and the reviewers for many helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arnaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The netflix prize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Lanning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD cup and workshop</title>
		<meeting>KDD cup and workshop</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning collaborative information filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Billsus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gintare</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karolina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06443</idno>
		<title level="m">Neural network matrix factorization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1311.1704</idno>
		<title level="m">Scalable recommendation with poisson factorization</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric poisson factorization for recommendation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="275" to="283" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Content-based recommendations with poisson factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><forename type="middle">K</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3176" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The movielens datasets: History and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems (TiiS)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2008. ICDM&apos;08. Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A neural autoregressive topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2708" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-linear matrix factorization with gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Local low-rank matrix approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seungyeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Divide-and-conquer matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lester</forename><forename type="middle">W</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast maximum margin matrix factorization for collaborative prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasson</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="713" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grouplens: an open architecture for collaborative filtering of netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iacovou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neophytos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suchak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bergstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 ACM conference on Computer supported cooperative work</title>
		<meeting>the 1994 ACM conference on Computer supported cooperative work</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using markov chain monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suvash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web Companion</title>
		<meeting>the 24th International Conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">List-wise learning to rank with matrix factorization for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM conference on Recommender systems</title>
		<meeting>the fourth ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="269" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ordinal boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><surname>Truyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svetha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rnade: The real-valued neural autoregressive density-estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benigno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2175" to="2183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benigno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR: W&amp;CP</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="467" to="475" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dzmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Dmitriy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00619</idno>
		<title level="m">Yoshua. Blocks and fuel: Frameworks for deep learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Listwise approach to learning to rank: theory and algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tie-Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1192" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A deep and autoregressive approach for topic modeling of multimodal data. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larochelle</forename><surname>Yu-Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2476802</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A neural autoregressive approach to attention-based recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu-Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Topic modeling of multimodal data: An autoregressive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larochelle</forename><surname>Yu-Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.178</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1370" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
