<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Forward Prediction for Physical Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Adcock</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Forward Prediction for Physical Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action</head><p>Rollout Action Rollout Action Rollout (a) Task not solved (b) Task solved (c) Task not solved <ref type="figure">Figure 1</ref>: Physical reasoning on the challenging PHYRE tasks require placing an object (the red ball) in the scene, such that when the simulation is rolled out, the blue and green objects touch for at least three seconds. In (a), the ball is too small and does not knock the green ball off the platform. In (b), the ball is larger and solves the task. In (c), the ball is placed slightly farther left, which results in the task not being solved. Small variations in the selected action (or the scene) can have a large effect on the efficacy of the action, making the task highly challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Physical reasoning requires forward prediction: the ability to forecast what will happen next given some initial world state. We study the performance of state-of-the-art forwardprediction models in the complex physical-reasoning tasks of the PHYRE benchmark <ref type="bibr" target="#b1">[2]</ref>. We do so by incorporating models that operate on object or pixel-based representations of the world into simple physical-reasoning agents. We find that forward-prediction models can improve physicalreasoning performance, particularly on complex tasks that involve many objects. However, we also find that these improvements are contingent on the test tasks being small variations of train tasks, and that generalization to completely new task templates is challenging. Surprisingly, we observe that forward predictors with better pixel accuracy do not necessarily lead to better physical-reasoning performance. Nevertheless, our best models set a new state-of-the-art on the PHYRE benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When presented with a picture of a Rube Goldberg machine, we can predict how the machine works. We do so by using our intuitive understanding of concepts such as force, mass, energy, collisions, etc., to imagine how the machine state would evolve once released. This ability allows us to solve real world physical-reasoning tasks, such as how to hit a billiards cue such that the ball ends up in the pocket, or how to balance the weight of two children on a see-saw. In contrast, physical-reasoning abilities of machine-learning models have largely been limited to closed domains such as predicting dynamics of multi-body gravitational systems <ref type="bibr" target="#b3">[4]</ref>, stability of block towers <ref type="bibr" target="#b36">[37]</ref>, or physical plausibility of observed dynamics <ref type="bibr" target="#b46">[47]</ref>. In this work, we explore the use of imaginative, forward-prediction approaches to solve complex physical-reasoning puzzles. We study modern object-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b58">59]</ref> and pixel-based <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b64">65]</ref> forward-prediction models in simple search-based agents on the PHYRE benchmark <ref type="bibr" target="#b1">[2]</ref>. PHYRE tasks involve placing one or two balls in a 2D world, such that the world reaches a state with a particular property (e.g., two balls are touching) after being played forward. PHYRE tasks are very challenging because small changes in the action (or the world) can have a very large effect on the efficacy of an action; see Figure 1 for an example. Moreover, PHYRE tests models' ability to generalize to completely new physical environments at test time, a significantly harder task than prior work that mostly varies number or properties of objects in the same environment. As a result, physical-reasoning agents may struggle even when their forward-prediction model works well.</p><p>Nevertheless, our best agents substantially outperform the prior state-of-the-art on PHYRE. Specifically, we find that forward-prediction models can improve the performance of physical-reasoning agents when the models are trained on tasks that are very similar to the tasks that need to be solved at test time. However, we find forward-prediction based agents struggle to generalize to truly unseen tasks, presumably, because small deviations in forward predictions tend to compound over time. We also observe that better forward prediction does not always lead to better physical-reasoning performance on PHYRE (c.f. <ref type="bibr" target="#b5">[6]</ref> for similar observations in RL). In particular, we find that object-based forwardprediction models make more accurate forward predictions but pixel-based models are more helpful in physical reasoning. This observation may be the result of two key advantages of models using pixel-based state representations. First, it is easier to determine whether a task is solved in a pixelbased representation than in an object-based one, in fully observable 2D environments like PHYRE. Second, pixelbased models facilitate end-to-end training of the forwardprediction model and the task-solution model in a way that object-based models do not in the absence of a differentiable renderer <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our study builds on a large body of prior research on forward prediction and physical reasoning. Forward prediction models attempt to predict the future state of objects in the world based on observations of past states. Learning of such models, including using neural networks, has a long history <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>. These models operate either on object-based (proprioceptive) representations or on pixel-based state representations. A popular class of object-based models use graph neural networks to model interactions between objects <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref>, for example, to simulate environments with thousands of particles <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49]</ref>. Another class of object-based models explicitly represents the Hamiltonian or Lagrangian of the physical system <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. While promising, such models are currently limited to simple point objects and physical systems that conserve energy. Hence, they cannot currently be used on PHYRE, which contains dissipative forces and extended objects. Modern pixel-based forward-prediction models extract state representations by applying a convolutional network on the observed frame(s) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b58">59]</ref> or on object segments <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b64">65]</ref>. The models perform forward prediction on the resulting state representation using graph neural networks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b64">65]</ref>, recurrent neural networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b62">63]</ref>, or a physics engine <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b61">62]</ref>. The models can be trained to predict object state <ref type="bibr" target="#b58">[59]</ref>, perform pixel reconstruction <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b64">65]</ref>, transform the previous frames <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref>, or produce a contrastive state representation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>. Physical reasoning tasks gauge a system's ability to intuitively reason about physical phenomena <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36]</ref>. Prior work has developed models that predict whether physical structures are stable <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, predict whether physical phenomena are plausible <ref type="bibr" target="#b46">[47]</ref>, describe or answer questions about physical systems <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b65">66]</ref>, perform counterfactual prediction in physical worlds <ref type="bibr" target="#b2">[3]</ref>, predict effect of forces <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b57">58]</ref>, or solve physical puzzles/games <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16]</ref>. Unlike other physical reasoning tasks, physical-puzzle benchmarks such as PHYRE <ref type="bibr" target="#b1">[2]</ref> and Tools <ref type="bibr" target="#b0">[1]</ref> incorporate a full physics simulator, and contain a large set of physical environments to study generalization. This makes them particularly suitable for studying the effectiveness of forward prediction for physical reasoning, and we adopt the PHYRE benchmark in our study for that reason. Inferring object representations involve techniques like generative models and attention mechanisms to decompose scenes into objects <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. Many techniques also leverage the motion information for better decomposition or to implicitly learn object dynamics <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54]</ref>. While relevant to our exploration of pixel-based methods as well, we leverage the simplicity of PHYRE visual world to extract object-like representations simply using connected component algorithm in our approaches (c.f. STN in Section 4.1). However, more sophisticated approaches could help further improve the performance, and would be especially useful for more visually complex and 3D environments. Video Prediction or conditional pixel generation typically requires an implicit understanding of physics. Popular approaches model the past frames using a variant of recurrent neural network <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b62">63]</ref> and make predictions directly using a decoder <ref type="bibr" target="#b56">[57]</ref>, or as a transformation of the previous frames using optical flow <ref type="bibr" target="#b63">[64]</ref> or spatial transformations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b64">65]</ref>. Our work is complementary to such approaches, building upon them to solve physical reasoning tasks.</p><p>Model-based RL approaches rely on building models of the environment of the agent to plan in. Such approaches typically use recurrent stochastic state transition models supervised with a reconstruction <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b31">32]</ref> or contrastive <ref type="bibr" target="#b25">[26]</ref> objective. Given the learned forward model, the planning is typically performed using a variant of cross entropy method (CEM) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b47">48]</ref>. Our setup is similar to model-based RL, with the crucial difference being that we take only a single action, and the long-horizon dynamics we need to model is significantly more complex than typical RL control environments <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>. Given the simplicity of our action space, we learn a value function over actions using the predicted rollouts, and use it to search for the optimal action at test time. Future work involving more complex or even continuous action spaces can perhaps benefit from learning a more sophisticated sampling approach using CEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PHYRE Benchmark</head><p>In PHYRE, each task consists of an initial state that is a 256 × 256 image. Colors indicate object properties; for instance, black objects are static while gray objects are dynamic and neither are involved in the goal state. PHYRE defines two task tiers (B and 2B) that differ in their action space. An action involves placing one ball (in the B tier) or two balls (in the 2B tier) in the image. Balls are parameterized by their position and radius, which determine the ball's mass. An action solves the task if the blue or purple object touches the green object (the goal state) for a minimum of three seconds when the simulation is rolled out. <ref type="figure">Figure 1</ref> illustrates the challenging nature of PHYRE tasks: small variations can change incorrect actions <ref type="figure">(Figure 1</ref> Each tier in PHYRE contains 25 task templates. A task template contains 100 tasks that are structurally similar but that differ in the initial positions of the objects. Performance on PHYRE is measured in two settings. The within-template setting defines a train-test split over tasks, such that training and test tasks can contain different instantiations of the same template. The cross-template setting splits across templates, such that training and test tasks never correspond to the same template. A PHYRE agent can make multiple attempts at solving a task. The performance of the agent is measured by the area under the success curve (AUCCESS; <ref type="bibr" target="#b1">[2]</ref>), which ranges from 0 to 100 and is higher when the agent needs fewer attempts to solve a task. Performance is averaged over 10 random splits of tasks or templates. In addition to AUCCESS, we also measure a forward-prediction accuracy (FPA) that does not consider whether an action solves a task. We define FPA as the percentage of pixels that match the ground-truth in a 10-second rollout at 1 frame per second (fps); we only consider pixels that correspond to dynamic objects when computing forward-prediction accuracy. Please refer to Appendix D for exact implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>We develop physical-reasoning agents for PHYRE that use learned forward-prediction models in a search strategy to find actions that solve a task. The search strategy maximizes the score of a task-solution model that, given a world state, predicts whether that state will lead to a task solution. <ref type="figure">Figure 2</ref> illustrates how our forward-prediction and task-solution models are combined. We describe both types of models, as well as the search strategy we use, separately below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Forward-Prediction Models</head><p>At time t, a forward-prediction model F aims to predict the next state,x t+1 , of a physical system based on a series of τ past states of that system. F consists of a state encoder e, a forward-dynamics model f , and a state decoder d. The past states {x t−τ , . . . , x t } are first encoded into latent representations {z t−τ , . . . , z t } using a learned encoder e with parameters θ e , i.e., z t = e(x t ; θ e ). The latent representations are then passed into the forward-dynamics model f with param-</p><formula xml:id="formula_0">eters θ f : f ({x t−τ , . . . , x t }, {z t−τ , . . . , z t }; θ f ) →ẑ t+1 .</formula><p>Finally, the predicted future latent representation is decoded using the decoder d with parameters θ d : d(ẑ t+1 ; θ d ) →x t+1 . We learn the model parameters Θ = (θ e , θ f , θ d ) on a large training set of observations of the system's dynamics. We experiment with forward-prediction models that use either object-based or pixel-based state representations.</p><p>Object-based models. We experiment with two objectbased forward-prediction models that capture interactions between objects: interaction networks <ref type="bibr" target="#b3">[4]</ref> and transformers <ref type="bibr" target="#b54">[55]</ref>. Both object-based forward-prediction models represent the system's state as a set of tuples that contain object type (ball, stick, etc.), location, size, color, and orientation. The models are trained by minimizing the mean squared error between the predicted and observed state.</p><p>• Interaction networks (IN; <ref type="bibr" target="#b3">[4]</ref>) maintain a vector representation for each object in the system at time t. Each vector captures information about the object's type, position, and velocity. A relationship is computed for each ordered pair of objects, designating the first object as the sender and the second as the receiver of the relation. The relation is characterized by the concatenation of the two objects' feature vectors and a one-hot encoding representing the sender object's attribute of static or dynamic. The dynamics model embeds the relations into "effects" per object using a multilayer perceptron (MLP). The effects exerted on an object are summed into a single effect per object. This aggregated effect is concatenated with the object's previous state vector, from a particular temporal offset, along with a placeholder for external effects, e.g., gravity. The result is passed through another MLP to predict velocity of the object. We use two interaction networks with different temporal offsets <ref type="bibr" target="#b58">[59]</ref>, and aggregate the results in an MLP to generate the final velocity prediction. The decoder then sums the object's predicted velocity with the previous position to obtain the new position of the object. • Transformers (Tx; <ref type="bibr" target="#b54">[55]</ref>) also maintain a representation per object: they encode each object's state using a 2-layer MLP. In contrast to IN, the dynamics model f in Tx is a Transformer that uses self-attention layers over the latent representation to predict the future state. We add a State Decoder:</p><formula xml:id="formula_1">( ! ; " )</formula><p>Task-solution model:</p><formula xml:id="formula_2">ℂ( !#$:! , !#$:! ; )</formula><p>Dynamics model:</p><formula xml:id="formula_3">( !#$:! , !#$:! ; &amp; )</formula><p>State encoders:  <ref type="figure">Figure 2</ref>: We study models that take as input a set of initial state via an object-based or a pixel-based representation (blue box). We input the representation into a range of forward-prediction models, which generally comprise an encoder (yellow box), a dynamics model (green box), and a decoder (gray box). We feed that output to a task-solution model (red box) that predicts whether the goal state is reached. At inference time, we search over actions that alter the initial state by adding additional objects to the state. For each action (and corresponding initial state) we predict a task-solution probability; we then select the action most likely to solve the task.</p><formula xml:id="formula_4">( ! ; ' ) Input Representations: ! Type Pos … … !#( !#) ! !#( !#) !</formula><p>sinusoidal temporal position encoding <ref type="bibr" target="#b54">[55]</ref> of time t to the features of each object. The resulting representation is fed into a Transformer encoder with 6 layers and 8 heads. The output representation is decoded using a MLP and added to the previous state to obtain the future state prediction.</p><p>Pixel-based models. In contrast to object-based models, pixel-based forward-prediction models do not assume direct access to the attribute values of the objects. Instead, they operate on images depicting the object configuration, and maintain a single, global world state that is extracted by an image encoder. Our image encoder e is a ResNet-18 network <ref type="bibr" target="#b28">[29]</ref> that is clipped at the res4 block. Objects in PHYRE can have seven different colors; hence, the input of the network consists of seven channels with binary values that indicate object presence, consistent with prior work <ref type="bibr" target="#b1">[2]</ref>. The representations extracted from the past τ frames are concatenated before being input into the two models we study.</p><p>• Spatial transformer networks (STN; <ref type="bibr" target="#b30">[31]</ref>) split the input frame into segments by detecting objects <ref type="bibr" target="#b64">[65]</ref>, and then encode each object using the encoder e. Specifically, we use a simple connected components algorithm <ref type="bibr" target="#b59">[60]</ref> to split each frame channel into object segments. The dynamics model concatenates the object channels for the τ input frames, and predicts a rotation and translation for each channel corresponding to the last frame using a small convolutional network. The decoder applies the predicted transformation to each channel. The resulting channels are combined into a single frame prediction by summing them. Inspired by modern keypoint localizers <ref type="bibr" target="#b27">[28]</ref>, we train STNs by minimizing the spatial cross-entropy, which sums the cross-entropies of H ×W softmax predictions over all seven channels.</p><p>• Deconvolutional networks (Dec) directly predict the pixels in the next frame using a deconvolutional network that does not rely on a segmentation of the input frame(s).</p><p>The representations for the last τ frames are concatenated along the channel dimension, and passed through a small convolutional network to generate a latent representation for the t + 1 th frame. Latent representationẑ t+1 is then decoded to pixels using a deconvolutional network, implemented as series of five transposed-convolution and (bilinear) upsampling layers, with intermediate ReLU activation functions. We found Decs are best trained by minimizing the per-pixel cross-entropy, which sums the cross-entropy of seven-way softmax predictions at each pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Task-Solution Models</head><p>We use our forward-prediction models in combination with a task-solution model that predicts whether a rollout solves a physical-reasoning task. In the physical-reasoning tasks we consider, the task-solution model needs to recognize whether two particular target objects are touching (task solved) or not (task not solved). This recognition is harder than it seems, particularly when using an object-based state representation. For example, evaluating whether or not the centers of two balls are "near" is insufficient because the radiuses of the balls need to be considered as well. For more complex objects, the model needs to evaluate complex relations between the two objects, as well as recognize other objects in the scene that may block contact. We note that good task-solution models may also correct for errors made by the forward-prediction model.</p><p>Per . AUCCESS of the OPTIMAL agent is shown for reference. Shaded regions indicate standard deviation across 10 folds. We note that object-based task-solution models struggle compared to pixel-based ones, especially in cross-template settings.</p><p>latent representations as input from the forward-prediction model. We provide the input frames as well to account for potentially poor performance of the dynamics model on certain tasks; in those cases the task-solution model can learn to ignore the rollout and only use input frames to make a prediction. It then produces a binary prediction:</p><formula xml:id="formula_5">C(x 0 . . . x τ , z 0 . . . z τ ,x τ +1 . . .x τ +τ ,ẑ τ +1 . . .ẑ τ +τ ; ψ) → [−1, +1].</formula><p>Because both types of forward-prediction models produce different outputs, we experiment with object-based classifiers and pixel-based classifiers that make predictions based on simulation state represented by object features or pixels respectively. We also experiment with pixel-based classifiers on object-based forward-prediction models by rendering the object-based state to pixels first.</p><p>• Object-based classifier (Tx-Cls). We use a Transformer <ref type="bibr" target="#b54">[55]</ref> model that encodes the object type and position into a 128-dimensional encoding using a two-layer MLP. As before, a sinusoidal temporal position encoding is added to each object's features. The resulting encodings for all objects over the τ + τ time steps are concatenated, and used in a 16-head, 8-layer transformer encoder with LayerNorm. The resulting representations are input into another MLP that performs a binary classification that predicts whether or not the task is solved. • Pixel-based classifier (Conv3D-{Latent,Pixel}).</p><p>Our pixel-based classifier poses the problem of classifying task solutions as a video-classification problem. Specifically, we adopt a 3D convolutional network for video classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. We experiment with two variants of this model: Conv3D-Pixel can also be used in combination with object-based forward-prediction models, as the predictions of those models can be rendered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Search Strategy</head><p>We compose a forward-prediction model F and a tasksolution model C to form a scoring function for action proposals. An action adds one or more additional objects to the initial world state. We sample K actions uniformly at random and evaluate the value of the scoring function for the sampled actions. To evaluate the scoring function, we alter the initial state with the action, use the resulting state as input into the forward-prediction model, and evaluate the task-solution model on the output of the forward-prediction model. The search strategy selects the action that is most likely to solve the task according to the task-solution model, based on the output of the forward-prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the performance of the forward-prediction models on the B-tier of the challenging PHYRE benchmark ([2]; see <ref type="figure">Figure 1</ref>). We present our experimental setup and the results of our experiments below. We provide trained models and code reproducing our results online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Training. To generate training data for our models, we sample task-action pairs in a balanced way: half of the sam- ples solve the task and the other half do not. We generate training examples for the forward-prediction models by obtaining frames from the simulator at 1 fps, and sampling τ consecutive frames used to bootstrap the forward model from a random starting point in this obtained rollout. The model is trained to predict frames that succeed the selected τ frames.</p><p>For the task-solution model, we always sample τ frames from the starting point of the rollout, or frame 0. Along with these τ frames, the task-solution model also gets the τ autoregressively predicted frames from the forward-prediction model as input. We use τ = 3 for most experiments, and eventually relax this constraint to use τ = 1 frame when comparing to the state-of-the-art in the next section.</p><p>We train most forward-prediction models using teacher forcing <ref type="bibr" target="#b60">[61]</ref>: we only use ground-truth states as input into the forward model during training. The only exception is Dec, for which we observed better performance when predicted states are used as input when training. Furthermore, since Dec is trainable without teacher forcing, we are able to train it jointly with the task-solution model, as it no longer requires picking a random point in the rollout to train the forward model. In this case, we train both models from frame 0 of each simulation with equal weights on both losses, and refer to this model as Dec <ref type="bibr">[Joint]</ref>. For object-based models, we add a small amount of Gaussian noise to object states during training to make the model robust <ref type="bibr" target="#b3">[4]</ref>. We train all task-solution and pixel-based forward-prediction models using mini-batch SGD, and train object-based forwardprediction models with Adam. We selected hyperparameters for each model based on the AUCCESS on the first fold in the within-template setting; see Appendix H.</p><p>Evaluation. At inference time, we bootstrap the forwardprediction models with τ initial ground-truth states from the <ref type="table">Table 1</ref>: AUCCESS and success percentage of our Dec [Joint] agents using τ = 0 (no roll-out, frame-level model) and τ = 10 (full roll-out) compared to current stateof-the-art agents <ref type="bibr" target="#b1">[2]</ref> on the PHYRE benchmark. In contrast to prior experiments, all agents here are conditioned on τ = 1 initial frame. Our agents outperform all prior work on both settings and metrics of the PHYRE benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUCCESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Success %age Within</head><p>Cross Within Cross RAND <ref type="bibr" target="#b1">[2]</ref> 13.7±0.5 13.0±5.0 7.7±0.8 6.8±5.0 MEM <ref type="bibr" target="#b1">[2]</ref> 2.4±0.3 18.5±5.1 2.7±0.5 15.2±5.9 DQN <ref type="bibr" target="#b1">[2]</ref> 77.6±1. simulator for a given action, and autoregressively predict τ future states. The τ + τ states are then passed into the task-solution model to predict whether the task will be solved or not by this action. Following <ref type="bibr" target="#b1">[2]</ref>, we use the tasksolution model to score a fixed set of K = 1, 000 (unless otherwise specified) randomly selected actions for each task.</p><p>We rank these actions based on the task-solution model score to measure AUCCESS. We also measure forward-prediction accuracy (FPA; see Section 3) on the validation tasks for 10 random actions each, half of which solve the task and the other half that do not. Following <ref type="bibr" target="#b1">[2]</ref>, we repeat all experiments for 10 folds and report the mean and standard deviation of the AUCCESS and FPA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>We organize our experimental results based on a series of research questions.</p><p>Can perfect forward prediction solve PHYRE physical reasoning? We first evaluate if perfect forward-prediction can solve physical reasoning on PHYRE. We do so by using the PHYRE simulator as the forward-prediction model, and applying task-solution models on the predicted state. We exclude the Conv3D-Latent task-solution model in this experiment because it requires the latent representation of learned forward-prediction model, which the simulator can not provide. <ref type="figure">Figure 3</ref> shows the AUCCESS of these models as a function of the number of seconds the forwardprediction model is rolled out. We compare model performance with that of the OPTIMAL agent <ref type="bibr" target="#b1">[2]</ref>, which is an agent that achieves the maximum attainable performance given that we rank only K actions. We observe that task-solution models work nearly perfectly in the within-template setting when 1f with τ = 0 (no forward prediction) and τ = 10 (forward prediction) of five task templates that benefit the least from forward prediction (left) and five templates that benefit the most (right). the forward-prediction is rolled out for τ ≈ 10 seconds. We also observe that pixel-based task-solution models outperform object-based models, especially in the cross-template setting. This suggests that it is more difficult for objectbased models to determine whether or not two objects are touching than for pixel-based models, presumably, because the computations required are more complex. In preliminary experiments, we found that Conv3D-Latent outperforms Conv3D-Pixel when combined with learned pixel-based forward-prediction models (see Appendix C.1). Therefore, we use Conv3D-Latent as the task-solution model for pixel-based models and Conv3D-Pixel for object-based models (by rendering object-based predictions) in the following experiments.</p><p>How well do forward-prediction models solve PHYRE physical reasoning? We evaluate performance of our learned forward-prediction models on the PHYRE tasks. Akin to the previous experiment, we roll out the forward-prediction model for τ seconds and evaluate the corresponding task-solution model on the τ state predictions and the τ = 3 input states. <ref type="figure">Figure 4</ref> presents the AUCCESS of this approach as a function of the number of seconds (τ ) that the forward-prediction models were rolled out. The AUCCESS of an agent without forward prediction (No-fwd) is shown for reference. The results show that forward prediction can improve AUCCESS by up to 2% in the within-template setting. The pixel-based Dec model performs similarly to models that operate on object-based states, either extracted (STN) or ground truth (IN and Tx). Furthermore, Dec allows for end-to-end training of the forward-prediction and the pixelbased task-solution models. The resulting Dec [Joint] model performs the best in our experiments, which is why we focus on it in subsequent experiments. Similar joint training of object-based models (c.f. Appendix C.2) yields smaller improvements due to limitations of object-based task-solution models. Although the within-template results suggest that forward prediction can help physical reasoning, AUCCESS plateaus after τ ≈ 5 seconds. This suggests that forward-prediction models are only truly accurate on PHYRE for a short period of time. Also, forward-prediction models help little in the cross-template setting, suggesting limited generalization across templates.</p><p>Does better forward-prediction imply better PHYRE physical reasoning? <ref type="figure" target="#fig_4">Figure 5 (</ref> Action (red ball) solving the task. Using a slightly smaller ball. Using a slightly larger ball. <ref type="figure">Figure 8</ref>: Rollouts for three slightly different actions on the same task by: (1) the simulator and (2) a STN trained only on tasks from the corresponding task template. Although the STN produces realistic rollouts, its predictions do not perfectly match the simulator. The small variations in action change whether the action solves the task. The STN model is unable to capture those variations effectively.</p><p>detail, <ref type="figure" target="#fig_4">Figure 5</ref> (right) shows FPA averaged over 10 seconds as a function of the maximum AUCCESS over that time.</p><p>The results confirm that more pixel-accurate forward predictions do not necessarily increase performance on PHYRE's physical-reasoning tasks.</p><p>How do forward-prediction agents compare to the stateof-the-art on PHYRE? Hitherto, all our experiments assumed access to τ = 3 input frames, which is not the setting considered by <ref type="bibr" target="#b1">[2]</ref>. To facilitate comparisons with prior work, we develop an Dec agent that requires only τ = 1 input frame: we pad the first frame with 2 empty frames and train the model exclusively on roll-outs that start from the first frame and do not use teacher forcing. We refer to the resulting model as Dec [Joint] 1f. <ref type="table">Table 1</ref> compares the performance of Dec [Joint] 1f to the state-of-the-art on PHYRE in terms of AUCCESS and success percentage @ 10 (i.e., the percentage of tasks that were solved within 10 attempts). The results show that Dec [Joint] 1f outperforms the previous best reported agents in terms of metrics in both the within and the cross-template settings. In the within-template setting, the performance of Dec [Joint] 1f increases substantially for large τ . This demonstrates the benefits of using a forward-prediction modeling approach to PHYRE in that setting. Having said that, forward-prediction did not help in the cross-template setting, presumably, because rollouts on unseen templates, while realistic, were not accurate enough to solve the tasks.</p><p>Which PHYRE templates benefit from using a forwardprediction model? To investigate this, we compare Dec [Joint] 1f at τ = 0 (i.e., no forward-prediction) and τ = 10 seconds in terms of per-template AUCCESS. We define per-template AUCCESS as the average AUCCESS over all tasks in a template in the within-template setting. <ref type="figure" target="#fig_5">Figure 6</ref> shows the per-template AUCCESS for the five templates in which forward-prediction models help the least (left five groups) and the five templates in which these models help the most (right five). Qualitatively, we observe that forward prediction does not help much in "simple" tasks that comprise a few objects, whereas it helps a lot in more "complex" tasks. This is corroborated by the results in <ref type="figure" target="#fig_6">Figure 7</ref>, in which we show AUCCESS and the improvement in AUCCESS due to forward modeling (∆ AUCCESS) as a function of the number of objects in the task. We observe that AUCCESS decreases (Pearson's correlation coefficient, ρ = −0.4) with the number of objects in the task, but that the benefits of forward predictions increase (ρ = 0.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>While the results of our experiments demonstrate the potential of forward prediction for physical reasoning, they also highlight that much work is still needed for the full potential to materialize. The main challenge is that physical environments such as PHYRE are inherently chaotic: a small change in an action (or scene) may drastically affect the action's efficacy. <ref type="figure">Figure 8</ref> shows an example of this: our models produce realistic forward predictions (also see Appendix A), but they may still select incorrect actions.</p><p>Furthermore, PHYRE expects a single model to learn all templates and generalize across templates, exacerbating this challenge. Notably, recent work on particle-based representations <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49]</ref> has shown successful initial results in terms of generalization by limiting the set of object types to only one (viz., particles). However, such approaches are yet to be studied in extreme generalization settings as expected in PHYRE, and are constrained in accuracy by the underlying particle representation of extended objects, i.e. a pixel level decomposition would be most accurate though computationally infeasible. Nevertheless, we believe this is an interesting direction and we plan to study it in the context of PHYRE in future work.</p><p>Finally, much of our analysis is specific to 2D environments, as used in the physical challenge benchmarks like TOOLS <ref type="bibr" target="#b0">[1]</ref> and PHYRE. Extending this analysis to 3D or partially observable environments would also be interesting future work, where object-based models may have an advantage over pixel-based. Also, experimenting with more sophisticated scene-decomposition approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54]</ref>, can allow for better joint training of object-centric approaches and further improve performance on the PHYRE tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Rollout Visualizations</head><p>We first show rollouts when our model is trained on train tasks from a single template, and evaluated from the val tasks on that template. We chose one of the hardest templates in terms of FPA, Template 18. This evaluation is comparable to most prior work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49]</ref>, where variations of single environment are used during training and testing. Note that our model's rollouts are quite high fidelity in this case, as expected.</p><p>• Within-Template Template 18 (fold 0):</p><p>http://fwd-pred-phyre.s3-website. us-east-2.amazonaws.com/ within-temp18only/ However, PHYRE requires training a single model for all the templates in the dataset. We now show a comprehensive visualization of rollouts for all our forward-prediction models (as were used to compute forward-prediction accuracy), for both within and cross-template settings. Note that the rollout fidelity drops, understandably as the model needs to capture all the different templates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Rollout Accuracy in Cross-Template Setting</head><p>Similar to <ref type="figure" target="#fig_4">Figure 5</ref> in the main paper, we show the forward-prediction accuracy on the cross-template setting in <ref type="figure">Figure 9</ref>. As expected, the accuracy is generally lower in the cross-template setting, showing that the models struggle to generalize beyond training templates. Otherwise, we see similar trends as seen in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>It is interesting to note that Dec accuracy goes down and then up, similar to as observed in the within-template case. We find that it is likely because Dec is better able to predict the final position of the objects than the actual path the objects would take. Since it tends to smear out the object pixels when not confident of its position, the model ends up with lower accuracy during the middle part of the rollout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Other Task-Solution Models on Learned</head><p>Forward-Prediction Models C.1. Conv3D-Latent vs Conv3D-Pixel, on Pixel-Based Forward-Prediction Models</p><p>In <ref type="figure">Figure 10</ref>, we compare Conv3D-Latent and Conv3D-Pixel on learned pixel-based forward models. We find Conv3D-Latent generally performs better, especially for Dec, since that model does not produce accurate Conv3D-Latent performs as well or better than Conv3D-Pixel, and hence we use it for the experiments in the paper.</p><p>future predictions in terms of pixel accuracy (FPA). However, the latent space for that model still contains useful information, which the Conv3D-Latent is able to exploit successfully. Hence for pixel-based forward-prediction models, given the option between latent or pixel space task-solution classifiers, we choose Conv3D-Latent for experiments </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Tx-Cls vs Conv3D-Pixel, on Object-Based</head><p>Forward-Prediction models</p><p>In <ref type="figure" target="#fig_9">Figure 11</ref>, we compare the object-based task-solution model on learned object-based forward-prediction models. Similar to the observations with GT simulator in <ref type="figure">Figure 3</ref> (main paper), object-based task-solution model (Tx-Cls) performed worse than its pixel-based counterpart (Conv3D-Pixel), even with learned forward-prediction models. Jointly training the object-based forward models with object-based task-solution model improves performance, as shown for the IN model, however it is still worse than using a pixel-based task-solution model on the object-based forward model. Hence, for experiments in the paper, we render the object-based models' predictions to pixels, and use a pixel-based task-solution model (Conv3D-Pixel). Note that the other pixel-based tasksolution model, Conv3D-Latent, is not applicable here, as object-based forward-prediction models do not produce a spatial latent representation which Conv3D-Latent operates on. Moreover, training object-based models jointly with the pixel-based classifier is not possible in the absence of a differentiable renderer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Forward Prediction Accuracy (FPA) metric</head><p>To compute FPA, we first zero out all pixels with colors corresponding to non-moving objects, in both GT and prediction. This ensures that any motion of non moving objects over other non moving objects or background will incur no reduction in the FPA score. Then, FPA for a frame of the rollout is defined as the percentage of pixels that match between GT and prediction. Hence, if any object of colors corresponding to moving objects (red, green, blue, gray) is at an incorrect position (either overlapping with static or non static objects/background), it would incur a reduction in FPA. A python-style code for the computation is as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Joint model with only task-solution loss</head><p>For our best joint model, Dec [Joint], we evaluate the effect of setting the forward prediction loss to 0. As seen in <ref type="figure">Figure 12</ref>, the model performs about the same as it would without a forward model, obtaining similar performance at different number of rollout seconds (τ ). This further strengthens the claim that forward prediction leads to the improvements in performance (as also evident from the increase in AUCCESS on increasing τ ), as opposed to any changes in parameters or training dynamics. of actions being re-ranked at test time, in <ref type="figure">Figure 13</ref>. We find the performance varies nearly linearly with number of actions upto 10K actions, similar to the observations in <ref type="bibr" target="#b1">[2]</ref>.</p><p>G. Templates ranked by FPA <ref type="figure" target="#fig_3">Figure 14</ref> shows the easiest and hardest templates for each forward model. We find some of the hardest ones are indeed the ones that humans would also find hard, such as the template involving a see-saw system or complex extended objects like cups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Hyperparameters</head><p>All experiments were performed using upto 8 V100 32GB GPUs. Depending on the number of steps the model was rolled out for during training, the actual GPU requirements were adjusted. The training time for all forward-prediction models was around 2 days, and the task solution models took upto 4 days (depending on how far the forward-prediction model was rolled out). Our code will be made available to reproduce our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1. Forward-prediction models</head><p>We train all object-based models with teacher forcing. We use a batch size of 8 per GPU over 8 gpus. For each batch element, we sample clips of length 4 from the rollout, using the first three as context frames and the 4th as the ground truth prediction frame. We train for 200K iterations. We add Gaussian noise sampled from a N (0, 0.014435) distribution to the training data, similar to <ref type="bibr" target="#b3">[4]</ref>. We add noise to 20% of the data for the first 2.5% of training, decreasing the percentage of data that is noisy to 0% over the next 10% of training. The object based forward models only make predictions for dynamic objects, and use a hard tanh to clip the predicted state values between 0-1. The models don't use the state of static objects or the angle of ball objects when calculating the loss. For angles, we compute the mean squared error between the cosine of the predicted and ground truth angle. We now describe the other hyperparameters specific to each object-based and pixel-based forward-prediction model.</p><p>• IN (object-based): We train these models using Adam and a learning rate of 0.001. We use two interaction nets, one that makes predictions based on the last two context frames, the other that makes predictions based on the first two context frames. Using the same architecture as <ref type="bibr" target="#b3">[4]</ref>, the relation encoder is a five layer MLP, with hidden size 100 and ReLU activation, that embeds the relation into a dimension 50 vector. The aggregated and external effects are passed through a three layer MLP with hidden size 150 and ReLU activation. Each interaction net makes a velocity prediction for the object, and the results are concatenated with the object's previous state and passed to a three layer MLP with hidden size 64 and ReLU activation to make the final velocity prediction per object. The predicted state is a sum of the velocity and the object's previous state. • Tx (object-based): We train these models using Adam and a learning rate of 0.0001. We use a two layer MLP with a hidden size of 128 and ReLU activation to embed the objects into a 128 dimensional vector. A sinusoidal temporal position encoding <ref type="bibr" target="#b54">[55]</ref> of time t is added to the features of each object. The result is passed to a Transformer encoder with 8 heads and 6 layers. The embeddings corresponding to the last time step are passed to the final three layer MLP with ReLU activations and hidden size of 100 to make the final prediction. The model predicts the velocity of the object, which is summed with the object's last state to get a new state prediction. • STN (pixel-based): We also train these models with teacher forcing. Since pixel-based models involve a ResNet-18 image encoder, we use a batch size of 2 per GPU, over 8 GPUs. For each batch element, we sample clips of length 16 from the rollout, and construct all possible sets with 3 context frames and the 4th ground truth prediction frame. The models were trained using a learning rate of 0.00005, adam optimizer, with cosine annealing over 100K iterations. The scene was split into objects using the connected components algorithm, and we split each color channel into upto 2 objects. The model then predicts rotation and transformation for each object channel, which are used to construct an affine transforma-tion matrix. The last context frame is transformed using this affine matrix to generate the predicted frame, which is passed through the image encoder to get the latent representation for the predicted frame (i.e. for STN, the future frame is predicted before the future latent representation). • Dec (pixel-based): For these models, we do not use teacher forcing, and use the last predicted states to predict future states at training time. We use a batch size of 2 per gpu over 8 gpus. For each batch element, we sample a 20-length clip from the simulator rollout, and train the model to predict upto 10 steps into the future (note with teacher forcing, models are trained only to predict 1 step into the future given 3 GT states). The model is trained for 50K iterations with a learning rate of 0.01 and SGD+Momentum optimizer. • Dec [Joint] (pixel-based): For this model, we train both forward-prediction and task-solution models jointly, with equally weighted losses. For this, we sample 13length rollout, always starting from frame 0. Instead of considering all possible starting points from the 13 states (as in Dec and STN), we only use the first 3 states to bootstrap, and roll it out for upto 10 steps into the future. We only incur forward-prediction losses for upto the first 5 of those 10 steps, we observed instability in training on predicting for all steps. Here we use a batch size of 8/gpu, over 8 gpus. The model is trained with learning rate of 0.0125 with SGD+Momentum for 150K iterations. The task-solution model used is Conv3D-Latent, which operates on the latent representation being learned by the forward-prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2. Task-solution models</head><p>For all these models, we always sample τ = 3 frames from the start point of each simulation, and roll it out for different number of τ states autoregressively, before passing through one of these task-solution models.</p><p>• Tx-Cls (for object-based): We train a Transformer encoder model on the object states predicted by object-based forward-prediction models. The object states are first encoded using a two layer MLP with ReLU activation into an embedding size of 128. A sinusoidal temporal position encoding <ref type="bibr" target="#b54">[55]</ref> of time t is added to the features of each object. The result is passed to a Transformer encoder which has 16 heads and 8 layers and uses layer normalization. The encoding is passed to three layer MLP with hidden size 128 and ReLU activations to classify the embedding as solved or not solved. We use a batch size of 128 and train for 150K iterations with SGD optimizer, a learning rate of 0.002, and momentum of 0.9. • Conv3D-Latent (for pixel-based): We train a 5-layer 3D convolutional model (with ReLU in between) on the latent space learned by forward-prediction models. We use a batch size of 64 and train for 100K iterations with SGD optimizer and learning rate of 0.0125. • Conv3D-Pixel (for both object and pixel-based):</p><p>We train a 2D + 3D convolutional model on future states rendered as pixels. We use a batch size of 64 and train for 100K iterations with SGD optimizer, learning rate of 0.0125, and momentum of 0.9. This model consists of 4 ResNet-18 blocks to encode the frames, followed by 5 3D convolutional layers over the frames' latent representation, as used in Conv3D-Latent. When object-based models are trained with this task-solution model, we run the forward-prediction model and the renderer in the data loader threads (on CPU), and feed the predicted frames into the task-solution model (training on GPU). We found this approach to be more computationally efficient than running both forward-prediction and task-solution models on GPU, and in between the two, swapping out the data from GPU to CPU and back, to perform the rendering on CPU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) and (c)) into a correct solution(Figure 1(b)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 ,Figure 3 :</head><label>23</label><figDesc>we implement the task solution model using a simple binary classifier C with parameters ψ. The classifier receives the τ + τ (initial and predicted) frames and/or WithinAUCCESS of object-based ( ) and pixel-based ( ) task-solution model (y-axis) applied on state obtained by rolling out an oracle forward-prediction model for τ seconds (x-axis)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 1 )Figure 4 :</head><label>14</label><figDesc>Conv3D-Latent: the latent state representations (z,ẑ) are concatenated along the temporal dimension, and passed through a stack of 3D convolu-AUCCESS of task-solution model applied on state obtained by rolling out five object-based ( ) and pixel-based ( ) forward-prediction models (y-axis) for τ seconds (xaxis). Forward-prediction models were initialized with τ = 3 ground-truth states. AUCCESS of agent without forward prediction (No-fwd) is shown for reference. Results are presented for the within-template (left) and cross-template (right) settings. tions with intermediate ReLUs followed by a linear classifier; and (2) Conv3D-Pixel: the pixel representations (x,x) are encoded using a ResNet-18 (up to res4), and classifications are made by the Conv3D-Latent model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Left: Within-template forward-prediction accuracy (FPA) after τ seconds roll out of five forwardprediction models. Right: Maximum AUCCESS value across roll-out as a function of forward-prediction accuracy averaged over τ = 10 seconds for the five models. Shaded regions and error bars indicate standard deviation over 10 folds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Per-template AUCCESS of Dec [Joint]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Per-template AUCCESS of the τ = 0 model as a function of the number of objects in the task template (left) and improvement in per-template AUCCESS, called ∆ AUCCESS, obtained by the τ = 10 model (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>•</head><label></label><figDesc>Within-Template (fold 0): http://fwd-pred-phyre.s3-website. us-east-2.amazonaws.com/within/ • Cross-Template (fold 0): http://fwd-pred-phyre.s3-website. us-east-2.amazonaws.com/cross/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Left: Forward-prediction accuracy (FPA; y-axis) after τ seconds (x-axis) rolling out five forward-prediction models. Right: Maximum AUCCESS value across rollout (y-axis) as a function of forward-prediction accuracy averaged over τ = 10 seconds (x-axis) for five forwardprediction models. Shaded regions and error bars indicate standard deviation over 10 folds. Both shown for crosstemplate setting. Comparison of Conv3D-{Latent, Pixel} classifiers on learned pixel-based forward-prediction models: Dec and STN, in the within-template setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Tx w/ Conv3D-Pixel IN w/ Tx-Cls TX w/ Tx-Cls IN w/ Tx-Cls [Joint]Seconds (τ ) Comparison of Tx-Cls and Conv3D-Pixel classifiers on learned object-based forward-prediction models: IN and Tx, in the within-template setting. Since Conv3D-Pixel generally performed better, we use it for the experiments shown in the paper.in the paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 20 axis=- 1 )</head><label>201</label><figDesc>def zero_out_non_moving_channels(img): 2 is_red = np.isclose(img, RED) 3 is_green = np.isclose(img, GREEN) 4 is_blue = np.isclose(img, BLUE) 5 is_gray = np.isclose(img, GRAY) 6 is_l_red = np.isclose(img, L_RED) 7 img[~(is_red | is_green | is_blue | is_gray | is_l_red)] = 0.0 == prediction.shape[-1] 21 frame_size = gt.shape[0] * gt.shape[1] 22 return np.sum(all_channels_close) / frame_size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>F.Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Performance with different number of actions ranked Similar to Figure 4 in [2], we analyze the performance of our best model, Dec [Joint] 1f, at different number AUCCESS Effect of setting forward prediction loss to 0 in Dec [Joint]. The performance is stagnant with the rollout if loss on the future prediction is set to 0, as expected. The model performs comparably to a model without forward prediction. Performance of Dec [Joint] 1f at different number of actions. The performance varies nearly linearly with number of actions ranked.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Easiest and hardest templates in terms of FPA.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors thank Rob Fergus, Denis Yarats, Brandon Amos, Ishan Misra, Eltayeb Ahmed, Anton Bakhtin and the entire Facebook AI Research team for many helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Tools challenge: Rapid trial-and-error learning in physical problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">A</forename><surname>Kelsey R Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<editor>CogSci</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">PHYRE: A new benchmark for physical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cophy: Counterfactual learning of physical dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gregor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03006</idno>
		<title level="m">Demis Hassabis, and Daan Wierstra. Learning and querying fast generative models for reinforcement learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11390</idno>
		<title level="m">Unsupervised scene decomposition and representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Se3-nets: Learning rigid body motion using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arunkumar</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13334</idno>
		<title level="m">Symplectic recurrent neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning in a handful of trials using probabilistic dynamics models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurtland</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lagrangian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Spergel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirley</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting spatial invariance for scalable unsupervised object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Task-agnostic dynamics priors for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Genesis: Generative scene inference and sampling with object-centric latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oiwi</forename><forename type="middle">Parker</forename><surname>Adam R Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Koray Kavukcuoglu, and Geoffrey E Hinton. Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szepesvari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning visual predictive models of physics for playing billiards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-object representation learning with iterative variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphaël</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hamiltonian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misko</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shapestacks: Learning vision-based physical intuition for generalised object stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neuroanimator: Fast neural network emulation and control of physics-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radek</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics and Interactive Techniques</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent world models facilitate policy evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dream to control: Learning behaviors by latent imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning latent dynamics for planning from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reasoning about physical interactions with object-oriented prediction and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Contrastive learning of structured world models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elise</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequential attend, infer, repeat: Generative modelling of moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Adam Roman Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Intuitive physics: Current research and controversies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James R Kubricht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjing</forename><surname>Holyoak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in cognitive sciences</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning physical intuition of block towers by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">To fall or not to fall: A visual approach to physical stability prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyedmajid</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00066</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tenenbaum, and Antonio Torralba. Visual grounding of learned physical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Tedrake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Soft rasterizer: A differentiable renderer for image-based 3D reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">OpenDR: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">learning to predict the effect of forces in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Flexible neural representation for physics prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Mrowca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">L</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning long-term visual dynamics with region proposal interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ESPRIT: Explaining Solutions to Physical Reasoning Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Nazneen Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadit</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Riochet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><forename type="middle">Ynocente</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07616</idno>
		<title level="m">Rob Fergus, Véronique Izard, and Emmanuel Dupoux. IntPhys: A framework and benchmark for visual intuitive physics reasoning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Optimization of computer simulation models with rare events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Reuven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Budden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00690</idno>
	</analytic>
	<monogr>
		<title level="j">Abbas Abdolmaleki</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Andrew Lefrancq</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Josh Merel. et al. Deepmind control suite</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">MuJoCo: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Relational neural expectation maximization: Unsupervised discovery of objects and their interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Entity abstraction in visual model-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Veerapaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungryull</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">3d-physnet: Learning the intuitive physics of non-rigid object deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Visual interaction networks: Learning a physics simulator from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tacchetti</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Centrosymmetric (cross-symmetric) matrices, their basic properties, eigenvalues, and eigenvectors. The American Mathematical Monthly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weaver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to see physics via visual de-animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Interpretable intuitive physics model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Compositional video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">CLEVRER: Collision events for video representation and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
