<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Video Classification via Temporal Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
							<email>kaidicao@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Ji</surname></persName>
							<email>jingweij@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yi</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
							<email>jniebles@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Video Classification via Temporal Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is a growing interest in learning a model which could recognize novel classes with only a few labeled examples. In this paper, we propose Temporal Alignment Module (TAM), a novel few-shot learning framework that can learn to classify a previous unseen video. While most previous works neglect long-term temporal ordering information, our proposed model explicitly leverages the temporal ordering information in video data through temporal alignment. This leads to strong data-efficiency for few-shot learning. In concrete, TAM calculates the distance value of query video with respect to novel class proxies by averaging the per frame distances along its alignment path. We introduce continuous relaxation to TAM so the model can be learned in an end-to-end fashion to directly optimize the few-shot learning objective. We evaluate TAM on two challenging real-world datasets, Kinetics and Something-Something-V2, and show that our model leads to significant improvement of few-shot video classification over a wide range of competitive baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The emergence of deep learning has greatly advanced the frontiers of action recognition <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b3">4]</ref>. The main focus tends to center around learning effective video representations for classification using large amounts of labeled data. In order to recognize novel classes that a pretrained network has not seen before, typically we need to manually collect hundreds of video samples for knowledge transferring. But such a procedure is rather tedious and labor intensive especially for videos, where the difficulty and cost of labeling is much higher compared to images.</p><p>There is a growing interest in learning models capable of effectively adapting themselves to recognize novel classes with only a few labeled examples. This is known as the fewshot learning task <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6]</ref>. Under the setup of meta-learning based few-shot learning, the model is explicitly trained to * Indicates equal contribution.  deal with scarce training data for previously unseen classes across different episodes. While the majority of recent fewshot learning works focus on image classification, adapting it to video data is not a trivial extension.</p><p>Videos are much more complicated than images, as recognizing some specific actions, such as opening the door, usually requires a complete modeling of temporal information. In the previous literature of video classification, 3D convolution and optical flow are two of the most popular methods to model temporal relations. The direct output of neural network encoders is always a temporal sequence of deep encoded features. State-of-the-art approaches commonly apply a temporal pooling module (usually mean pooling) in order to make final prediction. As observed before, averaging the deep features only captures the cooccurrence rather than the temporal ordering of patterns, which will unavoidably result in information loss.</p><p>Loss of information is even more severe for few-shot learning. It is hard to learn the local temporal patterns which are useful for few-shot classification with limited amount of data. Utilizing the long-term temporal ordering information, which is often neglected in previous works on video classification, might potentially help with few-shot learning. For example, if the model could verify that there is a procedure of pouring water before a close-up view of a just made tea, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the model will then become quite confident about predicting the class of this query video to be making tea, rather than some other potential predictions like boiling water or serving tea. In addition, <ref type="figure" target="#fig_1">Fig. 1</ref> also shows that for two videos in the same class, even though they both contain a procedure of pouring water followed by closed-up view of tea, the exact temporal duration of each atomic step can vary dramatically. This non-linear temporal variations of videos pose great challenges for fewshot video learning.</p><p>With these insights, we thus propose Temporal Alignment Module (TAM) for few-shot video classification, a novel temporal-alignment based approach that learns to estimate temporal alignment score of a query video with corresponding proxies in the support set. To be specific, we compute temporal alignment score for each potential querysupport pair by averaging per-frame distances along a temporal alignment path, which enforces the score we use to make prediction to preserve temporal ordering. Furthermore, TAM is fully differentiable so that the model can be trained end-to-end and optimize the few-shot objective directly. This in turn helps the model to better utilize longterm temporal information to make few-shot learning predictions. This module allows us to better model the temporal evolution of videos, while enabling stronger data efficiency in the few-shot setting. We evaluate our model for few-shot video classification task on two action recognition datasets: Kinetics <ref type="bibr" target="#b16">[17]</ref> and Something-Something V2 <ref type="bibr" target="#b11">[12]</ref>. We show that when there is only a single example available, our method outperforms the mean pooling baseline which does not consider temporal ordering information by approximately 8% in top-1 accuracy. We also show qualitatively that the proposed framework is able to learn meaningful alignment path in an endto-end manner.</p><p>In summary, our main contributions are: (i) We are the first to explicitly address the non-linear temporal variations issue in the few-shot video classification setting. (ii) We propose Temporal Alignment Module (TAM), a dataefficient few-shot learning framework that can dynamically align two video sequences while preserving the temporal ordering, which is often neglected in previous works. (iii) We use continuous relaxation to make our model fully differentiable and show that it outperforms previous state-of-the-art methods by a large margin on two challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Few-Shot Learning. To address few-shot learning, a direct approach is to train a model on the training set and finetune with the few data in the novel classes. Since the data in novel classes are not enough to fine-tune the model with general learning techniques, methods are proposed to learn a good initialization model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">32]</ref> or develop a novel optimizer <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b25">25]</ref>. These works aim to relieve the difficulty of fine-tuning the model with limited samples. However, such methods suffer from overfitting when the training data in novel classes are scarce but the variance is large. Another branch of works, which learns a common metric for both seen and novel classes, can avoid overfitting to some extent. Convolutional Siamese Net <ref type="bibr" target="#b20">[20]</ref> trains a Siamese network to compare two samples. Latent Embedding Optimization <ref type="bibr" target="#b39">[39]</ref> employs attention kernel to measure the distance. Prototypical Network <ref type="bibr" target="#b35">[35]</ref> utilizes the Euclidean distance to the class center. Graph Neural Networks <ref type="bibr" target="#b9">[10]</ref> constructs a weighted graph to represent all the data and measure the similarity between data. Other methods use data augmentation, which learns to augment labeled data in unseen classes for supervised training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">44]</ref>. However, video generation is still an under-explored problem at least generating videos condition on a typical category. Thus, in this paper, we employ the metric learning approach and designs a temporalaligned video metric for few-shot video classification.</p><p>There are works exploring few-shot recognition. OSS-Metric Learning <ref type="bibr" target="#b19">[19]</ref> proposes a novel OSS-Metric Learning to measure the similarity of video pairs to enable one-shot video classification. <ref type="bibr" target="#b23">[23]</ref> introduces a zero-shot method which learns a mapping function from an attribute to a class center. It has an extension to few-shot learning by integrating labeled data on unseen classes. CMN <ref type="bibr" target="#b47">[47]</ref> is the most related work to ours. They introduce a multi-saliency embedding algorithm to encode video into a fixed-size matrix representation. Then they propose a compound memory network (CMN) to compress and store the representation and classify videos by matching and ranking. However, previous works collapse the order of frames at representation <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b47">47]</ref>. Thus, the learned model is sub-optimal for video datasets where sequence order is important. In this paper, we preserve the frame order in video representation and estimate distance with temporal alignment, which utilizes video sequence order to solve few-shot video classification. Video Classification. A significant amount of research has tackled the problem of video classification. State-of-theart video classification methods have evolved from handcrafted representation learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b40">40]</ref> to deep-learning based models. C3D <ref type="bibr" target="#b37">[37]</ref> utilizes 3D spatial-temporal convolutional filters to extract deep features from sequences of RGB frames. TSN <ref type="bibr" target="#b41">[41]</ref> and I3D <ref type="bibr" target="#b3">[4]</ref> uses two-stream 2D or 3D CNNs with larger size on both RGB and opti- Overview of our method. We first extract per-frame deep features using the embedding network. We then compute the distance matrices between the query video and videos in the support set. Next, an alignment score is computed out of the matrix representation. Finally we apply softmax operator over the alignment score of each novel class. cal flow sequences. By factorizing 3D convolutional filters into separate spatial and temporal components, P3D <ref type="bibr" target="#b29">[29]</ref> and R(2+1)D <ref type="bibr" target="#b38">[38]</ref> yield models with comparable or superior classification accuracy but smaller in size. An issue of these video representation learning methods is their dependence on large-scale video datasets for training. Models with an excessive amount of learnable parameters tend to fail when only a small number of training samples are available.</p><p>Another concern of video representation learning is the lack of temporal relational reasoning. Classification on the videos sensitive to temporal ordering poses a more significant challenge to the above networks which are tailored to capture short-term temporal features. Non-local neural networks <ref type="bibr" target="#b42">[42]</ref> introduce self-attention to aggregate temporal information in the long-term. Wang et al. <ref type="bibr" target="#b43">[43]</ref> further employ space-time region graphs to model the spatialtemporal reasoning. Recently, TRN <ref type="bibr" target="#b46">[46]</ref> proposes a temporal relational module to achieve superior performance. Still, these networks inevitably pool/fuse features from different frames in the last layers to extract a single feature vector representing the whole video. In contrast, our model is able to learn video representation without loss of temporal ordering in order to generate more accurate final predictions. Sequence Alignment. Sequence alignment is of great importance in the field of bioinformatics, which describes the way of arrangement of DNA/RNA or protein sequences, in order to identify the regions of similarity among them <ref type="bibr" target="#b1">[2]</ref>. In the vision community, researchers have growing interests in tackling the sequence alignment problem with high dimensional multi-modal data, such as finding the alignment between untrimmed video sequence and the corresponding textual action sequence <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">31]</ref>. The main technique that has been applied to this line of work is dynamic programming. While dynamic programming is guar-anteed to find the optimal alignment between two sequences given a prescribed distance function, the discrete operations used in dynamic programming are non-differentiable and hence prevent learning distance functions with gradientbased methods. Our work is closely related to recent progress on using continuous relaxation of discrete operations to tackle sequence alignment problem <ref type="bibr" target="#b4">[5]</ref> and hence allow us to train our entire model end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Our goal is to learn a model which can classify novel classes of videos with only a few labeled examples. The wide range of intra-class spatial-temporal variations of videos poses great challenges for few-shot video classifications. We address this challenge by proposing a fewshot learning framework with Temporal Alignment Module (TAM), which is to our best knowledge the first model that can explicitly learn a distance measure independent of non-linear temporal variations in videos. The use of TAM sets our approach apart from previous works that fail to preserve temporal ordering and relation during meta training and meta testing. <ref type="figure" target="#fig_2">Fig.2</ref> shows the outline of our model.</p><p>In the following, we will first provide a problem formulation of few-shot video classification task, and then define our model and show how it can be used at training and test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>In the few-shot video classification setting, we split the classes we have annotation of into C train : the base classes that have sufficient data for representation learning and C test : the novel or unseen classes that have only a few labeled data during testing stage. The goal of few-shot learn-ing is then to train a network that can generalize well to new episodes over novel classes. In a n-way, k-shot problem, for each episode the support set will contain n novel classes, and each class will have a very small amount of samples (k in our setting). The algorithm will have to classify videos from query set to one of the novel classes in support set. Episodes are randomly drawn from a larger collection of data, which we hereby denote them as meta set. In our setting, we introduce 3 splits over classes as meta training T train , meta validation T val and meta testing T test sets.</p><p>We formulate the few-shot learning as a representation learning problem through a distance function</p><formula xml:id="formula_0">φ(f ϕ (x 1 ), f ϕ (x 2 ))</formula><p>, where x 1 and x 2 are two samples drawn from C train and f ϕ (·) is an embedding function that maps samples to their representations. The difference between our problem formulation with the majority of previous fewshot learning researches lies in the fact that we are now dealing with higher dimensional inputs, i.e. (2+1)D volumes instead of 2D images. The addition of the time dimension in few-shot setting demands the model to be able to learn temporal ordering and relation with limited data in order to generalize to novel classes, which pose challenges that have not been properly addressed by previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model</head><p>With the above problem formulation, our goal is to learn a video distance function by minimizing the few-shot learning objective. Our key insight is that we want to explicitly learn a distance function independent of non-linear temporal variations by aligning the frames of two videos. Unlike previous works which use weighted average or mean pooling along the time dimension <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b47">47]</ref>, our model is able to infer temporal ordering and relationship during meta training or meta testing in an explicit and data efficient manner. In this subsection, we will breakdown our model following the pipeline as illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. Embedding Module: The purpose of the embedding module f ϕ is to generate a compact representation of a trimmed action video that encapsulates its visual content. A raw video usually consists of hundreds of frames, whose information could be redundant were to perform per frame inference. Thus frame sampling is usually adopted as a preproccessing stage for video inputs. Existing frame sampling schemes can be mainly divided into two categories: dense sampling (randomly cropping out T consecutive frames from the original full-length video and then dropping every other frame) <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b45">45]</ref> and sparse sampling (samples distribute uniformly along the temporal dimension) <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b21">21]</ref>. We follow the sparse sampling protocol first described in TSN <ref type="bibr" target="#b41">[41]</ref>, which divides the video sequence into T segments and extracts a short snippets in each segment. The sparse sampling scheme allows each video sequence to be represented by a fix number of snippets. The sampled snippets span the whole video, which enables longterm temporal modeling.</p><p>Given a input sequence S = {X 1 , X 2 , ..., X T }, we will apply a CNN backbone network f ϕ to process each of the snippets individually. After the encoding, raw video snippets will then turn into a sequence of feature vectors</p><formula xml:id="formula_1">f ϕ (S) = {f ϕ (X 1 ), f ϕ (X 2 ), ..., f ϕ (X T )}.</formula><p>It is worth noticing that for the embedding of each video f ϕ (S), its dimension is T × D f , rather than D f for image embedding, which is usually chosen as the activation before final fullyconnected layer of a CNN network. Distance Measure with Temporal Alignment Module (TAM):</p><p>Given two videos S i , S j and their embedded features</p><formula xml:id="formula_2">f ϕ (S i ), f ϕ (S j ) ∈ R T ×D , we can calculate the frame-level distance matrix D ∈ R T ×T as D l,m = 1 − f ϕ (S i ) l, · f ϕ (S j ) m, ||f ϕ (S i ) l, || ||f ϕ (S j ) m, || ,<label>(1)</label></formula><p>where D l,m is the frame-level distance value between the lth frame of video S i and the mth frame of video S j . We further define W ⊂ {0, 1} T ×T to be the set of possible binary alignment matrices, where ∀W ∈ W, W ij = 1 if the ith frame of video S i is aligned to the jth frame of video S j . Our goal is to find the best alignment W * ∈ W</p><formula xml:id="formula_3">W * = argmin W ∈W W, D(f ϕ (S i ), f ϕ (S j )) ,<label>(2)</label></formula><p>which minimizes the inner product between the alignment matrix W and the frame-level distance matrix D defined in Eq. (1). The video distance measure is thus given by</p><formula xml:id="formula_4">φ(f ϕ (S i ), f ϕ (S j )) = W * , D .<label>(3)</label></formula><p>We propose to use a variant of Dynamic Time Warping (DTW) algorithm <ref type="bibr" target="#b24">[24]</ref> to solve Eq. (2). This is achieved by solving for a cumulative distance function</p><formula xml:id="formula_5">γ(i, j) = Dij + min{γ(i − 1, j − 1), γ(i − 1, j), γ(i, j − 1)}.<label>(4)</label></formula><p>In this setting of plain DTW, an alignment path is a contiguous set of matrix elements which defines a mapping between two sequences that satisfies the following conditions: boundary conditions, continuity and monotonicity. The boundary condition poses constraints on alignment matrix W such that W 11 = 1 and W T T = 1 must be true in all possible alignment paths. In our alignment formulation, though the videos are trimmed, the action in the query video does not have to match exactly about its start and end action with the proxy. For example, consider the action of making coffee, there might be a snippet of stirring coffee at the end of action and it might not. To address this issue, we propose to relax this the boundary condition. Instead of having a path aligning the two videos from start to end, we allow the algorithm to find a path with flexible starting and ending points, while maintaining continuity and monotonicity. To work through this, we pad two column of 0s at the start and end of the distance matrix so that it functions as enabling the alignment process to start and end at arbitrary position. So for our method, instead of computing the alignment score on a T × T matrix, we work with the padded matrix of size T × (T + 2). We further denote the indexes of the first dimension as 1, 2, ..., T , and indexes of the second dimension as 0, 1, 2, ..., T, T + 1, for simplicity. The compute of cumulative distance function is then changed into function</p><formula xml:id="formula_6">γ(i, j) = Dij +      min{γ(i − 1, j − 1), γ(i − 1, j), γ(i, j − 1)}, j = 0 or j = T + 1 min{γ(i − 1, j − 1), γ(i, j − 1), otherwise<label>(5)</label></formula><p>Note that if we follow the Eq. 5 to compute the alignment score, the score by itself is naturally normalized. Since at each time step except j = 0 and j = T − 1, the alignment function forces a path from γ(·, j − 1) to γ(·, j), the final alignment score will be a summation of exactly T scores. In this light, alignment scores computed from different pairs of query videos and support videos are normalized, which means that the scale will not be affected by the path chosen. Differentiable TAM with Continuous Relaxation: While the above formulation is straightforward, the key technical challenge is that γ is not differentiable with respect to the distance function D. Following the recent works on continuous relaxation of discrete operation and its application in video temporal segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">22]</ref>, we introduce a continuous relaxation to our Temporal Alignment Module (TAM). We use log-sum-exp with a smoothing parameter λ &gt; 0 to approximate the non-differentiable minimum operator in Eq. <ref type="formula" target="#formula_6">(5)</ref> min(x 1 , x 2 , ..., x n ) ≈ −λ log</p><formula xml:id="formula_7">n i=1 e −xi/λ if λ → 0. (6)</formula><p>While the use of continuous relaxation in Eq. (6) does not convexify the the objective function, it helps the optimization process and allows gradients to be backpropagated through TAM. Training and Inference: We have shown how to compute the cumulative distance function γ and use continuous relaxation to make it differentiable given a pair of input videos (S i , S j ). The video distance measure is given by</p><formula xml:id="formula_8">φ(f ϕ (S i ), f ϕ (S j )) = γ(T, T + 1).<label>(7)</label></formula><p>In training time, given ground-truth video pair (S,Ŝ) and support set S, we train our entire model end-to-end by directly minimizing the loss function</p><formula xml:id="formula_9">L = − log exp(−φ(f ϕ (S), f ϕ (Ŝ))) Z∈S exp(−φ(f ϕ (S), f ϕ (Z)))</formula><p>.</p><p>At test time we are given an unseen query video Q and its support set S, our goal is to find the video S * ∈ S that minimize the video distance function</p><formula xml:id="formula_11">S * = argmin S∈S φ(Q, S).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this work, our task is few-shot video classification, where the objective is to classify novel classes with only a few examples from the support set. We divide our experiments into the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>As pointed out by <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46]</ref>, existing action recognition datasets can be roughly classified into two groups: Youtube type videos: UCF101 <ref type="bibr" target="#b36">[36]</ref>, Sports 1M <ref type="bibr" target="#b15">[16]</ref>, Kinetics <ref type="bibr" target="#b16">[17]</ref>, and crowd-sourced videos: Jester <ref type="bibr" target="#b0">[1]</ref>, Charades <ref type="bibr" target="#b34">[34]</ref>, Something-Something V1&amp;V2 <ref type="bibr" target="#b11">[12]</ref>, in which the videos are collected by asking the crowd-source workers to record themselves performing instructed activities. Crowdsourced videos usually focus more on modeling the temporal relationships, since visual contents among different classes are more similar than those of Youtube type videos. To demonstrate the effectiveness of our approach on these two groups of video data, we base our few-shot evaluation on two action recognition datasets, Kinetics <ref type="bibr" target="#b16">[17]</ref> and Something-Something V2 <ref type="bibr" target="#b11">[12]</ref>.</p><p>Kinetics <ref type="bibr" target="#b16">[17]</ref> and Something-Something V2 <ref type="bibr" target="#b11">[12]</ref> are constructed to serve as action recognition datasets so we have to construct their few-shot versions. For Kinetics dataset, we follow the same split as CMN <ref type="bibr" target="#b47">[47]</ref> and sample 64 classes for meta training, 12 classes for validation and 24 classes for meta testing. Since there is no existing split for few-shot classification on Something-Something V2, we construct a few-shot dataset following the same rule as CMN <ref type="bibr" target="#b47">[47]</ref>. We randomly selected 100 classes from the whole dataset. The 100 classes are then split into 64, 12 and 24 classes as the meta-training, meta-validation and metatesting set, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For a n-way, k-shot test setting, we randomly sample n classes with each class containing k examples as the support set. We construct the query set to have n examples, where each unlabeled sample in the query set belongs to one of the n classes in the support set. Thus each episode has a total of n(k + 1) examples. We report the mean accuracy by randomly sampling 10,000 episodes in the following experiments.</p><p>We follow the video preprocessing procedure introduced in TSN <ref type="bibr" target="#b41">[41]</ref>. During training we first resize each frame in the video to 256 × 256 and then randomly crop a 224 × 224 region from the video clip. For inference we change the random crop to center crop. For Kinetics dataset we randomly apply horizontal flip during training. Since the label in Something-Something V2 dataset incorporates an assumption of left and right, e.g. pulling something from left to right and pulling something from right to left, so we do not use horizontal flip for this dataset.</p><p>Following the experiment setting of CMN, we use ResNet-50 <ref type="bibr" target="#b13">[14]</ref> as the backbone network for TSN. We initialize network using pre-trained models on ImageNet <ref type="bibr" target="#b6">[7]</ref>. We optimized our model with SGD <ref type="bibr" target="#b2">[3]</ref>, with a starting learning rate of 0.001 and decaying every 30 epochs by 0.1. We use meta-validation set to tune the parameters, and stop the training process when the accuracy on the metavalidation set is about to decrease. We implemented the whole framework with PyTorch <ref type="bibr" target="#b27">[27]</ref> framework. The whole model takes 4 TITAN Xp GPUs to train for 10 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluating Few-Shot Learning</head><p>We now evaluate the representations we learned after optimizing few-shot learning objective. We compare our method with the two following categories of baselines: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Train from ImageNet Pretrained Features</head><p>For baselines that use ImageNet pretrained features, we follow the same setting as described in CMN. As the fact that previous few-shot learning algorithms are all designed to deal with images, they usually take image-level feature encoded by some backbone network as input. To circumvent this discrepancy, we first feed frames of a video to a ResNet-50 network pretrained on ImageNet, and then average frame-level features to obtain a video-level feature. The averaged video-level feature is then served as the input of few-shot algorithms.</p><p>Matching Net <ref type="bibr" target="#b39">[39]</ref> We use an FCE classification layer in the original paper without fine-tuning in all experiments. The FCE module uses a bidirectional-LSTM and each training example could be viewed as an embedding of all the other examples. MAML <ref type="bibr" target="#b8">[9]</ref> Given the video-level feature as the input, we train the model following the default hyper-parameter and other settings described in <ref type="bibr" target="#b8">[9]</ref>. CMN <ref type="bibr" target="#b47">[47]</ref> As CMN is specially designed for few-shot video classification, it could handle video feature inputs directly. The encoded feature sequence is first fed into a multi-saliency embedding function to get a video-level feature. Final few-shot prediction is done by a compound memory structure similar to <ref type="bibr" target="#b14">[15]</ref>. For the experiment results using ImageNet pretrained backbones, we directly take the numbers from CMN <ref type="bibr" target="#b47">[47]</ref> to ensure fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Finetune from Backbone on Meta Training Set</head><p>As raised by <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">28]</ref>, using cosine distances between the input feature and the trainable proxy for each class could explicitly reduce intra-class variations among features during training. The rigorous experiments in <ref type="bibr" target="#b5">[6]</ref> has shown that the Baseline++ model is competitive or even surpass when compared with other few-shot learning methods. So in finetuned settings we adapt several previous approaches with the structure of Baseline++ to serve as strong baselines. TSN++ For TSN++ baseline, we also use episode-based  Although the averaged score is quite high given the false matching and the query image, our algorithm is able to find the correct alignment path the minimize the alignment score, which ultimately results in the correct prediction.</p><p>training to simulate the few-shot setting at meta-train stage to directly optimize for generalization to unseen novel classes. In order to get a video-level representation, we average over the temporal dimension of extracted per frame features for both query sets and support sets. The video level feature from support set could then serve as proxies for each novel class. We can then obtain the prediction probability for each class by normalizing these cosine distance values with a softmax function. For inference during metatesting stage, we first forward each video in the support set to get proxies for each class. Given the proxies we can then make prediction for videos in query set. CMN++ We follow the setting of CMN and reimplement this method by ourselves. The only difference about CMN++ and CMN is that we replace the ImageNet pretrained feature with the feature extracted by TSN++ mentioned above. TRN++ We also compare our approach against methods that attempt to learn a compact video-level representation given a sequence of image-level features. TRN <ref type="bibr" target="#b46">[46]</ref> proposes a temporal relation module, which uses multilayer perceptrons (MLP) to fuse features of different frames. We refer TRN++ to one of baselines by replacing average consensus module in TSN++ with temporal relation module.</p><p>By default we conduct 5-way few-shot classification if there is no further clarification. The 1-shot and 5-shot video classification results on both the Kinetics and Something-Something V2 datasets are listed in <ref type="table" target="#tab_0">Table 1</ref>. It can be concluded that our approach significantly outperforms all the baselines on both datasets. In CMN paper, the experimental observations show that fine-tuning the backbone module on the meta-training set does not improve the few-shot video classification performance. In contradiction, we find that with proper data augmentation and training strategy, a model could be trained to generalize better on unseen classes in a new domain given the meta-training set. By comparing the results of TSN++ and TRN++, we could conclude that considering temporal relation explicitly helps with model generalization on unseen classes. Compare to TSN++, the improvement brought by CMN++ is not as large as the gap on ImageNet pretrained features reported in the original paper. This may be due to the reason that we are now using a more suitable distance function (cosine distance) during meta-training so that the frame-level feature is more discriminative among unseen classes. This in turn makes it harder to improve the final prediction given those strong features as the input. Finally it is worthwhile to note that TAM outperforms all the finetuned baselines by a large margin. This demonstrates the importance of taking temporal ordering information into consideration while dealing with few-shot video classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results and Visualizations</head><p>We show qualitative results comparing CMN and TAM in <ref type="figure" target="#fig_5">Fig. 4</ref>. In particular, we observe that CMN has difficulty in differentiating two actions from different classes with very similar visual clues among all the frames, e.g., backgrounds. As can be seen from the distance matrices in <ref type="figure" target="#fig_5">Fig. 4</ref>, though our method cannot alter the fact that the two visually similar action clips will have an averagely lower frame-wise distance value, it is able to find a temporal alignment that minimize the cumulative distance score between the query action video and the true support class video while the per-frame visual clue is not evident enough. Though the mean score of TAM is lower than the match of CMN, TAM succeeds in making the right prediction via calculating a lower alignment score out of the distance matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>Here we perform ablation experiments to demonstrate the effectiveness of our selections of the final model. We have shown in Section 4.3 that explicitly modeling the tem- poral ordering plays an important role for generalization to unseen classes. We now analyze the effect of different temporal alignment approaches. While having the cosine distance matrix D, there are several choices we could adopt to extract the alignment score out of the matrix, as visualized in <ref type="figure" target="#fig_3">Fig. 3</ref>. In addition to our proposed method, we consider several heuristics for generating the scores. The first is "Min", where we use the minimum element in the matrix D to represent the video distance value. The second is "Mean", for which we average over the cosine distance value of all pairs of frames. These two choices both neglect the temporal ordering. We will then introduce a few potential choices that explicitly consider sequence ordering when computing the temporal alignment score. An immediate scheme is to take an average over the diagonal of the distance matrix. The assumption made behind this approach is that the query video sequence shall be perfectly aligned with its corresponding support proxy of the same class, which could be somewhat ideal in read world applications. To allow for more adaptive alignment strategy, we introduce Plain DTW and our method. Here the Plain DTW in <ref type="table">Table.</ref> 2 means that there is no padding so that W 11 and W T T are assumed to be in the alignment path, and for each time step during computing alignment score we allow a possible movement choice among −→, and ↓.</p><p>The results are shown in <ref type="table">Table.</ref> 2. It can be observed that we are able to improve the few-shot learning by considering temporal ordering explicitly. There are some slight differences in performance between method Diagonal and Mean regarding to the two datasets here. There are less visual clues in each frame of Something-Something V2 than that of Kinetics, so the improvement of using Diagonal with regard to using Mean is prominent for Something-Something V2, while the gap is closed for Kinetics. However, we see that through adaptive temporal alignment, our method consistently improve the baselines on two datasets by more than 3% accross 1-shot and 5-shots. This shows that by reinforcing the model to learn an adaptive alignment path across query videos and proxies, the final model could learn to encode better representations for the video, as well as a more accurate alignment score which could in turn help with fewshot classification.</p><p>The next ablation study is on the sensitivity of smooth-  ing parameter λ. Previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">22]</ref> have shown that using λ empirically helps optimization in many tasks. Intuitively, a smaller λ functions more like the min operation and a larger λ means a heavier smoothing effect over the values in nearby positions. We experimented on λ within the value set of [0.01, 0.05, 0.1, 0.5, 1].</p><p>The results are shown in <ref type="figure" target="#fig_7">Fig. 5</ref>. In general, the performance is stable across values of λ. We observe that in practice λ ranges from 0.05 to 0.1 works relatively good under the setting of both two datasets. Thus we notice that a suitable λ is essential for the representation learning. When λ is too small, though it is able to function most similarly as the real min operator, the gradient is too imbalanced so that some pairs of frames are not adequately trained. On the contrary, a large λ might be too smooth so that the difference among all kinds of alignments are not notable enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose Temporal Alignment Module (TAM), a novel few-shot framework that can explicitly learn distance measure and representation independent of non-linear temporal variations in videos using very few data. In contrast to previous works, TAM dynamically aligns two video sequences while preserving the temporal ordering and it further uses continuous relaxation to directly optimize for the few-shot learning objective in an end-to-end fashion. Our results and ablations show that our model significantly outperforms a wide range of competitive baselines and achieves state-of-the-art results on two challenging realworld datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Our few-shot video classification setting. Pairs of semantically matched frames are connected with a blue dashed line. The arrows show the direction of the temporal alignment path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of our method. We first extract per-frame deep features using the embedding network. We then compute the distance matrices between the query video and videos in the support set. Next, an alignment score is computed out of the matrix representation. Finally we apply softmax operator over the alignment score of each novel class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Methods for calculating alignment score. Each subplot shows a distance matrix. The darker of the color of an entry, the smaller the distance value is of a pair of relevant frames. The entries with green border denotes the entries contributing to the final alignment score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>match: Holding something in front of something Our match: Pushing something from right to left Query : Pushing something from right to left</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of our learning results. Comparison of our matched with CMN's matched results in an episode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Smoothing factor sensitivity. We compare the effect of using different smoothing factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Few-shot video classification results. We report 5-way video classification accuracy on meta-testing set.</figDesc><table><row><cell></cell><cell cols="2">Kinetics</cell><cell cols="2">Something V2</cell></row><row><cell>Method</cell><cell cols="4">1-shot 5-shot 1-shot 5-shot</cell></row><row><cell>Matching Net [47]</cell><cell>53.3</cell><cell>74.6</cell><cell>-</cell><cell>-</cell></row><row><cell>MAML [47]</cell><cell>54.2</cell><cell>75.3</cell><cell>-</cell><cell>-</cell></row><row><cell>CMN [47]</cell><cell>60.5</cell><cell>78.9</cell><cell>-</cell><cell>-</cell></row><row><cell>TSN++</cell><cell>64.5</cell><cell>77.9</cell><cell>33.6</cell><cell>43.0</cell></row><row><cell>CMN++</cell><cell>65.4</cell><cell>78.8</cell><cell>34.4</cell><cell>43.8</cell></row><row><cell>TRN++</cell><cell>68.4</cell><cell>82.0</cell><cell>38.6</cell><cell>48.9</cell></row><row><cell>TAM (ours)</cell><cell>73.0</cell><cell>85.8</cell><cell>42.8</cell><cell>52.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Temporal matching ablation study. We compare our method to temporal-agnostic and temporal-aware baselines.</figDesc><table><row><cell></cell><cell cols="2">Kinetics</cell><cell cols="2">Something V2</cell></row><row><cell cols="5">matching type 1-shot 5-shot 1-shot 5-shot</cell></row><row><cell>Min</cell><cell>52.4</cell><cell>71.6</cell><cell>29.7</cell><cell>38.5</cell></row><row><cell>Mean</cell><cell>67.8</cell><cell>78.9</cell><cell>35.2</cell><cell>45.3</cell></row><row><cell>Diagonal</cell><cell>66.2</cell><cell>79.3</cell><cell>38.3</cell><cell>48.7</cell></row><row><cell>Plain DTW</cell><cell>69.2</cell><cell>80.6</cell><cell>39.6</cell><cell>49.0</cell></row><row><cell>TAM(Ours)</cell><cell>73.0</cell><cell>85.8</cell><cell>42.8</cell><cell>52.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work has been partially supported by JD.com American Technologies Corporation (JD) under the SAILJD AI Research Initiative. This article solely reflects the opinions and conclusions of its authors and not JD or any entity associated with JD.com.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://20bn.com/datasets/jester.5" />
		<title level="m">The 20bn-jester dataset v1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gapped blast and psi-blast: a new generation of protein database search programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Schäffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="3389" to="3402" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">D 3 TW : Discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02598</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A neural multisequence alignment technique (neumatch)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8749" to="8758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03129</idno>
		<title level="m">Learning to remember rare events</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">British Machine Vision Association</title>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="275" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">One shot similarity metric learning for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="31" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08383</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Differentiable dynamic programming for structured prediction and attention. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A generative approach to zero-shot and few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arulkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="372" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dynamic time warping. Information retrieval for music and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neuralnetwork-viterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7386" to="7395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<title level="m">Meta-learning with latent embedding optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM international conference on Multimedia</title>
		<meeting>the 15th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lowshot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7278" to="7286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Compound memory networks for fewshot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
