<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RESA: Recurrent Feature-Shift Aggregator for Lane Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Zheng</surname></persName>
							<email>zhengtuzju@gmail.comfanghaozju@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjian</forename><surname>Tang</surname></persName>
							<email>tangwenjian@fabu.ai</email>
							<affiliation key="aff1">
								<orgName type="institution">Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
							<email>yangzheng@fabu.ai</email>
							<affiliation key="aff1">
								<orgName type="institution">Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
							<email>haifengliu@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<email>dcai@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RESA: Recurrent Feature-Shift Aggregator for Lane Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lane detection is one of the most important tasks in selfdriving. Due to various complex scenarios (e.g., severe occlusion, ambiguous lanes, etc.) and the sparse supervisory signals inherent in lane annotations, lane detection task is still challenging. Thus, it is difficult for the ordinary convolutional neural network (CNN) to train in general scenes to catch subtle lane feature from the raw image. In this paper, we present a novel module named REcurrent Feature-Shift Aggregator (RESA) to enrich lane feature after preliminary feature extraction with an ordinary CNN. RESA takes advantage of strong shape priors of lanes and captures spatial relationships of pixels across rows and columns. It shifts sliced feature map recurrently in vertical and horizontal directions and enables each pixel to gather global information. RESA can conjecture lanes accurately in challenging scenarios with weak appearance clues by aggregating sliced feature map. Moreover, we propose a Bilateral Up-Sampling Decoder that combines coarse-grained and fine-detailed features in the upsampling stage. It can recover the low-resolution feature map into pixel-wise prediction meticulously. Our method achieves state-of-the-art results on two popular lane detection benchmarks (CULane and Tusimple). Code has been made available at: https://github.com/ZJULearning/resa.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Lane detection is an essential task in the computer vision community. It could serve as significant cues for autonomous driving and Advanced Driver Assistance System (ADAS) <ref type="bibr">(bar hillel et al. 2014)</ref> to keep a car from staying beyond lane markings. Detecting lanes in-the-wild is challenging due to severe occlusion caused by other vehicles, bad weather conditions, ambiguous pavement, and the inherent long and thin property of the lane itself.</p><p>Modern algorithms , <ref type="bibr" target="#b2">(Bergasa et al. 2018)</ref>, <ref type="bibr" target="#b7">(Chen, Liu, and Lian 2019)</ref> typically adopt a pixelwise prediction formulation, i.e., treat lane detection as a semantic segmentation problem, where each pixel in an image is assigned with a binary label to indicate whether it belongs * Equal Contribution † Deng Cai is the corresponding author. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  <ref type="figure">Figure 1</ref>: Feature aggregation illustration. (a) Comparison between CNN semantic segmentation and our method (RESA). The segmentation method with ordinary CNN suffers from bad performance due to severe occlusion. (b) Illustration of feature aggregation. Spatial lane feature can be enriched, owing to horizontal and vertical feature aggregation in a layer. Thus, RESA can infer lanes even if they are occluded. We add different strides to gather features within different distances, which relieves information loss problem during long-distance propagation.</p><p>to a lane. These methods solve the problem with an encoderdecoder framework. They first apply a CNN as the encoder to extract high semantic information into a feature map, then use an up-sampling decoder to recover the feature map to its original size and finally perform a pixel-wise prediction. Due to the thin and long property of lanes, the number of annotated lane pixels is far fewer than background pixels. These methods often struggle to extract subtle lane feature and may ignore the strong shape prior or high relevance between lanes, yielding inferior detection performance. The more challenging case is that the lane may be almost entirely occluded by crowded cars, and we can only conjecture the lane with common sense. Therefore, low-quality feature extracted by ordinary CNN tends to drop subtle lane features. Several methods try to pass spatial information within feature maps, e.g., SCNN <ref type="bibr" target="#b20">(Pan et al. 2018)</ref>. SCNN typically proposes a spatial convolution to pass information between adjacent rows or columns within feature map.</p><p>Nevertheless, the sequential information passing operation is time-consuming, which leads to a slow inference speed. Meanwhile, passing information between adjacent rows or columns sequentially takes many iterations, and information may be lost during long-distance propagation.</p><p>In this paper, we develop a REcurrent Feature-Shift Aggregator (RESA) to gather information within feature maps and pass spatial information more directly and efficiently. As shown in <ref type="figure">Fig. 1</ref> RESA can aggregate information vertically and horizontally by shifting the sliced feature map recurrently. RESA will first slice the feature map in vertical and horizontal directions, then make each sliced feature receive another sliced feature adjacent to a certain stride. Each pixel is updated simultaneously in several steps, and finally each location can gather information in the whole space. In this way, the information could be propagated between pixels in the feature map. RESA has three main advantages: 1) RESA passes information in a parallel way, thus reducing time cost significantly. 2) Information will be passed with different strides in RESA. Thus different sliced feature maps can be gathered without information loss during propagation. 3) RESA is simple and flexible to be incorporated into other networks.</p><p>Then we propose the Bilateral Up-Sampling Decoder (BUSD). BUSD has two branches. One is to catch the coarse-grained feature, and the other is to capture the fine-detailed feature. The coarse branch applies the bilinear up-sample directly and produces a blurry image. In contrast, the detailed branch implements up-sample with a transpose convolution and is followed by two non-bottleneck blocks <ref type="bibr" target="#b24">(Romera et al. 2017)</ref> to fix fine-detailed loss. Combined with two branches, our decoder can recover the lowresolution feature map into pixel-wise prediction meticulously.</p><p>We evaluate our method on two popular lane detection benchmarks, i.e., CULane and Tusimple. Qualitatively, RESA could well preserve the smoothness and continuity of lane detection, as shown in <ref type="figure">Fig. 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Lane Detection</head><p>Lane detection methods can be classified into two classes: traditional methods and deep learning-based methods. Traditional methods try to exploit hand-crafted low-level feature or specialized feature. <ref type="bibr" target="#b27">Sun, Tsai, and Chan (2006)</ref> tries to detect lanes in HSI color representation and <ref type="bibr" target="#b30">Yu and Jain (1997)</ref> extracts lane boundaries via Hough Transform. These methods require a complex feature selection process and have the weakness of poor scalability due to road scene variations. Recently, deep learning methods have shown superiority in lane detection with the high capacity to learn lane features in the end-to-end manner. <ref type="bibr" target="#b12">Huval et al. (2015)</ref> are the first to apply the deep learning method in lane detection with CNN. <ref type="bibr" target="#b19">Neven et al. (2018)</ref> propose to cast the lane detection problem as an instance segmentation problem. <ref type="bibr" target="#b22">Philion (2019)</ref> integrates the lane decoding step into the network and draws lanes iteratively without the recurrent neural network. Self-attention distillation (SAD) is proposed to allow a model to learn from itself and gains substantial improvement without any additional supervision or labels <ref type="bibr" target="#b10">(Hou et al. 2019</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Information Utilization</head><p>There have been some other attempts to utilize spatial information in neural networks. ION <ref type="bibr" target="#b1">(Bell et al. 2016</ref>) explores the use of spatial Recurrent Neural Networks (RNNs). These RNNs pass spatially varying contextual information both horizontally and vertically across an image. <ref type="bibr" target="#b16">Liang et al. (2016)</ref> constructs Graph LSTM to provide information propagation route for semantic object parsing. SCNN <ref type="bibr" target="#b20">(Pan et al. 2018)</ref> proposes to generalize traditional layer-by-layer convolutions to slice-by-slice convolutions within feature maps, thus enabling message passing between pixels across rows and columns in the same layer. SCNN propagates message as residual and makes it easier to train than previous work, but still suffers from expensive computation and information loss during long-distance propagation. RESA is much more computationally efficient than SCNN while gathering information from sliced features with different strides to avoid information loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>This section will demonstrate the details of our designed model, including the overall network architecture, RESA, and Bilateral Up-Sampling Decoder.</p><p>To take advantage of the strong shape priors of lanes and captures spatial relationships of pixels across rows and columns, we propose a novel RESA module to gather information and enrich the feature map. After inserting RESA into the encoder-decoder framework, our model is constructed with three components: encoder, aggregator, and decoder. We select commonly used backbone like ResNet <ref type="bibr" target="#b9">(He et al. 2016)</ref>, VGG <ref type="bibr" target="#b26">(Simonyan and Zisserman 2015)</ref>, etc as our encoder to extract preliminary feature from raw image. Then RESA module is applied to aggregate lane feature and get rich feature map. A novel Bilateral Up-Sampling Decoder with coarse-grained branch and finedetailed branch is proposed to recover lanes smoothly and continuously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture Design</head><p>The overall network architecture is shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. The framework is composed of three components: 'Dk', 'Uk', 'Lk', 'Rk' denotes "up-to-down", "down-to-up", "right-to-left", and "left-to-right" respectively at k-th iteration in RESA. (b) RESA U module. In this module, information propagates "down-to-up" with different strides recurrently and simultaneously. (c) RESA R module. In this module, information propagates "left-to-right" with different strides recurrently and simultaneously.</p><p>1. Encoder: A commonly used backbone network like VGG, ResNet, and etc, is applied as a feature extractor. The size of the raw input image is reduced to 1/8 after passing the encoder. The preliminary feature will be extracted in this stage. 2. RESA: REcurrent Feature Shift Aggregator (RESA) is proposed for gathering spatial feature. In every iteration, the sliced feature map will shift recurrently in 4 directions and pass information vertically and horizontally. Finally, RESA needs K iterations to ensure that each location can receive information in the whole feature map. 3. Decoder: Decoder consists of bilateral up-sampling blocks. Each block up-samples two times and finally recover the 1/8 feature map to the original size. Bilateral Up-Sampling Decoder is composed of the coarse-grained branch and fine-detailed branch. After up-sampled by the decoder, the output feature map is used to predict each lane's existence and probability distribution. A fully-connected layer is followed for existence prediction, and the binary classification will be performed. A pixel-wise prediction will be conducted for lanes probability distribution prediction, which is the same as the semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent Feature-Shift Aggregator</head><p>We propose REcurrent Feature-Shift Aggregator (RESA) to gather spatial information by shifting the sliced feature map horizontally and vertically. Specifically, assume we have a 3-D feature map tensor X of size C × H × W , where C, H, and W denote the number of channels, rows, and columns, respectively. X k c,i,j means the value of feature map X at kth iteration where c, i and j indicate indexes of channel, row and column, respectively. Then the forward computation of RESA is defined as follow:</p><formula xml:id="formula_0">Z k c,i,j = m,n F m,c,n · X k m,(i+s k ) mod H,j+n−1 ,<label>(1)</label></formula><formula xml:id="formula_1">Z k c,i,j = m,n F m,c,n · X k m,i+n−1,(j+s k ) mod W , (2) X k c,i,j = X k c,i,j + f (Z k c,i,j ),<label>(3)</label></formula><formula xml:id="formula_2">s k = L 2 K−k , k = 0, 1, · · · , K − 1,<label>(4)</label></formula><p>where K = log 2 L , k is the iteration number. L in Eq. (1) and Eq. <ref type="formula">(2)</ref> is W and H, respectively. f is a nonlinear activation function as ReLU. The X with superscript denotes the element that has been updated. s k is the shift stride in kth iteration. Eq. (1) and Eq. <ref type="formula">(2)</ref> show vertical and horizontal information passing formulas. F is a group of 1-d convolution kernel, which size is N in × N out × w, where N in , N out and w denote the number of input channels, the number of output channels and kernel width. Both N in and N out are equal to C. Z in Eq.</p><p>(1) and Eq. <ref type="formula">(2)</ref> is intermediate results for information passing. Note that feature map X is split into H slices in the horizontal direction and W slices in the vertical direction as shown in <ref type="figure" target="#fig_1">Fig 2(b)</ref> and <ref type="figure" target="#fig_1">Fig. 2(c)</ref>. We implement recurrent feature-shift information passing simply by index calculation with no other complicated operations. Shift stride s k is controlled by iteration number k, which determines the information passing distance dynamically.</p><formula xml:id="formula_3">W s1 = 1 s2 = 2 X0 X1 X2 X3 X4 ... Xw2 Xw1 X0 X1 X2 X3 X4 ... Xw2 Xw1</formula><p>H H <ref type="figure">Figure 3</ref>: Information passing in RESA when s 1 = 1 and s 2 = 2. X 0 can receive information from X 0 ,X 1 ,X 2 and X 3 only in two iterations.</p><p>Also, note that the information passing has four directions. We use "down-to-up" (shown in <ref type="figure" target="#fig_1">Fig. 2</ref>(b) RESA U), "up-to-down" as vertical information aggregator and "leftto-right" <ref type="figure" target="#fig_1">(Fig. 2</ref>(c) RESA R), "right-to-left" as horizontal information aggregator. The convolution layer weights with the same shift stride are shared across all slices in the same direction.</p><p>We take "right-to-left" information passing as a demonstration, and the detail is shown in <ref type="figure">Fig. 3</ref>. At k = 0 iteration, s 1 = 1 and X i in each column can receive X i+1 shifted feature. Because of recurrently shifting, columns at tail can also receive feature on the other side, i.e., X w−1 can receive X 0 shifted feature. At k = 1 iteration, s 2 = 2 and X i in each column can receive X i+2 shifted feature. Take X 0 as an example, X 0 can receive X 2 information in the second iteration, considering X 0 has received information from X 1 while X 2 has received information from X 3 in the previous iteration, now X 0 has received information from X 0 , X 1 , X 2 , and X 3 in total only in two iterations. The next iterations are similar to the above procedure. After the all K iterations, each X i can aggregate information in the whole feature map when iteration k = K finally.</p><p>Analysis. RESA applies feature-shift operation recurrently in 4 directions and enables every location to perceive and aggregate all spatial information in the same feature map. Lane detection is a task that highly relies on surrounding clues. For example, a lane is occluded by several cars, but we can still inference it from other lanes, car direction, road shape, or other visual clues. RESA aggregates feature from other locations to enrich the feature map and helps model to conjecture lanes like humans. The novel and powerful RESA module mainly has three advantages, which are concluded as follows:</p><p>1. Computationally efficient. Traditional information passing methods like Markov Random Field (MRF) or Conditional Random Field (CRF) <ref type="bibr" target="#b15">(Krähenbühl and Koltun 2011)</ref>, where each pixel receives all other pixel information in a fully connected way, always suffer from its intensive and redundant computation. Some methods like SCNN <ref type="bibr" target="#b20">(Pan et al. 2018</ref>) implement a more effective information passing scheme, i.e., slice-by-slice convolution. However, this RNN-like way still consumes much time as the complexity is increased linearly with the spatial size grows, and the sequential propagation cannot fully utilize computation resources. RESA's complexity is related to spatial size at a log level, and all locations are updated in a parallel way at every iteration. Each location can aggregate information from in the whole feature map in log 2 L iteration.</p><p>2. Feature information gathered effectively. The sliced feature information will not only be passed to adjacent slice but also be passed to sliced feature map with different strides, i.e., s k = 1, 2, 4, 8, · · · . Therefore, each pixel can gather information from the sliced feature map without information loss during propagation. As shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, RESA can get better performance than SCNN since SCNN only passes feature information to adjacent and loses information during propagation.</p><p>3. Easy to be plugged into other networks. Without bells and whistles, the structure is quite concise. Firstly, the implementation of RESA is simple, which only needs index operation in the feature map. Secondly, RESA doesn't change the shape of the input feature map, which can be treated as a feature enhancement module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilateral Up-Sampling Decoder</head><p>The main task of the decoder is up-sampling the feature map to the input resolution. Most decoders utilize the bilinear upsampling procedure to recover the final pixel-wise prediction, which is easy to obtain coarse results but may lose details. Some methods <ref type="bibr" target="#b24">(Romera et al. 2017</ref>) use stacking convolutional operations and deconvolutional operations to get refined upsampling results. For the motivation above, we combine their advantages and propose Bilateral Up-Sampling Decoder (BUSD). The decoder is composed of two branches, one is to recover the coarse-grained feature, and the other is to fix fine-detailed loss. The structure is illustrated in <ref type="figure">Fig. 4</ref>. Input will pass two branches, and 2x up-sampled output with half number of channel will be produced. After passing these stacked decoder blocks, the 1/8 feature map produced by RESA will be recovered to the same size as the input image.</p><p>Coarse Grained Branch. The coarse-grained branch will output a rough up-sampled feature from the last layer quickly, which may ignore details. A simple and shallow path is designed. We first apply 1 × 1 convolution to reduce the number of channel by a factor 2 of the input feature map, and a BN (Ioffe and Szegedy 2015) is followed. A bilinear interpolation is used directly to up-sample the input feature map. At last, the ReLU is performed.</p><p>Fine Detailed branch. The fine-detailed branch is used to fine-tune subtle information loss from the coarse-grained  <ref type="figure">Figure 4</ref>: Bilateral Up-Sampling Decoder. Decoder upsamples feature map to 2x size. It is composed by coarse grained branch (left) and fine detailed branch (right). Coarse grained branch is used to get a rough up-sampled feature quickly and ignore much detail. Fine detailed branch is used to fine-tune subtle information loss.</p><p>branch, and the path is deeper than the other. We use transpose convolution with stride 2 to up-sample feature map and reduce the number of channel by a factor 2 simultaneously. ReLU is followed the up-sampling as the similar design used in the coarse-grained branch. Non-bottleneck block <ref type="bibr" target="#b24">(Romera et al. 2017)</ref> consists of four 3 × 1 and 1 × 3convolutions with BN and ReLU, which can keep the shape of the feature map and extract information efficiently in a factorized way. We stack two non-bottleneck after the up-sampling operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Dataset</head><p>We conduct experiments on two widely used lane detection benchmark datasets: CULane dataset <ref type="bibr" target="#b20">(Pan et al. 2018</ref>) and Tusimple Lane detection benchmark 1 . The CULane dataset consists of 55 hours of videos which comprises urban and highway scenarios. It consists of nine different scenarios, including normal, crowd, curve, dazzle night, night, no line, and arrow in the urban area. Tusimple dataset is collected with stable lighting conditions in highways. The details of datasets are showed in <ref type="table">Table 1</ref>.</p><p>CULane. For CULane dataset, each lane is treated as a 30-pixel-width line. Intersection-over-union (IoU) is calculated between predictions and ground truth. Predicted lanes whose IoU are larger than a threshold (0.5) are considered 1 https://github.com/TuSimple/tusimple-benchmark/ as true positives (TP). The F1-measure is taken as the evaluation metric, which is defined as: F 1 = 2×P recision×Recall P recision+Recall , where P recision = T P T P +F P and Recall = T P T P +F N , F P and F N is false positive and false negative respectively.</p><p>Tusimple. For Tusimple dataset, the evaluation metric is accuracy. It is defined as follow: accuracy = clip C clip clip S clip . in which C clip is the number of lane points predicted correctly (mismatch distance between prediction and ground truth is within a certain range) and S clip is the total number of ground truth points in each clip. We also evaluate the rate of false positive (FP) and false negative (FN) on prediction results.</p><p>Following <ref type="bibr" target="#b10">(Hou et al. 2019)</ref>, we first resize the original images to 288 × 800 for CULane and 368 × 640 for Tusimple, respectively. We use SGD (Bottou 2010) with momentum 0.9 and weight decay 1e-4 as the optimizer to train our model, and the learning rate is set 2.5e-2 for CU-Lane and 2.0e-2 for Tusimple, respectively. We use warmup <ref type="bibr" target="#b8">(Doll, Girshick, and Noordhuis 2017)</ref> strategy in the first 500 batches and then apply polynomial learning rate decay policy <ref type="bibr" target="#b18">(Mishra and Sarawadekar 2019)</ref> with power set to 0.9.</p><p>The loss function is the same as SCNN <ref type="bibr" target="#b20">(Pan et al. 2018</ref>), which consists of segmentation BCE loss and existence classification CE loss. Considering the imbalanced label between background and lane markings, the segmentation loss of background is multiplied by 0.4. The batch size is set 8 for CULane and 4 for Tusimple, respectively. The total number of training epoch is set 50 for the TuSimple dataset and 12 for the CULane dataset. All models are trained with 4 NVIDIA 2080Ti GPUs (11G Memory) in Ubuntu. All experiments are implemented with Pytorch1.1.</p><p>In our experiments, we use ResNet <ref type="bibr" target="#b9">(He et al. 2016</ref>) and VGG (Simonyan and Zisserman 2014) as backbone. In ResNet, we add an extra 1 × 1 convolution to reduce the output channel to 128. The modification of VGG is the same as SCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>We show the results of our method on two lane detection benchmark datasets and compare it with other popular lane detection methods. For CULane dataset, several popular lane detection methods, including ResNet50 ), Res34-VP <ref type="bibr" target="#b17">(Liu, Zeng, and Meng 2020)</ref>, SCNN, Res34-SAD <ref type="bibr" target="#b10">(Hou et al. 2019</ref>), Res34-Ultra <ref type="bibr" target="#b23">(Qin, Wang, and Li 2020)</ref>, PINet <ref type="bibr" target="#b14">(Ko et al. 2020)</ref>, CurveLane <ref type="bibr" target="#b29">(Xu et al. 2020)</ref> are used for comparison. Our RESA adopts ResNet-50 as the backbone, which is marked as RESA-50. The result is shown in  achieve 36fps, which denotes our method is efficient in computation and can be used in real-time applications. Moreover, it is observed that our method obtains superior performance in almost all scenarios, which strongly suggests the effectiveness and the generality of RESA.For Tusimple lane detection benchmark, six methods are used for comparison, including ResNet18, ResNet34, ENet <ref type="bibr" target="#b21">(Paszke et al. 2016)</ref>, LaneNet <ref type="bibr" target="#b28">(Wang, Ren, and Qiu 2018)</ref>, ENet-SAD, and SCNN. We use ResNet-18/34 as the backbone, and they are marked as RESA-18/34. The result is shown in Table 3. RESA-34 achieves 96.82% accuracy, which also outperforms the state-of-the-art. We also analyze FP and FN for each method. It is noteworthy that the FP of RESA is far below other algorithms, which means that RESA gains higher precision on the lane detection task and contributes to achieving higher accuracy.</p><p>To further explain the effectiveness of our method, we show the qualitative results of our algorithm and others in the CULane dataset. As <ref type="figure" target="#fig_2">Fig. 5</ref> shows, segmentation methods cannot preserve lane markings' smoothness and continuity due to severe occlusion. In contrast, SCNN could partially address the problem by passing spatial information and improve the performance, but the result is still unsatisfying. It can be observed that the predictions of SCNN become imprecise at the bottom of the image, where can only be inferred by the surrounding feature. It indicates that information may be lost in SCNN during long-distance propagation. Among these methods, RESA could capture the spatial relationship of the pixel across rows and columns and aggregate information from the sliced feature map with different strides. Therefore, the results of RESA are more robust and contain less noise. This demonstrates that RESA owns much stronger capability to capture structural prior objects than traditional segmentation modules and SCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In the Method section, we discuss Recurrent Feature-Shift Aggretator (RESA) and Bilateral Up-Sampling Decoder (BUSD) and analyze the advantages of each module, respectively. To verify the importance of each proposed component, we make detailed ablation studies in this section.   Effect of Each Component. We first investigated the effectiveness of the Bilateral Up-Sampling Decoder module and RESA module. For the baseline, we select ResNet-34 as the backbone. After being extracted from the backbone, the feature map is up-sampled 8x directly using bilinear interpolation as SCNN does. The output is used as regression and finally gets the probability distribution of each lane.</p><p>To make comparison, we replace bilinear interpolation with Bilateral Up-Sampling Decoder and then insert RESA between backbone and decoder step by step. We summarize the performance of each module in <ref type="table">Table 4</ref>. As we can see, both modules can strongly improve lane detection performance, which proves the capabilities of proposed modules.</p><p>Effectiveness of Feature Aggregation. In this section, we investigate the effect of the direction in RESA. As we add Baseline BUSD RESA F1 65.1 68.6 (+3.5) 74.3 (+9.2) 74.5 (+9.4) <ref type="table">Table 4</ref>: Experiments of the proposed modules on CULane dataset with ResNet-34 backbone. Baseline stands for 8x upsampling directly after backbone. more directions in RESA, we can get higher F1-measure. The result is shown in <ref type="table">Table.</ref> 5. Furthermore, we study the feature aggregation method in Eq. 3. We replace the addition operator by maximum operator. The result shows that the maximum operator has similar performance as addition operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Precision  Iteration In RESA. In this section, we explore the effect of different iterations in RESA. Theoretically, as the iteration increases, each slice of feature map can aggregate more information, which contributes to obtain better performance. To illustrate more iterations can bring up better performance, we make comparison between different iterations, i.e., iteration = 1, · · · , 5. As shown in <ref type="table" target="#tab_10">Table 6</ref>, the performance will be better as the iteration increases. However, more iterations lead to more computational time cost. It is a trade-off between performance and computational resources. To strike a balance between them, we select iteration = 4 as our final choice.  Compare RESA with SCNN. SCNN <ref type="bibr" target="#b20">(Pan et al. 2018</ref>) has shown message passing scheme could improve the lane detection performance but extra more parameters could merely bring about little improvement. Thus, we compare the RESA with SCNN to verify the effectiveness of our method. We try to add RESA and SCNN with different backbones (e.g. ResNet, VGG). We conduct experiment to compare the performance with SCNN. The experiment results are shown in  Computational Efficiency. We also conduct experiment to compare the running time of our method with LSTM, SCNN. The running time of these methods are recorded with the average time for 1000 runs. We use different convolution kernel widths <ref type="bibr">(7,</ref><ref type="bibr">9,</ref><ref type="bibr">11)</ref> to compare the efficiency. SCNN propagates information in a sequential way, i.e., a slice does not pass information to the next slice until it has received information from previous slice. Thus, this kind of message passing requires much computational cost due to sequential computing. In contrast, our RESA passes information in a parallel way. As shown in <ref type="table">Table 8</ref>, RESA is around 10 times faster than SCNN with the same kernel width, which makes it promising to apply our method to real-time applications.</p><p>Method LSTM SCNN RESA Kernel width -7 9 11 7 9 11 Runtime (ms) 108.0 43.5 44.0 44.6 3.8 4.0 4.4 <ref type="table">Table 8</ref>: Runtime of LSTM, SCNN, and RESA. The iteration in RESA is 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose two components tailored for lane detection: Recurrent Feature-Shift Aggretator (RESA) and Bilateral Up-Sampling Decoder (BUSD). RESA takes the advantage of strong shape priors of lanes and captures spatial relationships of pixels across rows and columns. It shifts sliced feature map recurrently in vertical and horizontal directions and enables each pixel to gather global information. Besides, it can be plugged into other networks easily. The Bilateral Up-Sampling Decoder is proposed to combine coarse grained feature and fine detailed feature in upsampling stage. Our method is evaluated on two popular lane detection benchmark datasets, i.e., Tusimple and CULane and achieves the state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Architecture Design. (a) Overall architecture of our model, which is composed by encoder, RESA and decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Examples results from CULane dataset with segmentation method, SCNN, and RESA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Through the overall design, RESA outperforms all baselines in the CULane dataset and achieves state-of-the-art result. Meanwhile, RESA-50 can</figDesc><table><row><cell>Category</cell><cell cols="9">Res50 Res34-VP SCNN Res34-SAD Res34-Ulrta PINet CurveLane-L RESA-34 RESA-50</cell></row><row><cell>Normal</cell><cell>87.4</cell><cell>90.4</cell><cell>90.6</cell><cell>89.9</cell><cell>90.7</cell><cell>90.3</cell><cell>90.7</cell><cell>91.9</cell><cell>92.1</cell></row><row><cell>Crowded</cell><cell>64.1</cell><cell>69.2</cell><cell>69.7</cell><cell>68.5</cell><cell>70.2</cell><cell>72.3</cell><cell>72.3</cell><cell>72.4</cell><cell>73.1</cell></row><row><cell>Night</cell><cell>60.6</cell><cell>63.8</cell><cell>66.1</cell><cell>64.6</cell><cell>66.7</cell><cell>67.7</cell><cell>68.2</cell><cell>69.8</cell><cell>69.9</cell></row><row><cell>No line</cell><cell>38.1</cell><cell>43.1</cell><cell>43.4</cell><cell>42.2</cell><cell>44.4</cell><cell>49.8</cell><cell>49.4</cell><cell>46.3</cell><cell>47.7</cell></row><row><cell>Shadow</cell><cell>60.7</cell><cell>62.5</cell><cell>66.9</cell><cell>67.7</cell><cell>69.3</cell><cell>68.4</cell><cell>70.1</cell><cell>72.0</cell><cell>72.8</cell></row><row><cell>Arrow</cell><cell>79.0</cell><cell>83.5</cell><cell>84.1</cell><cell>83.8</cell><cell>85.7</cell><cell>83.7</cell><cell>85.8</cell><cell>88.1</cell><cell>88.3</cell></row><row><cell>Dazzle light</cell><cell>54.1</cell><cell>61.4</cell><cell>58.5</cell><cell>59.9</cell><cell>59.5</cell><cell>66.3</cell><cell>67.7</cell><cell>66.5</cell><cell>69.2</cell></row><row><cell>Curve</cell><cell>59.8</cell><cell>64.7</cell><cell>64.4</cell><cell>66.0</cell><cell>69.5</cell><cell>65.6</cell><cell>68.4</cell><cell>68.6</cell><cell>70.3</cell></row><row><cell>Crossroad</cell><cell>2505</cell><cell>2141</cell><cell>1990</cell><cell>1960</cell><cell>2037</cell><cell>1427</cell><cell>1746</cell><cell>1896</cell><cell>1503</cell></row><row><cell>Total</cell><cell>66.7</cell><cell>70.9</cell><cell>71.6</cell><cell>70.7</cell><cell>72.3</cell><cell>74.4</cell><cell>74.8</cell><cell>74.5</cell><cell>75.3</cell></row><row><cell>Runtime (ms)</cell><cell>-</cell><cell>26</cell><cell>116</cell><cell>&lt;51</cell><cell>6</cell><cell>40</cell><cell>-</cell><cell>22</cell><cell>28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art results on CULane dataset with IoU threshold = 0.5. For crossroad, only FP are shown. Res50 indicates deeplab) using resnet50 as backbone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art results on Tusimple dataset. ResNet-18/34 indicates deeplab) using resnet18 and resnet34 as backbone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Effectiveness of feature aggregation of RESA on CULane dataset with ResNet-34 backbone. † means maximum feature aggregation method.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>The performance of the model by using different iterations on CULane dataset with ResNet-34 backbone.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 .</head><label>7</label><figDesc>The result shows that RESA outperforms SCNN and brings significant improvement.</figDesc><table><row><cell>Method</cell><cell cols="3">Precision Recall F1-measure</cell></row><row><cell>VGG16</cell><cell>62.2</cell><cell>60.3</cell><cell>61.2</cell></row><row><cell>VGG16 + SCNN</cell><cell>72.4</cell><cell>72.1</cell><cell>72.3</cell></row><row><cell>VGG16 + RESA</cell><cell>74.1</cell><cell>72.5</cell><cell>73.3</cell></row><row><cell>ResNet34</cell><cell>66.2</cell><cell>64.2</cell><cell>65.1</cell></row><row><cell>ResNet34 + SCNN</cell><cell>73.9</cell><cell>71.5</cell><cell>72.7</cell></row><row><cell>ResNet34 + RESA</cell><cell>76.1</cell><cell>72.9</cell><cell>74.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>The comparison between SCNN and RESA trained using VGG16 and ResNet34 as backbone.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Recent progress in road and lane detection: A survey. Machine Vision and Applications 25</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<idno type="DOI">10.1007/s00138-011-0404-2</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alvarez</surname></persName>
		</author>
		<ptr target="https://github.com/Eromera/erfnet" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-Scale Machine Learning with Stochastic Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMP</title>
		<meeting>COMP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stat&amp;apos;</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-7908-2604-316</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">RBNet: A Deep Neural Network for Unified Road and Road Boundary Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-70087-870</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="677" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">PointLaneNet: Efficient end-to-end CNNs for accurate real-time lane detection. IV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<idno type="DOI">10.1109/IVS.2019</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2563" to="2568" />
			<pubPlace>June(Iv</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate, large minibatch SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<idno>10495258. doi:10.1124/ dmd.107.016501.CYP3A4-Mediated</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv ISSN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mo-bileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An empirical evaluation of deep learning on highway driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pazhayampallil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Migimatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng-Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01716</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Key Points Estimation and Point Instance Segmentation Approach for Lane Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06604</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<editor>Shawe-Taylor, J.</editor>
		<editor>Zemel, R. S.</editor>
		<editor>Bartlett, P. L.</editor>
		<editor>Pereira, F.</editor>
		<editor>and Weinberger, K. Q.</editor>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15602</idno>
		<title level="m">Heatmapbased Vanishing Point boosts Lane Detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Polynomial Learning Rate Policy with Warm Restart for Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sarawadekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TENCON 2019 -2019 IEEE Region 10 Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2087" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FastDraw: Addressing the long tail of lane detection by adapting a sequential prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11582" to="11591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ultra Fast Structure-aware Deep Lane Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for realtime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">HSI color model based lane-marking detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1168" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Lanenet: Realtime lane detection networks for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01726</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12147</idno>
		<title level="m">CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive Point Blending</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lane boundary detection using a multiresolution hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="748" to="751" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
