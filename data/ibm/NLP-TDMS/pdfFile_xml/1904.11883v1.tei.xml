<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Optimized Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-04-26">26 Apr 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
							<email>jiangbo@ahu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Optimized Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-04-26">26 Apr 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) have been widely studied for graph data representation and learning tasks. Existing GCNs generally use a fixed single graph which may lead to weak suboptimal for data representation/learning and are also hard to deal with multiple graphs. To address these issues, we propose a novel Graph Optimized Convolutional Network (GOCN) for graph data representation and learning. Our GOCN is motivated based on our re-interpretation of graph convolution from a regularization/optimization framework. The core idea of GOCN is to formulate graph optimization and graph convolutional representation into a unified framework and thus conducts both of them cooperatively to boost their respective performance in GCN learning scheme. Moreover, based on the proposed unified graph optimization-convolution framework, we propose a novel Multiple Graph Optimized Convolutional Network (M-GOCN) to naturally address the data with multiple graphs. Experimental results demonstrate the effectiveness and benefit of the proposed GOCN and M-GOCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional Neural Networks (CNNs) have been widely applied for grid-like structure data representation and learning in computer vision and machine learning area. However, in many real applications, data are not coming with grid-like structure but instead have some irregular structures which are usually represented as structured graphs. Traditional CNNs generally fail to address graph-structured data.</p><p>Recently, Graph Convolutional Networks (GCNs) have been widely studied to deal with arbitrary graph-structured data representation and learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>. The aim of GCNs is trying to define some reasonable convolution operations on arbitrary structured graphs. For example, Bruna et al. <ref type="bibr" target="#b2">[3]</ref> propose to define a graph convolution by using the eigen-decomposition of graph Laplacian matrix. Henaff et al. <ref type="bibr" target="#b9">[10]</ref> further introduce a spatially constrained spectral filters to define graph convolution. Kipf et al. <ref type="bibr" target="#b14">[15]</ref> propose to explore the first-order approximation of spectral filters and present a simple Graph Convolutional Network (GCN) for semi-supervised learning. Li et al. <ref type="bibr" target="#b29">[30]</ref> present an adaptive graph CNNs, in which the graph is learned adaptively by employing a metric learning method. Hamilton et al. <ref type="bibr" target="#b8">[9]</ref> present a general inductive representation and learning framework to generate embeddings for the unseen nodes. Velickovic et al. <ref type="bibr" target="#b29">[30]</ref> propose Graph Attention Networks (GAT) for graph based semi-supervised learning. Klicpera et al. <ref type="bibr" target="#b12">[13]</ref> propose to combine GCN and PageRank together to derive an improved propagation scheme in layer-wise propagation. Some recent works also explore specific graph neural networks for computer vision tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>The above GCNs have been widely used for graph data representation and learning. One important aspect of GCNs is the graph structure representation of data. In general, the graph data we feed to existing GCNs should be a single graph which is obtained from either domain knowledge (e.g., social network) or human establishment, such as k-NN graph. However, there are three main issues. First, traditional human established graphs generally use fixed parameters to determine the graph structure and thus are usually sensitive to the local noises and errors. Second, the graphs obtained from domain knowledge or established by human are generally independent of graph convolutional learning, which thus are not guaranteed to be optimal for graph convolutional representation/learning in GCNs. Third, existing GCNs are generally hard to deal with multiple graphs, although some heuristic fusion strategies can be utilized for multiple graph convolutional network learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>. It is known that the convolution operation on multiple graphs is not as well-defined as on single graph.</p><p>To address these issues, in this paper, we propose a novel Graph Optimized Convolutional Network (GOCN) for data representation and learning problem. Our GOCN is motivated based on our new re-interpretation of graph convolution from a regularization/optimization framework. The core idea of GOCN is to formulate graph optimization and graph convolutional representation into a unified framework and thus conducts both of them cooperatively to boost their respective performance in GCN learning scheme. The main advantage of GOCN is that the learned representation of data can provide useful "weakly" supervised information for learning a better graph which simultaneously facilitates graph convolutional representation and learning. Furthermore, based on the proposed unified graph optimization-convolution framework, we propose a novel Multiple Graph Optimized Convolutional Network (M-GOCN) to naturally address the data with multiple graphs.</p><p>Overall, the main contributions of this paper are summarized as follows:</p><p>• We propose to reformulate graph convolution learning as a regularization framework, based on which a unified graph optimization-convolution (GOC) framework is derived to learn an optimal graph for graph convolutional representation and learning.</p><p>• Based on the proposed unified GOC model, we propose a novel Graph Optimized Convolutional Network (GOCN) which conducts both graph construction and graph convolution simultaneously in GCN scheme for data representation and semi-supervised learning.</p><p>• We extend GOC to deal with multiple graphs and provide a novel Multiple Graph Optimized Convolutional Network (M-GOCN) for multi-graph data representation and learning.</p><p>Experimental results on several datasets demonstrate the effectiveness of the proposed GOCN and M-GOCN methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Revisiting GCN</head><p>Recently, Graph Convolutional Networks (GCNs) have been widely studied for graph data representation and learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref>. The core aspect of GCNs is the specific definition of graph convolution in layer-wise propagation. Here, we briefly review the widely used GCN model proposed in work <ref type="bibr" target="#b14">[15]</ref>. Given an input feature matrix H (0) = X ∈ R n×d0 and graph A ∈ R n×n with A ii = 0, GCN defines the layer-wise propagation as <ref type="bibr" target="#b14">[15]</ref>,</p><formula xml:id="formula_0">H (k+1) = σ (I + D − 1 2 AD − 1 2 )H (k) Θ (k)<label>(1)</label></formula><p>where k = 0, 1, · · · K − 1 and I denotes an identity matrix, and D is a diagonal degree matrix.</p><formula xml:id="formula_1">Θ (k) ∈ R d k ×d k+1</formula><p>is a layer-specific trainable weight matrix, and σ(·) denotes an activation function.</p><p>The last layer of GCN outputs the final representation H (K) of graph nodes, which can be used for many learning tasks, such as clustering, visualization and (semi-supervised) classification etc. In this paper, we focus on semi-supervised classification. For this task, a softmax activation function is further used to output the label prediction P ∈ R n×c for graph nodes, where c denotes class number. The weight matrices of GCN network {Θ (0) , Θ <ref type="bibr" target="#b0">(1)</ref> , · · · Θ (K−1) } are optimized by minimizing the cross-entropy loss as <ref type="bibr" target="#b14">[15]</ref>,</p><formula xml:id="formula_2">L Semi-GCN = − i∈L c j=1 Y ij lnP ij<label>(2)</label></formula><p>where L indicates the set of labeled nodes and each row Y i· , i ∈ L of Y denotes the corresponding label indication vector for the i-th labeled node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Optimized Convolutional Network</head><p>In this section, we present our Graph Optimized Convolutional Network (GOCN) model. GOCN is motivated based on our re-interpretation of GCN by using a regularization framework. In the following, we first present our regularization reformulation of graph convolution in §3.1. Based on it, we then derive a unified framework of graph optimization-convolution operation in §3.2. Finally, we present our GOCN architecture in §3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Regularization framework of GCN</head><p>The propagation rule Eq.(1) in GCN can be decomposed into two operations, i.e.,</p><formula xml:id="formula_3">Z (k) = (I + D − 1 2 AD − 1 2 )H (k)<label>(3)</label></formula><formula xml:id="formula_4">H (k+1) = σ Z (k) Θ (k)<label>(4)</label></formula><p>where Eq. </p><formula xml:id="formula_5">whereÂ = D − 1 2 AD − 1 2 andÂ ii = 0 (because A ii = 0). From Eq.(5)</formula><p>, we can note that GCN employs an one-step feature aggregation on normalized graphÂ (biased by feature itself) to obtain contextual feature representation in layer-wise propagation.</p><p>First, here one can also explore a more flexible T -step aggregation in GCN layer-wise propagation as</p><formula xml:id="formula_6">Z (t+1) = αÂZ (t) + (1 − α)H = (αÂ) t+1 + (1 − α) t i=0 (αÂ) i H<label>(6)</label></formula><p>where t = 0, 1 · · · T − 1 and Z (0) = H. Parameter α ∈ (0, 1) denotes the fraction of feature information that node v i receives from its neighbors on normalized graphÂ. Note that, when t = 0 and α = 0.5, the aggregation (Eq.(6)) degenerates to GCN update Eq.(5). When t → ∞, this aggregation converges to an equilibrium solution as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>,</p><formula xml:id="formula_7">Z * = (1 − α)(I − αÂ) −1 H<label>(7)</label></formula><p>Second, one important property of the update Eq. <ref type="formula" target="#formula_6">(6)</ref> is that it can be theoretically explained by using an optimization framework <ref type="bibr" target="#b5">[6]</ref>. Specifically, the converged solution of Eq. <ref type="formula" target="#formula_7">(7)</ref> is the optimal solution that minimizes the following optimization problem,</p><formula xml:id="formula_8">min Z R(Â, H; Z) = Tr(Z T (I −Â)Z) + µ Z − H 2 F<label>(8)</label></formula><p>where µ = 1 α − 1 is a replacement parameter of α to balance two terms. Tr(·) denotes the trace function and · F denotes the Frobenious norm. Moreover, the update Eq.(6) (or Eq. <ref type="formula" target="#formula_7">(7)</ref>) also provides an approximate solution to this problem by using a T -step power iteration algorithm.</p><p>In particular, we can note that GCN update (Eq.(5)) provides a very approximate solution for the problem R(Â, H; Z) by using an one-step power algorithm. This provides a regularization/optimizaton framework interpretation for GCN update rule, which motivates us to develop some more effective graph convolution (feature aggregation) variants in GCN layer-wise propagation. In this paper, we focus on graph constructionÂ and aim to learn a more effective graph for GCN representation/learning. To do so, in the following we first derive a unified framework of graph optimization and convolution, followed by a simple power iteration implementation. Based on this unified framework, we then develop our GOCN architecture in §3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unified graph optimization-convolution model</head><p>One important aspect of the above feature aggregation is the graph constructionÂ. Constructing a good graph to represent data relationship is generally important for data representation and (semisupervised) learning tasks. Existing GCNs generally use a fixed graph which may lead to weak suboptimal for data representation and learning. Our aim in this section is to propose a unified graph optimization-convolution model that aims to learn an optimal graph adaptively for feature aggregation in GCN scheme.</p><p>Formally, given an initial (normalized) graphÂ, we propose to learn an optimal graph S that best serves the above feature aggregation Eq.(8) while preserving the structure information encoded in A. This can be achieved by optimizing the following unified framework,</p><formula xml:id="formula_9">min S,Z U goc = G(Â; S) + γR(S, H; Z) (9) s.t. S = S T , S ij ≥ 0</formula><p>where G(Â; S) denotes the graph learning functions and γ is used to balance two terms. In this paper, we use the simple Frobenious norm and propose a graph optimized feature aggregation as,</p><formula xml:id="formula_10">min S,Z U goc = Â − S 2 F + γR(S, H; Z) (10) s.t. S = S T , S ij ≥ 0</formula><p>The optimal S and Z can be obtained via a simple algorithm which alternatively conducts the following step 1 and step 2 until convergence.</p><p>Step 1. Solving S while fixing Z, the problem becomes</p><formula xml:id="formula_11">min S Â − S 2 F + γTr(Z T (I − S)Z) (11) s.t. S = S T , S ij ≥ 0</formula><p>It has a simple closed-form solution which is given as</p><formula xml:id="formula_12">S ij = max (Â + γ 2 ZZ T ) ij , 0<label>(12)</label></formula><p>Step 2. Solving Z while fixing S, the problem becomes to Eq. <ref type="bibr" target="#b7">(8)</ref>. The optimal solution is</p><formula xml:id="formula_13">Z = (1 − α)(I − αS) −1 H<label>(13)</label></formula><p>and an approximate solution can be obtained by</p><formula xml:id="formula_14">Z = (αS) T + (1 − α) T −1 i=0 (αS) i H<label>(14)</label></formula><p>Remark. Here, to avoid the inversion computation in Eq.(13), we can instead use a T -step power iteration and obtain Eq. <ref type="bibr" target="#b13">(14)</ref> to compute the optimal Z approximately [6].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GOCN architecture</head><p>Using the proposed unified model U goc , we present our Graph Optimized Convolutional Network (GOCN). Similar to the architecture of standard GCN <ref type="bibr" target="#b14">[15]</ref>, our GOCN contains one input layer, several hidden propagation layers and one final perceptron layer, as introduced in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Hidden Propagation Layer</head><p>For hidden propagation layer, it takes features H (k) ∈ R n×d k and an initial graphÂ ∈ R n×n as the input and outputs feature map <ref type="formula" target="#formula_0">(10)</ref>), then GOCN conducts layer-wise propagation as</p><formula xml:id="formula_15">H (k+1) ∈ R n×d k+1 . Let Z * = Φ(A, H) be the optimal Z-solution of unified model U goc (Eq.</formula><formula xml:id="formula_16">H (k+1) = σ Φ(Â, H (k) )Θ (k)<label>(15)</label></formula><p>where k = 0, 1, · · · K − 1 and Θ (k) ∈ R d k ×d k+1 denotes the layer-specific trainable weight matrix. σ(·) denotes an activation function.</p><p>Efficient computation of Φ(Â, H (k) ). Exactly calculating Φ(Â, H (k) ) is time consuming due to (i) inversion operation (Eq.(13)) and (ii) alternative computation of step 1 and step 2 until convergence.</p><p>To overcome this issue, we first use update Eq. <ref type="bibr" target="#b13">(14)</ref> to compute the optimal Z in Eq.(13) approximately. Second, we use a M -step alternative iteration to optimize S and Z approximately. The detail propagation rule in GOCN hidden layer is summarized in Algorithm 1. In our experiments, we set T = 2 and M = 3.</p><p>Algorithm 1 GOCN layer-wise propagation 1: Input: Feature matrix H (k) ∈ R n×d k , initial graphÂ ∈ R n×n and weight parameter Θ (k) , parameters γ, µ, maximum iteration T, M 2: Output: Feature map H (k+1) ∈ R n×d k+1 3: Initialize Z = H (k) 4: for t m = 1, 2 · · · M do 5:</p><formula xml:id="formula_17">Compute S as S ij = max Â + γ 2 ZZ T ij , 0 6: Compute Z as Z = (αS) T + (1 − α) T −1 i=0 (αS) i H (k) 7: end for 8: Return H (k+1) = σ(ZΘ (k) )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Final Perceptron Layer</head><p>The last layer of GOCN outputs the final label prediction P ∈ R n×c for graph nodes, where c denotes the number of class. Similar to work <ref type="bibr" target="#b14">[15]</ref>, the optimal network weight parameters {Θ (0) , Θ <ref type="bibr" target="#b0">(1)</ref> , · · · Θ (K−1) } of GOCN are obtained by minimizing the following cross-entropy loss function over all the labeled nodes L, i.e.,</p><formula xml:id="formula_18">L Semi-GOCN = − i∈L c j=1 Y ij lnP ij<label>(16)</label></formula><p>where L indicates the set of labeled nodes and Y i· , i ∈ L denotes the corresponding label indication vector for the i-th labeled node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GOCN on Multiple Graphs</head><p>One desired property of the proposed GOCN is that it can be naturally adapted to address the data with multiple graph representations. This is because we can extend the unified model U goc (Eqs.(9,10)) to deal with multiple graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multi-graph convolution model</head><p>Given an input feature matrix X = H (0) ∈ R n×d0 with multiple graphs A = {Â <ref type="bibr" target="#b0">(1)</ref> ,Â (2) · · ·Â (m) }, we can obtain an optimal feature aggregation Z = Φ m (A, H) on graph set A by optimizing, </p><formula xml:id="formula_19">s.t. S = S T , S ij ≥ 0<label>(17)</label></formula><p>In particular, we use Frobenious norm and propose our multiple graph optimized feature aggregation as</p><formula xml:id="formula_20">min S,Z,w U mgoc = m v=1 w r v Â (v) − S 2 F + γR(S, H; Z) s.t. m v=1 w v = 1, w v ≥ 0, S = S T , S ij ≥ 0<label>(18)</label></formula><p>where w = (w 1 , w 2 · · · w m ) denote the important weights of different graphs which are learned adaptively. The parameter r &gt; 1 is used to control the weight distribution, as suggested in previous work <ref type="bibr" target="#b31">[32]</ref>. The optimal S, Z and w can be obtained by alternatively conducting the following Step 1∼3 until convergence.</p><p>Step 1. Solving S while fixing Z, w, the problem becomes</p><formula xml:id="formula_21">min S m v=1 w r v Â (v) − S 2 F + γTr(Z T (I − S)Z) s.t. S = S T , S ij ≥ 0<label>(19)</label></formula><p>It has a simple closed-form solution as</p><formula xml:id="formula_22">S ij = max ( m v=1 w r vÂ (v) + γ 2 ZZ T ) ij , 0<label>(20)</label></formula><p>Step 2. Solving Z while fixing S, w, the problem becomes to Eq. <ref type="bibr" target="#b7">(8)</ref>. The optimal solution is given as Eq.(13) exactly or Eq.(14) approximately.</p><p>Step 3. Solving w while fixing S, Z, the problem becomes</p><formula xml:id="formula_23">min w m v=1 w r v Â (v) − S 2 F + γR(S, H; Z) (21) s.t. m v=1 w v = 1, w v ≥ 0</formula><p>The Lagrangian function is</p><formula xml:id="formula_24">min w m v=1 w r v Â (v) − S 2 F + ξ( m v=1 w v − 1) (22) s.t. w v ≥ 0</formula><p>where ξ is the Lagrangian multiplier. With some simple algebraic manipulations, the optimal w is derived as</p><formula xml:id="formula_25">w v = (1/ Â (v) − S 2 F ) 1/(r−1) m v=1 (1/ Â(v) − S 2 F ) 1/(r−1)<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">M-GOCN architecture</head><p>Let A = {Â (1) ,Â (2) · · ·Â (m) } and Z = Φ m (A, H) be the optimal solution of problem Eq. <ref type="formula" target="#formula_0">(18)</ref>, then our Multiple GOCN (M-GOCN) conducts the layer-wise propagation as,</p><formula xml:id="formula_26">H (k+1) = σ Φ m (A, H (k) )Θ (k)<label>(24)</label></formula><p>where k = 0, 1 · · · K − 1 and Θ (k) denotes the layer-specific trainable weight matrix. σ(·) denotes an activation function, such as ReLU(·) = max(0, ·).</p><p>Remark. Similar to GOCN, here we can instead use a T -step power iteration to compute the optimal Z approximately to avoid the inversion computation. The complete algorithm to compute Φ m (A, H (k) ) in M-GOCN hidden propagation is summarized in Algorithm 2. In our experiments, we set T = 2 and M = 3 for M-GOCN.</p><p>Algorithm 2 M-GOCN layer-wise propagation 1: Input: Feature matrix H (k) ∈ R n×d k , initial multiple graphs A = {Â (1) ,Â (2) · · ·Â (m) } and weight parameter Θ (k) ∈ R n×d k+1 , parameters γ, µ, r, maximum iteration T, M 2: Output: Feature map H (k+1) ∈ R n×d k+1 3: Initialize Z = H (k) ; w = (1/m · · · 1/m) 4: for t m = 1, 2 · · · M do 5:</p><formula xml:id="formula_27">Compute S as S ij = max ( m v=1 w r vÂ (v) + γ 2 ZZ T ) ij , 0 6: Compute Z as Z = (αS) T + (1 − α) T −1 i=0 (αS) i H (k)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Update w as</p><formula xml:id="formula_28">w v = (1/ Â (v) − S 2 F ) 1/(r−1) m v=1 (1/ Â(v) − S 2 F ) 1/(r−1) 8: end for 9: Return H (k+1) = σ ZΘ (k)</formula><p>The perceptron layer of M-GOCN outputs the final label prediction P ∈ R n×c where c denotes the number of class, and the loss function is designed as a cross-entropy loss defined over labeled data which is the same as GOCN, as discussed in §3.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In order to evaluate the effectiveness of the proposed GOCN and M-GOCN, we test them on several datasets and compare them with some other baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>In single graph learning experiments, we test GOCN on six datasets including three standard network datasets (Citeseer, Cora and Pubmed <ref type="bibr" target="#b25">[26]</ref>) and three image datasets (CIFAR10 <ref type="bibr" target="#b15">[16]</ref>, SVHN <ref type="bibr" target="#b19">[20]</ref> and Scene15 <ref type="bibr" target="#b10">[11]</ref>). Their usages in our experiments are introduced below.</p><p>Citeseer contains 3327 nodes and 4732 edges whose nodes denote documents and edges encode the citation relationships between documents. Each node is represented by a 3703 dimension feature descriptor and all nodes are classified into six classes. Cora contains 2708 nodes and 5429 edges. Each node is represented by a 1433 dimension feature descriptor and all the nodes are falling into six classes. Pubmed contains 19717 nodes and 44338 edges. Each node is represented by a 500 dimension feature descriptor and all the nodes are falling into three classes. Scene15 dataset consists of 4485 scene images with 15 different categories <ref type="bibr" target="#b10">[11]</ref>. For each image, we use the feature descriptor provided by work <ref type="bibr" target="#b10">[11]</ref>. CIFAR10 dataset contains 50000 natural RGB color images and all images are falling into 10 classes <ref type="bibr" target="#b15">[16]</ref>. In our experiments, we select 10000 images in all with 1000 images per class for evaluation. We have not use all of images because large storage and high computational complexity are required for graph convolution operation in our GOCN and other comparing GCN based methods. For each image, a CNN feature descriptor is extracted to represent it. SVHN dataset contains 73257 training and 26032 test RGB images <ref type="bibr" target="#b19">[20]</ref>. Similarly, we select 10000 images in all with 1000 images for each class in our evaluation. For each image, a CNN feature descriptor is extracted.</p><p>In our multi-graph learning experiments, we evaluate M-GOCN on three datasets including MSRC-v1 <ref type="bibr" target="#b30">[31]</ref>, Caltech101-7 <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref> and Handwritten numerals <ref type="bibr" target="#b0">[1]</ref>. Their usages in our experiments are introduced below. MSRC-v1 <ref type="bibr" target="#b30">[31]</ref> contains 8 classes of 240 images. Each class contains 30 images. Similar to the setup in work <ref type="bibr" target="#b21">[22]</ref>, we obtain five graphs by using five different kinds of visual descriptors and the final input feature of each graph node is constructed by concatenating different descriptors together. Caltech101-7 <ref type="bibr" target="#b16">[17]</ref> contains 101 categories of images. Following the setup of work <ref type="bibr" target="#b20">[21]</ref>, we select the widely used 7 classes and obtain 1474 images in all in our experiments. We construct six graphs by using six different feature descriptors and obtain the input feature of each graph node by concatenating these descriptors together <ref type="bibr" target="#b20">[21]</ref>. Handwritten numerals <ref type="bibr" target="#b0">[1]</ref> contains 2000 digits from '0' to '9' and each digit class has 200 data samples. We construct six graphs by using six published feature descriptors and obtain the final input feature of each graph node by concatenating them together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Parameter setting</head><p>Similar to experimental setting of GCN <ref type="bibr" target="#b14">[15]</ref>, we use a two-layer graph convolutional network and the number of units in hidden layer is set to 16. We train our GOCN using an ADAM algorithm <ref type="bibr" target="#b13">[14]</ref> with 10000 maximum epochs and learning rate of 0.01. We stop training if the validation loss does not decrease for 100 consecutive epochs, as suggested in <ref type="bibr" target="#b14">[15]</ref>. All the network weights are initialized using Glorot initialization <ref type="bibr" target="#b7">[8]</ref>. The parameters α, γ are set to 0.9 and 20 respectively. Note that, GOCN is not insensitive to these parameters. We provide additional experiments across different settings of α, β and hidden layer number in §5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Data setting</head><p>For network datasets (Citeseer, Cora and Pubmed), we utilize the similar data setting used in previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>. For each class, we select 20 nodes as labeled data and 300 nodes as validation data, and then evaluate the performance of label prediction on the remaining 1000 test nodes. For image dataset CIFAR10 and SVHN, we randomly select 1000, 2000 and 3000 images as labeled samples and use the remaining data as unlabeled samples. For unlabeled samples, we select 1000 images for validation purpose and use the remaining 8000, 7000 and 6000 images as test samples, respectively. For image dataset Scene15 <ref type="bibr" target="#b10">[11]</ref>, we randomly select 500, 750 and 1000 images as labeled data and select 500 images for validation. The remaining images are used for testing. All the reported results are averaged over five runs with different data splits of training, validation and testing. In our multi-graph learning experiments, for all datasets (MSRC-v1, Caltech101-7 and Handwritten numerals), we select 10%, 20% and 30% nodes as labeled samples and use the remaining data as unlabeled samples. For unlabeled samples, we also use 5% nodes for validation purpose to determine the convergence criterion, and use the remaining 85%, 75% and 65% nodes respectively as test samples. All the reported results are averaged over five runs with different data splits of training, validation and testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Evaluation on single graph</head><p>We first compare our GOCN with the baseline model GCN <ref type="bibr" target="#b14">[15]</ref> to demonstrate the benefit of graph optimization. Also, we compare our method against some other graph neural network based semisupervised learning methods which contain two traditional graph based semi-supervised learning methods including Label Propagation (LP) <ref type="bibr" target="#b33">[34]</ref>, Manifold Regularization (ManiReg) <ref type="bibr" target="#b1">[2]</ref>, and four graph neural network methods including DeepWalk <ref type="bibr" target="#b22">[23]</ref>, Graph Convolutional Network (GCN) <ref type="bibr" target="#b14">[15]</ref>, Graph Attention Networks (GAT) <ref type="bibr" target="#b29">[30]</ref> and Deep Graph Informax (DGI) <ref type="bibr" target="#b23">[24]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the comparison results on three citation network benchmark datasets. <ref type="table" target="#tab_1">Table 2</ref> and 3 summarize the comparison results on three image datasets. The best results are marked as bold. We can note that <ref type="bibr" target="#b0">(1)</ref> Comparing with the baseline model GCN <ref type="bibr" target="#b14">[15]</ref>, GOCN obtains obviously better learning results on all datasets, especially on image datasets. This demonstrates the higher predictive ability of GOCN on semi-supervised classification by incorporating graph optimization, which indicates that GOCN conducts data representation and semi-supervised learning more optimal than GCN. (2) GOCN performs better than recent graph network GAT <ref type="bibr" target="#b29">[30]</ref> and DGI <ref type="bibr" target="#b23">[24]</ref>, which demonstrates the benefit of GOCN on data representation and learning. (3) GOCN generally performs better than other graph based semi-supervised method LP <ref type="bibr" target="#b33">[34]</ref>, ManiReg <ref type="bibr" target="#b1">[2]</ref> and DeepWalk <ref type="bibr" target="#b22">[23]</ref>, which further indicates the effectiveness of GOCN on conducting semi-supervised classification tasks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Evaluation on multiple graphs</head><p>For multi-graph learning tasks, we compare our M-GOCN against the state-of-the art baseline methods which contain GCN(v) that conducts traditional GCN <ref type="bibr" target="#b14">[15]</ref> on the v-th singe graphÂ (v) and node content features X; GCN-M that conducts traditional GCN <ref type="bibr" target="#b14">[15]</ref> on the averaged graph rep-resentationĀ = 1 m m v=1Â</p><p>(v) and node features X; Multi-GCN that first learns representations for multiple graphsÂ (v) by using/sharing the common parameters (as suggested in work <ref type="bibr" target="#b6">[7]</ref>), and then select the final representation with the lowest training loss function for multi-graph representation; MLAN <ref type="bibr" target="#b20">[21]</ref> that is a multi-view learning model for graph learning and semi-supervised classification. <ref type="table" target="#tab_3">Table 4</ref> summarizes the comparison results of semi-supervised classification on these datasets. Here, we can note that (1) The proposed M-GOCN performs obviously better than traditional GCN <ref type="bibr" target="#b14">[15]</ref> model conduced on each individual graph A (v) . This clearly demonstrates the effectiveness of the proposed M-GOCN on learning a compact representation for multiple graphs by intergrading the information of multiple graphs together. (2) M-GOCN performs better than the baseline method GCN-M, Multi-GCN <ref type="bibr" target="#b6">[7]</ref> and MLAN <ref type="bibr" target="#b20">[21]</ref>, which indicates the effectiveness of M-GOCN architecture on conduct multiple graph representation and semi-supervised learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Works</head><p>This paper proposes a novel Graph Optimized Convolutional Network (GOCN) for graph data representation and semi-supervised learning. GOCN is inspired based on the re-formulation of graph convolution as a regularization framework. GOCN integrates graph optimization and convolution in a unified scheme and thus can boost their respectively performance in graph neural network learning. Also, GOCN can be naturally extended to M-GOCN to deal with multiple graphs. Experimental results on several benchmarks demonstrate the effectiveness and benefits of the proposed GOCN and M-GOCN on semi-supervised learning tasks. In the future, we will explore GOCN and M-GOCN methods on some other learning tasks, such as graph clustering, embedding etc. Also, we will adapt them on some more computer vision tasks, such as object detection, image co-segmentation and multiple object tracking etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 3 )</head><label>3</label><figDesc>defines a kind of feature aggregation for features H (k) on graph and Eq.(4) gives a non-linear feature transformation via projection Θ (k) and non-linear activation σ(·). For simplicity, we rewrite Eq.(3) as Z = (I +Â)H =ÂH + H (5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>U</head><label></label><figDesc>mgoc = G(A; S) + γR(S, H; Z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison results of different methods on dataset Citeseer, Cora and Pubmed. The best results are marked by bold.</figDesc><table><row><cell>Methond</cell><cell>Citeseer</cell><cell>Cora</cell><cell>Pubmed</cell></row><row><cell>ManiReg</cell><cell>60.1%</cell><cell>59.5%</cell><cell>70.7%</cell></row><row><cell>LP</cell><cell>59.6%</cell><cell>59.0%</cell><cell>71.1%</cell></row><row><cell>DeepWalk</cell><cell>43.2%</cell><cell>67.2%</cell><cell>65.3%</cell></row><row><cell>DGI</cell><cell>71.5%</cell><cell>76.8%</cell><cell>77.2%</cell></row><row><cell>GCN</cell><cell>68.9%</cell><cell>82.9%</cell><cell>77.9%</cell></row><row><cell>GAT</cell><cell>71.0%</cell><cell>83.2%</cell><cell>78.0%</cell></row><row><cell>GOCN</cell><cell cols="3">71.8% 84.8% 79.7%</cell></row><row><cell cols="2">5.3 Comparison with state-of-the-art methods</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison results of different methods on Scene15 dataset. The best results are marked by bold.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Scene15</cell><cell></cell></row><row><cell>No. of label</cell><cell>500</cell><cell>750</cell><cell>1000</cell></row><row><cell>ManiReg</cell><cell cols="3">81.29±3.35 86.45±1.91 89.86±0.71</cell></row><row><cell>LP</cell><cell cols="3">89.40±4.74 92.12±2.87 92.98±2.45</cell></row><row><cell>Deep Walk</cell><cell cols="3">95.64±0.24 96.01±0.24 96.53±0.37</cell></row><row><cell>DGI</cell><cell cols="3">92.94±1.61 94.21±0.64 94.27±0.94</cell></row><row><cell>GCN</cell><cell cols="3">91.42±2.07 94.41±0.92 95.44±0.89</cell></row><row><cell>GAT</cell><cell cols="3">93.98±0.75 94.64±0.41 95.03±0.46</cell></row><row><cell>GOCN</cell><cell cols="3">95.87±0.56 97.40±0.34 98.00±0.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison results of different methods on dataset SVHN and CIFAR10. The best results are marked by bold. 44±0.69 72.73±0.44 74.63±0.45 52.30±0.66 57.08±0.80 59.69±0.71 LP 69.68±0.84 70.35±1.73 69.47±2.96 57.52±0.67 59.22±0.67 60.38±0.51 Deep Walk 74.64±0.23 76.21±0.23 77.04±0.42 56.16±0.54 59.73±0.35 61.26±0.32 DGI 70.82±1.22 72.83±0.79 73.16±1.20 58.97±0.61 60.26±0.56 60.56±0.36 GCN 71.33±1.48 73.43±0.46 73.63±0.52 60.43±0.56 60.91±0.50 60.99±0.49 GAT 73.87±0.32 74.85±0.55 75.17±0.43 63.25±0.50 65.55±0.58 66.56±0.58 GOCN 80.72±0.35 82.67±0.25 83.63±0.37 68.13±0.58 71.83±0.37 73.66±0.52</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>SVHN</cell><cell></cell><cell></cell><cell>CIFAR10</cell><cell></cell></row><row><cell>No. of label</cell><cell>1000</cell><cell>2000</cell><cell>3000</cell><cell>1000</cell><cell>2000</cell><cell>3000</cell></row><row><cell>ManiReg</cell><cell>69.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison results of different multi-graph learning methods on dataset Caltech101-7, MSRC-v1 and Handwritten numerals, respectively. The best results are marked by bold. 97±0.36 97.23±0.47 90.22±1.06 91.68±1.50 97.76±0.42 97.92±0.41</figDesc><table><row><cell>Dataset</cell><cell cols="2">Caltech101-7</cell><cell>MSRC-v1</cell><cell></cell><cell cols="2">Handwritten numerals</cell></row><row><cell>Ratio of label</cell><cell>10%</cell><cell>20%</cell><cell>10%</cell><cell>20%</cell><cell>10%</cell><cell>20%</cell></row><row><cell>GCN(1)</cell><cell cols="2">82.62±0.78 84.71±1.38</cell><cell cols="2">62.08±6.74 66.21±5.42</cell><cell cols="2">90.92±0.22 91.33±0.45</cell></row><row><cell>GCN(2)</cell><cell cols="2">85.00±1.90 86.34±1.73</cell><cell cols="2">81.97±1.61 85.84±2.90</cell><cell cols="2">94.72±0.22 95.39±0.77</cell></row><row><cell>GCN(3)</cell><cell cols="2">86.99±1.42 88.37±0.82</cell><cell cols="2">84.94±3.36 88.07±1.54</cell><cell cols="2">95.59±0.61 96.16±0.57</cell></row><row><cell>GCN(4)</cell><cell cols="2">93.18±0.98 93.44±0.55</cell><cell cols="2">77.47±3.20 83.10±1.54</cell><cell cols="2">95.78±0.31 96.45±0.68</cell></row><row><cell>GCN(5)</cell><cell cols="2">92.35±0.62 92.49±0.47</cell><cell cols="2">80.00±2.60 84.72±3.46</cell><cell cols="2">88.53±0.53 89.17±0.63</cell></row><row><cell>GCN(6)</cell><cell cols="2">92.05±0.78 92.45±0.86</cell><cell>-</cell><cell>-</cell><cell cols="2">82.34±0.67 83.11±0.25</cell></row><row><cell>GCN-M</cell><cell cols="2">90.39±2.02 91.91±0.33</cell><cell cols="2">86.81±3.60 90.31±1.87</cell><cell cols="2">96.42±0.47 97.08±0.48</cell></row><row><cell>Multi-GCN</cell><cell cols="2">95.09±0.62 96.08±0.58</cell><cell cols="2">87.14±2.13 90.43±1.45</cell><cell cols="2">97.14±0.23 97.88±0.30</cell></row><row><cell>MLAN</cell><cell cols="2">93.45±0.36 94.86±0.29</cell><cell cols="2">83.42±2.10 87.27±1.66</cell><cell cols="2">97.23±0.26 97.46±0.52</cell></row><row><cell>M-GOCN</cell><cell>95.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Arthur Asuncion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ranking on data manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G O B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N L J W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Label consistent k-svd: Learning a discriminative dictionary for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2651" to="2664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning region features for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W Y W J D</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="381" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining neural networks with personalized pagerank for classification on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Johannes Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale multi-view spectral clustering via bipartite graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2750" to="2756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural graph matching networks for fewshot 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><forename type="middle">H S S S Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="653" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5423" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-view clustering and semi-supervised classification with adaptive neighbours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2408" to="2414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parameter-free auto-weighted multiple graph learning: A framework for multiview clustering and semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1881" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L H P L Y B</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J J S S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-C</forename><forename type="middle">Z</forename><surname>Siyuan Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rgcnn: Regularized graph cnn for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02952</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Locus: Learning object classes with unsupervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="756" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiview spectral embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1438" to="1446" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Capsule graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xinyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
		<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dual graph convolutional networks for graph-based semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Wide Web Conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
