<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-world Multi-object, Multi-grasp Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018">2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Jen</forename><surname>Chu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruinian</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricio</forename><forename type="middle">A</forename><surname>Vela</surname></persName>
						</author>
						<title level="a" type="main">Real-world Multi-object, Multi-grasp Detection</title>
					</analytic>
					<monogr>
						<title level="m">and Automation Letters IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE</title>
						<imprint>
							<biblScope unit="issue">1</biblScope>
							<date type="published" when="2018">2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/LRA.2018.2852777</idno>
					<note>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Perception for Grasping</term>
					<term>Grasping</term>
					<term>Deep Learn- ing in Robotic Automation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A deep learning architecture is proposed to predict graspable locations for robotic manipulation. It considers situations where no, one, or multiple object(s) are seen. By defining the learning problem to be classification with null hypothesis competition instead of regression, the deep neural network with RGB-D image input predicts multiple grasp candidates for a single object or multiple objects, in a single shot. The method outperforms state-of-the-art approaches on the Cornell dataset with 96.0% and 96.1% accuracy on image-wise and object-wise splits, respectively. Evaluation on a multi-object dataset illustrates the generalization capability of the architecture. Grasping experiments achieve 96.0% grasp localization and 89.0% grasping success rates on a test set of household objects. The real-time process takes less than .25 s from image to plan.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W HILE manipulating objects is relatively easy for humans, reliably grasping arbitrary objects remains an open challenge for robots. Resolving it would advance the application of robotics to industrial use cases, such as part assembly, binning, and sorting. Likewise, it would advance the area of assistive robotics, where the robot interacts with its surroundings in support of human needs. Robotic grasping involves perception, planning, and control. As a starting point, knowing which object to grab and how to do so are essential aspects. Consequently, accurate and diverse detection of robotic grasp candidates for target objects should lead to a better grasp path planning and improve the overall performance of grasp-based manipulation tasks.</p><p>The proposed solution utilizes a deep learning strategy for identifying suitable grasp configurations from an input image. In the past decade, deep learning has achieved major success on detection, classification, and regression tasks <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. Its key strength is the ability to leverage large quantities of labelled and unlabelled data to learn powerful representations without hand-engineering the feature space. Deep neural networks have been shown to outperform hand-designed features and reach state-of-the-art performance.</p><p>In this research problem, we are interested in tackling the problem of identifying viable candidate robotic grasps of objects in a RGB-D image. The envisioned gripper is a parallel plate gripper (or similar in functionality). The principal difficulty comes from the variable shapes and poses  of objects as imaged by a camera. The structural features defining successful grasps for each object may be different; all possible features should be identified through the learning process. The proposed architecture associated to the grasp configuration estimation problem relies on the strengths of deep convolutional neural networks (CNNs) at detection and classification. Within this architecture, the identification of grasp configuration for objects is broken down into a grasp detection processes followed by a more refined grasp orientation classification process, both embedded within two coupled networks.</p><p>The proposed architecture includes a grasp region proposal network for identification of potential grasp regions. The network then partitions the grasp configuration estimation problem into regression over the bounding box parameters, and classification of the orientation angles, from RGB-D data. Importantly, the orientation classifier also includes a No Orientation competing class to reject spuriously identified regions for which no single orientation classification performs well, and to act as a competing no-grasp class. The proposed approach predicts grasp candidates in more realistic situations where no, single, and multiple objects may be visible; it also predicts multiple grasps with confidence scores (see <ref type="figure" target="#fig_0">Fig. 1</ref>, scores not shown for clarity). A new multi-objects grasp dataset is collected for evaluation with the traditional performance metric of false-positives-per-image. We show how the multi-grasp output and confidence scores can inform a subsequent planning process to take advantage of the multiple outputs for improved grasping success.</p><p>The main contributions of this paper are threefold: (1) A deep network architecture that predicts multiple grasp candidates in situations when none, single or multiple objects are in the view. Compared to baseline methods, the classification-based approach demonstrates improved out-comes on the Cornell dataset <ref type="bibr" target="#b3">[4]</ref> benchmark, achieving stateof-the-art performance on image-wise and object-wise splits.</p><p>(2) A multi-object, multi-grasp dataset is collected and manually annotated with grasp configuration ground-truth as the Cornell dataset. We demonstrate the generalization capabilities of the architecture and its prediction performance on the multi-grasp dataset with respect to false grasp candidates per image versus grasp miss rate. The dataset is available at github.com/ivalab/grasp multiObject.</p><p>(3) Experiments with a 7 degree of freedom manipulator and a time-of-flight RGB-D sensor quantify the system's ability to grasp a variety of household objects placed at random locations and orientations. Comparison to published works shows that the approach is effective, achieving a sensible balance for real-time object pick-up with an 89% success rate and less than 0.25 s from image to prediction to plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Research on grasping has evolved significantly over the last two decades. Altogether, the review papers <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref> provide a good context for the overall field. This section reviews grasping with an emphasis on learning-based approaches and on representation learning.</p><p>Early work on perception-based learning approaches to grasping goes back to <ref type="bibr" target="#b8">[9]</ref>, which identified a low dimensional feature space for identifying and ranking grasps. Importantly it showed that learning-based methods could generalize to novel objects. Since then, machine learning methods have evolved alongside grasping strategies. Exploiting the input/output learning properties of machine learning (ML) systems, <ref type="bibr" target="#b9">[10]</ref> proposed to learn the image to grasp mapping through the manual design of convolutional networks. As an end-to-end system, reconstruction of the object's 3D geometry is not needed to arrive at a grasp hypothesis. The system was trained using synthetic imagery, then demonstrated successful grasping on real objects. Likewise, <ref type="bibr" target="#b10">[11]</ref> employed a CNNlike feature space with random forests for grasp identification. In addition to where to grasp, several efforts learn the grasp approach or pre-grasp strategy <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, while some focus on whether the hypothesized grasp is likely to succeed <ref type="bibr" target="#b13">[14]</ref>. Many of these approaches exploited contemporary machine learning algorithms with manually defined feature spaces.</p><p>At the turn of this decade, two advances led to new methods for improving grasp identification: the introduction of low-cost depth cameras, and the advent of computational frameworks to facilitate the construction and training of CNNs. The advent of consumer depth cameras enabled models of grasping mapping to encode richer features. In particular <ref type="bibr" target="#b14">[15]</ref> represented grasps as a 2D oriented rectangle in the image space with the local surface normal as the approaching vector; this grasp configuration vector has been adopted as the well-accepted formulation. Generally, the early methods using depth cameras sought to recover the 3D geometry from point clouds for grasp planning <ref type="bibr" target="#b15">[16]</ref>, with manually derived feature space used in the learning process. Deep learning approaches arrived later <ref type="bibr" target="#b1">[2]</ref> and were quickly adopted by the computer vision community <ref type="bibr" target="#b16">[17]</ref>.</p><p>Deep learning avoids the need for engineering feature spaces, with the trade-off that larger datasets are needed. The trade-off is usually mitigated through the use of pretraining on pre-existing computer vision datasets followed by fine-tuning on a smaller, problem-specific dataset. Following a sliding window approach, <ref type="bibr" target="#b17">[18]</ref> trained a two stage multimodal network, with the first stage generating hypotheses for a more accurate second stage. Similarly, <ref type="bibr" target="#b18">[19]</ref> first performed an image-wide pre-processing step to identify candidate object regions, followed by application of a CNN classifier for each region. To avoid sliding windows or image-wide search, end-to-end approaches are trained to output a single grasp configuration from the input data <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>. Regressionbased approaches <ref type="bibr" target="#b19">[20]</ref> require compensation through image partitioning since the grasp configuration space is non-convex. Following the architecture in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref> experimented on real objects with physical grasping experiments. The two-stage network in <ref type="bibr" target="#b20">[21]</ref> first output a learnt feature, which was then used to provide a single grasp output. These approaches may suffer from averaging effects associated to the singleoutput nature of the mapping. Guo et al. <ref type="bibr" target="#b22">[23]</ref> employed a two-stage process with a feature learning CNN, followed by specific deep network branches (graspable, bounding box, and orientation). Alternatively, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> predicted grasp scores over the set of all possible grasps (which may be discretized). Such networks admit the inclusion of end-effector uncertainty. Most deep network approaches mentioned above start with the strong prior that every image contains a single object with a single grasp target (except <ref type="bibr" target="#b18">[19]</ref>). This assumption does not generically hold in practice: many objects have multiple grasp options and a scene may contain more than one object.</p><p>Another line of research is to learn the mapping from vision input to robot motion to achieve grasping. To directly plan grasps, Lu et al. <ref type="bibr" target="#b25">[26]</ref> proposed to predict and maximize grasp success by inferring grasp configurations from vision input for grasp planning. Research on empirical grasp planning with reinforcement learning (RL) acquired samples from robots in real experiments <ref type="bibr" target="#b26">[27]</ref>. The training time involved several weeks and led to limitation of its scalability. The work <ref type="bibr" target="#b27">[28]</ref> collected over 800k data points with up to 14 robotic arms running in parallel for learning visual servoing. The training time involved over 2 months. Generalization performance of RL solution to environmental changes remains unknown.</p><p>Inspired by <ref type="bibr" target="#b28">[29]</ref>, we propose to incorporate a grasp region proposal network to generate candidate regions for feature extraction. Furthermore, we propose to transform grasp configuration from a regression problem formulated in previous works <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> into a combination of region detection and orientation classification problems (with null hypothesis competition). We utilize ResNet <ref type="bibr" target="#b16">[17]</ref>, the current state-of-theart deep convolutional neural network, for feature extraction and grasp prediction. Compared to previous approaches, our method considers more realistic scenarios with multiple objects in a scene. The proposed architecture predicts multiple grasps with corresponding confidence scores, which aids the subsequent planning process and actual grasping. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM STATEMENT</head><p>Given corresponding RGB and depth images of a novel object, the objective is to identify the grasp configurations for potential grasp candidates of an object for the purpose of manipulation. The 5-dimensional grasp rectangle is the grasp representation employed <ref type="bibr" target="#b14">[15]</ref>. It is a simplification of the 7dimensional representation <ref type="bibr" target="#b14">[15]</ref> and describes the location, orientation, and opening distance of a parallel plate gripper prior to closing on an object. The 2D orientated rectangle, shown in <ref type="figure" target="#fig_1">Fig. 2a</ref> depicts the gripper's location (x, y), orientation θ, and opening distance h. An additional parameter describing the length w completes the bounding box grasp configuration,</p><formula xml:id="formula_0">g = {x, y, θ, w, h} T .<label>(1)</label></formula><p>Thinking of the center of the bounding box with its local (x, y) axes aligned to the w and h variables, respectively, the first three parameters represent the SE(2) frame of the bounding box in the image, while the last two describe the dimensions of the box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPROACH</head><p>Much like <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, the proposed approach should avoid sliding window such as in <ref type="bibr" target="#b17">[18]</ref> for real-time implementation purposes. We avoid the time-consuming sliding-window approach by harnessing the capacity of neural networks to perform bounding box regression, and thereby to predict candidate regions on the full image directly. Furthermore, we preserve all possible grasps and output all ranked candidates, instead of regressing a single outcome. To induce a richer feature representation and learn more of the structural cues, we propose to use a deeper network model compared to previous works <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b26">[27]</ref>, with the aim of improving feature extraction for robotic grasp detection. We adopt the ResNet-50 <ref type="bibr" target="#b16">[17]</ref> with 50 layers, which has more capacity and should learn better than the AlexNet <ref type="bibr" target="#b1">[2]</ref> used in previous works (8 layers). ResNet is known for its residual learning concept to overcome the challenge of learning mapping functions. A residual block is designed as an incorporation of a skip connection with standard convolutional neural network. This design allows the block to bypass the input, and encourage convolutional layers to predict the residual for the final mapping function of a residual block.</p><p>The next three subsections describe the overall architecture of the system. It includes integration of the proposal network with a candidate grasp region generator; a description of our choice to define grasp parameter estimation as a combination of regression and classification problems; and an explanation of the multi-grasp detection architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Grasp Proposals</head><p>The first stage of the deep network aims to generate grasp proposals across the whole image, avoiding the need for a separate object segmentation pipeline.</p><p>Inspired by Region Proposal Network (RPN) <ref type="bibr" target="#b28">[29]</ref>, the Grasp Proposal Network in our architecture ( <ref type="figure" target="#fig_2">Fig. 3</ref>) works as RPN and shares a common feature map (14×14×1024 feature map) of intermediate convolutional layers from ResNet-50 (layer 40). The Grasp Proposal Network outputs a 1×1×512 feature which is then fed into two sibling fully connected layers. The two outputs specify both probability of grasp proposal and proposal bounding box for each of r anchors on the shared feature map. The ROI layer extracts features with corresponding proposal bounding boxes and sends to the rest of the networks.</p><p>The Grasp Proposal Network works as sliding a mininetwork over the feature map. At each anchor of the feature map, by default 3 scales and 3 aspect ratios are used for grasp reset bounding box shape variations, as shown in <ref type="figure" target="#fig_1">Fig 2c.</ref> Hence r × 3 × 3 predictions would be generated in total. For ground truth, we reset each orientated ground truth bounding box to have vertical height and horizontal width, as shown in <ref type="figure" target="#fig_1">Fig.  2b</ref>. Let t i denote the 4-dimensional vector specifying the reset (x, y, w, h) of the i-th grasp configuration, and p i denote the probability of the i-th grasp proposal. For the index set of all proposals I, we define the loss of grasp proposal net (gpn) to be:</p><formula xml:id="formula_1">L gpn ({(p i , t i ) I i=1 }) = i L gp cls (p i , p * i ) + λ i p * i L gp reg (t i , t * i ). (2)</formula><p>where L gp cls is the cross entropy loss of grasp proposal classification (gp cls), L gp reg is the l 1 regression loss of grasp proposal (gp reg) with weight λ. We denote p * i = 0 for no grasp and p * i = 1 when a grasp is specified. The variable t * i is the ground truth grasp coordinate corresponding to p * i . Compared to the widely applied selective search used in R-CNN <ref type="bibr" target="#b29">[30]</ref>, RPN learns object proposals end-to-end from the input without generating region of interests beforehand. This latter, streamlined approach is more applicable to real-time robotic applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Grasp Orientation as Classification</head><p>Many prior approaches <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> regress to a single 5dimensional grasp representation g = {x, y, w, h, θ} for a RGB-D input image. Yet to predict either on SE(2) (planar pose) or on S 1 (orientation) involves predicting coordinates that lies in a non-Euclidean (non-convex) space where regression and its standard L2 loss may not perform well. Rather than performing regression, our multi-grasp localization pipeline quantizes the grasp representation orientation coordinate θ into R equal-length intervals (each interval is represented by its centroid), and formulates the input/ouput mapping as a classification task for grasp orientation. It differs from <ref type="bibr" target="#b22">[23]</ref> in that we add a non-grasp collecting orientation class for explicit competition with a null hypotheis. If none of the orientation classifiers outputs a score higher than the nongrasp class, then the grasp proposal is considered incorrect and rejected. In contrast, <ref type="bibr" target="#b22">[23]</ref> has a separate grasp confidence score, which may not capture well the orientation-dependent properties of grasps. The value of the non-grasp class is that it is necessary for the downstream multi-object, multigrasp component of the final algorithm. The total number of classes is</p><formula xml:id="formula_2">|C| = R + 1. Denote by {(l i , θ i )} I i=1</formula><p>where the i-th grasp configuration with classification label l i ∈ 1, ..., R is associated with the angle θ i . For the case of no possible orientation (i.e., the region is not graspable), the output label is l = 0 and there is no associated orientation. In this paper, R = 19 is utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-Grasp Detection</head><p>After the region proposal stage of the deep network, the last stage identifies candidate grasp configurations. This last stage classifies the predicted region proposals from previous stage into R regions for grasp configuration parameter θ. At the same time the last stage also refines the proposal bounding box to a non-oriented grasp bounding box (x, y, w, h).</p><p>To process the region proposals efficiently, we integrate an ROI pooling layer <ref type="bibr" target="#b30">[31]</ref> into ResNet-50 so that it may share ResNet's convolutional layers. Sharing the feature map with previous layers avoids re-computation of features within the region of interest. An ROI pooling layer stacks all of the features of the identified grasp proposals, which then get fed to two sibling fully connected layers for orientation parameter classification l and bounding box regression (x, y, w, h). The ROI pooling layer receives its input from the intermediate convolutional layer of ResNet-50 (layer 40).</p><p>Let ρ l denote the probability of class l after a softmax layer, and β l denote the corresponding predicted grasp bounding box. Define the loss function of the grasp configuration prediction (gcr) to be:</p><formula xml:id="formula_3">L gcr ({(ρ l , β l )} C c=0 ) = c L gcr cls (ρ l ) + λ 2 c 1 c =0 (c)L gcr reg (β c , β * c ). (3)</formula><p>where L gcr cls is the cross entropy loss of grasp angle classification (gcr cls), L gcr reg is the l 1 regression loss of grasp bounding boxes (gcr reg) with weight λ 2 , and β * c is the ground truth grasp bounding box.</p><p>With the modified ResNet-50 model, end-to-end training for grasp detection and grasp parameter estimation employs the total loss:</p><formula xml:id="formula_4">L total = L gpn + L gcr .<label>(4)</label></formula><p>The streamlined system generates grasp proposals at the ROI layer, stacks all ROIs using the shared feature, and the additional neurons of the two sibling layers output grasp bounding boxes and orientations, or reject the proposal.</p><p>V. EXPERIMENTS AND EVALUATION Evaluation of the grasp identification algorithm utilizes the Cornell Dataset for benchmarking against other state-of-theart algorithms. To demonstrate the multi-object, multi-grasp capabilities, a new dataset is carefully collected and manually annotated. Both datasets consist of color and depth images for multiple modalities. In practice, not all possible grasps are covered by the labelled ground truth, yet the grasp rectangles are comprehensive and representative for diverse examples of good candidates. The scoring criteria takes into account the potential sparsity of the grasp configuration by including an acceptable proximity radius to the ground truth grasp configuration.</p><p>a) Cornell Dataset: The Cornell Dataset <ref type="bibr" target="#b3">[4]</ref> consists of 885 images of 244 different objects, with several images taken of each object in various orientations or poses. Each distinct image is labelled with multiple ground truth grasps corresponding to possible ways to grab the object. b) Multi-Object Dataset: Since the Cornell Dataset scenarios consist of one object in one image, we collect a Multi-Object Dataset for the evaluation of the multi-object/multigrasp case. Our dataset is meant for evaluation and consists of 96 images with 3-5 different objects in a single image. We follow the same protocol as the Cornell Dataset by taking several images of each set of objects in various orientations or poses. Multiple ground truth grasps for each object in each image are annotated using the same configuration definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cornell Data Preprocessing</head><p>To reuse the pre-trained weights of ResNet-50 on COCO-2014 dataset <ref type="bibr" target="#b31">[32]</ref>, the Cornell dataset is preprocessed to fit the input format of the ResNet-50 network. For comparison purposes, we follow the same procedure in <ref type="bibr" target="#b19">[20]</ref> and substitute the blue channel with the depth channel. Since RGB data lies between 0 to 255, the depth information is normalized to the same range. The mean image is chosen to be 144, while the pixels on the depth image with no information were replaced with zeros. For the data preparation, we perform extensive data augmentation. First, the images are center cropped to obtain a 351x351 region. Then the cropped image is randomly rotated between 0 to 360 degree and center cropped to 321x321 in size. The rotated image is randomly translated in x and y direction by up to 50 pixels. The preprocessing generates 1000 augmented data for each image. Finally the image is resized to 227x227 to fit the input of ResNet-50 architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pre-Training</head><p>To avoid over-fitting and precondition the learning process, we start with the pretrained ResNet-50. As shown in <ref type="figure" target="#fig_2">Fig 3,</ref> we implement grasp proposal layer after the third residual block by sharing the feature map. The proposals are then sent to the ROI layer and fed into the fourth residual layer. The 7 × 7 average pool outputs are then fed to two fully connected layers for final classification and regression. All new layers beyond ResNet-50 are trained from scratch.</p><p>Because the orientation is specified as a class label and assigned a specific orientation, the Cornell dataset needs to be converted into the expected output format of the proposed network. We equally divide 180 degrees into R regions (due to symmetry of the gripper) and assign the continuous ground truth orientation to the nearest discrete orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head><p>For training, we train the whole network end-to-end for 5 epochs on a single nVidia Titan-X (Maxwell architecture). The initial learning rate is set to 0.0001. And we divide the learning rate by 10 every 10000 iterations. Tensorflow is the implementation framework with cudnn-5.1.10 and cuda-8.0 packages. The code will be publicly released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Metric</head><p>Accuracy evaluation of the grasp parameters involves checking for proximity to the ground truth according to established criteria <ref type="bibr" target="#b19">[20]</ref>. A candidate grasp configuration is reported correct if both: 1) the difference of angle between predicted grasp g p and ground truth g t is within 30 • , and 2) the Jaccard index of the predicted grasp g p and the ground truth g t is greater than 0.25, e.g.,</p><formula xml:id="formula_5">J(g p , g t ) = |g p ∩ g t | |g p ∪ g t | &gt; 0.25<label>(5)</label></formula><p>The Jaccard index is similar to the Intersection over Union (IoU) threshold for object detection.  <ref type="bibr" target="#b14">[15]</ref> 60.5 58.3 0.02 Lenz et al. <ref type="bibr" target="#b17">[18]</ref> 73.9 75.6 0.07 Redmon et al. <ref type="bibr" target="#b19">[20]</ref> 88.0 87.1 3.31 Wang et al. <ref type="bibr" target="#b18">[19]</ref> 81.8 N/A 7.10 Asif et al. <ref type="bibr" target="#b10">[11]</ref> 88.2 87.5 -Kumra et al. <ref type="bibr" target="#b20">[21]</ref> 89.2 88.9 16.03 Mahler et al. <ref type="bibr" target="#b24">[25]</ref> 93.0 N/A ∼1.25 Guo et al. <ref type="bibr" target="#b22">[23]</ref> 93. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single-object Single-grasp</head><p>Testing of the proposed architecture on the Cornell Dataset, and comparison with prior works lead to <ref type="table" target="#tab_1">Table I</ref>. For this single-object/single-grasp test, the highest output score of all grasp candidates output is chosen as the final output. The proposed architecture outperforms all competitive methods. On image-wise split, our architecture reaches 96.0% accuracy; on object-wise split for unseen objects, 96.1% accuracy is achieved. We also tested our proposed architecture by replacing ResNet-50 with VGG-16 architecture, a smaller deep net with 16 layers. With VGG-16, our model still outperforms competitive approaches. Yet the deeper ResNet-50 achieve 4.4% more on unseen objects. Furthermore, we experiment on RGB images without depth information with ResNet-50 version and both image-wise and object-wise split perform slightly worse than our proposed approach, indicating the effectiveness of depth. The third column contains the run-time of methods that have reported it, as well as the runtime of the proposed method. Computationally, our architecture detects and localize multiple grasps in 0.120s, which is around 8 fps and is close to usable in real time applications. The VGG-16 architecture doubles the speed with some prediction accuracy loss. <ref type="table" target="#tab_1">Table II</ref> contains the outcomes of stricter Jaccard indexes for the ResNet-50 model. Performance decreases with stricter conditions but maintains competitiveness even at 0.40 IoU condition. Typical output of the system is given in <ref type="figure">Fig. 4a</ref>, where four grasps are identified. Limiting the output to a single grasp leads to the outputs depicted in <ref type="figure">Fig. 4b</ref>. In the multi-grasp case, our system not only predicts universal grasps learned from ground truth, but also contains candidate grasps not contained in the ground truth, <ref type="figure">Fig. 4c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Single-object Multi-grasp</head><p>For realistic robotic application, a viable grasp usually depends both on the object and its surroundings. Given that (b) (a) (c) (d) <ref type="figure">Fig. 4</ref>. Output 5D grasp configuration of system for Cornell dataset inputs: (a) the multiple grasp options output for an object; (b) the top grasp outputs for several objects; (c) output grasps (red) and ground-truth grasps (green) showing that the system may output grasps for which there is no ground truth; (d) multi-grasp output for several objects. The green rectangles are ground truth and the red rectangles represent predicted grasps for each unseen object.</p><p>one grasp candidate may be impossible to achieve, there is benefit to provide a rank ordered list of grasp candidates. Our system provides a list of high quality grasp candidates for a subsequent planner to select from. <ref type="figure">Fig. 4d</ref> shows samples of the predicted grasps and corresponding ground truths.</p><p>To evaluate the performance of the multi-grasp detector, we employ the same scoring system as with the single grasp, then generate the miss rate as a function of the number of false positives per image (FPPI) by varying the detection threshold (see <ref type="figure" target="#fig_3">Fig. 5a</ref> for the single-object multi-grasp case). The model achieves 28% and 25% miss rate at 1 FPPI for object-wise split and image-wise split, respectively. A false positive means an incorrect grasp candidate for the object. Thus, accepting that there may be 1 incorrect candidate grasp per image, the system successfully detects 72% (75%) of possible grasps for objectwise (image-wise) split. The model performs slightly better in image-wise split than object-wise split due to unseen objects in the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-object Multi-grasp</head><p>Here, we apply the proposed architecture to a multi-object multi-grasp task using our Multi-Object dataset. The trained network is the same trained network we've been reporting the results for (trained only on the Cornell dataset with both image-split and object-split variants). Testing involves evaluating against the multi-object dataset, and represents a cross domain application with unseen objects. <ref type="figure" target="#fig_3">Fig. 5a</ref> depicts the plot of miss rate versus FPPI. At 1FPPI, the system achieves 53% and 49% prediction accuracy with image-split model and object-split networks, respectively. Visualizations of predicted grasp candidates are depicted in <ref type="figure" target="#fig_3">Fig.  5b</ref>. The model successfully locates multiple grasp candidates on multiple new objects in the scene with very few false positives, and hence is practical for robotic manipulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Physical Grasping</head><p>To confirm and test the grasp prediction ability in practice, a physical grasping system is set up for experiments (see <ref type="figure" target="#fig_3">Fig.  5c</ref>). As in <ref type="bibr" target="#b21">[22]</ref>, performance is given for both the vision subsystem and the subsequently executed grasp movement. The dual scores aid in understanding sources of error for the overall experiment. To evaluate the vision sub-system, each RGB-D input of vision sub-system is saved to disk and annotated with the same protocol as the Cornell and Multi-Object Top-1 Nearest to center detected physical detected physical banana 10/10 7/10 10/10 8/10 glasses 9/10 8/10 9/10 9/10 ball 10/10 9/10 10/10 9/10 tape 10/10 10/10 10/10 10/10 screwdriver 9/10 7/10 9/10 7/10 stapler 10/10 10/10 10/10 9/10 spoon 9/10 9/10 10/10 10/10 bowl 10/10 10/10 10/10 10/10 scissors 9/10 8/10 9/10 9/10 mouse 9/10 8/10 9/10 8/10 average (%) 95.0 86.0 96.0 89.0 datasets. The evaluation metric uses Jaccard index 0.25 and angle difference 30 • thresholds. A set of 10 commonly seen objects was collected from Cornell dataset for the experiment. For each experiment, an object was randomly placed on a reachable surface at different locations and orientations. Each object was tested 10 times. The outcome of physical grasping was marked as pass or fail. <ref type="table" target="#tab_1">Table III</ref> shows the performance of both the vision subsystem and the physical grasping sub-system for two different policies. For the first (Top-1), we used the the grasp candidate with the highest confidence score. For the second, the planner chose the grasp candidate closest to the image-based center of the object from the top-N candidates (N = 25 in this experiment). In real-world physical grasping, grasp candidates close to the image-based centroid of object should be helpful, by creating a more balanced grasp for many objects. The lowest performing objects are those with a less even distribution of mass or shape (screwdriver), meaning that object-specific grasp priors coupled to the multi-grasp output might improve grasping performance. We leave this for future work as our system does not perform object recognition. Also, the tested and Automation Letters This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication.   objects are unseen. For direct physical grasping comparisons, state-of-the-art approaches were implemented and applied to the same objects with the same robot. The reference approaches selected are <ref type="bibr" target="#b22">[23]</ref> due to its performance on the Cornell dataset, and <ref type="bibr" target="#b20">[21]</ref> due to the closely related back-bone architecture in the model design. <ref type="table" target="#tab_1">Table IV</ref> reports both the accuracies of our implementations on standard Cornell dataset and physical grasping success. The set of objects is the same as in table III. The method in <ref type="bibr" target="#b20">[21]</ref> has two parallel ResNet-50 for RGB and depth input, respectively. Our implementation achieves similar results on Cornell dataset. However, it overfits to the Cornell dataset as performance drops substantially for realworld objects (56% success rate). Our implementation of <ref type="bibr" target="#b22">[23]</ref> achieves slightly better than reported, and reaches 81% success rate on physical grasping. Our proposed approach outperforms both across the board.</p><p>Table V further compares our experimental outcomes with state-of-the-art published works with physical grasping. The testing sets for reported experiments may include different object class/instance. Even though the object classes may be the same, the actual objects used could differ. Nevertheless, the comparison should provide some context for grasping performance and computational costs relative to other published approaches. The experiments in <ref type="bibr" target="#b21">[22]</ref> had 6 objects in common with ours. On the common subset, <ref type="bibr" target="#b21">[22]</ref> reports a 55.0% success rate with a 60s execution time, while ours achieves 86.7% with a 15s execution time (mostly a consequence of joint rate limits). The approach described in <ref type="bibr" target="#b24">[25]</ref> reported 80.0% success rate on 10 household objects and 94.0% when using a cross entropy method <ref type="bibr" target="#b27">[28]</ref> to sample and re-fit grasp candidates (at the cost of greater grasp detection time). The RL approach taking several weeks achieved 66.0% on seen and unseen objects <ref type="bibr" target="#b26">[27]</ref>. Not included in the table are the reported results of <ref type="bibr" target="#b27">[28]</ref>, due to different experimental conditions. They reported 90% success on grasping objects from a bin with replacement, and 80% without replacement (100 trials using unseen objects). Our approach achieves 89.0% in real-time, with subsequent planning of the redundant manipulator taking 0.1 secs. Overall it exhibits a good balance between accuracy and speed for real world object grasping tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>This section reviews a set of experiments, summarized in <ref type="table" target="#tab_1">Table VI</ref>, examining the contributions of the proposed acrchitecture's components. Firstly, ResNet-50 was used to regress RGB input to 5D grasp configuration output (a). This architecture can be recognized as <ref type="bibr" target="#b19">[20]</ref> with a deeper network and without depth information. Then two ResNet-50 networks (b) processed RGB and depth data, respectively, with a small network regressing the concatenated feature for the grasp configuration. This architecture matches <ref type="bibr" target="#b20">[21]</ref> and boosts performance. However, the doubled number of parameters results in difficulties when deploying on real-world grasping. To keep the architecture size, one color channel (blue) is replaced with depth information, while the performance is maintained (c). Next, grasp orientation is quantized and an extra branch is trained to classify grasping orientation of an object (d). The last two instances integrate grasp proposals into the ResNet-50 back-bone with added layers, for color (e) and RGD (f) input data. The multi-grasp outputs overcome averaging effects <ref type="bibr" target="#b19">[20]</ref> without the need to separate an image into grids. The ablation study identifies the contribution of classification, grasp proposal and the selection policy. In addition, the RGB-only version of the proposed method is still able to achieve good performance, being slightly worse than including depth information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We presented a novel grasping detection system to predict grasp candidates for novel objects in RGB-D images. Com- pared to previous works, our architecture is able to predict multiple candidate grasps instead of single outcome, which shows promise to aid a subsequent grasp planning process. Our regression as classification approach transforms orientation regression to a classification task, which takes advantage of the high classification performance of CNNs for improved grasp detection outcomes. We evaluated our system on the Cornell grasping dataset for comparison with state-of-the-art system using a common performance metric and methodology to show the effectiveness of our design. We also performed experiments on self-collected multi-object dataset for multiobject multi-grasp scenario. Acceptable grasp detection rates are achieved for the case of 1 false grasp per image. Physical grasping experiments show a small performance loss (8.3%) when physically grasping the object based on correct candidate grasps found. The outcomes might be improved by fusing the multi-grasp output with object-specific grasp priors, which we leave to future work. All code and data will be publicly released.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Simultaneous multi-object, multi-grasp detection by the proposed model. Training used the Cornell dataset with the standard object-wise split. The red lines correspond to parallel plates of the grasping gripper. The white lines indicate the distance between the plates before the grasp is executed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) The 5D grasp representation. (b) A grasp rectangle is first set to the zero orientation for grasp proposal training. The angle θ is one of the discrete rotation angles. (c) Each element in the feature map is an anchor and corresponds to multiple candidate grasp proposal bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Complete structure of our multi-object multi-grasp predictor. The network takes RG-D inputs, and predicts multiple grasps candidates with orientations and rectangle bounding boxes for each object in the view. Blue blocks indicate network layers and gray blocks indicate images and feature maps. Green blocks show the two loss functions. The grasp proposal network slides across anchors of intermediate feature maps from ResNet-50 with k = 3 × 3 candidates predicted per anchor. The black lines of output bounding boxes denote the open length of a two-fingered gripper, while the red lines denote the parallel plates of the gripper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>CHUFig. 5 .</head><label>5</label><figDesc>et al.: REAL-WORLD MULTI-OBJECT, MULTI-GRASP DETECTION 7 Detection results of the system: (a) The ROC curves of our system on single-object multi-grasp scenario and multi-object multi-grasp scenario, respectively. The model was trained on Cornell Dataset and tested on our own multi-object dataset. (b) Detection results of our system on multi-object multi-grasp scenario. The model was trained on Cornell Dataset and tested on our own multi-object dataset. Red rectangle represents the predicted grasp on each unseen object. (c) Experiment setting for physical grasping test. The manipulator is a 7 degree of freedom redundant robotic arm. The vision device is META-1 AR glasses with time-of-flight for RGB-D input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received: February 24, 2018; Revised May 15, 2018; Accepted June 14, 2018. This paper was recommended for publication by Editor Han Ding upon evaluation of the Associate Editor and Reviewers' comments. This work was supported in part by NSF Award #1605228. Fu-Jen Chu, Ruinian Xu and Patricio A. Vela are with Institute for Robotics and Intelligent Machines, Georgia Institute of Technology, GA, USA.</figDesc><table /><note>{fujenchu, gtsimonxu, pvela}@gatech.edu Digital Object Identifier (DOI): see top of this page.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="2">SINGLE-OBJECT SINGLE-GRASP EVALUATION</cell></row><row><cell>approach</cell><cell cols="2">image-wise object-wise speed</cell></row><row><cell></cell><cell>Prediction Accuracy (%)</cell><cell>fps</cell></row><row><cell>Jiang et al.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell>PHYSICAL GRASPING COMPARISON</cell></row><row><cell>approach</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PHYSICAL</head><label>IV</label><figDesc>GRASPING EVALUATION ON SAME ROBOT AND OJBECTS</figDesc><table><row><cell>approach</cell><cell>Cornell splits</cell><cell cols="2">Physical grasp</cell></row><row><cell></cell><cell cols="3">image / object detected physical</cell></row><row><cell>Kumra et al. [21]</cell><cell>88.7 / 86.5</cell><cell>61/100</cell><cell>56/100</cell></row><row><cell>Guo et al. [23]</cell><cell>93.8 / 89.9</cell><cell>89/100</cell><cell>81/100</cell></row><row><cell>Ours (Top-1)</cell><cell>96.0 / 96.1</cell><cell>95/100</cell><cell>86/100</cell></row><row><cell>Ours (center)</cell><cell>96.0 / 96.1</cell><cell>96/100</cell><cell>89/100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Citation information: DOI 10.1109/LRA.2018.2852777, IEEE Robotics 2377-3766 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications standards/publications/rights/index.html for more information.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">PHYSICAL GRASPING COMPARISON</cell><cell></cell></row><row><cell>approach</cell><cell cols="2">Time (s)</cell><cell cols="2">Settings</cell><cell>Success (%)</cell></row><row><cell></cell><cell cols="4">detect plan object trial</cell><cell></cell></row><row><cell>[18]</cell><cell>13.50</cell><cell>-</cell><cell>30</cell><cell>100</cell><cell>84 / 89*</cell></row><row><cell>[22]</cell><cell>1.80</cell><cell>-</cell><cell>10</cell><cell>-</cell><cell>62</cell></row><row><cell>[27]</cell><cell>-</cell><cell>-</cell><cell>15</cell><cell>150</cell><cell>66</cell></row><row><cell>[26]</cell><cell>-</cell><cell>2∼3</cell><cell>10</cell><cell>-</cell><cell>84</cell></row><row><cell>[25]</cell><cell>0.80</cell><cell>-</cell><cell>10</cell><cell>50</cell><cell>80</cell></row><row><cell>[25]+refits</cell><cell>2.50</cell><cell>-</cell><cell>40</cell><cell>100</cell><cell>94</cell></row><row><cell>Ours</cell><cell>0.12</cell><cell>0.10</cell><cell>10</cell><cell>100</cell><cell>89.0</cell></row></table><note>* Outcomes are for Baxter / PR2 robots, respectively, with the diffence arising from the different gripper spans.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell cols="3">ABLATION STUDY</cell><cell></cell></row><row><cell>Architecture</cell><cell cols="3">Cornell Splits image object Parameters Number of</cell></row><row><cell>(a) RGB</cell><cell>86.4</cell><cell>85.4</cell><cell>24559685</cell></row><row><cell>(b) RGB + depth</cell><cell>88.7</cell><cell>86.5</cell><cell>51738757</cell></row><row><cell>(c) RGD</cell><cell>88.1</cell><cell>86.0</cell><cell>24559685</cell></row><row><cell>(d) RGD + cls*</cell><cell>89.8</cell><cell>89.3</cell><cell>24568919</cell></row><row><cell>(e) RGB + cls + gp</cell><cell>94.4</cell><cell>95.5</cell><cell>28184211</cell></row><row><cell>(f) RGD + cls + gp</cell><cell>96.0</cell><cell>96.1</cell><cell>28184211</cell></row></table><note>* cls: classification; gp: grasp proposal</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning for RGB-D based object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
	<note>in Experimental Robotics, 2013</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cornell grasping dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lab</surname></persName>
		</author>
		<ptr target="http://pr.cs.cornell.edu/grasping/rectdata/data.php" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2017" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robot grasp synthesis algorithms: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Shimoga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="266" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robotic grasping and contact: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bicchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An overview of 3D object grasp synthesis algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sahbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>El-Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bidaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="336" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data-driven grasp synthesisa survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="309" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to grasp using visual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Flash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2470" to="2476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robotic grasping of novel objects using vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Driemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RGB-D object recognition and grasp detection using hierarchical cascaded forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Robotics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning and evaluation of the approach vector for automatic grasp generation and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ekvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4715" to="4720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Selection of robot pre-grasps using boxbased shape approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1765" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to grasp objects with multiple contact points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="5062" to="5069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient grasping from RGBD images: Learning using a new rectangle representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moseson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Grasping novel objects with depth segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Phoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Quigley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sudsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2578" to="2585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning for detecting robotic grasps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="705" to="724" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robot grasp detection using multimodal deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Mechanical Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1687814016668077</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time grasp detection using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1316" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robotic grasp detection using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-world, real-time robotic grasping with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Towards Autonomous Robotic Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="617" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hybrid deep architecture for robotic grasp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1609" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning a grasp function for grasping under gripper pose uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Intelligent Robotic and Systems</title>
		<meeting>the IEEE International Conference on Intelligent Robotic and Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4461" to="4468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dex-Net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aparicio-Ojea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Planning multifingered grasps as probabilistic inference in a learned deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chenna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hermans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Robotics Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Robotics and Automation</title>
		<meeting>IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3406" to="3413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning handeye coordination for robotic grasping with large-scale data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
