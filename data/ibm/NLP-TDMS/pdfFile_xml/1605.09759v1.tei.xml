<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Zero-Shot Image Tagging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
							<email>yangzhang@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<postCode>32816</postCode>
									<settlement>Orlando</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
							<email>bgong@crcv.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<postCode>32816</postCode>
									<settlement>Orlando</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@crcv.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<postCode>32816</postCode>
									<settlement>Orlando</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Zero-Shot Image Tagging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The well-known word analogy experiments show that the recent word vectors capture fine-grained linguistic regularities in words by linear vector offsets, but it is unclear how well the simple vector offsets can encode visual regularities over words. We study a particular image-word relevance relation in this paper. Our results show that the word vectors of relevant tags for a given image rank ahead of the irrelevant tags, along a principal direction in the word vector space. Inspired by this observation, we propose to solve image tagging by estimating the principal direction for an image. Particularly, we exploit linear mappings and nonlinear deep neural networks to approximate the principal direction from an input image. We arrive at a quite versatile tagging model. It runs fast given a test image, in constant time w.r.t. the training set size. It not only gives superior performance for the conventional tagging task on the NUS-WIDE dataset, but also outperforms competitive baselines on annotating images with previously unseen tags.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in the vector-space representations of words <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47]</ref> have benefited both NLP <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b53">54]</ref> and computer vision tasks such as zeros-shot learning <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref> and image captioning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. The use of word vectors in NLP is grounded on the fact that the fine-grained linguistic regularities over words are captured by linear word vector offsets-a key observation from the well-known word analogy experiments <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47]</ref>, such as the syntactic relation dance − dancing ≈ f ly − f lying and semantic relation king − man ≈ queen − woman. However, it is unclear whether the visual regularities over words, which are implicitly used in the aforementioned computer vision problems, can still be encoded by the simple vector offsets.</p><p>In this paper, we are interested in the problem of image tagging, where an image (e.g., of a zoo in <ref type="figure">Figure 1</ref>) calls for a partition of a vocabulary of words into two disjoint sets according to the image-word relevance (e.g., relevant tags Y = {people, animal, zoo} and irrelevant <ref type="figure">Figure 1</ref>: Given an image, its relevant tags' word vectors rank ahead of the irrelevant tags' along some direction in the word vector space. We call that direction the principal direction for the image. To solve the problem of image tagging, we thus learn a function f (·) to approximate the principal direction from an image. This function takes as the input an image x m and outputs a vector f (x m ) for defining the principal direction in the word vector space. ones Y = {sailor, book, landscape}). This partitioning of words, (Y, Y ), is essentially different from the fine-grained syntactic (e.g., dance to dancing) or semantic (e.g., king to man) relation tested in the word analogy experiments. Instead, it is about the relationship between two sets of words due to a visual image. Such a relation in words is semantic and descriptive, and focuses on visual association, albeit relatively coarser. In this case, do the word vectors still offer the nice property, that the simple linear vector offsets can depict the visual (image) association relations in words? For the example of the zoo, while humans are capable of easily answering that the words in Y are more related to the zoo than those in Y , can such zoo-association relation in words be expressed by the 9 pairwise word vector offsets {people − sailor, people − book, · · · , zoo − landscape} between the relevant Y and irrelevant Y tags' vectors?</p><p>One of the main contributions of this paper is to empirically examine the above two questions (cf. Section 3). Every image introduces a visual association rule (Y, Y ) over words. Thanks to the large number of images in benchmark datasets for image tagging, we are able to examine many distinct visual association regulations in words and the corresponding vector offsets in the word vector space. Our results reveal a somehow surprising connection between the two: the offsets between the vectors of the relevant tags Y and those of the irrelevant Y are along about the same direction, which we call the principal direction. See <ref type="figure" target="#fig_0">Figure 2</ref> for the visualization of some vector offsets. In other words, there exists at least one vector (direction) w in the word vector space, such that its inner products with the vector offsets between Y and Y are greater than 0, i.e., ∀p ∈ Y , ∀n ∈ Y , w, p − n &gt; 0 equivalently, w, p &gt; w, n ,</p><p>where the latter reads that the vector w ranks all relevant words Y (e.g., for the zoo image) ahead of the irrelevant ones Y . For brevity, we overload the notations Y and Y to respectively denote the vectors of the words in them. The visual association relations in words thus represent themselves by the (linear) rank-abilities of the corresponding word vectors. This result reinforces the conclusion from the word analogy experiments that, for a single word multiple relations are embedded in the high dimensional space <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47]</ref>. Furthermore, those relations can be expressed by simple linear vector arithmetic.</p><p>Inspired by the above observation, we propose to solve the image tagging problem by estimating the principal direction, along which the relevant tags rank ahead of the irrelevant ones in the word vector space. Particularly, we exploit linear mappings and deep neural networks to approximate the principal direction from each input image. This is a grand new point of view to image tagging and results in a quite versatile tagging model. It operates fast given a test image, in constant time with respect to the training set size. It not only gives superior performance for the conventional tagging task, but is also capable of assigning novel tags from an open vocabulary, which are unseen at the training stage. We do not assume any a priori knowledge about these unseen tags as long as they are in the same vector space as the seen tags for training. To this end, we name our approach fast zero-shot image tagging (Fast0Tag) to recognize that it possesses the advantages of both FastTag <ref type="bibr" target="#b7">[8]</ref> and zero-shot learning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>In sharp contrast to our approach, previous image tagging methods can only annotate test images with the tags seen at training except <ref type="bibr" target="#b18">[19]</ref>, to the best of our knowledge. Limited by the static and usually small number of seen tags in the training data, these models are frequently challenged in practice. For instance, there are about 53M tags on Flickr and the number is rapidly growing. The work of <ref type="bibr" target="#b18">[19]</ref> is perhaps the first attempt to generalize an image tagging model to unseen tags. Compared to the proposed method, it depends on two extra assumptions. One is that the unseen tags are known a priori in order to tune the model towards their combinations. The other is that the test images are known a priori, to regularize the model. Furthermore, the generalization of <ref type="bibr" target="#b18">[19]</ref> is limited to a very small number, U, of unseen tags, as it has to consider all the 2 U possible combinations.</p><p>To summarize, our first main contribution is on the analyses of the visual association relations in words due to images, and how they are captured by word vector offsets. We hypothesize and empirically verify that, for each visual association rule (Y, Y ), in the word vector space there exists a principal direction, along which the relevant words' vectors rank ahead of the others'. Built upon this finding, the second contribution is a novel image tagging model, Fast0Tag, which is fast and generalizes to open-vocabulary unseen tags. Last but not least, we explore three different image tagging scenarios: conventional tagging which assigns seen tags to images, zero-shot tagging which annotates images by (a large number of) unseen tags, and seen/unseen tagging which tags images with both seen and unseen tags. In contrast, the existing work tackles either conventional tagging, or zero-shot tagging with very few unseen tags. Our Fast0Tag gives superior results over competitive baselines under all the three testing scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image tagging. Image tagging aims to assign relevant tags to an image or to return a ranking list of tags. In the literature this problem has been mainly approached from the tag ranking perspective. In the generative methods, which involve topic models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b43">44]</ref> and mixture models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>, the candidate tags are naturally ranked according to their probabilities conditioned on the test image. For the non-parametric nearest neighbor based methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b60">61]</ref>, the tags for the test image are often ranked by the votes from some training images. The nearest neighbor based algorithms, in general, outperform those depending on generative models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>, but suffer from high computation costs in both training and testing. The recent FastTag algorithm <ref type="bibr" target="#b7">[8]</ref> is magnitude faster and achieves comparable results with the nearest neighbor based methods. Our Fast0Tag shares the same level of low complexity as FastTag. The embedding method <ref type="bibr" target="#b56">[57]</ref> assigns ranking scores to the tags by a crossmodality mapping between images and tags. This idea is further exploited using deep neural networks <ref type="bibr" target="#b19">[20]</ref>. Interestingly, none of these methods learn their models explicitly for the ranking purpose except <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b19">20]</ref>, although they all rank the candidate tags for the test images. Thus, there exists a mismatch between the models learned and the actual usage of the models, violating the principle of Occam's razor. We use a ranking loss in the same spirit as <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>In contrast to our Fast0Tag, which can rank both seen and an arbitrary number of unseen tags for test images, the aforementioned approaches only assign tags to images from a closed vocabulary seen at the training stage. An exception is by Fu et al. <ref type="bibr" target="#b17">[18]</ref>, where the authors consider pre-fixed U unseen tags and learn a multi-label model to account for all the 2 U possible combinations of them. This method is limited to a small number U of unseen tags.</p><p>Word embedding. Instead of representing words using the traditional one-hot vectors, word embedding maps each word to a continuous-valued vector, by learning from primarily the statistics of word co-occurrences. Although there are earlier works on word embedding <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b11">12]</ref>, we point out that our work focuses on the most recent GloVe <ref type="bibr" target="#b46">[47]</ref> and word2vec vectors <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39]</ref>. As shown in the well-known word analogy experiments <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47]</ref>, both types of word vectors are able to capture fine-grained semantic and syntactic regularities using vector offsets. In this paper, we further show that the simple linear offsets also depict the relatively coarser visual association relations in words.</p><p>Zero-shot learning. Zero-shot learning is often used exchange-ably with zero-shot classification, whereas the latter is a special case of the former. Unlike weaklysupervised learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b16">17]</ref> which learn new concepts by mining noisy new samples, zero-shot classification learns classifiers from seen classes and aims to classify the objects of unseen classes <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref>. Attributes <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14]</ref> and word vectors are two of the main semantic sources making zero-shot classification feasible.</p><p>Our Fast0Tag along with <ref type="bibr" target="#b18">[19]</ref> enriches the family of zero-shot learning by zero-shot multi-label classification <ref type="bibr" target="#b54">[55]</ref>. Fu et al. <ref type="bibr" target="#b18">[19]</ref> reduce the problem to zero-shot classification by treating every combination of the multiple labels as a class. We instead directly model the labels and are able to assign/rank many unseen tags for an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The linear rank-ability of word vectors</head><p>Our Fast0Tag approach benefits from the finding that the visual association relation in words, i.e., the partition of a vocabulary of words according to their relevances to an image, expresses itself in the word vector space as the existence of a principal direction, along which the words/tags relevant to the image rank ahead of the irrelevant ones. This section details the finding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The regulation over words due to image tagging</head><p>We use S to denote the seen tags available for training image tagging models and U the tags unseen at the training stage. The training data are in the form of {(x m , Y m ); m = 1, 2, · · · , M}, where x m ∈ R D is the feature representation of image m and Y m ⊂ S are the seen tags relevant to that image. For brevity, we overload the notation Y m to also denote the collection of the corresponding word/tag vectors.</p><p>The conventional image tagging aims to assign seen tags in S to the test images. The zero-shot tagging, formalized in <ref type="bibr" target="#b18">[19]</ref>, tries to annotate test images using a pre-fixed set of unseen tags U. In addition to those two scenarios, this paper considers seen/unseen image tagging, which finds both relevant seen tags from S and relevant unseen tags from U for the test images. Furthermore, the set of unseen tags U could be open and dynamically growing.</p><p>Denote by Y m := S \Y m the irrelevant seen tags. An image m introduces a visual association regulation to wordsthe partition (Y m , Y m ) of the seen tags to two disjoint sets. Noting that many fine-grained syntactic and semantic regulations over words can be expressed by linear word vector offsets, we next examine what properties the vector offsets could offer for this new visual association rule.  <ref type="bibr" target="#b55">[56]</ref> and PCA for two visual association rules over words. One is imposed by an image with 5 relevant tags and the other is with 15 relevant tags. We observe two main structures from the vector offsets:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Principal direction and cluster structure</head><formula xml:id="formula_1">(p − n), ∀p ∈ Y m , ∀n ∈ Y m using t-SNE</formula><p>Principal direction. Mostly, the vector offsets point to about the same direction (relative to the origin), which we call the principal direction, for a given visual association rule (Y m , Y m ) in words for image m. This implies that the relevant tags Y m rank ahead of the irrelevant ones Y m along the principal direction (cf. eq. (1)).</p><p>Cluster structure. There exist cluster structures in the vector offsets for each visual association regulation over the words. Moreover, all the offsets pointing to the same relevant tag in Y m fall into the same cluster. We differentiate the offsets pointing to different relevant tags by colors in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Can the above two observations generalize? Namely, do they still hold in the high-dimensional word vector space for more visual association rules imposed by other images?</p><p>To answer the questions, we next design an experiment to verify the existence of the principal directions in word vector spaces, or equivalently the linear rank-ability of word vectors. We leave the cluster structure for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Testing the linear rank-ability hypothesis</head><p>Our experiments in this section are conducted on the validation set (26,844 images, 925 seen tags S, and 81 unseen tags U) of NUS-WIDE <ref type="bibr" target="#b8">[9]</ref>. The number of relevant seen/unseen tags associated with an image ranges from 1 to 20/117 and on average is 1.7/4.9. See Section 5 for details.</p><p>Our objective is to investigate, for any visual association rule (Y m , Y m ) in words by image m, the existence of the principal direction along which the relevant tags Y m rank ahead of the irrelevant tags Y m . The proof completes once we find a vector w in the word vector space that satisfies the ranking constraints w, p &gt; w, n , ∀p ∈ Y m , ∀n ∈ Y m .  <ref type="figure" target="#fig_0">Figure 2</ref>: Visualization of the offsets between relevant tags' word vectors and irrelevant ones'. Note that each vector from the origin to a point is an offset between two word vectors. The relevant tags are shown beside the images <ref type="bibr" target="#b8">[9]</ref>.</p><p>To this end, we train a linear ranking SVM <ref type="bibr" target="#b25">[26]</ref> for each visual association rule using all the corresponding pairs (p, n), then rank the word vectors by the SVM, and finally examine how many constraints are violated. In particular, we employ MiAP, the larger the better (cf. Section 5), to compare the SVM's ranking list with those ranking constraints. We repeat the above process for all the validation images, resulting in 21,863 unique visual association rules.</p><p>Implementation of ranking SVM. In this paper, we use the implementation of solving ranking SVM in the primal <ref type="bibr" target="#b6">[7]</ref> with the following formulation:</p><formula xml:id="formula_2">min w λ 2 w 2 + yi∈Ym yj ∈Ym max(0, 1 − wy i + wy j )</formula><p>where λ is the hyper-parameter controlling the trade-off between the objective and the regularization.  Results. The MiAP results averaged over all the distinct regulations are reported in <ref type="figure" target="#fig_1">Figure 3</ref>(left), in which we test the 300D GloVe vectors <ref type="bibr" target="#b46">[47]</ref> and word2vec <ref type="bibr" target="#b40">[41]</ref> of dimensions 100, 300, 500, and 1000. The horizontal axis shows different regularizations we use for training the ranking SVMs. Larger λ regularizes the models more. In the 300D GloVe space and the word2vec spaces of 300, 500, and 1000 dimensions, more than two ranking SVMs, with small λ values, give rise to nearly perfect ranking results (MiAP ≈ 1), showing that the seen tags S are linearly rank-able under almost every visual association rule-all the ranking constraints imposed by the relevant Y m and irrelevant Y m tags to image m are satisfied. However, we shall be cautious before drawing any conclusions beyond the experimental vocabulary S of seen tags. An image m incurs a visual association rule essentially over all words, though the same rule implies different partitions of distinct experimental vocabularies (e.g., the seen tags S and unseen ones U). Accordingly, we would expect the principal direction for the seen tags is also shared by the unseen tags under the same rule, if the answer is YES to the questions at the end of Section 3.2.</p><p>Generalization to unseen tags. We test whether the same principal direction exists for the seen tags and unseen ones under every visual association rule induced by an image. This can be (only partially) justified by applying the ranking SVMs previously learned, to the unseen tags' vectors, because we do not know the "true" principal directions. We consider the with 81 unseen tags U as the "test data" for the trained ranking SVMs, each due to an image incurred visual association. NUS-WIDE provides the annotations of the 81 tags for the images. The results, shown in <ref type="figure" target="#fig_1">Figure 3</ref>(right), are significantly better than the most basic baseline, randomly ranking the tags (the black curve close to the origin), demonstrating that the directions output by SVMs are generalizable to the new vocabulary U of words.</p><p>Observation. Therefore, we conclude that the word vectors are an efficient media to transfer knowledge-the rankability along the principal direction-from the seen tags to the unseen ones. We have empirically verified that the visual association rule (Y m , Y m ) in words due to an image m can be represented by the linear rank-ability of the corresponding word vectors along a principal direction. Our experiments involve |S| + |U| = 1,006 words in total. Largerscale and theoretical studies are required for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approximating the linear ranking functions</head><p>This section presents our Fast0Tag approach to image tagging. We first describe how to solve image tagging by approximating the principal directions thanks to their existence and generalization, empirically verified in the last section. We then describe detailed approximation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image tagging by ranking</head><p>Grounded on the observation from Section 3, that there exists a principal direction w m , in the word vector space, for every visual association rule (Y m , Y m ) in words by an image m, we propose a straightforward solution to image tagging. The main idea is to approximate the principal direction by learning a mapping function f (·), between the visual space and the word vector space, such that</p><formula xml:id="formula_3">f (x m ) ≈ w m ,<label>(2)</label></formula><p>where x m is the visual feature representation of the image m. Therefore, given a test image x, we can immediately suggest a list of tags by ranking the word vectors of the tags along the direction f (x), namely, by the ranking scores,</p><formula xml:id="formula_4">f (x), t , ∀t ∈ S ∪ U<label>(3)</label></formula><p>no matter the tags are from the seen set S or unseen set U. We explore both linear and nonlinear neural networks for implementing the approximation function f (x) ≈ w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Approximation by linear regression</head><p>Here we assume a linear function from the input image representation x to the output principal direction w, i.e.,</p><formula xml:id="formula_5">f (x) := Ax,<label>(4)</label></formula><p>where A can be solved in a closed form by linear regression. Accordingly, we have the following from the training</p><formula xml:id="formula_6">w m = Ax m + m , m = 1, 2, · · · , M<label>(5)</label></formula><p>where w m is the principal direction of all offset vectors of the seen tags, for the visual association rule (Y m , Y m ) due to the image m, and m are the errors. Minimizing the mean squared errors gives us a closed form solution to A.</p><p>One caveat is that we do not know the exact principal directions w m at all-the training data only offer images x m and the relevant tags Y m . Here we take the easy alternative and use the directions found by ranking SVMs (cf. Section 3) in eq. (5). There are thus two stages involved to Discussion. We note that the the linear transformation between visual and word vector spaces has been employed before, e.g., for zero-shot classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref> and image annotation/classification <ref type="bibr" target="#b57">[58]</ref>. This work differs from them with a prominent feature, that the mapped image f (x) = Ax has a clear meaning; it depicts the principal direction, which has been empirically verified, for the tags to be assigned to the image. We next extend the linear transformation to a nonlinear one, through a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Approximation by neural networks</head><p>We also exploit a nonlinear mapping f (x; θ) by a multilayer neural network, where θ denotes the network parameters. <ref type="figure" target="#fig_2">Figure 4</ref> shows the network architecture. It consists of two RELU layers followed by a linear layer to output the approximated principal direction, w, for an input image x. We expect the nonlinear mapping function f (x; θ) to offer better modeling flexibility than the linear one.</p><p>Can we still train the neural network by regressing to the M directions obtained from ranking SVMs? Both our intuition and experiments tell that this is a bad idea. The number M of training instances is small relative to the number of parameters in the network, making it hard to avoid overfitting. Furthermore, the directions by ranking SVMs are not the true principal directions anyway. There is no reason for us to stick to the ranking SVMs for the principal directions.</p><p>We instead unify the two stages in Practical considerations. We use Theano <ref type="bibr" target="#b3">[4]</ref> to solve the optimization problem. A mini-batch consists of 1,000 images, each of which incurs on average 4,600 pairwise ranking constraints of the tags-we use all pairwise ranking constraints in the optimization. The normalization ω m for the per-image ranking loss suppresses the violations from the images with many positive tags. This is desirable since the numbers of relevant tags of the images are unbalanced, ranging from 1 to 20. Without the normalization the MiAP results drop by about 2% in our experiments. For regularization, we use early stopping and a dropout layer <ref type="bibr" target="#b22">[23]</ref> with the drop rate of 30%. The optimization hyper-parameters are selected by the validation set (cf. Section 5).</p><p>In addition to the RankNet loss <ref type="bibr" target="#b4">[5]</ref> in eq. (6), we have also experimented some other choices for the perimage loss, including the hinge loss <ref type="bibr" target="#b9">[10]</ref>, Crammer-Singer loss <ref type="bibr" target="#b10">[11]</ref>, and pairwise max-out ranking <ref type="bibr" target="#b25">[26]</ref>. The hinge loss performs the worst, likely because it is essentially not designed for ranking problems, though one can still understand it as a point-wise ranking loss. The Crammer-Singer, pairwise max-out, and RankNet are all pair-wise ranking loss functions. They give rise to comparable results and RankNet outperforms the other two by about 2% in terms of MiAP. This may attribute to the ease of control over the optimization process for RankNet. Finally, we note that the list-wise ranking loss <ref type="bibr" target="#b58">[59]</ref> can also be employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on NUS-WIDE</head><p>This section presents our experimental results. We contrast our approach to several competitive baselines for the conventional image tagging task on the large-scale NUS-WIDE <ref type="bibr" target="#b8">[9]</ref> dataset. Moreover, we also evaluate our method on the zero-shot and seen/unseen image tagging problems (cf. Section 3.1). For the comparison on these problems, we extend some existing zero-shot classification algorithms and consider some variations of our own approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset and configuration</head><p>NUS-WIDE. We mainly use the NUS-WIDE dataset <ref type="bibr" target="#b8">[9]</ref> for the experiments in this section. NUS-WIDE is a standard benchmark dataset for image tagging. It contains 269,648 images in the original release and we are able to retrieve 223,821 of them since some images are either corrupted or removed from Flickr. We follow the recommended experiment protocol to split the dataset into a training set with 134,281 images and a test set with 89,603 images. We further randomly separate 20% from the training set as our validation set for 1) tuning hyper-parameters in our method and the baselines and 2) conducting the empirical analyses in Section 3. Annotations of NUS-WIDE. NUS-WIDE releases three sets of tags associated with the images. The first set comprises of 81 "groundtruth" tags. They are carefully chosen to be representative of the Flickr tags, such as containing both general terms (e.g., animal) and specific ones (e.g., dog and f lower), corresponding to frequent tags on Flickr, etc. Moreover, they are annotated by high-school and college students and are much less noisy than those directly collected from the Web. This 81-tag set is usually taken as the groundtruth for benchmarking different image tagging methods. The second and the third sets of annotations are both harvested from Flickr. There are 1,000 popular Flickr tags in the second set and nearly 5,000 raw tags in the third. Image features and word vectors. We extract and 2 normalize the image feature representations of VGG-19 <ref type="bibr" target="#b49">[50]</ref>. Both GloVe <ref type="bibr" target="#b46">[47]</ref> and Word2vec <ref type="bibr" target="#b40">[41]</ref> word vectors are included in our empirical analysis experiments in Section 3 and the 300D GloVe vectors are used for the remaining experiments. We also 2 normalize the word vectors. Evaluation. We evaluate the tagging results of different methods using two types of metrics. One is the mean image average precision (MiAP), which takes the whole ranking list into account. The other consists of the precision, recall, and F-1 score for the top K tags in the list. We report the results for K = 3 and K = 5. Both metrics are commonly used in the previous works on image tagging. We refer the readers to Section 3.3 of <ref type="bibr" target="#b35">[36]</ref> for how to calculate MiAP and to Section 4.2 of <ref type="bibr" target="#b19">[20]</ref> for the top-K precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Conventional image tagging</head><p>Here we report the experiments on the conventional tagging. The 81 concepts with "groundtruth" annotations in NUS-WIDE are used to benchmark different methods.</p><p>Baselines. We include TagProp <ref type="bibr" target="#b21">[22]</ref> as the first competitive baseline. It is representative among the nearest neighbor based methods, which in general outperform the parametric methods built from generative models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, and gives rise to state-of-the-art results in the experimental study <ref type="bibr" target="#b35">[36]</ref>. We further compare with two most recent </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Zero-shot and Seen/Unseen image tagging</head><p>This section presents some results for the two novel image tagging scenarios, zero-shot and seen/unseen tagging.</p><p>Fu et al. <ref type="bibr" target="#b18">[19]</ref> formalized the zero-shot image tagging problem, aiming to annotate test images using a pre-fixed set U of unseen tags. Our Fast0Tag naturally applies to this scenario, by simply ranking the unseen tags with eq. (3). Furthermore, this paper also considers seen/unseen image tagging which finds both relevant seen tags from S and relevant unseen tags from U for the test images. The set of unseen tags U could be open and dynamically growing.</p><p>In our experiments, we treat the 81 concepts with highquality user annotations in NUS-WIDE as the unseen set U for evaluation and comparison. We use the remaining 925 out of the 1000 frequent Flickr tags to form the seen set S-75 tags are shared by the original 81 and 1,000 tags.</p><p>Baselines. Our Fast0Tag models can be readily applied to the zero-shot and seen/unseen image tagging scenarios. For comparison we study the following baselines. Seen2Unseen. We first propose a simple method which extends an arbitrary traditional image tagging method to also working with previously unseen tags. It originates from our analysis experiment in Section 3. First, we use any existing method to rank the seen tags for a test image. Second, we train a ranking SVM in the word vector space using the ranking list of the seen tags. Third, we rank unseen (and seen) tags using the learned SVM for zero-shot (and seen/unseen) tagging. LabelEM. The label embedding method <ref type="bibr" target="#b1">[2]</ref> achieves impressive results on zero-shot classification for finegrained object recognition. If we consider each tag of S ∪ U as a unique class, though this implies that some classes will have duplicated images, the LabelEM can be directly applied to the two new tagging scenarios. LabelEM+. We also modify the objective loss function of LabelEM when we train the model, by carefully removing the terms that involve duplicated images. This slightly improves the performance of LabelEM. ConSE. Again by considering each tag as a class, we include a recent zero-shot classification method, ConSE <ref type="bibr" target="#b44">[45]</ref> in the following experiments. Note that it is computationally infeasible to compare with <ref type="bibr" target="#b18">[19]</ref>, which might be the first work to our knowledge on expanding image tagging to handle unseen tags, because it considers all the possible combinations of the unseen tags. Results. <ref type="table" target="#tab_7">Table 5</ref> summarizes the results of the baselines and Fast0Tag when they are applied to the zero-shot andseen/unseen image tagging tasks. Overall, Fast0Tag, with either linear or neural network mapping, performs the best.</p><p>Additionally, in the table we add two special rows whose results are mainly for reference. The Random row corresponds to the case when we return a random list of tags in U for zero-shot tagging (and in U ∪ S for seen/unseen tagging) to each test image. We compare this row with the row of Seen2Unseen, in which we extend TagProp to handle the unseen tags. We can see that the results of Unseen2Seen are significantly better than randomly ranking the tags. This tells us that the simple Seen2Unseen is effective in expanding the labeling space of traditional image tagging methods. Some tag completion methods <ref type="bibr" target="#b48">[49]</ref> may also be employed for the same purpose as Seen2Unseen.</p><p>Another special row in <ref type="table" target="#tab_7">Table 5</ref> is the last one with RankSVM for zero-shot image tagging. We obtain its results through the following steps. Given a test image, we assume the annotation of the seen tags, S, are known and then learn a ranking SVM with the default regularization λ = 1. The learned SVM is then used to rank the unseen  tags for this image. One may wonder that the results of this row should thus be the upper bound of our Fast0Tag implemented based on linear regression, because the ranking SVM models are the targets of the linear regresson. However, the results show that they are not. This is not surprising, but rather it reinforces our previous statement that the learned ranking SVMs are not the "true" principal directions. The Fast0Tag implemented by the neural network is an effective alternative for seeking the principal directions. It would also be interesting to compare the results in Table 5 (zero-shot image tagging) with those in <ref type="table" target="#tab_6">Table 4</ref> (conventional tagging), because the experiments for the two tables share the same testing images and the same candidate tags; they only differ in which tags are used for training. We can see that the Fast0Tag (net.) results of the zero-shot tagging in <ref type="table" target="#tab_7">Table 5</ref> are actually comparable to the conventional tagging results in <ref type="table" target="#tab_6">Table 4</ref>, particularly about the same as FastTag's. These results are encouraging, indicating that it is unnecessary to use all the candidate tags for training in order to have high-quality tagging performance.</p><p>Annotating images with 4,093 unseen tags. What happens when we have a large number of unseen tags showing up at the test stage? NUS-WIDE provides noisy annotations for the images with over 5,000 Flickr tags. Excluding the 925 seen tags that are used to train models, there are 4,093 remaining unseen tags. We use the Fast0Tag models to rank all the unseen tags for the test images and the results are shown in <ref type="table" target="#tab_4">Table 3</ref>. Noting that the noisy annotations weaken the credibility of the evaluation process, the results are reasonably low but significantly higher than the random lists.</p><p>Qualitative results. <ref type="figure">Figure 6</ref> shows the top five tags for some exemplar images <ref type="bibr" target="#b8">[9]</ref>, returned by Fast0Tag under the conventional, zero-shot, and seen/unseen image tagging scenarios. <ref type="bibr">Those</ref>   ging are shown on the rightmost. The tags in green color appear in the groundtruth annotation; those in red color and italic font are the mistaken tags. Interestingly, Fast0Tag performs equally well for traditional and zero-shot tagging and makes even the same mistakes. More results are in Suppl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments on IAPRTC-12</head><p>We present another set of experiments conducted on the widely used IAPRTC-12 <ref type="bibr" target="#b20">[21]</ref> dataset. We use the same tag annotation and image training-test split as described in <ref type="bibr" target="#b21">[22]</ref> for our experiments.</p><p>There are 291 unique tags and 19627 images in IAPRTC-12. The dataset is split to 17341 training images and 2286 testing images. We further separate 15% from the training images as our validation set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Configuration</head><p>Just like the experiments presented in the last section, we evaluate our methods in three different tasks: conventional tagging, zero-shot tagging, and seen/unseen tagging.</p><p>Unlike NUS-WIDE where a relatively small set (81 tags) is considered as the groundtruth annotation, all the 291 tags of IAPRTC-12 are usually used in the previous work to compare different methods. We thus also use all of them conventional tagging.</p><p>As for zero-shot and seen/unseen tagging tasks, we exclude 20% from the 291 tags as unseen tags. At the end, we have 233 seen tags and 58 unseen tags.</p><p>The visual features, evaluation metrics, word vectors, and baseline methods remain the same as described in the main text. <ref type="table" target="#tab_6">Table 4</ref> and 5 show the results of all the three image tagging scenarios (conventional, zero-shot, and seen/unseen tagging). The proposed Fast0Tag still outperforms the other competitive baselines in this new IAPRTC-12 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results</head><p>A notable phenomenon, which is yet less observable on NUS-WIDE probably due to its noisier seen tags, is that the gap between LabelEM+ and LabelEM is significant. It indicates that the traditional zero-shot classification methods are not suitable for either zero-shot or seen/unseen image tagging task. Whereas we can improve the performance by tweaking LabelEM and by carefully removing the terms in its formulation involving the comparison of identical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">More qualitative results</head><p>In this section, we provide more qualitative results of different tagging methods on both the NUS-WIDE, shown in <ref type="figure">Figure 6</ref>.(a) supplementing <ref type="figure" target="#fig_4">Figure 5</ref> in main text, and the IAPRTC-12, shown in <ref type="figure">Figure 6</ref>.(b).</p><p>Due to incompletion and noise of tag groundtruth, many actually correct tag predictions are often evaluated as mistaken predictions since they mismatch with groundtruth. This phenomenon becomes especially apparent in 4k zeroshot tagging results in <ref type="figure">Figure 6</ref>.(a) where plentiful diverse tag candidates are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have systematically studied a particular visual regulation over words, the visual association rule which partitions words into two disjoint sets according to their relevances to an image, as well as how it can be captured by the vector offsets in the word vector space. Our empirical results show that, for any image, there exists a principal direction in the word vector space such that the relevant tags' vectors rank ahead of the irrelevant ones' along that direction. The experimental analyses involve 1,006 words; larger-scale and theoretical analyses are required for future work. Built upon this observation, we develop a Fast0Tag model to solve image tagging by estimating the principal directions for input images. Our approach is as efficient as FastTag <ref type="bibr" target="#b7">[8]</ref> and is capable of annotating images with a large number of previously unseen tags. Extensive experiments validate the effectiveness of our Fast0Tag approach.  <ref type="figure">Figure 6</ref>: The top five tags for exemplar images in <ref type="bibr" target="#b8">[9]</ref>(a) and <ref type="bibr" target="#b20">[21]</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>visualizes the vector offsets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The existence (left) and generalization (right) of the principal direction for each visual association rule in words induced by an image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The neural network used in our approach for implementing the mapping function f (x; θ) from the input image, which is represented by the CNN features x, to its corresponding principal direction in the word vector space.learn the linear function f (x) = Ax. The first stage trains a ranking SVM over the word vectors of seen tags for each visual association (Y m , Y m ). The second stage solves for the mapping matrix A by linear regression, in which the targets are the directions returned by the ranking SVMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Section 4.2. Recall that we desire the output of the neural network f (x m ; θ) to be the principal direction, along which all the relevant tag vectors p ∈ Y m of an image m rank ahead of the irrelevant ones n ∈ Y m . Denote by ν(p, n; θ) = f (x m ; θ), n − f (x m ; θ), p , the amount of violation to any of those ranking constraints.We minimize the following loss to train the neural network,θ ← arg min θ M m=1 ω m (x m , Y m ; θ), (6) (x m , Y m ; θ) = p∈Ym n∈Ym log (1 + exp{ν(p, n; θ)})where ω m = |Y m ||Y m | −1 normalizes the per-image RankNet loss<ref type="bibr" target="#b4">[5]</ref> (x m , Y m ; θ) by the number of ranking constraints imposed by the image m over the tags. This formulation enables the function f (x) to directly take account of the ranking constraints by relevant p and irrelevant n tags. Moreover, it can be optimized with no challenge at all by standard mini-batch gradient descent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The top five tags for exemplar images<ref type="bibr" target="#b8">[9]</ref> returned by Fast0Tag on the conventional, zero-shot, and seen/unseen image tagging tasks, and by TagProp for conventional tagging. (Correct tags: green; mistaken tags: red and italic. Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(b) returned by Fast0Tag on the conventional, zeroshot, seen/unseen and 4,093 zero-shot image tagging tasks, and by TagProp for conventional tagging. (Correct tags: green; mistaken tags: red and italic)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison results of the conventional image tagging with 81 tags on NUS-WIDE.</figDesc><table><row><cell>Method %</cell><cell>MiAP</cell><cell>P</cell><cell>K = 3 R</cell><cell>F1</cell><cell>P</cell><cell>K = 5 R</cell><cell>F1</cell></row><row><cell>CCA</cell><cell>19</cell><cell>9</cell><cell cols="2">15 11</cell><cell>7</cell><cell cols="2">20 11</cell></row><row><cell>WSABIE [58]</cell><cell>28</cell><cell cols="6">16 27 20 12 35 18</cell></row><row><cell>TagProp [22]</cell><cell>53</cell><cell cols="6">29 50 37 22 62 32</cell></row><row><cell>WARP [20]</cell><cell>48</cell><cell cols="6">27 45 34 20 57 30</cell></row><row><cell>FastTag [8]</cell><cell>41</cell><cell cols="6">23 39 29 19 54 28</cell></row><row><cell>Fast0Tag (lin.)</cell><cell>52</cell><cell cols="6">29 50 37 21 60 31</cell></row><row><cell>Fast0Tag (net.)</cell><cell>55</cell><cell cols="6">31 52 39 23 65 34</cell></row><row><cell cols="8">parametric methods, WARP [20] and FastTag [8], both of</cell></row><row><cell cols="8">which are built upon deep architectures though using dif-</cell></row><row><cell cols="8">ferent models. For a fair comparison, we use the same</cell></row><row><cell cols="8">VGG-19 features for all the methods-the code of TagProp</cell></row><row><cell cols="8">and FastTag is provided by the authors and we implement</cell></row><row><cell cols="8">WARP based on our neural network architecture. Finally,</cell></row><row><cell cols="8">we compare to WSABIE [58] and CCA, both correlating</cell></row><row><cell cols="8">images and relevant tags in a low dimensional space. All</cell></row><row><cell cols="8">the hyper-parameters (e.g., the number of nearest neighbors</cell></row><row><cell cols="8">in TagProp and early stopping for WARP) are selected using</cell></row><row><cell>the validation set.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Results. Table 4 shows the comparison results of Tag-</cell></row><row><cell cols="8">Prop, WARP, FastTag, WSABIE, CCA, and our Fast0Tag</cell></row><row><cell cols="8">models implemented respectively by the linear mapping and</cell></row><row><cell cols="8">nonlinear neural network. We can see that TagProp per-</cell></row><row><cell cols="8">forms significantly better than WARP and FastTag. How-</cell></row><row><cell cols="8">ever, TagProp's training and test complexities are very high,</cell></row><row><cell cols="8">being respectively O(M 2 ) and O(M) w.r.t. the training set</cell></row><row><cell cols="8">size M. In contrast, both WARP and FastTag are more ef-</cell></row><row><cell cols="8">ficient, with O(M) training complexity and constant test-</cell></row><row><cell cols="8">ing complexity, thanks to their parametric formulation. Our</cell></row><row><cell cols="8">Fast0Tag with linear mapping gives comparable results to</cell></row><row><cell cols="8">TagProp and Fast0Tag with the neural network outperforms</cell></row><row><cell cols="8">the other methods. Also, both implementations have as low</cell></row><row><cell cols="7">computation complexities as WARP and FastTag.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison results of the zero-shot and seen/unseen image tagging tasks with 81 unseen tags and 925 seen tags.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Zero-shot image tagging</cell><cell></cell><cell></cell><cell></cell><cell cols="5">Seen/unseen image tagging</cell><cell></cell></row><row><cell>Method %</cell><cell>MiAP</cell><cell>P</cell><cell>K = 3 R</cell><cell>F1</cell><cell>P</cell><cell>K = 5 R</cell><cell>F1</cell><cell>MiAP</cell><cell>P</cell><cell>K = 3 R</cell><cell>F1</cell><cell>P</cell><cell>K = 5 R</cell><cell>F1</cell></row><row><cell>Random</cell><cell>7.1</cell><cell>2.2</cell><cell>3.8</cell><cell>2.8</cell><cell>2.2</cell><cell>6.1</cell><cell>3.2</cell><cell>1.2</cell><cell cols="6">0.6 0.3 0.4 0.6 0.5 0.5</cell></row><row><cell>Seen2Unseen</cell><cell>16.7</cell><cell>7.3</cell><cell>12.5</cell><cell>9.2</cell><cell>7.0</cell><cell>19.7</cell><cell>10.3</cell><cell>2.8</cell><cell>2.1</cell><cell>1.1</cell><cell>1.4</cell><cell>1.9</cell><cell>1.6</cell><cell>1.8</cell></row><row><cell>LabelEM [2]</cell><cell>23.7</cell><cell>11.9</cell><cell>20.2</cell><cell>14.9</cell><cell>10.2</cell><cell>28.9</cell><cell>15.1</cell><cell>8.8</cell><cell>8.7</cell><cell>4.4</cell><cell>5.8</cell><cell>7.9</cell><cell>6.6</cell><cell>7.2</cell></row><row><cell>LabelEM+ [2]</cell><cell>24.9</cell><cell>12.5</cell><cell>21.4</cell><cell>15.8</cell><cell>10.7</cell><cell>30.4</cell><cell>15.8</cell><cell>10.2</cell><cell>11.3</cell><cell>5.7</cell><cell>7.6</cell><cell>9.6</cell><cell>8.1</cell><cell>8.8</cell></row><row><cell>ConSE [45]</cell><cell>32.4</cell><cell>17.7</cell><cell>30.1</cell><cell>22.3</cell><cell>13.7</cell><cell>38.8</cell><cell>20.2</cell><cell>12.5</cell><cell>16.7</cell><cell>8.4</cell><cell cols="4">11.2 13.5 11.3 12.3</cell></row><row><cell>Fast0Tag (lin.)</cell><cell>40.1</cell><cell>21.8</cell><cell>37.2</cell><cell>27.5</cell><cell>17.0</cell><cell>48.4</cell><cell>25.2</cell><cell>18.8</cell><cell cols="6">22.9 11.5 15.4 18.7 15.7 17.1</cell></row><row><cell>Fast0Tag (net.)</cell><cell>42.2</cell><cell>22.6</cell><cell>38.4</cell><cell>28.4</cell><cell>17.6</cell><cell>50.0</cell><cell>26.0</cell><cell>19.1</cell><cell cols="6">21.7 11.0 14.5 18.4 15.5 16.8</cell></row><row><cell>RankSVM</cell><cell cols="7">37.0 19.7 33.3 24.7 15.2 42.9 22.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Annotating images with up to 4,093 unseen tags.</figDesc><table><row><cell>Method %</cell><cell>MiAP</cell><cell>P</cell><cell>K = 3 R</cell><cell>F1</cell><cell>P</cell><cell>K = 5 R</cell><cell>F1</cell></row><row><cell>Random</cell><cell>0.3</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Fast0Tag (lin.)</cell><cell>9.8</cell><cell>9.4</cell><cell>7.2</cell><cell>8.2</cell><cell>7.4</cell><cell>9.5</cell><cell>8.4</cell></row><row><cell>Fast0Tag (net.)</cell><cell>8.5</cell><cell>8.0</cell><cell>6.2</cell><cell>7.0</cell><cell>6.5</cell><cell>8.3</cell><cell>7.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Clouds Military Plane Sky Airport Snow Clouds Aircraft Aviation Airplane Jet Flying Aircrafts Takeoff Airline Jets Airlines Airport Plane Clouds Sky Military Train Railroad Bridge Road Fire Railroad Train Bridge Road Fire</head><label></label><figDesc>by TagProp under the conventional tag-</figDesc><table><row><cell>Images</cell><cell>Conventional Tagging</cell><cell>Zero-Shot Tagging</cell><cell>See/Unseen Tagging</cell><cell>4k Zero-Shot Tagging</cell><cell>TagProp (Conventional)</cell></row><row><cell></cell><cell>Water</cell><cell>Water</cell><cell>Water</cell><cell>Water</cell><cell>Water</cell></row><row><cell></cell><cell>Beach</cell><cell>Ocean</cell><cell>Ocean</cell><cell>Ocean</cell><cell>Surf</cell></row><row><cell></cell><cell>Ocean</cell><cell>Surf</cell><cell>Wave</cell><cell>NGO</cell><cell>Ocean</cell></row><row><cell></cell><cell>Surf</cell><cell>Beach</cell><cell>Sea</cell><cell>Dam</cell><cell>Beach</cell></row><row><cell></cell><cell>Cat</cell><cell>Snow</cell><cell>Surf</cell><cell>Surf</cell><cell>Whales</cell></row><row><cell></cell><cell>Reflection</cell><cell>Cityscape</cell><cell>Night</cell><cell>Waterfront</cell><cell>Water</cell></row><row><cell></cell><cell>Water</cell><cell>Sunset</cell><cell>Skyline</cell><cell>Danbe</cell><cell>Reflection</cell></row><row><cell></cell><cell>Building</cell><cell>Bridge</cell><cell>Cityscape</cell><cell>Cityscape</cell><cell>Sky</cell></row><row><cell></cell><cell>Cityscape</cell><cell>Reflection</cell><cell>Sunset</cell><cell>Sunset</cell><cell>Building</cell></row><row><cell></cell><cell>Harbor</cell><cell>Harbor</cell><cell>City</cell><cell>Venice</cell><cell>Cityscape</cell></row><row><cell></cell><cell>Coral</cell><cell>Coral</cell><cell>Coral</cell><cell>Coral</cell><cell>Coral</cell></row><row><cell></cell><cell>Fish</cell><cell>Fish</cell><cell>Underwater</cell><cell>Korea</cell><cell>Fish</cell></row><row><cell></cell><cell>Ocean</cell><cell>Water</cell><cell>Marine</cell><cell>Mushroom</cell><cell>Water</cell></row><row><cell></cell><cell>Water</cell><cell>Ocean</cell><cell>Scuba</cell><cell>Lichen</cell><cell>Ocean</cell></row><row><cell></cell><cell>Rocks</cell><cell>Sand</cell><cell>Reef</cell><cell>Fish</cell><cell>Flowers</cell></row><row><cell></cell><cell>Plane</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Airport</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Sky</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Locomotive</cell><cell>Locomotives</cell><cell>Train</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Railroad</cell><cell>Railroad</cell><cell>Railroad</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Railway</cell><cell>Railways</cell><cell>Clouds</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Train</cell><cell>Train</cell><cell>Sky</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Rail</cell><cell>Trains</cell><cell>Road</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison results of the conventional image tagging with 291 tags on IAPRTC-12.</figDesc><table><row><cell>Method %</cell><cell>MiAP</cell><cell>P</cell><cell>K = 3 R</cell><cell>F1</cell><cell>P</cell><cell>K = 5 R</cell><cell>F1</cell></row><row><cell>TagProp [22]</cell><cell>52</cell><cell cols="6">54 29 38 46 41 43</cell></row><row><cell>WARP [20]</cell><cell>48</cell><cell cols="6">50 27 35 43 38 40</cell></row><row><cell>FastTag [8]</cell><cell>48</cell><cell cols="6">53 28 36 44 39 41</cell></row><row><cell>Fast0Tag (lin.)</cell><cell>46</cell><cell cols="6">52 28 37 43 38 40</cell></row><row><cell>Fast0Tag (net.)</cell><cell>56</cell><cell cols="6">58 31 41 50 44 47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :Lake Mountain Water Sky Reflection Valley Glacier Mountain Lake Snow Mountains Valley Glacier Landscape Mountain Valley Glacier Alpine Mountain Lake Mountain Snow Lake Water Sky Mountain Clouds Snow Sunset Sky Mountain Snow Glacier Valley Snow Mountains Mountain Snow Glacier Valley Mountain Snowy Snow Peaks Alps Images Conventional Tagging Zero-Shot Tagging Seen/Unseen Tagging TagProp (Conventional) Bed Room Wall Lamp Night Pillow curtain Clock Wood Picture Pillow Bed Room Wall Curtain Bed Room Wall Table Wood Jersey Cycling Short Cyclist Bike Racing Frame Helmet Horse Shirt Cyclist Cycling Bike Jersey Road Bike Cyclist Short Cycling Jersey Child Hand Woman Girl table Adult Kid Boy Towel Girl Adult Hand Kid Child Woman Child Table Tourist Round Hand Man Woman House Tree Bench Park Adult Lion Picture Short Park Man House Tree Woman Woman Wall Man Tree People Sky City Mountain Building Cloud Skyline Fountain Cup Boy Shore Skyline Sky Fountain Building Street Sky Building City Boat Cloud Mountain Snow Cloud Peak View Glacier Valley Frame Ridge Skyline Mountain Snow Cloud Summit Sky Mountain Cloud Sky Snow Landscape Table Woman Plate Man Wall Bar Shirt Girl Adult Picture Table Woman Plate Man Wall Table Woman Man Wall Restaurant</head><label>5</label><figDesc>Comparison results of the zero-shot and seen/unseen image tagging tasks with 58 unseen tags and 233 seen tags.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Zero-shot image tagging</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Seen/unseen image tagging</cell></row><row><cell>Method %</cell><cell></cell><cell>MiAP</cell><cell>P</cell><cell>K = 3 R</cell><cell>F1</cell><cell>P</cell><cell></cell><cell>K = 5 R</cell><cell>F1</cell><cell>MiAP</cell><cell>P</cell><cell>K = 3 R</cell><cell>F1</cell><cell>P</cell><cell>K = 5 R</cell><cell>F1</cell></row><row><cell>Random</cell><cell></cell><cell>8.1</cell><cell>2.0</cell><cell>4.5</cell><cell>2.8</cell><cell cols="2">2.2</cell><cell>2.2</cell><cell>8.1</cell><cell>3.5</cell><cell>2.2</cell><cell cols="3">1.2 1.5 1.9</cell><cell>1.7 1.8</cell></row><row><cell>Seen2Unseen</cell><cell></cell><cell>15.6</cell><cell>6.1</cell><cell>13.5</cell><cell>8.4</cell><cell>5.3</cell><cell></cell><cell>19.5</cell><cell>8.4</cell><cell>7.2</cell><cell>3.6</cell><cell>1.9</cell><cell>2.5</cell><cell>4.2</cell><cell>3.7</cell><cell>3.9</cell></row><row><cell>LabelEM [2]</cell><cell></cell><cell>11.5</cell><cell>3.6</cell><cell>7.9</cell><cell>4.9</cell><cell>3.6</cell><cell></cell><cell>13.3</cell><cell>5.7</cell><cell>13.8</cell><cell>3.1</cell><cell>1.7</cell><cell>2.2</cell><cell>4.4</cell><cell>3.9</cell><cell>8.7</cell></row><row><cell cols="2">LabelEM+ [2]</cell><cell>17.6</cell><cell>7.3</cell><cell>16.1</cell><cell>10.0</cell><cell>6.4</cell><cell></cell><cell>23.4</cell><cell>10.0</cell><cell>20.1</cell><cell>13.9</cell><cell>7.4</cell><cell>9.7</cell><cell>13.2</cell><cell>11.8</cell><cell>12.5</cell></row><row><cell>ConSE [45]</cell><cell></cell><cell>24.1</cell><cell>9.7</cell><cell>21.3</cell><cell>13.3</cell><cell>8.9</cell><cell></cell><cell>32.5</cell><cell>13.9</cell><cell>32.5</cell><cell>38.8</cell><cell>20.6</cell><cell>26.9</cell><cell>31.1</cell><cell>27.6</cell><cell>29.2</cell></row><row><cell cols="2">Fast0Tag (lin.)</cell><cell>23.1</cell><cell>11.3</cell><cell>24.9</cell><cell>15.6</cell><cell>9.0</cell><cell></cell><cell>33.2</cell><cell>14.2</cell><cell>42.9</cell><cell>50.6</cell><cell>27.0</cell><cell>35.2</cell><cell>40.8</cell><cell>36.2</cell><cell>38.4</cell></row><row><cell cols="2">Fast0Tag (net.)</cell><cell>20.3</cell><cell>8.5</cell><cell>18.6</cell><cell>11.6</cell><cell>7.2</cell><cell></cell><cell>26.4</cell><cell>11.3</cell><cell>45.9</cell><cell>48.2</cell><cell>25.7</cell><cell>33.5</cell><cell>42.2</cell><cell>37.4</cell><cell>39.7</cell></row><row><cell>RankSVM</cell><cell></cell><cell>21.6</cell><cell cols="3">10.2 22.6 14.1</cell><cell cols="3">8.6 31.7</cell><cell>13.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Images</cell><cell cols="2">Conventional Tagging</cell><cell>Zero-Shot Tagging</cell><cell>Seen/Unseen Tagging</cell><cell cols="2">4k Zero-Shot Tagging</cell><cell cols="2">TagProp (Conventional)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially supported by NSF IIS 1566511. We thank the anonymous area chair and reviewers, especially the assigned Reviewer 30, for their helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-Embedding for Attribute-Based Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="819" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of Output Embeddings for Fine-Grained Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matching words and pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1107" to="1135" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for scientific computing conference (SciPy)</title>
		<meeting>the Python for scientific computing conference (SciPy)<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Supervised learning of semantic classes for image annotation and retrieval. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="394" to="410" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient algorithms for ranking with SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="215" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th international conference on Machine Learning</title>
		<meeting>the 30th international conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1274" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">NUS-WIDE: a real-world web image database from National University of Singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on image and video retrieval</title>
		<meeting>the ACM international conference on image and video retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAsIs</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving Semantic Concept Detection through the Dictionary of Visually-distinct Elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2585" to="2592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiple bernoulli relevance models for image and video annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1002</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Relaxing From Vocabulary: Robust Weakly-Supervised Deep Learning for Vocabulary-Free Image Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1985" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Transductive Multi-label Zero-shot Learning. pages 7.1-7.11. British Machine Vision Association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Transductive Multi-class and Multi-label Zero-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.07884</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4894</idno>
		<title level="m">Deep convolutional ranking for multilabel image annotation</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The iapr tc-12 benchmark: A new evaluation resource for visual information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop OntoImage</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation. In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="309" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3464" to="3472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic image annotation and retrieval using cross-media relevance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">NMF-KNN: Image Annotation Using Weighted Multi-view Nonnegative Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2306</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A model for learning the semantics of pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">page None</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Phrasebased Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<editor>D. Blei and F. Bach</editor>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2085" to="2094" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Visually weighted neighbor voting for image tag relevance learning. Multimedia tools and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>De Neve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1363" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning social tag relevance by neighbor voting. Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1310" to="1322" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08248</idno>
		<idno>arXiv: 1503.08248</idno>
		<title level="m">Socializing the Semantic Gap: A Comparative Survey on Image Tag Assignment, Refinement and Retrieval</title>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Baselines for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="105" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Coherent image annotation by learning semantic distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<idno>arXiv: 1301.3781</idno>
		<title level="m">Efficient Estimation of Word Representations in Vector Space</title>
		<imprint>
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PLSA-based image auto-annotation: constraining the latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th annual ACM international conference on Multimedia</title>
		<meeting>the 12th annual ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="348" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video annotation through search and graph reinforcement mining. Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Moxley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="184" to="193" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semi-supervised relational topic model for weakly annotated image recognition in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4233" to="4240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Zeroshot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document</title>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Flickr tag recommendation based on collective knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sigurbjrnsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Feature-Independent Context Estimation for Automatic Image Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1958" to="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Marton. Quantitative evaluation of passage retrieval algorithms for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Multi-label classification: An overview. Dept. of Informatics, Aristotle University of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Thessaloniki, Greece</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Large scale image annotation: learning to rank with joint wordimage embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Listwise approach to learning to rank: theory and algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1192" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Annotating images and image objects using a hierarchical dirichlet process model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Multimedia Data Mining: held in conjunction with the ACM SIGKDD 2008</title>
		<meeting>the 9th International Workshop on Multimedia Data Mining: held in conjunction with the ACM SIGKDD 2008</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An adaptive teleportation random walk model for learning social tag relevance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georgescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bilingual Word Embeddings for Phrase-Based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
