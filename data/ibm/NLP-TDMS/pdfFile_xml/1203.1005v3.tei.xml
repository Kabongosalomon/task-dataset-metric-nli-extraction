<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Subspace Clustering: Algorithm, Theory, and Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">René</forename><surname>Vidal</surname></persName>
						</author>
						<title level="a" type="main">Sparse Subspace Clustering: Algorithm, Theory, and Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-High-dimensional data</term>
					<term>intrinsic low-dimensionality</term>
					<term>subspaces</term>
					<term>clustering</term>
					<term>sparse representation</term>
					<term>1 -minimization</term>
					<term>convex programming</term>
					<term>spectral clustering</term>
					<term>principal angles</term>
					<term>motion segmentation</term>
					<term>face clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many real-world problems deal with collections of high-dimensional data, such as images, videos, text and web documents, DNA microarray data, and more. Often, such high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories to which the data belong. In this paper, we propose and study an algorithm, called Sparse Subspace Clustering (SSC), to cluster data points that lie in a union of low-dimensional subspaces. The key idea is that, among the infinitely many possible representations of a data point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of the data into subspaces. Since solving the sparse optimization program is in general NP-hard, we consider a convex relaxation and show that, under appropriate conditions on the arrangement of the subspaces and the distribution of the data, the proposed minimization program succeeds in recovering the desired sparse representations. The proposed algorithm is efficient and can handle data points near the intersections of subspaces. Another key advantage of the proposed algorithm with respect to the state of the art is that it can deal directly with data nuisances, such as noise, sparse outlying entries, and missing entries, by incorporating the model of the data into the sparse optimization program. We demonstrate the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>H IGH-DIMENSIONAL data are ubiquitous in many areas of machine learning, signal and image processing, computer vision, pattern recognition, bioinformatics, etc. For instance, images consist of billions of pixels, videos can have millions of frames, text and web documents are associated with hundreds of thousands of features, etc. The high-dimensionality of the data not only increases the computational time and memory requirements of algorithms, but also adversely affects their performance due to the noise effect and insufficient number of samples with respect to the ambient space dimension, commonly referred to as the "curse of dimensionality" <ref type="bibr" target="#b0">[1]</ref>. However, high-dimensional data often lie in low-dimensional structures instead of being uniformly distributed across the ambient space. Recovering low-dimensional structures in the data helps to not only reduce the computational cost and memory requirements of algorithms, but also reduce the effect of high-dimensional noise in the data and improve the performance of inference, learning, and recognition tasks.</p><p>In fact, in many problems, data in a class or category can be well represented by a low-dimensional subspace of the highdimensional ambient space. For example, feature trajectories of a rigidly moving object in a video <ref type="bibr" target="#b1">[2]</ref>, face images of a subject under varying illumination <ref type="bibr" target="#b2">[3]</ref>, and multiple instances of a handwritten digit with different rotations, translations, and thicknesses <ref type="bibr" target="#b3">[4]</ref> lie in a low-dimensional subspace of the ambient space. As a result, the collection of data from multiple classes or categories lie in a union of low-dimensional subspaces. Subspace clustering • E. <ref type="bibr">Elhamifar</ref>  (see <ref type="bibr" target="#b4">[5]</ref> and references therein) refers to the problem of separating data according to their underlying subspaces and finds numerous applications in image processing (e.g., image representation and compression <ref type="bibr" target="#b5">[6]</ref>) and computer vision (e.g., image segmentation <ref type="bibr" target="#b6">[7]</ref>, motion segmentation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and temporal video segmentation <ref type="bibr" target="#b9">[10]</ref>), as illustrated in <ref type="figure" target="#fig_0">Figures 1 and 2</ref>. Since data in a subspace are often distributed arbitrarily and not around a centroid, standard clustering methods <ref type="bibr" target="#b10">[11]</ref> that take advantage of the spatial proximity of the data in each cluster are not in general applicable to subspace clustering. Therefore, there is a need for having clustering algorithms that take into account the multi-subspace structure of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Prior Work on Subspace Clustering</head><p>Existing algorithms can be divided into four main categories: iterative, algebraic, statistical, and spectral clustering-based methods.</p><p>Iterative methods. Iterative approaches, such as K-subspaces <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> and median K-flats <ref type="bibr" target="#b13">[14]</ref> alternate between assigning points to subspaces and fitting a subspace to each cluster. The main drawbacks of such approaches are that they generally require to know the number and dimensions of the subspaces, and that they are sensitive to initialization.</p><p>Algebraic approaches. Factorization-based algebraic approaches such as <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref> find an initial segmentation by thresholding the entries of a similarity matrix built from the factorization of the data matrix. These methods are provably correct when the subspaces are independent, but fail when this assumption is violated. In addition, they are sensitive to noise and outliers in the data. Algebraic-geometric approaches such as Generalized Principal Component Analysis (GPCA) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, fit the data with a polynomial whose gradient at a point gives the normal vector to the subspace containing that point. While GPCA can arXiv:1203.1005v3 [cs.CV] 5 Feb 2013  deal with subspaces of different dimensions, it is sensitive to noise and outliers, and its complexity increases exponentially in terms of the number and dimensions of subspaces.</p><p>Statistical methods. Iterative statistical approaches, such as Mixtures of Probabilistic PCA (MPPCA) <ref type="bibr" target="#b16">[17]</ref>, Multi-Stage Learning (MSL) <ref type="bibr" target="#b17">[18]</ref>, or <ref type="bibr" target="#b18">[19]</ref>, assume that the distribution of the data inside each subspace is Gaussian and alternate between data clustering and subspace estimation by applying Expectation Maximization (EM). The main drawbacks of these methods are that they generally need to know the number and dimensions of the subspaces, and that they are sensitive to initialization. Robust statistical approaches, such as Random Sample Consensus (RANSAC) <ref type="bibr" target="#b19">[20]</ref>, fit a subspace of dimension d to randomly chosen subsets of d points until the number of inliers is large enough. The inliers are then removed, and the process is repeated to find a second subspace, and so on. RANSAC can deal with noise and outliers, and does not need to know the number of subspaces. However, the dimensions of the subspaces must be known and equal. In addition, the complexity of the algorithm increases exponentially in the dimension of the subspaces. Information-theoretic statistical approaches, such as Agglomerative Lossy Compression (ALC) <ref type="bibr" target="#b20">[21]</ref>, look for the segmentation of the data that minimizes the coding length needed to fit the points with a mixture of degenerate Gaussians up to a given distortion. As this minimization problem is NP-hard, a suboptimal solution is found by first assuming that each point forms its own group, and then iteratively merging pairs of groups to reduce the coding length. ALC can handle noise and outliers in the data. While, in principle, it does not need to know the number and dimensions of the subspaces, the number of subspaces found by the algorithms is dependent on the choice of a distortion parameter. In addition, there is no theoretical proof for the optimality of the agglomerative algorithm.</p><p>Spectral clustering-based methods. Local spectral clusteringbased approaches such as Local Subspace Affinity (LSA) <ref type="bibr" target="#b21">[22]</ref>, Locally Linear Manifold Clustering (LLMC) <ref type="bibr" target="#b22">[23]</ref>, Spectral Local Best-fit Flats (SLBF) <ref type="bibr" target="#b23">[24]</ref>, and <ref type="bibr" target="#b24">[25]</ref> use local information around each point to build a similarity between pairs of points. The segmentation of the data is then obtained by applying spectral clustering <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> to the similarity matrix. These methods have difficulties in dealing with points near the intersection of two subspaces, because the neighborhood of a point can contain points from different subspaces. In addition, they are sensitive to the right choice of the neighborhood size to compute the local information at each point. Global spectral clustering-based approaches try to resolve these issues by building better similarities between data points using global information. Spectral Curvature Clustering (SCC) <ref type="bibr" target="#b27">[28]</ref> uses multi-way similarities that capture the curvature of a collection of points within an affine subspace. SCC can deal with noisy data but requires to know the number and dimensions of subspaces and assumes that subspaces have the same dimension. In addition, the complexity of building the multi-way similarity grows exponentially with the dimensions of the subspaces, hence, in practice, a sampling strategy is employed to reduce the computational cost. Using advances in sparse <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> and low-rank <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> recovery algorithms, Sparse Subspace Clustering (SSC) <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, Low-Rank Recovery (LRR) <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and Low-Rank Subspace Clustering (LRSC) <ref type="bibr" target="#b40">[41]</ref> algorithms pose the clustering problem as one of finding a sparse or low-rank representation of the data in the dictionary of the data itself. The solution of the corresponding global optimization algorithm is then used to build a similarity graph from which the segmentation of the data is obtained. The advantages of these methods with respect to most state-of-the-art algorithms are that they can handle noise and outliers in data, and that they do not need to know the dimensions and, in principle, the number of subspaces a priori.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Paper Contributions</head><p>In this paper, we propose and study an algorithm based on sparse representation techniques, called Sparse Subspace Clustering (SSC), to cluster a collection of data points lying in a union of low-dimensional subspaces. The underlying idea behind the algorithm is what we call the self-expressiveness property of the data, which states that each data point in a union of subspaces can be efficiently represented as a linear or affine combination of other points. Such a representation is not unique in general because there are infinitely many ways in which a data point can be expressed as a combination of other points. The key observation is that a sparse representation of a data point ideally corresponds to a combination of a few points from its own subspace. This motivates solving a global sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of data. As a result, we can overcome the problems of local spectral clustering-based algorithms, such as choosing the right neighborhood size and dealing with points near the intersection of subspaces, since, for a given data point, the sparse optimization program automatically picks a few other points that are not necessarily close to it but belong to the same subspace.</p><p>Since solving the sparse optimization program is in general NP-hard, we consider its 1 relaxation. We show that, under mild conditions on the arrangement of subspaces and data distribution, the proposed 1 -minimization program recovers the desired solution, guaranteeing the success of the algorithm. Our theoretical analysis extends the sparse representation theory to the multisubspace setting where the number of points in a subspace is arbitrary, possibly much larger than its dimension. Unlike blocksparse recovery problems <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> where the bases for the subspaces are known and given, we do not have the bases for subspaces nor do we know which data points belong to which subspace, making our case more challenging. We only have the sparsifying dictionary for the union of subspaces given by the matrix of data points.</p><p>The proposed 1 -minimization program can be solved efficiently using convex programming tools <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> and does not require initialization. Our algorithm can directly deal with noise, sparse outlying entries, and missing entries in the data as well as the more general class of affine subspaces by incorporating the data corruption or subspace model into the sparse optimization program. Finally, through experimental results, we show that our algorithm outperforms state-of-the-art subspace clustering methods on the two real-world problems of motion segmentation ( <ref type="figure" target="#fig_0">Fig. 1</ref>) and face clustering ( <ref type="figure" target="#fig_1">Fig. 2</ref>).</p><p>Paper Organization. In Section 2, we motivate and introduce the SSC algorithm for clustering data points in a union of linear subspaces. In Section 3, we generalize the algorithm to deal with noise, sparse outlying entries, and missing entries in the data as well as the more general class of affine subspaces. In Section 4, we investigate theoretical conditions under which the 1minimization program recovers the desired sparse representations of data points. In Section 5, we discuss the connectivity of the similarity graph and propose a regularization term to increase the connectivity of points in each subspace. In Section 6, we verify our theoretical analysis through experiments on synthetic data. In Section 7, we compare the performance of SSC with the state of the art on the two real-world problems of motion segmentation and face clustering. Finally, Section 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SPARSE SUBSPACE CLUSTERING</head><p>In this section, we introduce the sparse subspace clustering (SSC) algorithm for clustering a collection of multi-subspace data using sparse representation techniques. We motivate and formulate the algorithm for data points that perfectly lie in a union of linear subspaces. In the next section, we will generalize the algorithm to deal with data nuisances such as noise, sparse outlying entries, and missing entries as well as the more general class of affine subspaces.</p><p>Let {S } n =1 be an arrangement of n linear subspaces of R D of dimensions {d } n =1 . Consider a given collection of N noisefree data points {y i } N i=1 that lie in the union of the n subspaces. Denote the matrix containing all the data points as</p><formula xml:id="formula_0">Y y 1 . . . y N = Y 1 . . . Y n Γ,<label>(1)</label></formula><p>where Y ∈ R D×N is a rank-d matrix of the N &gt; d points that lie in S and Γ ∈ R N ×N is an unknown permutation matrix. We assume that we do not know a priori the bases of the subspaces nor do we know which data points belong to which subspace. The subspace clustering problem refers to the problem of finding the number of subspaces, their dimensions, a basis for each subspace, and the segmentation of the data from Y . To address the subspace clustering problem, we propose an algorithm that consists of two steps. In the first step, for each data point, we find a few other points that belong to the same subspace. To do so, we propose a global sparse optimization program whose solution encodes information about the memberships of data points to the underlying subspace of each point. In the second step, we use these information in a spectral clustering framework to infer the clustering of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sparse Optimization Program</head><p>Our proposed algorithm takes advantage of what we refer to as the self-expressiveness property of the data, i.e., each data point in a union of subspaces can be efficiently reconstructed by a combination of other points in the dataset.</p><p>More precisely, each data point for data point y i ∈ ∪ n =1 S can be written as</p><formula xml:id="formula_1">y i = Y c i , c ii = 0,<label>(2)</label></formula><p>where c i c i1 c i2 . . . c iN and the constraint c ii = 0 eliminates the trivial solution of writing a point as a linear combination of itself. In other words, the matrix of data points Y is a self-expressive dictionary in which each point can be written as a linear combination of other points. However, the representation of y i in the dictionary Y is not unique in general. This comes from the fact that the number of data points in a subspace is often greater than its dimension, i.e., N &gt; d . As a result, each Y , and consequently Y , has a non-trivial nullspace giving rise to infinitely many representations of each data point.</p><p>The key observation in our proposed algorithm is that among all solutions of (2), there exists a sparse solution, c i , whose nonzero entries correspond to data points from the same subspace as y i . We refer to such a solution as a subspace-sparse representation.</p><p>More specifically, a data point y i that lies in the d -dimensional subspace S can be written as a linear combination of d other points in general directions from S . As a result, ideally, a sparse representation of a data point finds points from the same subspace where the number of the nonzero elements corresponds to the dimension of the underlying subspace.</p><p>For a system of equations such as (2) with infinitely many solutions, one can restrict the set of solutions by minimizing an objective function such as the q -norm of the solution 1 as</p><formula xml:id="formula_2">min c i q s. t. y i = Y c i , c ii = 0.</formula><p>(3)  <ref type="figure">Fig. 3</ref>. Three subspaces in R 3 with 10 data points in each subspace, ordered such that the fist and the last 10 points belong to S1 and S3, respectively. The solution of the q -minimization program in (3) for y i lying in S1 for q = 1, 2, ∞ is shown. Note that as the value of q decreases, the sparsity of the solution increases. For q = 1, the solution corresponds to choosing two other points lying in S1.</p><p>Different choices of q have different effects in the obtained solution. Typically, by decreasing the value of q from infinity toward zero, the sparsity of the solution increases, as shown in <ref type="figure">Figure 3</ref>. The extreme case of q = 0 corresponds to the general NP-hard problem <ref type="bibr" target="#b50">[51]</ref> of finding the sparsest representation of the given point, as the 0 -norm counts the number of nonzero elements of the solution. Since we are interested in efficiently finding a non-trivial sparse representation of y i in the dictionary Y , we consider minimizing the tightest convex relaxation of the 0 -norm, i.e.,</p><formula xml:id="formula_3">min c i 1 s. t. y i = Y c i , c ii = 0,<label>(4)</label></formula><p>which can be solved efficiently using convex programming tools <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> and is known to prefer sparse solutions <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. We can also rewrite the sparse optimization program (4) for all data points i = 1, . . . , N in matrix form as</p><formula xml:id="formula_4">min C 1 s. t. Y = Y C, diag(C) = 0,<label>(5)</label></formula><p>where C c 1 c 2 . . . c N ∈ R N ×N is the matrix whose i-th column corresponds to the sparse representation of y i , c i , and diag(C) ∈ R N is the vector of the diagonal elements of C.</p><p>Ideally, the solution of (5) corresponds to subspace-sparse representations of the data points, which we use next to infer the clustering of the data. In Section 4, we study conditions under which the convex optimization program in (5) is guaranteed to recover a subspace-sparse representation of each data point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Clustering using Sparse Coefficients</head><p>After solving the proposed optimization program in (5), we obtain a sparse representation for each data point whose nonzero elements ideally correspond to points from the same subspace. The next step of the algorithm is to infer the segmentation of the data into different subspaces using the sparse coefficients.</p><p>To address this problem, we build a weighted graph G = (V, E, W ), where V denotes the set of N nodes of the graph corresponding to N data points and E ⊆ V × V denotes the set of edges between nodes. W ∈ R N ×N is a symmetric non-negative similarity matrix representing the weights of the edges, i.e., node i is connected to node j by an edge whose weight is equal to w ij . An ideal similarity matrix W , hence an ideal similarity graph G, is one in which nodes that correspond to points from the same subspace are connected to each other and there are no edges between nodes that correspond to points in different subspaces.</p><p>Note that the sparse optimization program ideally recovers to a subspace-sparse representation of each point, i.e., a representation whose nonzero elements correspond to points from the same subspace of the given data point. This provides an immediate choice of the similarity matrix as W = |C| + |C| . In other words, each node i connects itself to a node j by an edge whose weight is equal to |c ij | + |c ji |. The reason for the symmetrization is that, in general, a data point y i ∈ S can write itself as a linear combination of some points including y j ∈ S . However, y j may not necessarily choose y i in its sparse representation. By this particular choice of the weight, we make sure that nodes i and j get connected to each other if either y i or y j is in the sparse representation of the other. <ref type="bibr" target="#b1">2</ref> The similarity graph built this way has ideally n connected components corresponding to the n subspaces, i.e.,</p><formula xml:id="formula_5">W =    W 1 · · · 0 . . . . . . . . . 0 · · · W n   Γ,<label>(6)</label></formula><p>where W is the similarity matrix of data points in S . Clustering of data into subspaces follows then by applying spectral clustering <ref type="bibr" target="#b25">[26]</ref> to the graph G. More specifically, we obtain the clustering of data by applying the Kmeans algorithm <ref type="bibr" target="#b10">[11]</ref> to the normalized rows of a matrix whose columns are the n bottom eigenvectors of the symmetric normalized Laplacian matrix of the graph. Remark 1: An optional step prior to building the similarity graph is to normalize the sparse coefficients as c i ← c i / c i ∞ . This helps to better deal with different norms of data points. More specifically, if a data point with a large Euclidean norm selects a few points with small Euclidean norms, then the values of the nonzero coefficients will generally be large. On the other hand, if a data point with a small Euclidean norm selects a few points with large Euclidean norms, then the values of the nonzero coefficients will generally be small. Since spectral clustering puts more emphasis on keeping the stronger connections in the graph, by the normalization step we make sure that the largest edge weights for all the nodes are of the same scale.</p><p>Algorithm 1 summarizes the SSC algorithm. Note that an advantage of spectral clustering, which will be shown in the experimental results, is that it provides robustness with respect to a few errors in the sparse representations of the data points. In other words, as long as edges between points in different subspaces are weak, spectral clustering can find the correct segmentation. <ref type="bibr" target="#b1">2</ref>. To obtain a symmetric similarity matrix, one can directly impose the constraint of C = C in the optimization program. However, this results in increasing the complexity of the optimization program and, in practice, does not perform better than the post-symmetrization of C, as described above. See also <ref type="bibr" target="#b51">[52]</ref> for other processing approaches of the similarity matrix.</p><p>Algorithm 1 : Sparse Subspace Clustering (SSC) Input: A set of points {y i } N i=1 lying in a union of n linear subspaces {S i } n i=1 . 1: Solve the sparse optimization program <ref type="bibr" target="#b4">(5)</ref> in the case of uncorrupted data or <ref type="bibr" target="#b12">(13)</ref> in the case of corrupted data. <ref type="bibr">2:</ref> Normalize the columns of C as c i ← ci ci ∞ . 3: Form a similarity graph with N nodes representing the data points. Set the weights on the edges between the nodes by W = |C| + |C| . <ref type="bibr" target="#b3">4</ref>: Apply spectral clustering <ref type="bibr" target="#b25">[26]</ref> to the similarity graph. Output: Segmentation of the data: Y 1 , Y 2 , . . . , Y n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2:</head><p>In principle, SSC does not need to know the number of subspaces. More specifically, under the conditions of the theoretical results in Section 4, in the similarity graph there will be no connections between points in different subspaces. Thus, one can determine the number of subspaces by finding the number of graph components, which can be obtained by analyzing the eigenspectrum of the Laplacian matrix of G <ref type="bibr" target="#b26">[27]</ref>. However, when there are connections between points in different subspaces, other model selection techniques should be employed <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRACTICAL EXTENSIONS</head><p>In real-world problems, data are often corrupted by noise and sparse outlying entries due to measurement/process noise and adhoc data collection techniques. In such cases, the data do not lie perfectly in a union of subspaces. For instance, in the motion segmentation problem, because of the malfunctioning of the tracker, feature trajectories can be corrupted by noise or can have entries with large errors <ref type="bibr" target="#b20">[21]</ref>. Similarly, in clustering of human faces, images can be corrupted by errors due to specularities, cast shadows, and occlusions <ref type="bibr" target="#b53">[54]</ref>. On the other hand, data points may have missing entries, e.g., when the tracker loses track of some feature points in a video due to occlusions <ref type="bibr" target="#b54">[55]</ref>. Finally, data may lie in a union of affine subspaces, a more general model which includes linear subspaces as a particular case.</p><p>In this section, we generalize the SSC algorithm for clustering data lying perfectly in a union of linear subspaces, to deal with the aforementioned challenges. Unlike state-of-the-art methods, which require to run a separate algorithm first to correct the errors in the data <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b54">[55]</ref>, we deal with these problems in a unified framework by incorporating a model for the corruption into the sparse optimization program. Thus, the sparse coefficients again encode information about memberships of data to subspaces, which are used in a spectral clustering framework, as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Noise and Sparse Outlying Entries</head><p>In this section, we consider clustering of data points that are contaminated with sparse outlying entries and noise. Let</p><formula xml:id="formula_6">y i = y 0 i + e 0 i + z 0 i<label>(7)</label></formula><p>be the i-th data point that is obtained by corrupting an errorfree point y 0 i , which perfectly lies in a subspace, with a vector of sparse outlying entries e 0 i ∈ R D that has only a few large nonzero elements, i.e., e 0 i 0 ≤ k for some integer k, and with a noise z 0 i ∈ R D whose norm is bounded as z 0 i 2 ≤ ζ for some ζ &gt; 0. Since error-free data points perfectly lie in a union of subspaces, using the self-expressiveness property, we can reconstruct y 0 i ∈ S in terms of other error-free points as</p><formula xml:id="formula_7">y 0 i = j =i c ij y 0 j .<label>(8)</label></formula><p>Note that the above equation has a sparse solution since y 0 i can be expressed as a linear combination of at most d other points from S . Rewriting y 0 i using <ref type="bibr" target="#b6">(7)</ref> in terms of the corrupted point y i , the sparse outlying entries vector e 0 i , and the noise vector z 0 i and substituting it into <ref type="formula" target="#formula_7">(8)</ref>, we obtain</p><formula xml:id="formula_8">y i = j =i c ij y j + e i + z i ,<label>(9)</label></formula><p>where the vectors e i ∈ R D and z i ∈ R D are defined as</p><formula xml:id="formula_9">e i e 0 i − j =i c ij e 0 j ,<label>(10)</label></formula><formula xml:id="formula_10">z i z 0 i − j =i c ij z 0 j .<label>(11)</label></formula><p>Since <ref type="formula" target="#formula_7">(8)</ref> has a sparse solution c i , e i and z i also correspond to vectors of sparse outlying entries and noise, respectively. More precisely, when a few c ij are nonzero, e i is a vector of sparse outlying entries since it is a linear combination of a few vectors of outlying entries in <ref type="bibr" target="#b9">(10)</ref>. Similarly, when a few c ij are nonzero and do not have significantly large magnitudes 3 , z i is a vector of noise since it is linear combination of a few noise vectors in <ref type="bibr" target="#b10">(11)</ref>.</p><p>Collecting e i and z i as columns of the matrices E and Z, respectively, we can rewrite (9) in matrix form as</p><formula xml:id="formula_11">Y = Y C + E + Z, diag(C) = 0.<label>(12)</label></formula><p>Our objective is then to find a solution (C, E, Z) for <ref type="formula" target="#formula_0">(12)</ref>, where C corresponds to a sparse coefficient matrix, E corresponds to a matrix of sparse outlying entries, and Z is a noise matrix. To do so, we propose to solve the following optimization program</p><formula xml:id="formula_12">min C 1 + λ e E 1 + λ z 2 Z 2 F s. t. Y = Y C + E + Z, diag(C) = 0,<label>(13)</label></formula><p>where the 1 -norm promotes sparsity of the columns of C and E, while the Frobenius norm promotes having small entries in the columns of Z. The two parameters λ e &gt; 0 and λ z &gt; 0 balance the three terms in the objective function. Note that the optimization program in <ref type="formula" target="#formula_0">(13)</ref> is convex with respect to the optimization variables (C, E, Z), hence, can be solved efficiently using convex programming tools. When data are corrupted only by noise, we can eliminate E from the optimization program in <ref type="bibr" target="#b12">(13)</ref>. On the other hand, when the data are corrupted only by sparse outlying entries, we can eliminate Z in <ref type="bibr" target="#b12">(13)</ref>. In practice, however, E can also deal with small errors due to noise. The following proposition suggests setting λ z = α z /µ z and λ e = α e /µ e , where α z , α e &gt; 1 and</p><formula xml:id="formula_13">µ z min i max j =i |y i y j |, µ e min i max j =i y j 1 .<label>(14)</label></formula><p>The proofs of all theoretical results in the paper are provided in the supplementary material.</p><p>Proposition 1: Consider the optimization program <ref type="bibr" target="#b12">(13)</ref>. Without the term Z, if λ e ≤ 1/µ e , then there exists at least 3. One can show that, under broad conditions, sum of |c ij | is bounded above by the square root of the dimension of the underlying subspace of y i . Theoretical guarantees of the proposed optimization program in the case of corrupted data is the subject of the current research. one data point y for which in the optimal solution we have (c , e ) = (0, y ). Also, without the term E, if λ z ≤ 1/µ z , then there exists at least one data point y for which (c , z ) = (0, y ).</p><p>After solving the proposed optimization programs, we use C to build a similarity graph and infer the clustering of data using spectral clustering. Thus, by incorporating the corruption model of data into the sparse optimization program, we can deal with clustering of corrupted data, as before, without explicitly running a separate algorithm to correct the errors in the data <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Missing Entries</head><p>We consider now the clustering of incomplete data, where some of the entries of a subset of the data points are missing. Note that when only a small fraction of the entries of each data point is missing, clustering of incomplete data can be cast as clustering of data with sparse outlying entries. More precisely, one can fill in the missing entries of each data point with random values, hence obtain data points with sparse outlying entries. Then clustering of the data follows by solving <ref type="bibr" target="#b12">(13)</ref> and applying spectral clustering to the graph built using the obtained sparse coefficients. However, the drawback of this approach is that it disregards the fact that we know the locations of the missing entries in the data matrix.</p><p>It is possible, in some cases, to cast the clustering of data with missing entries as clustering of complete data. To see this, consider a collection of data points</p><formula xml:id="formula_14">{y i } N i=1 in R D . Let J i ⊂ {1, .</formula><p>. . , D} denote indices of the known entries of y i and define J N i=1 J i . Thus, for every index in J, all data points have known entries. When the size of J, denoted by |J|, is not small relative to the ambient space dimension, D, we can project the data, hence, the original subspaces, into a subspace spanned by the columns of the identity matrix indexed by J and apply the SSC algorithm to the obtained complete data. In other words, we can only keep the rows of Y indexed by J, obtain a new data matrix of complete dataȲ ∈ R |J|×N , and solve the sparse optimization program <ref type="bibr" target="#b12">(13)</ref>. We can then infer the clustering of the data by applying spectral clustering to the graph built using the sparse coefficient matrix. Note that the approach described above is based on the assumption that J is nonempty. Addressing the problem of subspace clustering with missing entries when J is empty or has a small size is the subject of the future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Affine Subspaces</head><p>In some real-world problems, the data lie in a union of affine rather than linear subspaces. For instance, the motion segmentation problem involves clustering of data that lie in a union of 3-dimensional affine subspaces <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b54">[55]</ref>. A naive way to deal with this case is to ignore the affine structure of the data and perform clustering as in the case of linear subspaces. This comes from the fact that a d -dimensional affine subspace S can be considered as a subset of a (d + 1)-dimensional linear subspace that includes S and the origin. However, this has the drawback of possibly increasing the dimension of the intersection of two subspaces, which in some cases can result in indistinguishability of subspaces from each other. For example, two different lines x = −1 and x = +1 in the x-y plane form the same 2dimensional linear subspace after including the origin, hence become indistinguishable.</p><p>To directly deal with affine subspaces, we use the fact that any data point y i in an affine subspace S of dimension d can be <ref type="figure">Fig. 4</ref>. Left: the three 1-dimensional subspaces are independent as they span the 3-dimensional space and the sum of their dimensions is also 3. Right: the three 1-dimensional are disjoint as any two subspaces intersect at the origin.</p><formula xml:id="formula_15">S 1 S 2 S 3 S 3 S 1 S 2</formula><p>written as an affine combination of d + 1 other points from S . In other words, a sparse solution of</p><formula xml:id="formula_16">y i = Y c i , 1 c i = 1, c ii = 0,<label>(15)</label></formula><p>corresponds to d + 1 other points that belong to S containing y i . Thus, to cluster data points lying close to a union of affine subspaces, we propose to solve the sparse optimization program</p><formula xml:id="formula_17">min C 1 + λ e E 1 + λ z 2 Z 2 F s. t. Y = Y C + E + Z, 1 C = 1 , diag(C) = 0,<label>(16)</label></formula><p>which, in comparison to <ref type="bibr" target="#b12">(13)</ref> for the case of linear subspaces, includes additional linear equality constraints. Note that <ref type="formula" target="#formula_0">(16)</ref> can deal with linear subspaces as well since a linear subspace is also an affine subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SUBSPACE-SPARSE RECOVERY THEORY</head><p>The underlying assumption for the success of the SSC algorithm is that the proposed optimization program recovers a subspacesparse representation of each data point, i.e., a representation whose nonzero elements correspond to the subspace of the given point. In this section, we investigate conditions under which, for data points that lie in a union of linear subspaces, the sparse optimization program in (4) recovers subspace-sparse representations of data points. We investigate recovery conditions for two classes of subspace arrangements: independent and disjoint subspace models <ref type="bibr" target="#b35">[36]</ref>.</p><formula xml:id="formula_18">Definition 1: A collection of subspaces {S i } n i=1 is said to be independent if dim(⊕ n i=1 S i ) = n i=1 dim(S i ),</formula><p>where ⊕ denotes the direct sum operator.</p><p>As an example, the three 1-dimensional subspaces shown in <ref type="figure">Figure 4</ref> (left) are independent since they span a 3-dimensional space and the sum of their dimensions is also 3. On the other hand, the subspaces shown in <ref type="figure">Figure 4</ref> (right) are not independent since they span a 2-dimensional space while the sum of their dimensions is 3.</p><formula xml:id="formula_19">Definition 2: A collection of subspaces {S i } n i=1</formula><p>is said to be disjoint if every pair of subspaces intersect only at the origin. In other words, for every pair of subspaces we have dim(S i ⊕S j ) = dim(S i ) + dim(S j ).</p><p>As an example, both subspace arrangements shown in <ref type="figure">Figure 4</ref> are disjoint since each pair of subspaces intersect at the origin. Note that, based on the above definitions, the notion of disjointness is weaker than independence as an independent subspace model is always disjoint while the converse is not necessarily true. An important notion that can be used to characterize two disjoint subspaces is the smallest principal angle, defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3:</head><p>The smallest principal angle between two subspaces S i and S j , denoted by θ ij , is defined as</p><formula xml:id="formula_20">cos(θ ij ) max vi∈Si,vj ∈Sj v i v j v i 2 v j 2 .<label>(17)</label></formula><p>Note that two disjoint subspaces intersect at the origin, hence their smallest principal angle is greater than zero and cos(θ ij ) ∈ [0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Independent Subspace Model</head><p>In this section, we consider data points that lie in a union of independent subspaces, which is the underlying model of many subspace clustering algorithms. We show that the 1 -minimization program in (4) and more generally the q -minimization in <ref type="formula">(3)</ref> for q &lt; ∞ always recover subspace-sparse representations of the data points. More specifically, we show the following result.</p><p>Theorem 1: Consider a collection of data points drawn from</p><formula xml:id="formula_21">n independent subspaces {S i } n i=1 of dimensions {d i } n i=1 . Let Y i denote N i data points in S i , where rank(Y i ) = d i , and let Y −i denote data points in all subspaces except S i . Then, for every S i and every nonzero y in S i , the q -minimization program c * c * − = argmin c c − q s. t. y = [Y i Y −i ] c c − ,<label>(18)</label></formula><p>for q &lt; ∞, recovers a subspace-sparse representation, i.e., c * = 0 and c * − = 0. Note that the subspace-sparse recovery holds without any assumption on the distribution of the data points in each subspace, other than rank(Y i ) = d i . This comes at the price of having a more restrictive model for the subspace arrangements. Next, we will show that for the more general class of disjoint subspaces, under appropriate conditions on the relative configuration of the subspaces as well as the distribution of the data in each subspace, the 1 -minimization in (4) recovers subspace-sparse representations of the data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Disjoint Subspace Model</head><p>We consider now the more general class of disjoint subspaces and investigate conditions under which the optimization program in (4) recovers a subspace-sparse representation of each data point. To that end, we consider a vector x in the intersection of S i with ⊕ j =i S j and let the optimal solution of the 1 -minimization when we restrict the dictionary to data points from S i be</p><formula xml:id="formula_22">a i = argmin a 1 s. t. x = Y i a.<label>(19)</label></formula><p>We also let the optimal solution of the 1 -minimization when we restrict the dictionary to points from all subspaces except S i be</p><formula xml:id="formula_23">a −i = argmin a 1 s. t. x = Y −i a. 4<label>(20)</label></formula><p>We show in the supplementary material that the SSC algorithm succeeds in recovering subspace-sparse representations of data points in each S i , if for every nonzero x in the intersection of <ref type="bibr" target="#b3">4</ref>. In fact, a i and a −i depend on x, Y i , and Y −i . Since this dependence is clear from the context, we drop the arguments in</p><formula xml:id="formula_24">a i (x, Y i ) and a −i (x, Y −i ).</formula><p>S i with ⊕ j =i S j , the 1 -norm of the solution of (19) is strictly smaller than the 1 -norm of the solution of (20), i.e.,</p><formula xml:id="formula_25">∀ x ∈ S i ∩ (⊕ j =i S j ), x = 0 =⇒ a i 1 &lt; a −i 1 .<label>(21)</label></formula><p>More precisely, we show the following result. Theorem 2: Consider a collection of data points drawn from</p><formula xml:id="formula_26">n disjoint subspaces {S i } n i=1 of dimensions {d i } n i=1 . Let Y i denote N i data points in S i , where rank(Y i ) = d i , and let Y −i denote data points in all subspaces except S i . The 1 -minimization c * c * − = argmin c c − 1 s. t. y = [Y i Y −i ] c c − ,<label>(22)</label></formula><p>recovers a subspace-sparse representation of every nonzero y in S i , i.e., c * = 0 and c * − = 0, if and only if (21) holds. While the necessary and sufficient condition in (21) guarantees a successful subspace-sparse recovery via the 1 -minimization program, it does not explicitly show the relationship between the subspace arrangements and the data distribution for the success of the 1 -minimization program. To establish such a relationship, we show that a i 1 ≤ β i , where β i depends on the singular values of data points in S i , and β −i ≤ a −i 1 , where β −i depends on the subspace angles between S i and other subspaces. Then, the sufficient condition β i &lt; β −i establishes the relationship between the subspace angles and the data distribution under which the 1minimization is successful in subspace-sparse recovery, since it implies that</p><formula xml:id="formula_27">a i 1 ≤ β i &lt; β −i ≤ a −i 1 ,<label>(23)</label></formula><p>i.e., the condition of Theorem 2 holds.</p><p>Theorem 3: Consider a collection of data points drawn from n disjoint subspaces</p><formula xml:id="formula_28">{S i } n i=1 of dimensions {d i } n i=1 . Let W i be the set of all full-rank submatricesỸ i ∈ R D×di of Y i , where rank(Y i ) = d i . If the condition max Y i∈Wi σ di (Ỹ i ) &gt; d i Y −i 1,2 max j =i cos(θ ij )<label>(24)</label></formula><p>holds, then for every nonzero y in S i , the 1 -minimization in <ref type="formula" target="#formula_1">(22)</ref> recovers a subspace-sparse solution, i.e., c * = 0 and c * − = 0. 5 Loosely speaking, the sufficient condition in Theorem 3 states that if the smallest principal angle between each S i and any other subspace is larger than a certain value that depends on the data distribution in S i , then the subspace-sparse recovery holds. This bound can be rather high when the norms of the data points are oddly distributed, e.g., when the maximum norm of data points in S i is much smaller than the maximum norm of data points in all other subspaces. Since the segmentation of the data does not change when data points are scaled, we can apply SSC to linear subspaces after normalizing the data points to have unit Euclidean norms. In this case, the sufficient condition in <ref type="bibr" target="#b23">(24)</ref> </p><formula xml:id="formula_29">reduces to max Y i∈Wi σ di (Ỹ i ) &gt; d i max j =i cos(θ ij ).<label>(25)</label></formula><p>Remark 3: For independent subspaces, the intersection of a subspace with the direct sum of other subspaces is the origin, hence, the condition in (21) always holds. As a result, from Theorem 2, the 1 -minimization always recovers subspace-sparse representations of data points in independent subspaces. <ref type="bibr" target="#b4">5</ref>. The induced norm Y −i 1,2 denotes the maximum 2 -norm of the columns of Y −i . <ref type="figure">Fig. 5</ref>. Left: for any nonzero x in the intersection of S1 and S2 ⊕ S3, the polytope αP1 reaches x for a smaller α than αP−1, hence, subspace-sparse recovery holds. Middle: when the subspace angle decreases, the polytope αP−1 reaches x for a smaller α than αP1. Right: when the distribution of the data in S1 becomes nearly degenerate, in this case close to a line, the polytope αP−1 reaches x for a smaller α than αP1. In both cases, in the middle and right, the subspace-sparse recovery does not hold for points at the intersecion.</p><formula xml:id="formula_30">S 1 S 2 S 3 x O P 1 P −1 S 1 S 2 S 3 x O P 1 P −1 S 1 S 2 S 3 x O P 1 P −1</formula><p>Remark 4: The condition in <ref type="bibr" target="#b20">(21)</ref> is closely related to the nullspace property in the sparse recovery literature <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b57">[58]</ref>. The key difference, however, is that we only require the inequality in <ref type="bibr" target="#b20">(21)</ref> to hold for the optimal solutions of (19) and <ref type="bibr" target="#b19">(20)</ref> instead of any feasible solution. Thus, while the inequality can be violated for many feasible solutions, it can still hold for the optimal solutions, guaranteeing successful subspace-sparse recovery from Theorem 2. Thus, our result can be thought of as a generalization of the nullspace property to the multi-subspace setting where the number of points in each subspace is arbitrary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Geometric interpretation</head><p>In this section, we provide a geometric interpretation of the subspace-sparse recovery conditions in <ref type="formula" target="#formula_0">(21)</ref> and <ref type="bibr" target="#b23">(24)</ref>. To do so, it is necessary to recall the relationship between the 1 -norm of the optimal solution of min a 1 s. t. x = Ba,</p><p>and the symmetrized convex polytope of the columns of B <ref type="bibr" target="#b58">[59]</ref>. More precisely, if we denote the columns of B by b i and define the symmetrized convex hull of the columns of B by</p><formula xml:id="formula_32">P conv(±b 1 , ±b 2 , · · · ),<label>(27)</label></formula><p>then the 1 -norm of the optimal solution of (26) corresponds to the smallest α &gt; 0 such that the scaled polytope αP reaches x <ref type="bibr" target="#b58">[59]</ref>. Let us denote the symmetrized convex polytopes of Y i and Y −i by P i and P −i , respectively. Then the condition in (21) has the following geometric interpretation:</p><p>the subspace-sparse recovery in S i holds if and only if for any nonzero x in the intersection of S i and ⊕ j =i S j , αP i reaches x before αP −i , i.e., for a smaller α.</p><p>As shown in the left plot of <ref type="figure">Figure 5</ref>, for x in the intersection of S 1 and S 2 ⊕ S 3 , the polytope αP 1 reaches x before αP −1 , hence the subspace-sparse recovery condition holds. On the other hand, when the principal angles between S 1 and other subspaces decrease, as shown in the middle plot of <ref type="figure">Figure 5</ref>, the subspacesparse recovery condition does not hold since the polytope αP −1 reaches x before αP 1 . Also, as shown in the right plot of <ref type="figure">Figure 5</ref>, when the distribution of the data in S 1 becomes nearly degenerate, in this case close to a 1-dimensional subspace orthogonal to the direction of x, then the subspace-sparse recovery condition does not hold since αP −1 reaches x before αP 1 . Note that the sufficient condition in <ref type="bibr" target="#b23">(24)</ref> translates the relationship between the polytopes, mentioned above, explicitly in terms of a relationship between the subspace angles and the singular values of the data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GRAPH CONNECTIVITY</head><p>In the previous section, we studied conditions under which the proposed 1 -minimization program recovers subspace-sparse representations of data points. As a result, in the similarity graph, the points that lie in different subspaces do not get connected to each other. On the other hand, our extensive experimental results on synthetic and real data show that data points in the same subspace always form a connected component of the graph, hence, for n subspaces the similarity graph has n connected components. <ref type="bibr" target="#b59">[60]</ref> has theoretically verified the connectivity of points in the same subspace for 2 and 3 dimensional subspaces. However, it has shown that, for subspaces of dimensions greater than or equal to 4, under odd distribution of the data, it is possible that points in the same subspace form multiple components of the graph.</p><p>In this section, we consider a regularization term in the sparse optimization program that promotes connectivity of the points within each subspace. <ref type="bibr" target="#b5">6</ref> We use the idea that if data points in each subspace choose a few common points from the same subspace in their sparse representations, then they form a single component of the similarity graph. Thus, we add to the sparse optimization program the regularization term</p><formula xml:id="formula_33">C r,0 N i=1 I( c i 2 &gt; 0),<label>(28)</label></formula><p>where I(·) denotes the indicator function and c i denotes the i-th row of C. Hence, minimizing (28) corresponds to minimizing the number of nonzero rows of C <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, i.e., choosing a few common data points in the sparse representation of each point. <ref type="bibr" target="#b5">6</ref>. Another approach to deal with the connectivity issue is to analyze the subspaces corresponding to the components of the graph and merge the components whose associated subspaces have a small distance from each other, i.e., have a small principal angle. However, the result can be sensitive to the choice of the dimension of the subspaces to fit to each component as well as the threshold value on the principal angles to merge the subspaces.  The similarity graph of C1 has three components corresponding to the three subspaces. Right: C2 corresponds to the solution of (30) for λr → +∞ and θ ∈ (0, 4π 10 ). The similarity graph of C2 has only one connected component.</p><formula xml:id="formula_34">C 1 =         0 −1 0 0 0 0 −1 0 0 0 0 0 0 0 0 −1 0 0 0 0 −1 0 0 0 0 0 0 0 0 −1 0 0 0 0 −1 0         C 2 =         0 0 0 0 0 0 0 0 0 0 0 0 + 1 4 cos θ</formula><p>Since a minimization problem that involves <ref type="formula" target="#formula_1">(28)</ref> is in general NP-hard, we consider its convex relaxation as</p><formula xml:id="formula_35">C r,1 N i=1 c i 2 .<label>(29)</label></formula><p>Thus, to increase the connectivity of data points from the same subspace in the similarity graph, we propose to solve</p><formula xml:id="formula_36">min C 1 + λ r C r,1 s. t. Y = Y C, diag(C) = 0,<label>(30)</label></formula><p>where λ r &gt; 0 sets the trade-off between the sparsity of the solution and the connectivity of the graph. <ref type="figure" target="#fig_3">Figure 6</ref> shows how adding this regularization term promotes selecting common points in sparse representations. The following example demonstrates the reason for using the row-sparsity term as a regularizer but not as an objective function instead of the 1 -norm. Example 1: Consider the three 1-dimensional subspaces in R 2 , shown in <ref type="figure" target="#fig_5">Figure 7</ref>, where the data points have unit Euclidean norms and the angle between S 1 and S 2 as well as between S 1 and S 3 is equal to θ. Note that in this example, the sufficient condition in <ref type="bibr" target="#b23">(24)</ref> holds for all values of θ ∈ (0, π 2 ). As a result, the solution of (30) with λ r = 0 recovers a subspace-sparse representation for each data point, which in this example is uniquely given by C 1 shown in <ref type="figure" target="#fig_5">Figure 7</ref>. Hence, the similarity graph has exactly 3 connected components corresponding to the data points in each subspace. Another feasible solution of (30) is given by C 2 , shown in <ref type="figure" target="#fig_5">Figure 7</ref>, where the points in S 1 choose points from S 2 and S 3 in their representations. Hence, the similarity graph has only one connected component. Note that for a large range of subspace angles θ ∈ (0, 4π 10 ) we have C 2 r,1 = 16 + 2/ cos 2 (θ) &lt; C 1 r,1 = 6.</p><p>As a result, for large values of λ r , i.e., when we only minimize the second term of the objective function in (30), we cannot recover subspace-sparse representations of the data points. This suggests using the row-sparsity regularizer with a small value of λ r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS WITH SYNTHETIC DATA</head><p>In Section 4, we showed that the success of the 1 -minimization for subspace-sparse recovery depends on the principal angles between subspaces and the distribution of the data in each subspace. In this section, we verify this relationship through experiments on synthetic data. We consider three disjoint subspaces {S i } 3 i=1 of the same dimension d embedded in the D-dimensional ambient space. To make the problem hard enough so that every data point in a subspace can also be reconstructed as a linear combination of points in other subspaces, we generate subspace bases</p><formula xml:id="formula_38">{U i ∈ R D×d } 3 i=1</formula><p>such that each subspace lies in the direct sum of the other two subspaces, i.e., rank( U 1 U 2 U 3 ) = 2d. In addition, we generate the subspaces such that the smallest principal angles θ 12 and θ 23 are equal to θ. Thus, we can verify the effect of the smallest principal angle in the subspace-sparse recovery by changing the value of θ. To investigate the effect of the data distribution in the subspace-sparse recovery, we generate the same number of data points, N g , in each subspace at random and change the value of N g . Typically, as the number of data points in a subspace increases, the probability of the data being close to a degenerate subspace decreases. <ref type="bibr" target="#b6">7</ref> After generating three d-dimensional subspaces associated to (θ, N g ), we solve the 1 -minimization program in (4) for each data point and measure two different errors. First, denoting the sparse representation of y i ∈ S ki by c i c i1 c i2 c i3 , with c ij corresponding to points in S j , we measure the subspacesparse recovery error by</p><formula xml:id="formula_39">ssr error = 1 3N g 3Ng i=1 (1 − c iki 1 c i 1 ) ∈ [0, 1],<label>(32)</label></formula><p>where each term inside the summation indicates the fraction of the 1 -norm of c i that comes from points in other subspaces. The error being zero corresponds to y i choosing points only in its own subspace, while the error being equal to one corresponds to y i choosing points from other subspaces. Second, after building the similarity graph using the sparse coefficients and applying spectral clustering, we measure the subspace clustering error by subspace clustering error = # of misclassified points total # of points .</p><p>In our experiments, we set the dimension of the ambient space to D = 50. We change the smallest principal angle between subspaces as θ ∈ <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b59">60]</ref> degrees and change the number of points in each subspace as N g ∈ [d + 1, 32d]. For each pair (θ, N g ) we <ref type="bibr" target="#b6">7</ref>. To remove the effect of different scalings of data points, i.e., to consider only the effect of the principal angle and number of points, we normalize the data points.  <ref type="figure">Fig. 9</ref>. Left: percentage of pairs of subspaces whose smallest principal angle is smaller than a given value. Right: average percentage of data points in pairs of subspaces that have one or more of their K-nearest neighbors in the other subspace. compute the average of the errors in (32) and (33) over 100 trials (randomly generated subspaces and data points). The results for d = 4 are shown in <ref type="figure" target="#fig_6">Figure 8</ref>. Note that when either θ or N g is small, both the subspace-sparse recovery error and the clustering error are large, as predicted by our theoretical analysis. On the other hand, when θ or N g increases, the errors decrease, and for (θ, N g ) sufficiently large we obtain zero errors. The results also verify that the success of the clustering relies on the success of the 1 -minimization in recovering subspace-sparse representations of data points. Note that for small θ as we increase N g , the subspacesparse recovery error is large and slightly decreases, while the clustering error increases. This is due to the fact that increasing the number of points, the number of undesirable edges between different subspaces in the similarity graph increases, making the spectral clustering more difficult. Note also that, for the values of (θ, N g ) where the subspace-sparse recovery error is zero, i.e., points in different subspaces are not connected to each other in the similarity graph, the clustering error is also zero. This implies that, in such cases, the similarity graph has exactly three connected components, i.e., data points in the same subspace form a single component of the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS WITH REAL DATA</head><p>In this section, we evaluate the performance of the SSC algorithm in dealing with two real-world problems: segmenting multiple motions in videos ( <ref type="figure" target="#fig_0">Fig. 1</ref>) and clustering images of human faces <ref type="figure" target="#fig_1">(Fig. 2)</ref>. We compare the performance of SSC with the best stateof-the-art subspace clustering algorithms: LSA <ref type="bibr" target="#b21">[22]</ref>, SCC <ref type="bibr" target="#b27">[28]</ref>, LRR <ref type="bibr" target="#b37">[38]</ref>, and LRSC <ref type="bibr" target="#b40">[41]</ref>. Implementation details. We implement the SSC optimization algorithm in (13) using an Alternating Direction Method of Multipliers (ADMM) framework <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b63">[64]</ref> whose derivation is provided in the supplementary material. For the motion segmentation experiments, we use the noisy variation of the optimization program (13), i.e., without the term E, with the affine constraint, and choose λ z = 800/µ z in all the experiments (µ z is defined in <ref type="bibr" target="#b13">(14)</ref>). For the face clustering experiments, we use the sparse outlying entries variation of the optimization program (13), i.e., without the term Z, and choose λ e = 20/µ e in all the experiments (µ e is defined in <ref type="formula" target="#formula_0">(14)</ref>). It is also worth mentioning that SSC performs better with the ADMM approach than with general interior point solvers <ref type="bibr" target="#b48">[49]</ref>, which typically return many small nonzero coefficients, degrading the spectral clustering result.</p><p>For the state-of-the-art algorithms, we use the codes provided by their authors. For LSA, we use K = 8 nearest neighbors and dimension d = 4, to fit local subspaces, for motion segmentation and use K = 7 nearest neighbors and dimension d = 5 for face clustering. For SCC, we use dimension d = 3, for the subspaces, for motion segmentation and d = 9 for face clustering. For LRR, we use λ = 4 for motion segmentation and λ = 0.18 for face clustering. Note that the LRR algorithm according to <ref type="bibr" target="#b37">[38]</ref>, similar to SSC, applies spectral clustering to a similarity graph built directly from the solution of its proposed optimization program. However, the code of the algorithm applies a heuristic post-processing step, similar to <ref type="bibr" target="#b64">[65]</ref>, to the low-rank solution prior to building the similarity graph <ref type="bibr" target="#b39">[40]</ref>. Thus, to compare the effectiveness of sparse versus low-rank objective function and to investigate the effect of the post-processing step of LRR, we report the results for both cases of without (LRR) and with (LRR-H) the heuristic post-processing step. <ref type="bibr" target="#b7">8</ref> For LRSC, we use the method in [41, Lemma 1] with parameter τ = 420 for motion segmentation, and an ALM variant of the method in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr">Section 4.2]</ref> with parameters α = 3τ = 0.5 * (1.25/σ 1 (Y )) 2 , γ = 0.008 and ρ = 1.5 for face clustering. Finally, as LSA and SCC need to know the number of subspaces a priori and the estimation of the number of subspaces from the eigenspectrum of the graph Laplacian in the noisy setting is often unreliable, in order to have a fair comparison, we provide the number of subspaces as an input to all the algorithms.</p><p>Datasets and some statistics. For the motion segmentation problem, we consider the Hopkins 155 dataset <ref type="bibr" target="#b65">[66]</ref>, which consists of 155 video sequences of 2 or 3 motions corresponding to 2 or 3 low-dimensional subspaces in each video <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b66">[67]</ref>. For the face clustering problem, we consider the Extended Yale B dataset <ref type="bibr" target="#b67">[68]</ref>, which consists of face images of 38 human subjects, where images of each subject lie in a low-dimensional subspace <ref type="bibr" target="#b2">[3]</ref>.</p><p>Before describing each problem in detail and presenting the experimental results, we present some statistics on the two datasets that help to better understand the challenges of subspace clustering and the performance of different algorithms. First, we compute the smallest principal angle for each pair of subspaces, which in the motion segmentation problem corresponds to a pair of motions in a video and in the face clustering problem corresponds to a pair of subjects. Then, we compute the percentage of the subspace pairs whose smallest principal angle is below a certain value, which ranges from 0 to 90 degrees. <ref type="figure">Figure 9</ref> (left) shows the corresponding graphs for the two datasets. As shown, subspaces in both datasets have relatively small principal angles. In the Hopkins-155 dataset, principal angles between subspaces are always smaller than 10 degrees, while in the Extended Yale B dataset, principal angles between subspaces are between 10 and 20 degrees. Second, for each pair of subspaces, we compute the percentage of data points that have one or more of their K-nearest neighbors in the other subspace. <ref type="figure">Figure 9</ref> (right) shows the average percentages over all possible pairs of subspaces in each dataset. As shown, in the Hopkins-155 dataset for almost all data points, their few nearest neighbors belong to the same subspace. On the other hand, for the Extended Yale B dataset, there is a relatively large number of data points whose nearest neighbors come from the other subspace. This percentage rapidly increases as the number of nearest neighbors increases. As a result, from the two plots in <ref type="figure">Figure 9</ref>, we can conclude that in the Hopkins 155 dataset the challenge is that subspaces have small principal angles, while in the Extended Yale B dataset, beside the principal angles between subspaces being small, the challenge is that data points in a subspace are very close to other subspaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Motion Segmentation</head><p>Motion segmentation refers to the problem of segmenting a video sequence of multiple rigidly moving objects into multiple spatiotemporal regions that correspond to different motions in the scene <ref type="figure" target="#fig_0">(Fig. 1</ref>). This problem is often solved by extracting and tracking a set of N feature points {x f i ∈ R 2 } N i=1 through the frames f = 1, . . . , F of the video. Each data point y i , which is also called a feature trajectory, corresponds to a 2F -dimensional vector obtained by stacking the feature points x f i in the video as</p><formula xml:id="formula_41">y i x 1i x 2i · · · x F i ∈ R 2F .<label>(34)</label></formula><p>Motion segmentation refers to the problem of separating these feature trajectories according to their underlying motions. Under the affine projection model, all feature trajectories associated with a single rigid motion lie in an affine subspace of R 2F of dimension at most 3, or equivalently lie in a linear subspace of R 2F of dimension at most 4 <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b66">[67]</ref>. Therefore, feature trajectories of n rigid motions lie in a union of n low-dimensional subspaces of R 2F . Hence, motion segmentation reduces to clustering of data points in a union of subspaces.</p><p>In this section, we evaluate the performance of the SSC algorithm as well as that of state-of-the-art subspace clustering methods for the problem of motion segmentation. To do so, we consider the Hopkins 155 dataset <ref type="bibr" target="#b65">[66]</ref> that consists of 155 video sequences, where 120 of the videos have two motions and 35 of the videos have three motions. On average, in the dataset, each sequence of 2 motions has N = 266 feature trajectories and F = 30 frames, while each sequence of 3 motions has N = 398 feature trajectories and F = 29 frames. The left plot of <ref type="figure" target="#fig_0">Figure  10</ref> shows the singular values of several motions in the dataset. Note that the first four singular values are nonzero and the rest of the singular values are very close to zero, corroborating the 4dimensionality of the underlying linear subspace of each motion. <ref type="bibr" target="#b8">9</ref> In addition, it shows that the feature trajectories of each video can be well modeled as data points that almost perfectly lie in a union of linear subspaces of dimension at most 4.</p><p>The results of applying subspace clustering algorithms to the dataset when we use the original 2F -dimensional feature trajectories and when we project the data into a 4n-dimensional subspace (n is the number of subspaces) using PCA are shown in <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref>, respectively. From the results, we make the following conclusions:</p><p>-In both cases, SSC obtains a small clustering error outperforming the other algorithms. This suggests that the separation of different motion subspaces in terms of their principal angles and the distribution of the feature trajectories in each motion subspace are sufficient for the success of the sparse optimization program, hence clustering. The numbers inside parentheses show the clustering errors of SSC without normalizing the similarity matrix, i.e., without step 2 in Algorithm 1. Notice that, as explained in Remark 1, the normalization step helps to improve the clustering results. However, this improvement is small (about 0.5%), i.e., SSC performs well with or without the post-processing of C.</p><p>-Without post-processing of its coefficient matrix, LRR has higher errors than other algorithms. On the other hand, postprocessing of the low-rank coefficient matrix significantly improves the clustering performance (LRR-H).</p><p>-LRSC tries to find a noise-free dictionary for data while finding their low-rank representation. This helps to improve over LRR. Also, note that the errors of LRSC are higher than the reported ones in <ref type="bibr" target="#b40">[41]</ref>. This comes from the fact that <ref type="bibr" target="#b40">[41]</ref> has used the erroneous compacc.m function from <ref type="bibr" target="#b31">[32]</ref> to compute the errors.</p><p>-The clustering performances of different algorithms when using the 2F -dimensional feature trajectories or the 4n-dimensional PCA projections are close. This comes from the fact that the feature trajectories of n motions in a video almost perfectly lie in a 4n-dimensional linear subspace of the 2F -dimensional ambient space. Thus, projection using PCA onto a 4n-dimensional subspace preserves the structure of the subspaces and the data, <ref type="bibr" target="#b8">9</ref>. If we subtract the mean of the data points in each motion from them, the singular values drop at 3, showing the 3-dimensionality of the affine subspaces. hence, for each algorithm, the clustering error in <ref type="table" target="#tab_1">Table 1</ref> is close to the error in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>In <ref type="figure" target="#fig_0">Figure 11</ref> we show the effect of the regularization parameter λ z = α z /µ z in the clustering performance of SSC over the entire Hopkins 155 dataset. Note that the clustering errors of SSC as a function of α z follow a similar pattern using both the 2Fdimensional data and the 4n-dimensional data. Moreover, in both cases the clustering error is less than 2.5% in both cases for a large range of values of α z .</p><p>Finally, notice that the results of SSC in <ref type="table" target="#tab_1">Tables 1-2</ref> do not coincide with those reported in <ref type="bibr" target="#b34">[35]</ref>. This is mainly due to the fact in <ref type="bibr" target="#b34">[35]</ref> we used random projections for dimensionality reduction, while here we use PCA or the original 2F -dimensional data. In addition, in <ref type="bibr" target="#b34">[35]</ref> we used a CVX solver to compute a subspacesparse representation, while here we use an ADMM solver. Also, notice that we have improved the overall clustering error of LSA, for the the case of 4n-dimensional data, from 4.94%, reported in <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b34">[35]</ref>, to 4.52%. This is due to using K = 8 nearest neighbors here instead of K = 5 in [66].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Face Clustering</head><p>Given face images of multiple subjects, acquired with a fixed pose and varying illumination, we consider the problem of clustering images according to their subjects <ref type="figure" target="#fig_1">(Fig. 2)</ref>. It has been shown that, under the Lambertian assumption, images of a subject with a fixed pose and varying illumination lie close to a linear subspace of dimension 9 <ref type="bibr" target="#b2">[3]</ref>. Thus, the collection of face images of multiple subjects lie close to a union of 9-dimensional subspaces.</p><p>In this section, we evaluate the clustering performance of SSC as well as the state-of-the-art methods on the Extended Yale B dataset <ref type="bibr" target="#b67">[68]</ref>. The dataset consists of 192 × 168 pixel cropped face images of n = 38 individuals, where there are N i = 64 frontal face images for each subject acquired under various lighting conditions. To reduce the computational cost and the memory requirements of all algorithms, we downsample the images to 48×42 pixels and treat each 2, 016-dimensional vectorized image as a data point, hence, D = 2, 016. The right plot in <ref type="figure" target="#fig_0">Figure 10</ref> shows the singular values of data points of several subjects in the dataset. Note that the singular value curve has a knee around 9, corroborating the approximate 9-dimensionality of the face data in each subject. In addition, the singular values gradually decay to zero, showing that the data are corrupted by errors. Thus, the face images of n subjects can be modeled as corrupted data points lying close to a union of 9-dimensional subspaces.</p><p>To study the effect of the number of subjects in the clustering performance of different algorithms, we devise the following experimental setting: we divide the 38 subjects into 4 groups, where the first three groups correspond to subjects 1 to 10, 11 to 20, 21 to 30, and the fourth group corresponds to subjects 31 to 38. For each of the first three groups we consider all choices of n ∈ {2, 3, 5, 8, 10} subjects and for the last group we consider all choices of n ∈ {2, 3, 5, 8}. <ref type="bibr" target="#b9">10</ref> Finally, we apply clustering algorithms for each trial, i.e., each set of n subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Applying RPCA separately on each subject</head><p>As shown by the SVD plot of the face data in <ref type="figure" target="#fig_0">Figure 10 (right)</ref>, the face images do not perfectly lie in a linear subspace as they are corrupted by errors. In fact, the errors correspond to the cast shadows and specularities in the face images and can be modeled as sparse outlying entries. As a result, it is important for a subspace clustering algorithm to effectively deal with data with sparse corruptions.</p><p>In order to validate the fact that corruption of faces is due to sparse outlying errors and show the importance of dealing with corruptions while clustering, we start by the following experiment. We apply the Robust Principal Component Analysis (RPCA) algorithm <ref type="bibr" target="#b31">[32]</ref> to remove the sparse outlying entries of the face data in each subject. Note that in practice, we do not know the clustering of the data beforehand, hence cannot apply the RPCA to the faces of each subject. However, as we will show, this experiment illustrates some of the challenges of the face clustering and validates several conclusions about the performances of different algorithms. <ref type="table" target="#tab_3">Table 3</ref> shows the clustering error of different algorithms after applying RPCA to the data points in each subject and removing the sparse outlying entries, i.e., after bringing the data points back to their low-dimensional subspaces. From the results, we make the following conclusions:</p><p>-The clustering error of SSC is very close to zero for different number of subjects suggesting that SSC can deal well with face clustering if the face images are corruption free. In other words, while the data in different subspaces are very close to each other, as shown in <ref type="figure">Figure 9</ref> (right), the performance of the SSC is more dependent on the principal angles between subspaces which, while small, are large enough for the success of SSC.</p><p>-The LRR and LRSC algorithms have also low clustering errors (LRSC obtains zero errors) showing the effectiveness of removing sparse outliers in the clustering performance. On the other hand, while LRR-H has a low clustering error for 2, 3, and 5 subjects, it has a relatively large error for 8 and 10 subjects, showing that the post processing step on the obtained low-rank coefficient matrix not always improves the result of LRR.</p><p>-For LSA and SCC, the clustering error is relatively large and the error increases as the number of subjects increases. This comes from the fact that, as shown in <ref type="figure">Figure 9</ref> (right), for face images, the neighborhood of each data point contains points that belong to other subjects and, in addition, the number of neighbors from other subjects increases as we increase the number of subjects.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Applying RPCA simultaneously on all subjects</head><p>In practice, we cannot apply RPCA separately to the data in each subject because the clustering is unknown. In this section, we deal with sparse outlying entries in the data by applying the RPCA algorithm to the collection of all data points for each trial prior to clustering. The results are shown in <ref type="table" target="#tab_4">Table 4</ref> from which we make the following conclusions: -The clustering error for SSC is low for all different number of subjects. Specifically, SSC obtains 2.09% and 11.46% for clustering of data points in 2 and 10 subjects, respectively.</p><p>-Applying RPCA to all data points simultaneously may not be as effective as applying RPCA to data points in each subject separately. This comes from the fact that RPCA tends to bring the data points into a common low-rank subspace which can result in decreasing the principal angles between subspaces and decreasing the distances between data points in different subjects. This can explain the increase in the clustering error of all clustering algorithms with respect to the results in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">Using original data points</head><p>Finally, we apply the clustering algorithms to the original data points without pre-processing the data. The results are shown in <ref type="table" target="#tab_5">Table 5</ref> from which we make the following conclusions: -The SSC algorithm obtains a low clustering error for all numbers of subjects, obtaining 1.86% and 10.94% clustering error for 2 and 10 subjects, respectively. In fact, the error is smaller than when applying RPCA to all data points. This is due to the fact that SSC directly incorporates the corruption model of the data by sparse outlying entries into the sparse optimization program, giving it the ability to perform clustering on the corrupted data.</p><p>-While LRR also has a regularization term to deal with the corrupted data, the clustering error is relatively large especially as the number of subjects increases. This can be due to the fact that there is not a clear relationship between corruption of each data point and the LRR regularization term in general <ref type="bibr" target="#b37">[38]</ref>. On the other hand, the post processing step of LRR-H on the lowrank coefficient matrix helps to significantly reduce the clustering error, although it is larger than the SSC error.</p><p>-As LRSC tries to recover error-free data points while finding their low-rank representation, it obtains smaller errors than LRR.</p><p>-LSA and SCC do not have an explicit way to deal with corrupted data. This together with the fact that the face images of each subject have relatively a large number of neighbors in other subjects, as shown in <ref type="figure">Figure 9</ref> (right), result in low performances of these algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.4">Computational time comparison</head><p>The average computational time of each algorithm as a function of the number of subjects (or equivalently the number of data points) is shown in <ref type="figure" target="#fig_0">Figure 12</ref>. Note that the computational time of SCC is drastically higher than other algorithms. This comes from the fact that the complexity of SCC increases exponentially in the dimension of the subspaces, which in this case is d = 9.</p><p>On the other hand, SSC, LRR and LRSC use fast and efficient convex optimization techniques which keeps their computational time lower than other algorithms. The exact computational times are provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS AND FUTURE WORK</head><p>We studied the problem of clustering a collection of data points that lie in or close to a union of low-dimensional subspaces. We proposed a subspace clustering algorithm based on sparse representation techniques, called SSC, that finds a sparse representation of each point in the dictionary of the other points, builds a similarity graph using the sparse coefficients, and obtains the segmentation of the data using spectral clustering. We showed that, under appropriate conditions on the arrangement of subspaces and the distribution of data, the algorithm succeeds in recovering the desired sparse representations of data points.</p><p>A key advantage of the algorithm is its ability to directly deal with data nuisances, such as noise, sparse outlying entries, and missing entries as well as the more general class of affine subspaces by incorporating the corresponding models into the sparse optimization program. Experiments on real data such as face images and motions in videos showed the effectiveness of our algorithm and its superiority over the state of the art.</p><p>Interesting avenues of research, which we are currently investigating, include theoretical analysis of the subspace-sparse recovery in the presence of noise, sparse outlying entries, and missing entries in the data. As our extensive experiments on synthetic and real data show, the points in each subspace, in general, form a single component of the similarity graph. Theoretical analysis of the connectivity of the similarity graph for points in the same subspace in a probabilistic framework would provide a better understanding for this observation. Finally, making the two steps of solving a sparse optimization program and spectral clustering applicable to very large datasets is an interesting and a practical subject for the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX PROOF OF PROPOSITION 1</head><p>In this section, we prove the result of Proposition 1 in the paper regarding the optimization program</p><formula xml:id="formula_42">min C 1 + λ e E 1 + λ z 2 Z 2 F s. t. Y = Y C + E + Z, diag(C) = 0.<label>(35)</label></formula><p>The result of the proposition suggests to set the regularization parameters as</p><formula xml:id="formula_43">λ e = α e /µ e , λ z = α z /µ z ,<label>(36)</label></formula><p>where α e , α z &gt; 1 and µ e and µ z are defined as</p><formula xml:id="formula_44">µ e min i max j =i y j 1 , µ z min i max j =i |y i y j |.<label>(37)</label></formula><p>We use the following Lemma in the theoretical proof of the proposition. Proof of this Lemma can be found in <ref type="bibr" target="#b48">[49]</ref>.</p><p>Lemma 1: Consider the optimization program</p><formula xml:id="formula_45">min c 1 + λ 2 y − Ac 2 2 .<label>(38)</label></formula><p>For λ &lt; A y ∞ , we have c = 0.</p><p>Proposition 1: Consider the optimization program <ref type="bibr" target="#b34">(35)</ref>. Without the term Z, if λ e ≤ 1/µ e , then there exists at least one data point y for which in the optimal solution we have (c , e ) = (0, y ). Also, without the term E, if λ z ≤ 1/µ z , then there exists at least one data point y for which (c , z ) = (0, y ).</p><p>Proof: Note that solving the optimization program (35) is equivalent to solving N optimization programs as</p><formula xml:id="formula_46">min c i 1 + λ e e i 1 + λ z 2 z i 2 2 s. t. y i = Y c i + e i + z i , c ii = 0,<label>(39)</label></formula><p>where c i , e i , and z i are the i-th columns of C, E, and Z, repsectively.</p><p>(a) Consider the optimization program (39) without the term z i and denote the objective function value by</p><formula xml:id="formula_47">cost(c i , e i ) c i 1 + λ e e i 1 .<label>(40)</label></formula><p>Note that a feasible solution of (39) is given by (0, e i ) for which the value of the objective function is equal to cost(0, e i ) = λ e y i 1 .</p><p>On the other hand, using matrix norm properties, for any feasible solution (c i , e i ) of (39) we have</p><formula xml:id="formula_49">y i 1 = Y c i + e i 1 ≤ (max j =i y j 1 ) c i 1 + e i 1 ,<label>(42)</label></formula><p>where we used the fact that c ii = 0. Multiplying both sides of the above inequality by λ e we obtain</p><formula xml:id="formula_50">cost(0, y i ) = λ e Y 1 ≤ (λ e max j =i y j 1 ) c i 1 + λ e e i 1 ,<label>(43)</label></formula><p>Note that if λ e &lt; 1 max j =i y j 1 , then from the above equation we have</p><formula xml:id="formula_51">cost(0, y i ) ≤ cost(c i , e i ).<label>(44)</label></formula><p>In other words, (c i = 0, e i = y i ) achieve the minimum cost among all feasible solutions of <ref type="bibr" target="#b38">(39)</ref>. Hence, if λ e &lt; max i 1 max j =i y j 1 , then there exists ∈ {1, · · · , N } such that in the solution of the optimization program <ref type="bibr" target="#b34">(35)</ref> we have (c , e ) = (0, y ).</p><p>(b) Consider the optimization program (39) without the term e i , which, using z i = y i − Y c i , can be rewritten as</p><formula xml:id="formula_52">min c i 1 + λ z 2 y i − Y c i 2 2 s. t. c ii = 0.<label>(45)</label></formula><p>From Lemma 1 we have that, for λ z &lt; 1 max j =i |y j y i | , the solution of (45) is equal to c i = 0, or equivalently, the solution of (39) is given by (c i , z i ) = (0, y i ). As a result, if λ z &lt; max i 1 max j =i |y j y i | , then there exists ∈ {1, · · · , N } such that in the solution of the optimization program <ref type="bibr" target="#b34">(35)</ref> we have (c , z ) = (0, y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOF OF THEOREM 1</head><p>In this section, we prove Theorem 1 in the paper, where we showed that for data points in a union of independent subspaces, the solution of the q -minimization recovers subspace-sparse representations of data points.</p><p>Theorem 1: Consider a collection of data points drawn from n independent subspaces</p><formula xml:id="formula_53">{S i } n i=1 of dimensions {d i } n i=1 . Let Y i denote N i data points in S i , where rank(Y i ) = d i ,</formula><p>and let Y −i denote data points in all subspaces except S i . Then, for every S i and every nonzero y in S i , the q -minimization program</p><formula xml:id="formula_54">c * c * − = argmin c c − q s. t. y = [Y i Y −i ] c c − ,<label>(46)</label></formula><p>for q &lt; ∞, recovers a subspace-sparse representation, i.e., c * = 0 and c * − = 0. Proof: We prove the result using contradiction. Assume c * − = 0. Then we can write</p><formula xml:id="formula_55">y = Y i c * + Y −i c * − .<label>(47)</label></formula><p>Since y is a data point in subspace S i , there exists a c such that y = Y i c. Substituting this into (47) we get</p><formula xml:id="formula_56">Y i (c − c * ) = Y −i c * − .<label>(48)</label></formula><p>Note that the left hand side of equation <ref type="formula" target="#formula_3">(48)</ref> corresponds to a point in the subspace S i while the right hand side of (48) corresponds to a point in the subspace ⊕ j =i S j . By the independence assumption, the two subspaces S i and ⊕ j =i S j are also independent hence disjoint and intersect only at the origin. Thus, from (48) we must have Y −i c * − = 0 and from (47) we obtain y = Y i c * . In other words, c * 0 is a feasible solution of the optimization problem <ref type="bibr" target="#b45">(46)</ref>. Finally, from the assumption of c * − = 0, we have</p><formula xml:id="formula_57">c * 0 q &lt; c * c * − q<label>(49)</label></formula><p>that contradicts the optimality of c * c * − . Thus, we must have c * = 0 and c * − = 0, obtaining the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOF OF THEOREM 2</head><p>In this section, we prove Theorem 2 in the paper, where we provide a necessary and sufficient condition for subspace-sparse recovery in a union of disjoint subspaces. To do so, we consider a vector x in the intersection of S i with ⊕ j =i S j and let the optimal solution of the 1 -minimization when we restrict the dictionary to the points from S i be</p><formula xml:id="formula_58">a i = argmin a 1 s. t. x = Y i a.<label>(50)</label></formula><p>We also let the optimal solution of the 1 minimization when we restrict the dictionary to the points from all subspaces except S i be</p><formula xml:id="formula_59">a −i = argmin a 1 s. t. x = Y −i a.<label>(51)</label></formula><p>We show that if for every nonzero x in the intersection of S i with ⊕ j =i S j , the 1 -norm of the solution of (50) is strictly smaller than the 1 -norm of the solution of (51), i.e.,</p><formula xml:id="formula_60">∀ x ∈ S i ∩ (⊕ j =i S j ), x = 0 =⇒ a i 1 &lt; a −i 1 ,<label>(52)</label></formula><p>then the SSC algorithm succeeds in recovering subspace-sparse representations of all the data points in S i .</p><p>Theorem 2: Consider a collection of data points drawn from n disjoint subspaces</p><formula xml:id="formula_61">{S i } n i=1 of dimensions {d i } n i=1 . Let Y i denote N i data points in S i , where rank(Y i ) = d i , and let Y −i denote data points in all subspaces except S i . The 1 -minimization c * c * − = argmin c c − 1 s. t. y = [Y i Y −i ] c c − ,<label>(53)</label></formula><p>recovers a subspace-sparse representation of every nonzero y in S i , i.e., c * = 0 and c * − = 0, if and only if (52) holds. Proof: (⇐=) We prove the result using contradiction. Assume c * − = 0 and define</p><formula xml:id="formula_62">x y − Y i c * = Y −i c * − .<label>(54)</label></formula><p>Since y lies in S i and Y i c * is a linear combination of points in S i , from the first equality in <ref type="bibr" target="#b53">(54)</ref> we have that x is a vector in S i . Let a i be the solution of (50) for x. We have</p><formula xml:id="formula_63">x = y − Y i c * = Y i a i ⇒ y = Y i (c * + a i ).<label>(55)</label></formula><p>On the other hand, since Y −i c * − is a linear combination of points in all subspaces except S i , from the second equality in <ref type="bibr" target="#b53">(54)</ref> we have that x is a vector in ⊕ j =i S j . Let a −i be the solution of (51) for x. We have</p><formula xml:id="formula_64">x = Y −i c * − = Y −i a −i ⇒ y = Y i c * + Y −i a −i .<label>(56)</label></formula><p>Note that the left hand side of (56) together with the fact that a −i is the optimal solution of (51) imply that</p><formula xml:id="formula_65">a −i 1 ≤ c * − 1 .<label>(57)</label></formula><p>From <ref type="formula" target="#formula_4">(55)</ref> and <ref type="bibr" target="#b55">(56)</ref> we have that c * + a i 0 and c * a −i are feasible solutions of the original optimization program in <ref type="bibr" target="#b52">(53)</ref>. Thus, we have</p><formula xml:id="formula_66">c * + a i 0 1 ≤ c * 1 + a i 1 &lt; c * 1 + a −i 1 ≤ c * c * − 1 ,<label>(58)</label></formula><p>where the first inequality follows from triangle inequality, the second strict inequality follows from the sufficient condition in <ref type="bibr" target="#b51">(52)</ref>, and the last inequality follows from <ref type="bibr" target="#b56">(57)</ref>. This contradicts the optimality of c * c * − for the original optimization program in <ref type="bibr" target="#b52">(53)</ref>, hence proving the desired result. (⇐=) We prove the result using contradiction. Assume the condition in (52) does not hold, i.e., there exists a nonzero x in the intersection of S i and ⊕ j =i S j for which we have a −i 1 ≤ a i 1 . As a result, for y = x, a solution of the 1minimization program (53) corresponds to selecting points from all subspaces except S i , which contradicts the subspace-sparse recovery assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROOF OF THEOREM 3</head><p>Theorem 3: Consider a collection of data points drawn from n disjoint subspaces</p><formula xml:id="formula_67">{S i } n i=1 of dimensions {d i } n i=1 . Let W i be the set of all full-rank submatricesỸ i ∈ R D×di of Y i , where rank(Y i ) = d i . If the condition max Y i∈Wi σ di (Ỹ i ) &gt; d i Y −i 1,2 max j =i cos(θ ij )<label>(59)</label></formula><p>holds, then for every nonzero y in S i , the 1 -minimization in (53) recovers a subspace-sparse solution, i.e., c * = 0 and c * − = 0. 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>We prove the result in two steps. In step 1, we show that a i 1 ≤ β i . In step 2, we show that β −i ≤ a −i 1 . Then, the sufficient condition β i &lt; β −i establishes the result of the theorem, since it implies</p><formula xml:id="formula_68">a i 1 ≤ β i &lt; β −i ≤ a −i 1 ,<label>(60)</label></formula><p>i.e., the condition of Theorem 2 holds.</p><p>Step 1: Upper bound on the 1 -norm of (50) Let W i be the set of all submatricesỸ i ∈ R D×di of Y i that are full column rank. We can write the vector x ∈ S i ∩ (⊕ j =i S j )</p><formula xml:id="formula_69">x =Ỹ iã =⇒ã = (Ỹ iỸ i ) −1Ỹ i x.<label>(61)</label></formula><p>Using vector and matrix norm properties, we have</p><formula xml:id="formula_70">ã 1 ≤ d i ã 2 = d i (Ỹ iỸ i ) −1Ỹ i x 2 ≤ d i (Ỹ iỸ i ) −1Ỹ i 2,2 x 2 = √ d i σ di (Ỹ i ) x 2 ,<label>(62)</label></formula><p>where σ di (Ỹ i ) denotes the d i -th largest singular value ofỸ i . Thus, for the solution of the optimization problem in (50), we have</p><formula xml:id="formula_71">a i 1 ≤ miñ Y i∈Wi ã 1 ≤ miñ Y i∈Wi √ d i σ di (Ỹ i ) x 2 β i ,<label>(63)</label></formula><p>which established the upper bound on the 1 -norm of the solution of the optimization program in (50).</p><p>Step 2: Lower bound on the 1 -norm of (51) For the solution of (51) we have x = Y −i a −i . If we multiply both sides of this equation from left by x , we get</p><formula xml:id="formula_72">x 2 2 = x x = x Y −i a −i .<label>(64)</label></formula><p>Applying the Holder's inequality (|u v| ≤ u ∞ v 1 ) to the above equation, we obtain</p><formula xml:id="formula_73">x 2 2 ≤ Y −i x ∞ a −i 1 .<label>(65)</label></formula><p>11. Y −i 1,2 denotes the maximum 2 -norm of the columns of Y −i .</p><p>By recalling the definition of the smallest principal angle between two subspaces, we can write</p><formula xml:id="formula_74">x 2 2 ≤ max j =i cos(θ ij ) Y −i 1,2 x 2 a −i 1 ,<label>(66)</label></formula><p>where θ ij is the first principal angle between S i and S j and Y −i 1,2 is the maximum 2 -norm of the columns of Y −i , i.e., data points in all subspaces except S i . We can rewrite <ref type="bibr" target="#b65">(66)</ref> as</p><formula xml:id="formula_75">β −i x 2 max j =i cos(θ ij ) Y −i 1,2 ≤ a −i 1<label>(67)</label></formula><p>which establishes the lower bound on the 1 norm of the solution of the optimization program in <ref type="bibr" target="#b50">(51)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOLVING THE SPARSE OPTIMIZATION PROGRAM</head><p>Note that the proposed convex programs can be solved using generic convex solvers such as CVX 12 . However, generic solvers typically have high computational costs and do not scale well with the dimension and the number of data points.</p><p>In this section, we study efficient implementations of the proposed sparse optimizations using an Alternating Direction Method of Multipliers (ADMM) method <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b63">[64]</ref>. We fist consider the most general optimization program min (C,E,Z)</p><formula xml:id="formula_76">C 1 + λ e E 1 + λ z 2 Z 2 F s. t. Y = Y C + E + Z, C 1 = 1, diag(C) = 0,<label>(68)</label></formula><p>and present an ADMM algorithm to solve it. First, note that using the equality constraint in (68), we can eliminate Z from the optimization program and equivalently solve min (C,E)</p><formula xml:id="formula_77">C 1 + λ e E 1 + λ z 2 Y − Y C − E 2 F s. t. C 1 = 1, diag(C) = 0.<label>(69)</label></formula><p>The overall procedure of the ADMM algorithm is to introduce appropriate auxiliary variables into the optimization program, augment the constraints into the objective function, and iteratively minimize the Lagrangian with respect to the primal variables and maximize it with respect to the Lagrange multipliers. With an abuse of notation, throughout this section, we denote by diag(C) both a vector whose elements are the diagonal entries of C and a diagonal matrix whose diagonal elements are the diagonal entries of C.</p><p>To start, we introduce an auxiliary matrix A ∈ R N ×N and consider the optimization program min (C,E,A)</p><formula xml:id="formula_78">C 1 + λ e E 1 + λ z 2 Y − Y A − E 2 F s. t. A 1 = 1, A = C − diag(C).<label>(70)</label></formula><p>whose solution for (C, E) coincides with the solution of (69). As we will see shortly, introducing A helps to obtain efficient updates on the optimization variables. Next, using a parameter ρ &gt; 0, we add to the objective function of (70) two penalty terms <ref type="bibr" target="#b11">12</ref>. CVX is a Matlab-based software for convex programming and can be downloaded from http://cvxr.com. Algorithm 2 : Solving (68) via an ADMM Algorithm Initialization: Set maxIter = 10 4 , k = 0, and Terminate ← False. Initialize C (0) , A (0) , E (0) , δ (0) , and ∆ (0) to zero. <ref type="bibr">1:</ref> while (Terminate == False) do <ref type="bibr">2:</ref> update A (k+1) by solving the following system of linear equations (λ z Y Y + ρI + ρ11 )A (k+1) = λ z Y (Y − E (k) ) + ρ(11 + C (k) ) − 1δ (k) − ∆ (k) , <ref type="bibr">3:</ref> update C (k+1) as C (k+1) = J − diag(J ), where J T 1 ρ (A (k+1) + ∆ (k) /ρ), <ref type="bibr">4:</ref> update E (k+1) as E (k+1) = T λe λz (Y − Y A (k+1) ), <ref type="bibr">5:</ref> update δ (k+1) as δ (k+1) = δ (k) + ρ (A (k+1) 1 − 1), <ref type="bibr">6:</ref> update ∆ (k+1) as ∆ (k+1) = ∆ (k) + ρ (A (k+1) − C (k+1) ), <ref type="bibr">7:</ref> k ← k + 1, <ref type="bibr">8:</ref> if ( A (k) 1 − 1 ∞ ≤ and A (k) − C (k) ∞ ≤ and A (k) − A (k−1) ∞ ≤ and E (k) − E (k−1) ∞ ≤ or (k ≥ maxIter) then corresponding to the constraints A 1 = 1 and A = C −diag(C) and consider the following optimization program min (C,E,A)</p><formula xml:id="formula_79">C 1 + λ e E 1 + λ z 2 Y − Y A − E 2 F + ρ 2 A 1 − 1 2 2 + ρ 2 A − (C − diag(C)) 2 F s. t. A 1 = 1, A = C − diag(C).<label>(71)</label></formula><p>Note that adding the penalty terms to (70) do not change its optimal solution, i.e., both (70) and (71) have the same solutions, since for any feasible solution of (71) that satisfies the constraints, the penalty terms vanish. However, adding the penalty terms makes the objective function strictly convex in terms of the optimization variables (C, E, A), which allows using the ADMM approach. Introducing a vector δ ∈ R N and and a matrix ∆ ∈ R N ×N of Lagrange multipliers for the two equality constraints in (71), we can write the Lagrangian function of (71) as L(C, A, E, δ,</p><formula xml:id="formula_80">∆) = C 1 + λ e E 1 + λ z 2 Y − Y A − E 2 F + ρ 2 A 1 − 1 2 2 + ρ 2 A − (C − diag(C)) 2 F + δ (A 1 − 1) + tr(∆ (A − C + diag(C))),<label>(72)</label></formula><p>where tr(·) denotes the trace operator of a given matrix. The ADMM approach then consists of an iterative procedure as follows: Denote by (C (k) , E (k) , A (k) ) the optimization variables at iteration k, and by (δ (k) , ∆ (k) ) the Lagrange multipliers at iteration k and • Obtain A (k+1) by minimizing L with respect to A, while (C (k) , E (k) , δ (k) , ∆ (k) ) are fixed. Note that computing the derivative of L with respect to A and setting it to zero, we obtain</p><formula xml:id="formula_81">(λ z Y Y + ρI + ρ11 )A (k+1) = λ z Y (Y − E (k) ) + ρ(11 + C (k) ) − 1δ (k) − ∆ (k) .<label>(73)</label></formula><p>In other words, A (k+1) is obtained by solving an N × N system of linear equations. When N is not very large, one can simply matrix inversion to obtain A (k+1) from (73).</p><p>For large values of N , conjugate gradient methods should be employed to solve for A (k+1) . • Obtain C (k+1) by minimizing L with respect to C, while (A (k) , E (k) , δ (k) , ∆ (k) ) are fixed. Note that the update on C also has a closed-form solution given by</p><formula xml:id="formula_82">C (k+1) = J − diag(J ), (74) J T 1 ρ (A (k+1) + ∆ (k) /ρ),<label>(75)</label></formula><p>where T η (·) is the shrinkage-thresholding operator acting on each element of the given matrix, and is defined as</p><formula xml:id="formula_83">T η (v) = (|v| − η) + sgn(v).<label>(76)</label></formula><p>The operator (·) + returns its argument if it is non-negative and returns zero otherwise. • Obtain E (k+1) by minimizing L with respect to E, while (C (k+1) , A (k+1) , δ (k) , ∆ (k) ) are fixed. The update on E can also be computed in closed-form as</p><formula xml:id="formula_84">E (k+1) = T λe λz (Y A (k+1) − Y ),<label>(77)</label></formula><p>• Having (C (k+1) , A (k+1) , E (k+1) fixed, perform a gradient ascent update with the step size of ρ on the Lagrange multipliers as</p><formula xml:id="formula_85">δ (k+1) = δ (k) + ρ (A (k+1) 1 − 1),<label>(78)</label></formula><formula xml:id="formula_86">∆ (k+1) = ∆ (k) + ρ (A (k+1) − C (k+1) ).<label>(79)</label></formula><p>These three steps are repeated until convergence is achieved or the number of iterations exceeds a maximum iteration number. Convergence is achieved when we have A (k) 1 − 1 ∞ ≤ ,</p><formula xml:id="formula_87">A (k) − C (k) ∞ ≤ , A (k) − A (k−1) ∞ ≤ and E (k) − E (k−1)</formula><p>∞ ≤ , where denotes the error tolerance for the primal and dual residuals. In practice, the choice of = 10 −4 works well in real experiments. In summary, Algorithm 2 shows the updates for the ADMM implementation of the optimization program (68).  subjects. Note that these computational times are based on the codes of the algorithms used by their authors. It is important to mention that LRR and SSC can be implemented using faster optimization solvers. More specifically, LRR can be made faster using LADMAP method proposed in: "Z. Lin and R. Liu and Z. Su, Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation, NIPS 2011." Also, SSC can be made faster using LADM method proposed in "J. Yang and Y. Zhang. Alternating direction algorithms for 1 problems in compressive sensing. SIAM J. Scientific Computing, 2010."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COMPUTATIONAL TIME COMPARISON</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Motion segmentation: given feature points on multiple rigidly moving objects tracked in multiple frames of a video (top), the goal is to separate the feature trajectories according to the moving objects (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Face clustering: given face images of multiple subjects (top), the goal is to find images that belong to the same subject (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Coefficient matrix obtained from the solution of (30) for data points in two subspaces. Left: λr = 0. Right: λr = 10. Increasing λr results in concentration of the nonzero elements in a few rows of the coefficient matrix, hence choosing a few common data points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Left: three 1-dimensional subspaces in R 2 with normalized data points. Middle: C1 corresponds to the solution of (30) for λr = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Subspace-sparse recovery error (left) and subspace clustering error (right) for three disjoint subspaces. Increasing the number of points or smallest principal angle decreases the errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Left: singular values of several motions in the Hopkins 155 dataset. Each motion corresponds to a subspace of dimension at most 4. Right: singular values of several faces in the Extended Yale B dataset. Each subject corresponds to a subspace of dimension around 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Clustering error (%) of SSC as a function of αz in the regularization parameter λz = αz/µz for the two cases of clustering of 2F -dimensional data and 4n-dimensional data obtained by PCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Average computational time (sec.) of the algorithms on the Extended Yale B dataset as a function of the number of subjects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Optimal sparse coefficient matrix C * = C (k) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is with the Department of Electrical Engineering and Computer Science, University of California, Berkeley, USA. E-mail: ehsan@eecs.berkeley.edu. • R. Vidal is with the Center for Imaging Science and the Department of Biomedical</figDesc><table /><note>Engineering, The Johns Hopkins University, USA. E-mail: rvi- dal@cis.jhu.edu.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Clustering error (%) of different algorithms on the Hopkins 155 dataset with the 2F -dimensional data points.</figDesc><table><row><cell cols="4">Algorithms LSA SCC LRR LRR-H LRSC</cell><cell>SSC</cell></row><row><cell>2 Motions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>4.23 2.89 4.10</cell><cell>2.13</cell><cell cols="2">3.69 1.52 (2.07)</cell></row><row><cell>Median</cell><cell>0.56 0.00 0.22</cell><cell>0.00</cell><cell cols="2">0.29 0.00 (0.00)</cell></row><row><cell>3 Motions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>7.02 8.25 9.89</cell><cell>4.03</cell><cell cols="2">7.69 4.40 (5.27)</cell></row><row><cell>Median</cell><cell>1.45 0.24 6.22</cell><cell>1.43</cell><cell cols="2">3.80 0.56 (0.40)</cell></row><row><cell>All</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>4.86 4.10 5.41</cell><cell>2.56</cell><cell cols="2">4.59 2.18 (2.79)</cell></row><row><cell>Median</cell><cell>0.89 0.00 0.53</cell><cell>0.00</cell><cell cols="2">0.60 0.00 (0.00)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Clustering error (%) of different algorithms on the Hopkins 155 dataset with the 4n-dimensional data points obtained by applying PCA.</figDesc><table><row><cell cols="4">Algorithms LSA SCC LRR LRR-H LRSC</cell><cell>SSC</cell></row><row><cell>2 Motions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>3.61 3.04 4.83</cell><cell>3.41</cell><cell cols="2">3.87 1.83 (2.14)</cell></row><row><cell>Median</cell><cell>0.51 0.00 0.26</cell><cell>0.00</cell><cell cols="2">0.26 0.00 (0.00)</cell></row><row><cell>3 Motions</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>7.65 7.91 9.89</cell><cell>4.86</cell><cell cols="2">7.72 4.40 (5.29)</cell></row><row><cell>Median</cell><cell>1.27 1.14 6.22</cell><cell>1.47</cell><cell cols="2">3.80 0.56 (0.40)</cell></row><row><cell>All</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>4.52 4.14 5.98</cell><cell>3.74</cell><cell cols="2">4.74 2.41 (2.85)</cell></row><row><cell>Median</cell><cell>0.57 0.00 0.59</cell><cell>0.00</cell><cell cols="2">0.58 0.00 (0.00)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Clustering error (%) of different algorithms on the Extended Yale B dataset after applying RPCA separately to the data points in each subject.</figDesc><table><row><cell cols="5">Algorithm LSA SCC LRR LRR-H LRSC SSC</cell></row><row><cell>2 Subjects</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell cols="2">6.15 1.29 0.09</cell><cell>0.05</cell><cell>0.00 0.06</cell></row><row><cell>Median</cell><cell>0.00</cell><cell>0.00 0.00</cell><cell>0.00</cell><cell>0.00 0.00</cell></row><row><cell>3 Subjects</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell cols="2">11.67 19.33 0.12</cell><cell>0.10</cell><cell>0.00 0.08</cell></row><row><cell>Median</cell><cell cols="2">2.60 8.59 0.00</cell><cell>0.00</cell><cell>0.00 0.00</cell></row><row><cell>5 Subjects</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell cols="2">21.08 47.53 0.16</cell><cell>0.15</cell><cell>0.00 0.07</cell></row><row><cell cols="3">Median 19.21 47.19 0.00</cell><cell>0.00</cell><cell>0.00 0.00</cell></row><row><cell>8 Subjects</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell cols="3">30.04 64.20 4.50 11.57</cell><cell>0.00 0.06</cell></row><row><cell cols="4">Median 29.00 63.77 0.20 15.43</cell><cell>0.00 0.00</cell></row><row><cell>10 Subjects</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell cols="3">35.31 63.80 0.15 13.02</cell><cell>0.00 0.89</cell></row><row><cell cols="4">Median 30.16 64.84 0.00 13.13</cell><cell>0.00 0.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Clustering error (%) of different algorithms on the Extended Yale B dataset after applying RPCA simultaneously to all the data in each trial. 53.05 19.92 15.33 10.99 6.79 Median 56.87 51.25 19.38 15.94 10.94 5.31 8 Subjects Mean 62.32 66.27 31.39 28.67 16.14 10.28 Median 62.50 64.84 33.30 31.05 14.65 9.57 10 Subjects Mean 62.40 63.07 35.89 32.55 21.82 11.46 Median 62.50 60.31 34.06 30.00 25.00 11.09</figDesc><table><row><cell cols="4">Algorithm LSA SCC LRR LRR-H LRSC SSC</cell></row><row><cell>2 Subjects</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>32.53 9.29 7.27</cell><cell>5.72</cell><cell>5.67 2.09</cell></row><row><cell cols="2">Median 47.66 7.03 6.25</cell><cell>3.91</cell><cell>4.69 0.78</cell></row><row><cell>3 Subjects</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell cols="2">53.02 32.00 12.29 10.01</cell><cell>8.72 3.77</cell></row><row><cell cols="3">Median 51.04 37.50 11.98 9.38</cell><cell>8.33 2.60</cell></row><row><cell>5 Subjects</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>58.76</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Clustering error (%) of different algorithms on the Extended Yale B dataset without pre-processing the data. 66.11 41.19 14.34 23.72 5.85 Median 58.59 64.65 43.75 10.06 28.03 4.49 10 Subjects Mean 60.42 73.02 38.85 22.92 30.36 10.94 Median 57.50 75.78 41.09 23.59 28.75 5.63</figDesc><table><row><cell cols="4">Algorithm LSA SCC LRR LRR-H LRSC SSC</cell></row><row><cell>2 Subjects</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>32.80 16.62 9.52</cell><cell>2.54</cell><cell>5.32 1.86</cell></row><row><cell cols="2">Median 47.66 7.82 5.47</cell><cell>0.78</cell><cell>4.69 0.00</cell></row><row><cell>3 Subjects</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell cols="2">52.29 38.16 19.52 4.21</cell><cell>8.47 3.10</cell></row><row><cell cols="3">Median 50.00 39.06 14.58 2.60</cell><cell>7.81 1.04</cell></row><row><cell>5 Subjects</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell cols="2">58.02 58.90 34.16 6.90</cell><cell>12.24 4.31</cell></row><row><cell cols="3">Median 56.87 59.38 35.00 5.63</cell><cell>11.25 2.50</cell></row><row><cell>8 Subjects</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>59.19</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 shows</head><label>6</label><figDesc>the computational time of different algorithms on the Extended Yale B dataset as a function of the number of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Average computational time (sec.) of the algorithms on the Extended Yale B dataset as a function of the number of subjects. Subjects 180.2 1020.5 22.1 16.3 42.6 10 Subjects 405.3 1439.8 255.0 96.9 160.3</figDesc><table><row><cell></cell><cell>LSA</cell><cell>SCC</cell><cell cols="3">LRR LRSC SSC</cell></row><row><cell>2 Subjects</cell><cell>5.2</cell><cell>262.8</cell><cell>1.6</cell><cell>1.1</cell><cell>1.8</cell></row><row><cell cols="3">3 Subjects 13.4 451.5</cell><cell>2.2</cell><cell>1.9</cell><cell>3.29</cell></row><row><cell cols="3">5 Subjects 62.7 630.3</cell><cell>7.6</cell><cell>5.7</cell><cell>11.4</cell></row><row><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">. The original published code of LRR contains the function "compacc.m" for computing the misclassification rate, which is erroneous. We have used the correct code for computing the misclassification rate and as a result, the reported performance for LRR-H is different from the published results in<ref type="bibr" target="#b37">[38]</ref> and<ref type="bibr" target="#b39">[40]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">. Note that choosing n out of 38 leads to extremely large number of trials. Thus, we have devised the above setting in order to have a repeatable experiment with a reasonably large number of trials for each n.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the financial support of grants nsf-iss 0447739 and nsf-csn 0931805.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape and motion from image streams under orthography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lambertian reflection and linear subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="218" to="233" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Metrics and models for handwritten character recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="65" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="52" to="68" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-scale hybrid linear models for lossy image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3655" to="3671" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of natural images via lossy data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="212" to="225" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multibody factorization method for independently moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Motion segmentation by subspace separation and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kanatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalized Principal Component Analysis (GPCA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2004-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nearest q-flat to m points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="249" to="252" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clustering appearances of objects under varying illumination conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Median k-flats for hybrid linear modeling with many outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Subspace Methods</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multibody grouping from motion images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Gear</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="150" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimation of subspace arrangements with applications in modeling and segmenting mixed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Derksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fossum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mixtures of probabilistic principal component analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="482" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric structure of degeneracy for multibody motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kanatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Methods in Video Processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multibody factorization with uncertainty and missing data using the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="707" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RANSAC random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Motion segmentation in the presence of outlying, incomplete, or corrupted trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="94" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Segmenting motions of different types by unsupervised manifold clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hybrid linear modeling via local best-fit flats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1927" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Degeneracies, dependencies and their implications in multi-body and multi-sequence factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="287" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On spectral clustering: analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral curvature clustering (SCC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">For most large underdetermined systems of linear equations the minimal 1 -norm solution is also the sparsest solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="797" to="829" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoding by linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4203" to="4215" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the LASSO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parrilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="501" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Clustering disjoint subspaces via sparse representation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A geometric analysis of subspace clustering with outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Latent low-rank representation for subspace segmentation and feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A closed form solution to robust subspace estimation and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust classification using structured sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Block-sparse recovery via convex optimization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recovering sparse signals using sparse measurement matrices in compressed dna microarrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Parvaresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vikalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="285" />
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the reconstruction of blocksparse signals with and optimal number of measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Parvaresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3075" to="3085" />
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust recovery of signals from a structured union of subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mishali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5302" to="5316" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Compressed sensing of blocksparse signals: Uncertainty relations and efficient recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuppinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bolcskei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3042" to="3054" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An interiorpoint method for large-scale l1-regularized least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorinevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="606" to="617" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="page" from="237" to="260" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Doubly stochastic normalization for spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Motion segmentation with missing data by Pow-erFactorization and Generalized PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="310" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Optimally sparse representation in general (nonorthogonal) dictionaries via 1 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2197" to="2202" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sparse representations in unions of bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3320" to="3325" />
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Theoretical and empirical results for recovery from multiple measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Friedlander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2516" to="2527" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Neighborly polytopes and sparse solution of underdetermined linear equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Graph connectivity in sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nasihatkon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">See all by looking at a few: Sparse modeling for finding representative objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Structured variable selection with sparsity-inducing norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2777" to="2824" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Signal Processing, special issue &quot;Sparse approximations in signal and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="589" to="602" />
		</imprint>
	</monogr>
	<note>Algorithms for simultaneous sparse approximation. part ii: Convex relaxation</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A dual algorithm for the solution of nonlinear variational problems via finite-element approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mercier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Math. Appl</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="17" to="40" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Spectral clustering of linear subspaces for motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schnörr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A benchmark for the comparison of 3-D motion segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Factorization-based segmentation of motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Motion Understanding</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="179" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
