<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STD: Sparse-to-Dense 3D Object Detector for Point Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Tencent</roleName><forename type="first">Youtu</forename><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">STD: Sparse-to-Dense 3D Object Detector for Point Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new two-stage 3D object detection framework, named sparse-to-dense 3D Object Detector (STD). The first stage is a bottom-up proposal generation network that uses raw point cloud as input to generate accurate proposals by seeding each point with a new spherical anchor. It achieves a high recall with less computation compared with prior works. Then, PointsPool is applied for generating proposal features by transforming their interior point features from sparse expression to compact representation, which saves even more computation time. In box prediction, which is the second stage, we implement a parallel intersection-over-union (IoU) branch to increase awareness of localization accuracy, resulting in further improved performance. We conduct experiments on KITTI dataset, and evaluate our method in terms of 3D object and Bird's Eye View (BEV) detection. Our method outperforms other stateof-the-arts by a large margin, especially on the hard set, with inference speed more than 10 FPS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D scene understanding with point cloud is a very important topic in computer vision, since it benefits many applications, such as autonomous driving <ref type="bibr" target="#b7">[8]</ref> and augmented reality <ref type="bibr" target="#b23">[24]</ref>. In this work, we focus on one essential 3D scene recognition task, object detection based on point cloud, which predicts the 3D bounding box and class label for each object in the scene.</p><p>Compared to RGB images, LiDAR point cloud has its own unique properties. On the one hand, they provide structural and spatial information of relative location and precise depth. On the other hand, they are unordered, sparse and locality sensitive, which brings difficulties in parsing raw LiDAR point cloud.</p><p>Most existing work transforms sparse point clouds to compact representations by projecting it to images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7]</ref> or subdividing it into equally distributed voxels <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35]</ref>. CNNs can be applied for parsing the point cloud. Nevertheless, these hand-crafted representa-tions may not be optimal. Instead of converting irregular point cloud to voxels, Qi et al. proposes PointNet <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> to directly operate on raw LiDAR point clouds for classification and semantic segmentation. Two streams of methods tackling 3D object detection follow. One is based on voxels, e.g., VoxelNet <ref type="bibr" target="#b36">[37]</ref> and SECOND <ref type="bibr" target="#b33">[34]</ref>, where voxelization is conducted on the entire point cloud, PointNet is applied to each voxel for feature extraction and CNNs are used for final bounding-box prediction. Although efficient, information loss is inevitable, degrading localization quality. The other stream is point-based, like F-PointNet <ref type="bibr" target="#b25">[26]</ref> and PointRCNN <ref type="bibr" target="#b29">[30]</ref>. They take raw point cloud data as input, and generate final predictions by PointNet++ <ref type="bibr" target="#b27">[28]</ref>. These methods achieve better performance with uncontrollable receptive fields and large computation cost.</p><p>Our Contributions Different from all previous methods, we propose a two-stage 3D object detection framework. In the first stage, we take each point in the point cloud as an element, and seed them with appropriate spherical anchors, aiming to preserve accurate location information. Then a PointNet++ backbone is applied for extracting semantic context feature for each point as well as generating objectness score to filter anchors.</p><p>To generate feature for each proposal, we propose the PointsPool layer by gathering canonical coordinates and semantic features of their interior points, retaining accurate localization and context information. This layer transforms sparse and unordered point-wise expression to more compact features, enabling utilization of efficient CNNs and end-to-end training. Final prediction is achieved in the second stage. Instead of predicting the box location and class label with a simple head, we propose to augment a novel 3D IoU branch for predicting 3D IoU between predictions and ground-truth bounding boxes to alleviate inappropriate removal during post-processing. We evaluate our model on KITTI dataset <ref type="bibr" target="#b0">[1]</ref>. Experiments show that our model outperforms other state-of-thearts in terms of both BEV and 3D object detection tasks, especially for difficult examples. Our primary contribution is manifold.</p><p>• We propose a point-based proposal generation paradigm for object detection on point cloud with spherical anchors. It is generic to achieve high recall.</p><p>• The proposed PointsPool layer integrates advantages of both point-and voxel-based methods, enabling efficient and effective prediction.</p><p>• Our new 3D IoU prediction branch helps alignment between classification score and localization, leading to notable improvement. Experimental results on KITTI dataset show that our framework handles many challenging cases with high occlusion and crowdedness, and achieves new state-of-the-art performance. Moreover, with our design, the good performance is achieved at a 10 FPS speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Semantic Segmentation There are several approaches to tackle semantic segmentation on point cloud. In <ref type="bibr" target="#b32">[33]</ref>, a projection function converts LiDAR points to a UV map, which is then classified by 2D semantic segmentation <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3]</ref> in pixel level. In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>, a multi-view-based function produces the segmentation mask. This method fuses information from different views. Other solutions, such as <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref>, segment point cloud from raw LiDAR data. They directly generate features on each point while keeping original structural information. A max-pooling method gathers the global feature. It is then concatenated with local feature for processing.</p><p>3D Object Detection There are three different lines for 3D object detection. They are multi-view, voxel, and pointbased methods.</p><p>For multi-view methods, MV3D <ref type="bibr" target="#b3">[4]</ref> projects LiDAR point cloud to BEV and trains a Region Proposal Network (RPN) to generate positive proposals. It merges features from BEV, image view and front view in order to generate refined 3D bounding boxes. AVOD <ref type="bibr" target="#b13">[14]</ref> improves MV3D by fusing image and BEV features like <ref type="bibr" target="#b19">[20]</ref>. Unlike MV3D, which only merges features in the refinement phase, it also merges features from multiple views in the RPN phase to generate positive proposals. These methods still have the limitation when detecting small objects such as pedestrians and cyclists. They do not deal with cases with multiple objects in depth direction.</p><p>There are several LiDAR-data based 3D object detection frameworks using voxel-grid representation. In <ref type="bibr" target="#b31">[32]</ref>, each non-empty voxel is encoded with 6 statistical quantities by the points within this voxel. Binary encoding is used in <ref type="bibr" target="#b15">[16]</ref> for each voxel grid. In PIXOR <ref type="bibr" target="#b34">[35]</ref>, each voxel grid is encoded as occupancy. All of these methods use handcrafted representation. VoxelNet <ref type="bibr" target="#b36">[37]</ref> instead stacks many VFE layers to generate machine-learned representation for each voxel. Compared to <ref type="bibr" target="#b36">[37]</ref>, SECOND <ref type="bibr" target="#b33">[34]</ref> uses sparse convolution layers <ref type="bibr" target="#b9">[10]</ref> for parsing the compact representation. PointPillars <ref type="bibr" target="#b14">[15]</ref> uses pseudo-images as the representation after voxelization. F-PointNet <ref type="bibr" target="#b25">[26]</ref> is the first method of utilizing raw point cloud to predict 3D objects. It uses frustum proposals from 2D object detection as candidate boxes and regresses predictions based on interior points. Therefore, performance heavily relies on the 2D object detector. Differently, PointRCNN <ref type="bibr" target="#b29">[30]</ref> uses the whole point cloud for proposal generation rather than 2D images. It directly uses the segmentation score of proposal's centric point for classification considering proposal location information. Other features like size and orientation are neglected. In contrast, our design is general to utilize the strong representation power of point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Framework</head><p>Our method is a two-stage 3D object detection framework exploiting advantages of voxel-and point-based methods. To generate accurate point-based proposals, we design spherical anchors and a new strategy in assigning labels to anchors. For each generated proposal, we deploy a new PointsPool layer to convert point-based features from sparse expression to dense representation. A box prediction network is applied for final prediction. The framework is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proposal Generation Module</head><p>Existing methods of 3D object detection mainly project point cloud to different views or divide them into voxels for utilizing CNNs. We instead design a generic strategy to seed anchors based on each point independently, which is the elementary component in the point cloud. Then features of interior points of each anchor are utilized to generate proposals. With this structure, we keep sufficient context information, achieving decent recall even with a small number of proposals.</p><p>Challenge Albeit elegant, point-based frameworks inevitably face many challenges. For example, the amount of points is prohibitively huge where high redundancy exists in anchors. They cost much computation during training and inference. Also, the way to assign ground-truth labels for anchors needs to be specially designed.</p><p>Spherical Anchor The first step of proposal generation module is to reasonably seed anchors for each point. Considering that a 3D object could be with any orientations, we design spherical anchors rather than traditional cuboid anchors. For each spherical anchor, it is with a spherical receptive field parametrized by the class-specific radius (i.e., 2-meter radius for car, and 1-meter radius for pedestrian and cyclist). Now the proposal predicted by each anchor is based on the points in the spherical receptive field. Each anchor is associated with a reference box for proposal generation, with pre-defined size. These anchors are located at the center of each point. Different from traditional anchor schemes, we do not pre-define the orientation of the reference box. It is instead directly predicted. As a result, the number of spherical anchors is not proportional to the number of pre-defined reference box orientation, leading to about 50% less anchors. With computation much reduced, we surprisingly achieve a much higher recall with spherical anchors than with traditional ones. This step reduces the amount of anchors to about 16K. To further compress them, we use a 3D semantic segmentation network to predict the class of each point and produce semantic feature for each point. It is followed by nonmaximal suppression (NMS) to remove redundant anchors. The final score of each anchor is the segmentation score on the center point. The IoU value is calculated based on the projection of each anchor to the BEV. With these operations, we reduce the number of anchors to around only 500.</p><p>Proposal Generation Network These computed useful anchors lead to accurate proposals. Inspired by PointNet <ref type="bibr" target="#b26">[27]</ref> in 3D classification, we gather 3D points within anchors for regression and classification. For points in an anchor, we pass their (X, Y, Z) locations, which are normalized by the anchor center coordinates, and semantic features from the segmentation network to a PointNet with several convolutional layers to predict classification scores, regression offsets and orientations. Details of the 3D segmentation networks and PointNet are illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>.</p><p>Then we compute offsets regarding anchor center coordinates (A x , A y , A z ) and their pre-defined sizes (A l , A w , A h ) so as to obtain precise proposals. The predefined size for "car", "cyclist" and "pedestrian" are (A l = 3.9,</p><formula xml:id="formula_0">A w = 1.6, A h = 1.6), (A l = 1.6, A w = 0.8, A h = 1.6) and (A l = 0.8, A w = 0.8, A h = 1.6), respectively.</formula><p>For angle prediction, we use a hybrid of classification and regression formulation following <ref type="bibr" target="#b25">[26]</ref>. That is, we predefine N a as equally split angle bins and classify the proposal angle into different bins. Residual is regressed with respect to the bin value. N a is set to 12 in our experiments. Finally, we apply NMS based on classification score and oriented BEV IoU to eliminate redundant proposals. Specifically, we keep up to 300 proposals during training and 100 for testing. Assignment Strategy Given that our anchors are with spherical receptive fields rather than cubes or cuboids, it is not appropriate to assign positive or negative labels according to traditional IoU calculation <ref type="bibr" target="#b36">[37]</ref> between the spherical receptive field and ground-truth boxes. We design a new criterion named PointsIoU to assign target labels. PointsIoU is defined as the quotient between the number of points in the intersection area of both regions and the number of points in the union area of both regions. An anchor is considered positive if its PointsIoU with a certain ground-truth box is higher than 0.55 and negative otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposal Feature Generation</head><p>With semantic features from the segmentation network for each point and refined proposals, we constitute compact features for each proposal.   Motivation For each proposal, the most straight-forward way to make final prediction is to perform PointNet++ based on interior points <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26]</ref>. Albeit simple, several operations such as set abstraction (SA) are computational expensive compared to traditional convolution or fully connected (FC) layers. As illustrated in <ref type="table" target="#tab_1">Table 1</ref>, with 100 proposals, PointNet++ baseline takes 41ms during inference, compared with 16ms with pure FC layers. It is almost 2.5× faster than the baseline, with only 0.4% performance drop. Moreover, compared to PointNet baseline, the model with FC layers yields 1.6% performance increase with only 6 extra milliseconds. It is because PointNet regression head uses less local information.</p><p>We apply a voxelization layer at this stage, named PointsPool, to compute compact proposal features that can be used in efficient FC layers for final predictions. Compared to voxelization in <ref type="bibr" target="#b36">[37]</ref>, this new layer is a gradientconductive voxelization layer, enabling end-to-end training.</p><p>PointsPool Layer PointsPool layer is composed of three steps. In the first step, we randomly choose N interior points for each proposal with their canonical coordinates and semantic features as initial feature. For each proposal, we obtain point canonical locations by subtracting the proposal center (X, Y, Z) values and rotating them to the proposal predicted orientation. These canonized coordinates enable the model to be robust under geometrical transformation, and be aware of inner points' relative locations for better performance than only using semantic features.</p><p>The second step is using the voxelization layer to subdivide each proposal into equally spaced voxels as <ref type="bibr" target="#b36">[37]</ref>. Specifically, we partition each proposal to (d l = 6, d w = 6, d h = 6) voxels. N r = 35 points are randomly sampled for each voxel. Concatenated features of canonical coordinates and semantic features of these points are used for each voxel. Compared to voxelization in <ref type="bibr" target="#b36">[37]</ref>, this layer has gradient representation, making it possible for end-to-end training. While passing gradients, we only need to pass  <ref type="table">Table 2</ref>. 3D object detection AP on KITTI val set. We conduct experiments to show importance of the post process. "Score-NMS" means using classification scores as NMS sorting scores. "IoU-NMS" means for each prediction, we use its largest IoU among all ground-truth boxes as the sorting score.</p><p>gradients of these randomly selected points. Finally, we apply a Voxel Feature Encoding (VFE) layer with channels (128, 128, 256) <ref type="bibr" target="#b36">[37]</ref> for extracting features of each voxel, so as to generate features of proposals with shapes (d l × d w × d h × 256). After getting features of each proposal, we flatten them for following FC layers in the box prediction head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Box Prediction Network</head><p>Our box prediction network has two branches for box estimation and IoU estimation respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Box Estimation Branch</head><p>In this branch, we use 2 FC layers with channels (512, 512) to extract features of each proposal. Then another 2 FC layers are applied for classification and regression respectively. We directly regress offsets between the ground-truth box and proposals, parametrized by (t l , t w , t h ). We further predict the shift (t x , t y , t z ) from proposal center to the ground-truth. As for angle prediction, we still use a hybrid of classification and regression formulation, same as the one described in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IoU Estimation Branch</head><p>In previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>, NMS is applied to results of box estimation to remove duplicate predictions. The classification score is used for ranking during NMS. Noted in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>, the classification scores of boxes are not highly correlated with the localization quality. Similarly, weak correlation between classification score and box quality affects point-based object detection tasks. Given that LiDAR for autonomous driving is usually gathered at a fixed angle, and objects are partially covered, localization accuracy is extremely sensitive to relative position between visible part and its full view while the classification branch cannot provide enough information. As shown in <ref type="table">Table 2</ref>, if we feed the oracle IoU value of each predicted box instead of the classification score to NMS for duplicate removal, the performance increases by around 12.6%.</p><p>Based on this fact, we develop an IoU estimation branch for predicting 3D IoU between boxes and corresponding ground-truth. Then, we multiply each box's classification score with its 3D IoU as a new sorting criterion. This design relieves the discrepancy between localization accuracy and classification score, effectively improving the final performance. Moreover, this IoU estimation branch is general and can be applied to other 3D object detectors. We expect similar performance improvement on other frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>We use a multi-task loss to train our network. Our total loss is composed of proposal generation loss L prop and box prediction loss L box as</p><formula xml:id="formula_1">L total = L prop + L box .<label>(1)</label></formula><p>The proposal generation loss is the summation of 3D semantic segmentation loss and proposal prediction loss. We use focal loss <ref type="bibr" target="#b20">[21]</ref> as segmentation loss L seg , keeping the original parameters α t = 0.25 and γ = 2. The prediction loss consists of proposal classification loss and regression loss. The overall proposal generation loss is defined as Eq. (2). s i and u i are the predicted classification score and ground-truth label for anchor i, respectively. N cls and N pos are the number of anchors and positive samples.</p><formula xml:id="formula_2">L prop = L seg + 1 N cls i L cls (s i , u i ) + λ 1 N pos i [u i ≥ 1](L loc + L ang ),<label>(2)</label></formula><p>where the Iverson bracket indicator function [u i ≥ 1] reaches 1 when u i ≥ 1 and 0 otherwise. L cls is simply the softmax cross-entropy loss. We parameterize an anchor A by its center (A x , A y , A z ) and size (A l , A w , A h ). Its ground truth box G is with (G x , G y , G z ) and (G l , G w , G h ).</p><p>Location regression loss is composed of center residual prediction loss and size residual prediction loss, formulated as</p><formula xml:id="formula_3">L loc = L dis (A ctr , G ctr ) + L dis (A size , G size ),<label>(3)</label></formula><p>where L dis is the smooth-l 1 loss. A ctr and A size are predicted center residual and size residual by the proposal generation network, while G ctr and G size are targets for them. The target of our network is defined as</p><formula xml:id="formula_4">G ctr = G j − A j , j ∈ (x, y, z) G size = (G j − A j )/A j , j ∈ (l, w, h).<label>(4)</label></formula><p>Angle loss includes orientation classification loss and residual prediction loss as</p><formula xml:id="formula_5">L angle = L cls (t a−cls , v a−cls )+L dis (t a−res , v a−res ), (5)</formula><p>where t a−cls and t a−res are predicted angle class and residual while v a−cls and v a−res are their targets.</p><p>The box prediction loss is almost the same as the proposal prediction loss, mentioned above, with two extra losses, which are 3D IoU loss and corner loss. When training IoU branch, we use 3D IoU between proposals and corresponding ground-truth boxes as ground-truth, and smooth-l 1 loss as the loss function. Corner loss is the distance between the predicted 8 corners and assigned groundtruth, expressed as</p><formula xml:id="formula_6">L corner = 8 k=1 P k − G k ,<label>(6)</label></formula><p>where P k and G k are the location of ground-truth and prediction for point k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on the widely used KITTI Object Detection Benchmark <ref type="bibr" target="#b0">[1]</ref>. There are 7,481 training images / point clouds and 7,518 test images / point clouds with three categories of Car, Pedestrian and Cyclist. We use average precision (AP) metric to compare with different methods. During evaluation, we follow the official KITTI evaluation protocol -that is, the IoU threshold is 0.7 for class Car and 0.5 for Pedestrian and Cyclist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Following former works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref>, in order to avoid IoU misalignment in KITTI evaluation protocol on Car, Pedestrian and Cyclist, we train two networks, one for car and the other for both pedestrian and cyclist.</p><p>Network Architecture To align network input, we randomly choose 16K points from the entire point cloud for each scene. Our 3D semantic segmentation network is  based on PointNet++ with four SA levels and four feature propagation (FP) layers. The proposal generation subnetwork is a multi-layer perception consisting of four hidden layers with channels (128, 128, 256, 512), followed by a PointsPool layer where we randomly sample N = 512 interior points per proposal as its initial input. These representations are then passed to the box regression network. Both the box estimation and IoU estimation branches consist of 2 fully connected layers with 512 channels.</p><p>Training Parameters Our model is trained stage-bystage to save GPU memory. The first stage consists of 3D semantic segmentation and proposal generation, while the second is for box prediction. For the first stage, we use ADAM <ref type="bibr" target="#b12">[13]</ref> optimizer with an initial learning rate 0.001 for the first 80 epochs and then decay it to 0.0001 for the last 20 epochs. Each batch consists of 16 point clouds evenly distributed on 4 GPU cards. For the second stage, we train 50 epochs with batch size 1. The learning rate is initialized as 0.001 for first 40 epochs and is then decayed by 0.1 in every 5 epochs. For each input point cloud, we sample 256 proposals, with ratio 1:1 for positives and negatives. Our implementation is based on Tensorflow <ref type="bibr" target="#b1">[2]</ref>. For the box prediction network, a proposal is considered positive if its maximum 3D IoU with all ground-truth boxes is higher than 0.55 and negative if its maximum 3D IoU is below 0.45 during training the car model. The positive and negative 3D IoU thresholds are 0.5 and 0.4 for the pedestrian and cyclist models. Besides, for the IoU branch, we only train on positive proposals.</p><p>Data Augmentation Data augmentation is important to prevent overfitting. First, similar to that of <ref type="bibr" target="#b33">[34]</ref>, we randomly add several ground-truth boxes with their interior points from other scenes to current point cloud in order to simulate objects with various environments. Then, for each bounding box, we randomly rotate it following a uniform distribution ∆θ 1 ∈ [−π/4, +π/4] and randomly add a translation (∆x, ∆y, ∆z). Third, each point cloud is flipped along the x-axis in camera coordinate with probability 0.5. We also randomly rotate each point cloud around z-axis (up axis) by a uniformly distributed random variable ∆θ 2 ∈ [−π/4, +π/4]. Finally, we apply a global scaling to point cloud with a random variable drawn from the uniform distribution [0.9, 1.1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>For evaluation on the test set, we train the model on split train/val sets at ratio 4:1. The performance of our method and comparison with previous methods are listed in <ref type="table" target="#tab_3">Table  3</ref>. Our model outperforms other methods by a large margin on Car and Cyclist classes, especially on the hard set. Compared to multi-view methods that use other sensors as extra information, our method still achieves higher AP with input of only raw point cloud. Compared to UberATG-MMF <ref type="bibr" target="#b18">[19]</ref>, which is the best multi-sensor detector, STD outperforms it by 0.88% on the moderate level on 3D detection of Cars. Also large increase 7.65% on the hard set is obtained, man-Indicator Pedestrian <ref type="figure">Figure 3</ref>. Small objects such as indicators are easy to detect on RGB images, but not on LiDAR data. ifesting the effectiveness of our proposal-generation module and IoU branch. Note that on Pedestrian class, STD is still the best among LiDAR-only detectors. Multi-sensor detectors work better because there are very few 3D points on pedestrians, making it difficult to distinguish them from other small objects like indicator or telegraph pole, as shown in <ref type="figure">Figure 3</ref>. Extra information of RGB would help in these cases.</p><p>Compared to LiDAR-only detectors, and voxel or point methods, our method works best on all three classes. Specifically, on Car detection, STD achieves a better AP by 1.87%, 2.64% and 3.97% compared to PointRCNN <ref type="bibr" target="#b29">[30]</ref>, PointPillars <ref type="bibr" target="#b14">[15]</ref> and SECOND <ref type="bibr" target="#b33">[34]</ref> respectively on the moderate set. The improvement on the hard set is more significant -7.74%, 7.76% and 9.86% increase respectively. We present several qualitative results in <ref type="figure">Figure 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation studies</head><p>For ablation studies, we follow VoxelNet <ref type="bibr" target="#b36">[37]</ref> to split the official training set into a train set of 3,717 images/scenes and a val set of 3,769 images/scenes. Images in train/val set belong to different video clips. Following <ref type="bibr" target="#b36">[37]</ref>, all ablation studies are conducted on the car class due to the relatively large amount of data to make system run stably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results On Validation Set</head><p>We first report the performance on KITTI val set in <ref type="table" target="#tab_4">Table 4</ref>. Our results on validation set compared to other methods are listed in <ref type="table" target="#tab_6">Table 5</ref>. Unlike voxel-based methods of <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b33">[34]</ref>, our model preserves more structure and appearance details, leading to better per-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Easy Moderate Hard MV3D <ref type="bibr" target="#b3">[4]</ref> 71.29 62.68 56.56 AVOD <ref type="bibr" target="#b13">[14]</ref> 84.41 74.44 68.65 VoxelNet <ref type="bibr" target="#b36">[37]</ref> 81.97 65.46 62.85 SECOND <ref type="bibr" target="#b33">[34]</ref> 87.43 76.48 69.10 F-PointNet <ref type="bibr" target="#b25">[26]</ref> 83    formance. Compared to point-based methods, the proposal generation module and IoU branch keep more accurate proposals and high-quality predictions, which brings higher AP especially on the hard set. We compare recall among different 2-stage object detectors in <ref type="table" target="#tab_7">Table 6</ref>, demonstrating the powerful of our proposal generation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Anchors Receptive Field</head><p>Given that anchors play an important role, it is important to make anchors cover as much as possible ground-truth area while not consuming much computation. We use spherical receptive field that has only one radius for each detection model. In order to justify the effectiveness of this design, we conduct experiments varying the shape and size of receptive fields. Average Recall (AR) with IoU threshold 0.7 is the metric. The result is shown in <ref type="table" target="#tab_8">Table 7</ref>. First, "cuboid" shape of receptive field needs more than one angles, i.e. (0, π/2), because of the disproportion between length and width, leading to 2× more data and accordingly more computation. "Cuboid" with only one orientation causes 1.5% decrease in terms of AR. Moreover, spherical receptive field brings additional context information, which benefits anchor classification and regression.</p><p>Effect of Proposal Feature Our proposal features are with canonical coordinates and 3D semantic features. We quantify their benefits using original points coordinates as our baseline. As shown in    <ref type="table" target="#tab_1">Table 10</ref>. 3D object detection AP on KITTI val moderate set. Our experiments analyze influence of different ways to use 3D IoU branch. "3D-IoU" means only using 3D IoU as NMS sorting score. "cls-score × 3D-IoU" indicates the way we describe in Section 3.3. transformation, AP increases by 11.1% on moderate set.</p><p>Effect of IoU Branch Our 3D IoU prediction branch estimates the localization quality to finally increase performance. As illustrated in <ref type="table" target="#tab_11">Table 9</ref>, our 3D IoU guided NMS outperforms traditional methods of NMS and soft-NMS by 1.1% and 0.8% on moderate set respectively, manifesting the usefulness of this branch. We note directly taking predicted 3D IoU as the NMS sorting criterion, as shown in <ref type="table" target="#tab_1">Table 10</ref>, performs not well. The reason is that only positive proposals are considered in IoU branch, while classification score can tell positive predictions from negative ones. Accordingly, combination of classification score and predicted IoU is very effective.</p><p>Inference Time The total inference time of STD is 80ms on a TitanV GPU where the PointNet++ backbone takes 54ms, the proposal generation module including PointNet and NMS takes 10ms, PointsPool layer takes about 6ms, and the second stage with two branches takes 10ms. STD is the fastest model among all point-based methods and multi-view methods, manifesting the reasonable design of STD. Note that, we merge batch normalization into convolution layers, and split the input point cloud of first SA level (16K) in PointNet++ to (32×512) for parallel computation so as to shorten inference time, resulting in 25ms and 50ms speedup respectively with performance unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a new two-stage 3D object detection framework that takes advantages of both voxel-and point-based methods. We introduce spherical anchors based on points and refine them for accurate proposal generation without loss of localization information in the first stage. Then a PointsPool layer is applied for generating compact representations for proposals which is beneficial to reduce inference time. The second stage reduces incorrect removal in post-process which further improves performance. Our model works decently on 3D detection, especially on the hard set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of our framework consisting of three different parts. The first is a proposal generation module (PGM) to generate accurate proposals from man-made point-based spherical anchors. The second part is a PointsPool layer to convert proposal features from sparse expression to compact representation. The final one is a box prediction network. It classifies and regresses proposals, and picks high-quality predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>N</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of networks in the proposal generation module. (a) 3D segmentation network (PointNet++). It takes a raw point cloud (x, y, z, r) as input, and generates semantic segmentation scores as well as global context features for each point by stacking SA layers and FP modules. (b) Proposal generation Network (PointNet). It treats normalized coordinates and semantic features of points within anchors as input, and produces classification and regression predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Methods</cell><cell cols="4">Proposal No. Inference Time Moderate</cell></row><row><cell>PointNet (4 conv layers)</cell><cell>100</cell><cell></cell><cell>10 ms</cell><cell>77.1</cell></row><row><cell>PointNet++ (3 SA)</cell><cell>100</cell><cell></cell><cell>41 ms</cell><cell>79.1</cell></row><row><cell>PointsPool + 2FC</cell><cell>100</cell><cell></cell><cell>16 ms</cell><cell>78.7</cell></row><row><cell>√ -</cell><cell>-√</cell><cell>88.8 90.9</cell><cell>78.7 90.9</cell><cell>78.2 90.6</cell></row></table><note>. 3D object detection AP on KITTI val moderate set. We compare inference time and AP among different box regression network architectures.Score-NMS IoU-NMS Easy Moderate Hard</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Performance on KITTI test set for both Car, Pedestrian and Cyclists.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>3D and BEV detection AP on KITTI val set.</figDesc><table><row><cell>Class</cell><cell cols="3">Easy Moderate Hard</cell></row><row><cell>Car (BEV)</cell><cell>90.5</cell><cell>88.5</cell><cell>88.1</cell></row><row><cell>Car (3D)</cell><cell>89.7</cell><cell>79.8</cell><cell>79.3</cell></row><row><cell cols="2">Pedestrian (BEV) 75.9</cell><cell>69.9</cell><cell>66.0</cell></row><row><cell>Pedestrian (3D)</cell><cell>73.9</cell><cell>66.6</cell><cell>62.9</cell></row><row><cell>Cyclist (BEV)</cell><cell>89.6</cell><cell>76.0</cell><cell>72.7</cell></row><row><cell>Cyclist (3D)</cell><cell>88.5</cell><cell>72.8</cell><cell>67.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>3D detection AP on KITTI val set of our model for "Car" compared to other state-of-the-art methods.</figDesc><table><row><cell>Methods</cell><cell cols="3">Proposals num IoU threshold Recall</cell></row><row><cell>AVOD [14]</cell><cell>50</cell><cell>0.5</cell><cell>91.0</cell></row><row><cell>Ours</cell><cell>50</cell><cell>0.5</cell><cell>96.3</cell></row><row><cell>PointRCNN [30]</cell><cell>100</cell><cell>0.7</cell><cell>74.8</cell></row><row><cell>Ours</cell><cell>100</cell><cell>0.7</cell><cell>76.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Recall of proposals on KITTI val set compared to other methods with the same proposal number and IoU threshold.</figDesc><table><row><cell cols="4">shape anchors amount proposals num Recall(IoU=0.7)</cell></row><row><cell>cuboid</cell><cell>1×</cell><cell>100</cell><cell>74.2</cell></row><row><cell>cuboid</cell><cell>2×</cell><cell>100</cell><cell>75.7</cell></row><row><cell>sphere</cell><cell>1×</cell><cell>100</cell><cell>76.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Recall of generated proposals on KITTI val set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>, using 3D segmentation features results in around 36.5% performance boost on moderate set in terms of AP. It means global context information enhances model capability greatly. With canonicalFigure 4. Visualization of our results on KITTI test set. Cars, pedestrians and cyclists are highlighted in yellow, red and green respectively. The upper row in each image is the 3D object detection result projected onto the RGB image. The other is the result in the LiDAR phase.</figDesc><table><row><cell cols="5">semantic canonized Easy Moderate Hard</cell></row><row><cell>-√ √</cell><cell>--√</cell><cell>38.7 82.5 88.8</cell><cell>31.1 67.6 78.7</cell><cell>26.0 67.2 78.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>3D object detection AP on KITTI val set. A tick in canonized item means using canonical coordinates rather than original coordinates as part of feature. A tick in "semantic" means using points feature from 3D semantic segmentation backbone in proposal feature.</figDesc><table><row><cell>NMS Soft-NMS 3D Easy Moderate Hard √ --88.8 78.7 78.2 -√ -88.9 79.0 78.4 --√ 89.7 79.8 79.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>3D object detection AP on KITTI val moderate set. Our experiments analyze influence of our 3D IoU branch. "3D" means using 3D IoU branch for post-processing.</figDesc><table><row><cell cols="4">NMS sorting score Easy Moderate Hard</cell></row><row><cell>3D-IoU</cell><cell>89.0</cell><cell>79.1</cell><cell>78.7</cell></row><row><cell cols="2">cls-score × 3D-IoU 89.7</cell><cell>79.8</cell><cell>79.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" />
		<title level="m">&quot;kitti 3d object detection benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Tensorflow: Large-scale machine learning on heterogeneous distributed systems. CoRR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multiview random forest of local experts combining RGB and LIDAR data for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Villalonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Amores</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<editor>IV</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Pointsift: A sift-like network module for 3d point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcnn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multitask multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Box aggregation for proposal decimation: Last mile of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiple 3d object tracking for augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pedestrian detection combining RGB and dense LIDAR data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Premebida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nunes</surname></persName>
		</author>
		<editor>ICoR</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequential context encoding for duplicate removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Roarnet: A robust 3d object detection based on region approximation refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03818</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems XI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent CRF for real-time roadobject segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PIXOR: real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
