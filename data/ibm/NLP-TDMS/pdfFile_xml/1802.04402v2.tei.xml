<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Slice Networks for 3D Segmentation of Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
							<email>qianguih@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
							<email>weiyuewa@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
							<email>uneumann@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Slice Networks for 3D Segmentation of Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point clouds are an efficient data format for 3D data. However, existing 3D segmentation methods for point clouds either do not model local dependencies <ref type="bibr" target="#b22">[22]</ref> or require added computations <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b24">24]</ref>. This work presents a novel 3D segmentation framework, RSNet 1 , to efficiently model local structures in point clouds. The key component of the RSNet is a lightweight local dependency module. It is a combination of a novel slice pooling layer, Recurrent Neural Network (RNN) layers, and a slice unpooling layer. The slice pooling layer is designed to project features of unordered points onto an ordered sequence of feature vectors so that traditional end-to-end learning algorithms (RNNs) can be applied. The performance of RSNet is validated by comprehensive experiments on the S3DIS[1], ScanNet <ref type="bibr" target="#b2">[3],</ref> and ShapeNet [35]  datasets. In its simplest form, RSNets surpass all previous state-of-the-art methods on these benchmarks. And comparisons against previous state-of-the-art methods <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24]</ref> demonstrate the efficiency of RSNets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most 3D data capturing devices (like LiDAR and depth sensors) produce point clouds as raw outputs. However, there are few state-of-the-art 3D segmentation algorithms that use point clouds as inputs. The main obstacle is that point clouds are usually unstructured and unordered, so it is hard to apply powerful end-to-end learning algorithms. As a compromise, many researchers transform point clouds into alternative data formats such as voxels <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b33">33]</ref> and multi-view renderings <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b20">20]</ref>.</p><p>Unfortunately, information loss and quantitation artifacts often occur in data format transformations. These can lead to 3D segmentation performance drops as a result due to loss of local contexts. Moreover, the 3D CNNs <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b14">14]</ref> and 2D multi-view CNNs <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b23">23]</ref> designed for <ref type="bibr" target="#b0">1</ref> Codes are released here https://github.com/qianguih/RSNet Recurrent Slice Network <ref type="figure">Figure 1</ref>: The RSNet takes raw point clouds as inputs and outputs semantic labels for each point. these data formats are often time-and memory-consuming.</p><p>In this paper, we approach 3D semantic segmentation tasks by directly dealing with point clouds. A simple network, a Recurrent Slice Network (RSNet), is designed for 3D segmentation tasks. As shown in <ref type="figure">Fig.1</ref>, the RSNet takes as inputs raw point clouds and outputs semantic labels for each of them.</p><p>The main challenge in handling point clouds is modeling local geometric dependencies. Since points are processed in an unstructured and unordered manner, powerful 2D segmentation methods like Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNNs) cannot be directly generalized to them.</p><p>In RSNets, the local context problem is solved by first projecting unordered points into ordered features and then applying traditional end-to-end learning algorithms. The projection is achieved by a novel slice pooling layer. In this layer, the inputs are features of unordered points and the output is an ordered sequence of aggregated features. Next, RNNs are applied to model dependencies in this sequence. Finally, a slice unpooling layer assigns features in the sequence back to points. In summary, the combination of the slice pooling layer, RNN layers, and the slice unpooling layer forms the local dependency module in RSNets. We note that the local dependency module is highly efficient. As shown in Section 3.2, the time complexity of the slice pooling/unpooling layer is O(n) w.r.t the number of input points and O(1) w.r.t the local context resolutions.</p><p>The performances of RSNets are validated on three challenging benchmarks. Two of them are large-scale realworld datasets, the S3DIS dataset <ref type="bibr" target="#b0">[1]</ref> and the ScanNet 1 arXiv:1802.04402v2 [cs.CV] 29 Mar 2018 dataset <ref type="bibr" target="#b2">[3]</ref>. Another one is the ShapeNet dataset <ref type="bibr" target="#b35">[35]</ref>, a synthetic dataset. RSNets outperform all prior results and significantly improve performances on the S3DIS and ScanNet datasets.</p><p>In following parts of the paper, we first review related works in Section 2. Then, details about the RSNet are presented in Section 3. Section 4 reports all experimental results and Section 5 draws conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Traditional 3D analysis algorithms are based on handcrafted features <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12]</ref>. Recently, there are some works that utilize end-to-end learning algorithms for 3D data analysis. They are categorized by their input data formats as follows.</p><p>Voxelized Volumes. <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref> made the early attempts of applying end-to-end deep learning algorithms for 3D data analysis, including 3D shape recognition, 3D urban scene segmentation <ref type="bibr" target="#b14">[14]</ref>. They converted raw point cloud data into voxelized occupancy grids and then applied 3D deep Convolutional Neural Networks to them. Due to the memory constraints of 3D convolutions, the size of input cubes in these methods was limited to 60 3 and the depth of the CNNs was relatively shallow. Many works have been proposed to ease the computational intensities. One direction is to exploit the sparsity in voxel grids. In <ref type="bibr" target="#b4">[5]</ref>, the authors proposed to calculate convolutions at sparse input locations by pushing values to their target locations. Benjamin Graham designed a sparse convolution network <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and applied it for 3D segmentation tasks <ref type="bibr" target="#b37">[37]</ref>. <ref type="bibr" target="#b16">[16]</ref> tried to reduce computation by sampling 3D data at sparse points before feeding them into networks. In <ref type="bibr" target="#b28">[28]</ref>, the authors designed a memory efficient data structure, hybrid gridoctree, and corresponding convolution/pooling/unpooling operations to handle higher resolution 3D voxel grids (up to 256 3 ). In <ref type="bibr" target="#b32">[32]</ref>, the authors managed to consume 3D voxel inputs of higher resolution (100 3 ) and build deeper networks by adopting early down-sampling and efficient convolutional blocks like residual modules. While most of these works were focusing on reducing computational requirements of 3D voxel inputs, few of them tried to deal with the quantitation artifacts and information loss in voxelization.</p><p>Multi-view Renderings. Another popular data representation for 3D data is its multi-view rendering images. <ref type="bibr" target="#b20">[20]</ref> designed a multi-view CNN for object detection in point clouds. In <ref type="bibr" target="#b30">[30]</ref>, 3D shapes were transformed into panoramic views, i.e., a cylinder project around its principal axis. <ref type="bibr" target="#b31">[31]</ref> designed a 2D CNN for 3D shape recognition by taking as inputs multi-view images. In <ref type="bibr" target="#b23">[23]</ref>, the authors conducted comprehensive experiments to compare the recognition performances of 2D multi-view CNNs against 3D volumetric CNNs. More recently, multi-view 2D CNNs have been applied to 3D shape segmentation and achieved promising results. Compared to volumetric methods, multiview based methods require less computational costs. However, there is also information loss in the multi-view rendering process.</p><p>Point Clouds. In the seminal work of PointNet <ref type="bibr" target="#b22">[22]</ref>, the authors designed a network to consume unordered and unstructured point clouds. The key idea is to process points independently and then aggregate them into a global feature representation by max-pooling. PointNet achieved state-ofthe-art results on several 3D classification and segmentation tasks. However, there were no local geometric contexts in PointNet. In the following work, PointNet++ <ref type="bibr" target="#b24">[24]</ref>, the authors improved PointNet by incorporating local dependencies and hierarchical feature learning in the network. It was achieved by applying iterative farthest point sampling and ball query to group input points. In another direction, <ref type="bibr" target="#b15">[15]</ref> proposed a KD-network for 3D point clouds recognition. In KD-network, a KD-tree was first built on input point clouds. Then, hierarchical groupings were applied to model local dependencies in points.</p><p>Both works showed promising improvements on 3D classification and segmentation tasks, which proved the importance of local contexts. However, their local context modeling methods all relied on heavy extra computations such as the iterate farthest point sampling and ball query in <ref type="bibr" target="#b24">[24]</ref> and the KD-tree construction in <ref type="bibr" target="#b15">[15]</ref>. More importantly, their computations will grow linearly when higher resolutions of local details are used. For example, higher local context resolutions will increase the number of clusters in <ref type="bibr" target="#b24">[24]</ref> and result in more computations in iterative farthest point sampling. And higher resolutions will enlarge the kd-tree in <ref type="bibr" target="#b15">[15]</ref> which also costs extra computations. In contrast, the key part of our local dependency module, the slice pooling layer, has a time complexity of O(1) w.r.t the local context resolution as shown in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given a set of unordered point clouds X = {x 1 , x 2 , ..., x i , ..., x n } with x i ∈ R d and a candidate label set L = {l 1 , l 2 , ..., l K }, our task is to assign each of input points x i with one of the K semantic labels. In RSNets, the input is raw point clouds X and output is Y = {y 1 , y 2 , ..., y i , ..., y n } where y i ∈ L is the label assigned to x i .</p><p>A diagram of our method is presented in <ref type="figure" target="#fig_0">Fig.2</ref>. The input and output feature extraction blocks are used for independent feature generation. In the middle is the local dependency module. Details are illustrated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Independent Feature Extraction</head><p>There are two independent feature extraction blocks in an RSNet. The input feature block consumes input points X ∈ R n×d in and produce features F in ∈ R n×d in . Output feature blocks take processed features F su ∈ R n×d su as inputs and produce final predictions for each point. The superscript in and su indicate the features are from the input feature block and the slice unpooling layer, respectively. Both blocks use a sequence of multiple 1 × 1 convolution layers to produce independent feature representations for each point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local Dependency Module</head><p>The key part of an RSNet is the local dependency module which is a combination of a slice pooling layer, RNN layers, and a slice unpooling layer. It supports efficient and effective local context modeling. The slice pooling layer is designed to project features of unordered points onto an ordered sequence. RNNs are then applied to model dependencies in the sequence. In the end, the slice unpooling layer reverses the projection and assigns updated features back to each point.</p><p>Slice Pooling Layer. The inputs of a slice pooling layer are features of unordered point clouds</p><formula xml:id="formula_0">F in = {f in 1 , f in 2 , ..., f in i , .</formula><p>.., f in n } and the output is an ordered sequence of feature vectors. This is achieved by first grouping points into slices and then generating a global representation for each slice via aggregating features of points within the slice.</p><p>Three slicing directions, namely slicing along x, y, and z axis, are considered in RSNets. We illustrate the details of slice pooling operation by taking z axis for example. A diagram of the slice pooling layer is presented in <ref type="figure" target="#fig_2">Fig.3</ref>. In a slice pooling layer, input points X are first split into slices by their spatial coordinates in z axis. The resolution of each slice is controlled by a hyper-parameter r. Assume input points span in the range [z min , z max ] in z axis. Then, the point x i is assigned to the k th slice, where k = (z i − z min )/r and z i is x i 's coordinate in z axis. In total, there are N slices where N = (z max − z min )/r . Here and indicate the ceil and floor function. After this process, all input points are grouped into N slices. They are also treated as N sets of points S = {S 1 , S 2 , ..., S i , ..., S N }, where S i denotes the set of points assigned to i th slice. In each slice, features of points are aggregated into one feature vector to represent the global information about this slice. Formally, after aggregation, a slice pooling layer produces an ordered sequence of feature vectors F s = {f s1 , f s2 , ..., f si , ..., f sN }, where f si is the global feature vector of slice set S i . The max-pooling operation is adopted as the aggregation operator in RSNets. It is formally defined in equation <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_1">f si = max xj ∈Si {f in j }<label>(1)</label></formula><p>The slice pooling layer has several important properties:</p><p>1. Order and Structure. F s is an ordered and structured sequence of feature vectors. In the aforementioned case, F s is ordered in the z axis. f s1 and f sN denote the feature representations of the bottommost and top-most set of points, respectively. Meanwhile, f si and f s(i−1) are features representing adjacent neighbors. This property makes traditional local dependency modeling algorithms applicable as F s is structured and ordered now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Efficiency. The time complexity of the slice pooling layer is O(n) (n is the number of the input points). And it is O(1) w. r. t the slicing resolution r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Local context trade-off. Given a fixed input, smaller r will produce more slices with richer local contexts </p><formula xml:id="formula_2">S 1 F s5 F s4 F s3 F s2 F s1 f s5 1 f s5 2 f s5 d-2 f s5 d-1 f s5 d f s4 1 f s4 2 f s4 d-2 f s4 d-1 f s4 d f s3 1 f s3 2 f s3 d-2 f s3 d-1 f s3 d f s2 1 f s2 2 f s2 d-2 f s2 d-1 f s2 d f s1 1 f s1 2 f s1 d-2 f s1 d-1 f s1 d (a)</formula><p>Illustration of the slice pooling operation. A set of points from a chair is used for illustration purpose here.</p><formula xml:id="formula_3">S 1 S 5 F R5 F R1 f R5 1 f R5 2 f R5 d-2 f R5 d-1 f R5 d f R1 1 f R1 2 f R1 d-2 f R1 d-1 f R1 d f R5 1 f R5 2 f R5 d-2 f R5 d-1 f R5 d f R5 1 f R5 2 f R5 d-2 f R5 d-1 f R5 d f R1 1 f R1 2 f R1 d-2 f R1 d-1 f R1 d f R1 1 f R1 2 f R1 d-2 f R1 d-1 f R1 d (b)</formula><p>Illustration of the slice unpooling operation. Global feature representation for one point set is replicated back to all points in the set. preserved while larger r produces fewer slices with coarse local contexts;</p><p>RNN Layer. As mentioned above, the slice pooling layer is essentially projecting features of unordered and unstructured input points onto an ordered and structured sequence of feature vectors. RNNs are a group of end-to-end learning algorithms naturally designed for a structured sequence. Thus, they are adopted to model dependencies in the sequence. By modeling one slice as one timestamp, the information from one slice interacts with other slices as the information is flowing through timestamps in RNN units. This enables contexts in slices impact with each other which in turn models the dependencies in them.</p><p>In an RSNet, the input of RNN layers is F s . In order to guarantee information from one slice could impact on all other slices, RSNets utilize the bidirectional RNN units <ref type="bibr" target="#b29">[29]</ref> to help information flow in both directions. After processing the inputs with a stack of bidirectional RNNs, the final outputs are F r = {f r1 , f r2 , ..., f ri , ..., f rN } with superscript r denoting the features are from RNN layers. Compared with F s , F r has been updated by interacting with neighboring points.</p><p>Slice Unpooling Layer. As the last part of an RSNet's local dependency module, the slice unpooling layer takes updated features F r as inputs and assigns them back to each point by reversing the projection. This can be easily achieved by storing the slice sets S. A diagram of the slice unpooling layer is presented in <ref type="figure" target="#fig_2">Fig.3</ref>. We note that the time complexity of slice unpooling layer is O(n) w. r. t the number of input points and is O(1) w. r. t slicing resolution as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In order to evaluate the performance of RSNets and compare with state-of-the-art, we benchmark RSNets on three datasets, the Stanford 3D dataset (S3DIS) <ref type="bibr" target="#b0">[1]</ref>, ScanNet dataset <ref type="bibr" target="#b2">[3]</ref>, and the ShapeNet dataset <ref type="bibr" target="#b35">[35]</ref>. The first two are large-scale realistic 3D segmentation datasets and the last one is a synthetic 3D part segmentation dataset.</p><p>We use the strategies in <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24]</ref> to process all datasets. For the S3DIS and ScanNet datasets, the scenes are first divided into smaller cubes using a sliding window of a fixed size. A fixed number of points are sampled as inputs from the cubes. In this paper, the number of points is fixed as 4096 for both datasets. Then RSNets are applied to segment objects in the cubes. Note that we only divide the scene on the xy plane as in <ref type="bibr" target="#b22">[22]</ref>. During testing, the scene is similarly split into cubes. We first run RSNets to get point-wise predictions for each cube, then merge predictions of cubes in the same scene. Majority voting is adopted when multiple predictions of one point are present.</p><p>We use one unified RSNet architecture for all datasets. In the input feature extraction block, there are three 1 × 1 convolutional layers with output channel number of 64, 64, and 64, respectively. In the output feature extraction block, there are also three 1 × 1 convolutional layers with output channel number of 512, 256, and K, respectively. Here K is the number of semantic categories. In each branch of the local dependency module, the first layer is a slice pooling layer and the last layer is a slice unpooling layer. The slicing resolution r varies for different datasets. There is a comprehensive performance comparison of different r values in Section 4.2. In the middle are the RNN layers. A stack of 6 bidirectional RNN layers is used in each branch. The numbers of channels for RNN layers are 256, 128, 64, 64, 128, and 256. In the baseline RSNet, Gated Recurrent Unit (GRU) <ref type="bibr" target="#b1">[2]</ref> units are used in all RNNs.</p><p>Two widely used metrics, mean intersection over union (mIOU) and mean accuracy (mAcc), are used to measure the segmentation performances. We first report the performance of a baseline RSNet on the S3DIS dataset. Then, comprehensive studies are conducted to validate various architecture choices in the baseline. In the end, we show state-of-the-art results on the ScanNet and ShapeNet dataset. Through experiments, the performances of RSNets are compared with various state-of-the-art 3D segmentation methods including 3D volumes based methods <ref type="bibr" target="#b32">[32]</ref>, spectral CNN based method <ref type="bibr" target="#b36">[36]</ref>, and point clouds based methods <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b15">15</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Segmentation on the S3DIS Dataset</head><p>We first present the performances of a baseline RSNet on the S3DIS dataset. The training/testing split in <ref type="bibr" target="#b32">[32]</ref> is used here to better measure the generalization ability of all methods. The slicing resolutions r along the x, y, z axis are  all set at 2cm. And the block size in x and y axis of each cube is 1m × 1m. Given these settings, there are 50 slices (N = 50) in x and y branch in an RSNet after the slice pooling layer. As we do not limit the block size in z axis, the number of slices along z axis varies on different inputs. In the S3DIS dataset, most of the scenes have a maximum z coordinate around 3m which produces around 150 slices. During testing, the sliding stride is set at 1m to generate non-overlapping cubes. The performance of our baseline network is reported in <ref type="table">Table.</ref>1. Besides the overall mean IOU and mean accuracy, the IOU of each category is also presented. Meanwhile, some segmentation results are visu-alized in <ref type="figure" target="#fig_3">Fig.4</ref>.</p><p>Previous state-of-the-art results <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b32">32]</ref> are reported in <ref type="table">Table.</ref>1 as well. In <ref type="bibr" target="#b32">[32]</ref>, the data representation is voxelized 3D volumes and a 3D CNN is built for segmenting objects in the volumes. Several geometric data augmentation strategies and end-to-end Conditional Random Filed (CRF) are utilized in their work. The PointNet <ref type="bibr" target="#b22">[22]</ref> takes the same inputs, point clouds, as our method. It adopted rotation along z axis to augment data. In contrast, our baseline RSN does not use any data augmentations.</p><p>The results in <ref type="table">Table.</ref>1 show that the RSNet has achieved state-of-the-art performances on the S3DIS dataset even without using any data augmentation. In particular, it improves previous 3D volumes based methods <ref type="bibr" target="#b32">[32]</ref> by 3.01 in mean IOU and 2.07 in mean accuracy. Compared with prior point clouds based method <ref type="bibr" target="#b22">[22]</ref>, it improves the mean IOU by 10.84 and mean accuracy by 10.44. The detailed percategory IOU results show that the RSNet is able to achieve better performances in more than half of all categories (7 out of 13).</p><p>We argue that the great performance improvements come from the local dependency module in the RSNet. While PointNet only relies on global features, the RSNet is equipped with local geometric dependencies among points. In summary, the significant performance gains against PointNet demonstrate: 1). local dependency modeling is crucial for 3D segmentation; 2). the combination of the novel slice pooling/unpooling layers and RNN layers is able to support effective spatial dependencies modeling in point clouds. Moreover, the performance improvements against 3D volumes based methods prove that directly handling point clouds can boost the 3D segmentation performances a lot as there are no quantitation artifacts and no local details lost anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>In this section, we validate the effects of various architecture choices and testing schemes. In particular, several key parameters are considered: 1). the slicing resolution r in RSNets; 2). the size of the sliding block; 3). the sliding stride during testing; 4). the type of RNN units. All set-     Slicing resolution. The slicing resolution r is an important hyper-parameter in RSNets. It controls the resolution of each slice which in turn controls how much local details are kept after slice pooling. By using a small slicing resolution, there are more local details preserved as the feature aggregation operation is executed in small local regions. However, a small slicing resolution will produce a large number of slices which requires RNN layers to consume a longer sequence. This may hurt the performance of RSNets as the RNN units may fail to model dependencies in the long sequence due to the "gradient vanishing" problem <ref type="bibr" target="#b9">[9]</ref>. On the other hand, a large slicing resolution will eliminate a lot of local details in input data as the feature aggregation is conducted on a wide spatial range. Thus, there is a trade-off of selecting the slicing resolution r.</p><p>Several experiments are conducted to show the impacts of different slicing resolutions. Two groups of slicing resolutions are tested. In the first group, we fix the slicing resolutions along x and y axis to be 2cm and vary the resolution along the z axis. In the second group, the slicing resolution along z axis is fixed as 2cm while varying resolutions along x and y axis. Detailed performances are reported in <ref type="table">Table.</ref>2. Results in <ref type="table">Table.</ref>2 show that the slicing resolution of 2cm, 2cm, 2cm along x, y, z axis works best for the S3DIS dataset. Both larger or smaller resolutions decrease the final performances.</p><p>Size of sliding block. The size of the sliding block is another key factor in training and testing. Small block sizes may result in too limited contexts in one cube. Large block sizes may put RSNets in a challenging trade-off between slicing resolutions as large block size will either produce more slices when the slicing resolution is fixed or increase the slicing resolution. In <ref type="table">Table.</ref>3, we report the results of three different block sizes, 1m, 2m, and 3m, along with different slicing resolution choices. The results show that larger block sizes actually decrease the performance. That is because larger block sizes produce a longer sequence of slices for RNN layers, which is hard to model using RNNs. Among various settings, the optimal block size for the S3DIS dataset is 1m on both x and y axis.</p><p>Stride of sliding during testing. When breaking down the scenes during testing, there are two options, splitting it into non-overlapping cubes or overlapping cubes. In Pon-intNet <ref type="bibr" target="#b22">[22]</ref> , non-overlapping splitting is used while Point-Net++ [24] adopted overlapping splitting. For RSNets, both options are tested. Specifically, we set the sliding stride into three values, 0.2m, 0.5m, and 1m. The first two produce overlapping cubes and the last one produces nonoverlapping cubes. All results are reported in <ref type="table">Table.</ref>4. Experimental results show that using overlapped division can slightly increase the performance (0.4∼1.9 in mean IOU and 1.1∼2.4 in mean accuracy on the S3DIS dataset). However, testing using overlapped division requires more computations as there are more cubes to process. Thus, we select the non-overlap sliding in our baseline RSNet.</p><p>RNN units. Due to the "gradient vanishing" problem in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Segmentation on the ScanNet dataset</head><p>We now show the performances of RSNets on the Scan-Net dataset. The exact same RSNet as Section 4.1 is used to process the ScanNet dataset. The performance of the RSNet is reported in <ref type="table">Table.</ref>6.</p><p>In the ScanNet dataset, the previous state-of-the-art method is PointNet++ <ref type="bibr" target="#b24">[24]</ref>. It only uses the xyz information of point clouds as inputs. To make a fair comparison, we also only use xyz information in the RSNet. <ref type="bibr" target="#b24">[24]</ref> only reported the global accuracy on the ScanNet dataset. However, as shown in the supplementary, the ScanNet dataset is highly unbalanced. In order to get a better measurement, we still use mean IOU and mean accuracy as evaluation metrics as previous sections. We reproduced the performances of PointNet <ref type="bibr" target="#b22">[22]</ref> and Pointnet++ <ref type="bibr" target="#b24">[24]</ref> (the single scale version) on the ScanNet dataset 2 and report them in <ref type="table">Table.</ref>6 as well. As shown in <ref type="table">Table.</ref>6, the RSNet has also achieved state-ofthe-art results on the ScanNet dataset. Compared with the PointNet++, the RSNet improves the mean IOU and mean accuracy by 5.09 and 4.60. Some comparisons between different methods are visualized in <ref type="figure" target="#fig_4">Fig.5</ref>. These visualizations show that as a benefit of the local dependency module, the RSNet is able to handle small details such the chairs, desks, and toilets in inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Segmentation on the ShapeNet Dataset</head><p>In order to compare the RSNet with some other methods <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref>, we also report the segmentation results of RSNets on the ShapeNet part segmentation dataset.</p><p>The same RSNet as in Section 4.1 is used here. It only takes the xyz information as the convention. Its performance are reported in <ref type="table">Table.</ref>7. <ref type="table">Table.</ref>7 also presents the results of other state-of-the-art methods including Point-Net <ref type="bibr" target="#b22">[22]</ref>, PointNet++ <ref type="bibr" target="#b24">[24]</ref>, KD-net <ref type="bibr" target="#b15">[15]</ref>, and spectral CNN <ref type="bibr" target="#b36">[36]</ref>. The RSNet outperforms all other methods except the PointNet++ <ref type="bibr" target="#b24">[24]</ref> which utilized extra normal information as inputs. However, the RSNet can also outperform Point-Net++ when it only takes xyz information. This validates the effectiveness of the RSNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Computation Analysis</head><p>We now demonstrate the efficiency of RSNets in terms of inference speed and GPU memory consumption. We follow the same time and space complexity measurement strategy as <ref type="bibr" target="#b24">[24]</ref>. We record the inference time and GPU memory consumption of a batch of 8 4096 points for vanilla Point-Net and the RSNet using PyTorch on a K40 GPU. Since <ref type="bibr" target="#b24">[24]</ref> reported the inference speed in TensorFlow, we use the relative speed w.r.t vanilla PointNet to compare speeds with each other. The speed and memory measurements are reported in <ref type="table">Table.</ref>8. <ref type="table" target="#tab_10">Table.8</ref> show that the RSNet is much faster than Point-Net++ variants. It is near 1.6 × faster than the single scale version of PointNet++ and 3.1 × faster than its multi-scale version. Moreover, the GPU memory consumption of the RSNet is even lower than vanilla PointNet. These prove that the RSNet is not only powerful but also efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduces a powerful and efficient 3D segmentation framework, Recurrent Slice Network (RSNet). An RSNet is equipped with a lightweight local dependency modeling module which is a combination of a slice pooling, RNN layers, and a slice unpooling layer. Experimental results show that RSNet can surpass previous state-of-the-art methods on three widely used benchmarks while requiring less inference time and memory.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>Here, we present more details of experimental settings and more experimental results and discussions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Results and Discussions on the S3DIS dataset</head><p>In the S3DIS dataset, there are 272 indoor scenes captured from 6 areas in 3 buildings. The points are annotated in 13 categories. To process this dataset, our RSNet takes points with 9 dimensional features as inputs as in <ref type="bibr" target="#b22">[22]</ref>. The first three, middle three, and last three dimensions represent the xyz coordinates, RGB intensities, and normalized xyz coordinates, respectively.</p><p>In the main text, we used the training/testing split in <ref type="bibr" target="#b32">[32]</ref> is used to avoid dividing areas from same building to both training and testing sets. However, in <ref type="bibr" target="#b22">[22]</ref>, the authors reported their performances using 6-fold validation. In order to comprehensively compare with <ref type="bibr" target="#b22">[22]</ref>, we also present the 6-fold validation performances of RSNet in <ref type="table">Table.</ref>9. The results show that our RSNet outperforms PointNet by a large margin while requiring less memories and reasonable extra inference times.</p><p>Both <ref type="table" target="#tab_12">Table.9 and the Table.</ref>1 in the main text show that while all the methods work well on some categories like ceiling, floor and wall, they all fail to achieve the same level of performances on the categories like beam, column, and bookcase. This is because the S3DIS dataset is a highly unbalanced dataset. From the data portion statistics in Table.10 we notice that ceiling, floor and wall are the dominant classes which have 7 ∼ 50 times more training data than the rare classes. This makes the segmentation algorithms fail to generalize well on the rare classes. In order to alleviate this problem, we adopt the median frequency balancing strategy <ref type="bibr" target="#b3">[4]</ref> in our RSNet training. The results are compared with the baseline in <ref type="table">Table.</ref>13. It shows that using median frequency balancing improves performances in terms of the mean accuracy. However, there is a slight decrease in mean IOU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Results and Discussions on the Scan-Net dataset</head><p>The ScanNet dataset contain 1,513 scenes captured by the Matterport 3D sensor. We follow the official training/testing split <ref type="bibr" target="#b2">[3]</ref> in this paper. The points are annotated in 20 categories and one background class. As shown in Table.11, the ScanNet dataset is also highly unbalanced. Thus, we use the mean IOU and mean accuracy as evaluation met-rics in the main text to better measure the performances for this dataset. To process the ScanNet dataset, out RSNet takes points with 3 dimensional features (xyz coordinates) as inputs as in <ref type="bibr" target="#b24">[24]</ref>.</p><p>In order to further improve the performances on the ScanNet dataset, we train a RSNet taking not only xyz coordinates but also RGB intensities as inputs. The results are reported in <ref type="table">Table.</ref>12. It shows that RGB information can slightly improve the performances of our baseline model. The mean IOU and mean accuracy are improved by 1.81 and 1.97. Moreover, detailed per-class IOUs show that the RGB information is particularly helpful for categories like door, window, and picture. These classes can be easily confused with walls when only geometric information (xyz coordinate) is present. However, RGB information helps the network distinguish them from each other.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Diagram of an RSNet. The three parallel branches denote the slicing direction along x, y, and z axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of slice pooling and slice unpooling operation and RNN modeling for slices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Sample segmentation results on the S3DIS dataset. From left to right are the input scenes, results produced by the RSNet, and ground truth. Best viewed with zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Sample segmentation results on the ScanNet dataset. From left to right are the input scenes, results produced by PointNet, PointNet++, RSNet, and ground truth. Interesting areas have been highlighted by red bounding boxes. Best viewed with zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Method mIOU mAcc ceiling floor wall beam column window door chair table bookcase sofa board clutter PointNet A [22] 41.09 48.98 88.80 97.33 69.80 0.05 3.92 46.26 10.76 52,61 58.93 40.28 5.85 26.38 33.22 3D-CNN [32] 43.67 CNN A [32] 47.46 54.91 90.17 96.48 70.16 0.00 11.40 33.36 21.12 76.12 70.07 57.89 37.46 11.16 41.61 3D-CNN AC [32] 48.92 57.35 90.06 96.05 69.86 0.00 18.37 38.35 23.12 75.89 70.40 58.42 40.88 12.96 41.60 Ours 51.93 59.42 93.34 98.36 79.18 0.00 15.75 45.37 50.10 65.52 67.87 22.45 52.45 41.02 43.64. Results on the Large-Scale 3D Indoor Spaces Dataset (S3DIS). Superscripts A and C denote data augmentation and post-processing (CRF) are used.</figDesc><table><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3D-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>r x (cm) r y (cm) r z (cm) mIOU mAcc</figDesc><table><row><cell>2</cell><cell>2</cell><cell>1</cell><cell>49.12 56.63</cell><cell></cell></row><row><cell>2</cell><cell>2</cell><cell>2</cell><cell>51.93 59.42</cell><cell></cell></row><row><cell>2</cell><cell>2</cell><cell>5</cell><cell>51.20 58.97</cell><cell></cell></row><row><cell>2</cell><cell>2</cell><cell>8</cell><cell>49.16 56.91</cell><cell>.</cell></row><row><cell>1</cell><cell>1</cell><cell>2</cell><cell>49.23 56.90</cell><cell></cell></row><row><cell>2</cell><cell>2</cell><cell>2</cell><cell>51.93 59.42</cell><cell></cell></row><row><cell>4</cell><cell>4</cell><cell>2</cell><cell>48.97 57.10</cell><cell></cell></row><row><cell>6</cell><cell>6</cell><cell>2</cell><cell>47.86 56.82</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Varying slice resolutions for RSNs on the S3DIS dataset. r x , r y , and r z indicate the slicing resolution along x, y, and z axis, respectively.</figDesc><table><row><cell cols="5">bs (m) r x (cm) r y (cm) r z (cm) mIOU mAcc</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>51.93 59.42</cell><cell></cell></row><row><cell>1</cell><cell>4 6</cell><cell>4 6</cell><cell>2 2</cell><cell>48.97 57.10 47.86 56.82</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>44.15 52.39</cell><cell></cell></row><row><cell>2</cell><cell>4 6</cell><cell>4 6</cell><cell>2 2</cell><cell>44.59 52.62 43.15 53.07</cell><cell>.</cell></row><row><cell></cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>39.08 49.61</cell><cell></cell></row><row><cell>3</cell><cell>4 6</cell><cell>4 6</cell><cell>2 2</cell><cell>37.77 47.89 37.55 49.01</cell><cell></cell></row><row><cell></cell><cell>8</cell><cell>8</cell><cell>2</cell><cell>37.21 46.35</cell><cell></cell></row><row><cell></cell><cell>16</cell><cell>16</cell><cell>2</cell><cell>35.25 44.70</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Varying sizes of sliding blocks for RSNs on the S3DIS dataset. bs indicates the block size.</figDesc><table><row><cell cols="2">sliding stride during testing mIOU mAcc</cell><cell></cell></row><row><cell>0.2 0.5</cell><cell>52.39 60.52 53.83 61.81</cell><cell>.</cell></row><row><cell>1.0</cell><cell>51.93 59.42</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Varying the testing stride on the S3DIS dataset</figDesc><table><row><cell cols="2">RNN unit mIOU mAcc</cell></row><row><cell cols="2">vanilla RNN 45.84 54.82 GRU 51.93 59.42</cell><cell>.</cell></row><row><cell>LSTM</cell><cell>50.08 57.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Varying RNN units for RSNs on the S3DIS dataset tings remain unchanged as the baseline RSNet in following control experiments except explicitly specified.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>] 14.69 19.90 69.44 88.59 35.93 32.78 2.63 17.96 3.18 32.79 0.00 PointNet++ [24] 34.26 43.77 77.48 92.50 64.55 46.60 12.69 51.32 52.93 52.27 30.23 Ours 39.35 48.37 79.23 94.10 64.99 51.04 34.53 55.95 53.02 55.41 34.84 42.72 31.37 32.97 20.04 2.02 3.56 27.43 18.51 0.00 23.81 2.20 Ours 49.38 54.16 6.78 22.72 3.00 8.75 29.92 37.90 0.95 31.29 18.98</figDesc><table><row><cell>Method</cell><cell cols="2">mIOU mAcc wall floor chair table</cell><cell>desk</cell><cell>bed</cell><cell>book-shelf</cell><cell>sofa</cell><cell>sink</cell></row><row><cell>PointNet [22Method</cell><cell cols="2">bathtub toilet curtain counter door window</cell><cell>shower curtain</cell><cell>refrid-gerator</cell><cell cols="2">picture cabinet</cell><cell>other furniture</cell></row><row><cell>PointNet [22]</cell><cell>0.17 0.00 0.00</cell><cell>5.09 0.00 0.00</cell><cell cols="4">0.00 0.00 0.00 4.99</cell><cell>0.13</cell></row><row><cell>PointNet++ [24]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results on the ScanNet dataset. IOU of each category is also reported here. the vanilla RNN unit, two RNN variants, LSTM and GRU, are proposed to model long-range dependencies in inputs. The effects of different RNN units are compared in Table.5. They show that GRU has the best performance for RSNets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>KD-net [15] 82.3 80.1 74.6 74.3 70.3 88.6 73.5 90.2 87.2 81.0 94.9 57.4 86.7 78.1 51.8 69.9 80.3 PN [22] 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6 PN++ * [24] 84.6 80.4 80.9 60.0 76.8 88.1 83.7 90.2 82.6 76.9 94.7 68.0 91.2 82.1 59.9 78.2 87.5 SSCNN [36] 84.7 81.6 81.7 81.9 75.2 90.2 74.9 93.0 86.1 84.7 95.6 66.7 92.7 81.6 60.6 82.9 82.1 PN++ [24] 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6 Ours 84.9 82.7 86.4 84.1 78.2 90.4 69.3 91.4 87.0 83.5 95.4 66.0 92.6 81.8 56.1 75.8 82.2</figDesc><table><row><cell cols="2">Method mean aero bag cap car chair</cell><cell>ear phone</cell><cell>guitar knife lamp laptop motor mug pistol rocket</cell><cell>skate board</cell><cell>table</cell></row><row><cell>Yi [35]</cell><cell cols="5">81.4 81.0 78.4 77.7 75.7 87.9 61.9 92.0 85.4 82.5 95.7 70.6 91.9 85.9 53.1 69.8 75.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Results on the ShapeNet dataset. PN++ * denotes the PointNet++ trained by us which does not use extra normal information as inputs.</figDesc><table><row><cell>Speed</cell><cell>PointNet (vanilla) [22] 1.0 ×</cell><cell>PointNet [22] 2.2 ×</cell><cell>PointNet++ (SSG) [24] 7.1 ×</cell><cell>PointNet++ (MSG) [24] 14.1 ×</cell><cell>PointNet++ (MRG) [24] 7.5 ×</cell><cell>RSNet 4.5 ×</cell><cell>.</cell></row><row><cell cols="2">Memory 844 MB</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>756 MB</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Computation analysis between PointNet, PointNet++, and RSNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Method mIOU mAcc ceiling floor wall beam column window door chair table bookcase sofa board clutter 66.45 92.48 92.83 78.56 32.75 34.37 51.62 68.11 59.72 60.13 16.42 50.22 44.85 52.03 .</figDesc><table><row><cell cols="2">PointNet [22] 47.71</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>56.47</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>6-fold validation results on the Large-Scale 3D Indoor Spaces Dataset (S3DIS). IOU of each category is also reported.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>ceiling floor wall beam column window door chair table bookcase sofa board clutter Data Per-centage (%) 25.3 23.3 17.3 2.42 1.6</figDesc><table><row><cell>1.1</cell><cell>4.6 3.4 5.3</cell><cell>0.5</cell><cell>3.3 0.7 11.2</cell><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Data portion of each category in the training set of the S3DIS dataset.</figDesc><table><row><cell cols="5">wall floor chair table desk</cell><cell>bed</cell><cell>bookshelf</cell><cell>sofa</cell><cell>sink</cell><cell>bathtub</cell></row><row><cell cols="10">Data Per-centage (%) 36.8 24.9 toilet curtain counter door window shower-curtain refridgerator picture cabinet other furniture 4.6 2.5 1.7 2.6 2.0 2.6 0.3 0.3</cell><cell>.</cell></row><row><cell>Data Per-centage (%) 0.3</cell><cell>1.5</cell><cell>0.6</cell><cell>2.3</cell><cell>0.9</cell><cell>0.2</cell><cell>0.4</cell><cell>0.4</cell><cell>2.6</cell><cell>2.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Data portion of each category in the training set of the ScanNet dataset. 48.37 79.23 94.10 64.99 51.04 34.53 55.95 53.02 55.41 34.84 RSNet with RGB 41.16 50.34 79.38 94.21 63.65 48.67 35.27 53.09 53.67 51.06 41.00 54.16 6.78 22.72 3.00 8.75 29.92 37.90 0.95 31.29 18.98 RSNet with RGB 60.37 63.20 8.30 20.90 15.32 15.67 24.36 39.76 4.30 30.06 20.98</figDesc><table><row><cell>Method</cell><cell>mIOU mAcc wall floor chair table</cell><cell>desk</cell><cell>bed</cell><cell>book-shelf</cell><cell>sofa</cell><cell>sink</cell></row><row><cell cols="2">RSNet 39.35 Method bathtub toilet curtain counter door window</cell><cell>shower curtain</cell><cell>refrid-gerator</cell><cell cols="2">picture cabinet</cell><cell>other furniture</cell></row><row><cell>RSNet</cell><cell>49.38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :Table 13 :</head><label>1213</label><figDesc>Results on the ScanNet dataset. IOU of each category is also reported here. Results of different training strategies on the S3DIS dataset.</figDesc><table><row><cell>Method</cell><cell>mIOU mAcc</cell><cell></cell></row><row><cell>RSNet</cell><cell>51.93 59.42</cell><cell>.</cell></row><row><cell cols="2">RSNet-median 48.68 62.09</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We reproduced the PonitNet and PointNet++ training on ScanNet by using the codes here and here which are published by the authors. The global accuracy of our version of PointNet and PointNet++ are 73.69% and 81.35%, respectively.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d semantic parsing of largescale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04405</idno>
		<title level="m">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<editor>Robotics and Automation (ICRA</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1355" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6070</idno>
		<title level="m">Spatially-sparse convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02890</idno>
		<title level="m">Sparse 3d convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Estimation of camera pose with respect to terrestrial lidar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Point cloud matching based on 3d self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detecting objects in scene point cloud: A combinational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision-3DV 2013, 2013 International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pole-like object detection and classification from urban point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2015 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3032" to="3038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Point cloud labeling using 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2016 23rd International Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2670" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vehicle detection in urban point clouds with orthogonal-view convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2593" to="2597" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Escape from cells: Deep kdnetworks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01222</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fpnn: Field probing neural networks for 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training-based object recognition in cluttered 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision-3DV 2013, 2013 International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and robust multi-view 3d object recognition in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2015 International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="171" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d point cloud object detection with multi-view convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2016 23rd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="585" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic 3d industrial point cloud modeling and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Vision Applications (MVA), 2015 14th IAPR International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exemplar-based 3d shape segmentation in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="203" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ipdc: Iterative part-based dense correspondence between point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pipe-run extraction and reconstruction from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05009</idno>
		<title level="m">Learning deep 3d representations at high resolutions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeppano: Deep panoramic representation for 3-d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2339" to="2343" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segcloud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07563</idno>
		<title level="m">Semantic segmentation of 3d point clouds</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shape inpainting using 3d generative adversarial network and recurrent convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2298" to="2306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00606</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<idno type="arXiv">arXiv:1710.06104</idno>
		<title level="m">Large-scale 3d shape reconstruction and segmentation from shapenet core55</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
