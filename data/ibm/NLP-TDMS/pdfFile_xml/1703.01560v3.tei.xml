<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LR-GAN: LAYERED RECURSIVE GENERATIVE AD- VERSARIAL NETWORKS FOR IMAGE GENERATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
							<email>jw2yang@vt.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
							<email>akannan@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<email>dbatra@gatech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<email>parikh@gatech.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech Blacksburg</orgName>
								<address>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LR-GAN: LAYERED RECURSIVE GENERATIVE AD- VERSARIAL NETWORKS FOR IMAGE GENERATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN. The code is available at https://github.com/jwyang/lr-gan.pytorch. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generative adversarial networks (GANs) <ref type="bibr">(Goodfellow et al., 2014)</ref> have shown significant promise as generative models for natural images. A flurry of recent work has proposed improvements over the original GAN work for image generation <ref type="bibr">(Radford et al., 2015;</ref><ref type="bibr" target="#b2">Denton et al., 2015;</ref><ref type="bibr">Salimans et al., 2016;</ref><ref type="bibr" target="#b0">Chen et al., 2016;</ref><ref type="bibr" target="#b11">Zhu et al., 2016;</ref><ref type="bibr" target="#b10">Zhao et al., 2016)</ref>, multi-stage image generation including part-based models <ref type="bibr">(Im et al., 2016;</ref><ref type="bibr">Kwak &amp; Zhang, 2016)</ref>, image generation conditioned on input text or attributes <ref type="bibr">(Mansimov et al., 2015;</ref><ref type="bibr">Reed et al., 2016b;</ref><ref type="bibr">a)</ref>, image generation based on 3D structure <ref type="bibr" target="#b7">(Wang &amp; Gupta, 2016)</ref>, and even video generation <ref type="bibr" target="#b5">(Vondrick et al., 2016)</ref>.</p><p>While the holistic 'gist' of images generated by these approaches is beginning to look natural, there is clearly a long way to go. For instance, the foreground objects in these images tend to be deformed, blended into the background, and not look realistic or recognizable.</p><p>One fundamental limitation of these methods is that they attempt to generate images without taking into account that images are 2D projections of a 3D visual world, which has a lot of structures in it. This manifests as structure in the 2D images that capture this world. One example of this structure is that images tend to have a background, and foreground objects are placed in this background in contextually relevant ways.</p><p>We develop a GAN model that explicitly encodes this structure. Our proposed model generates images in a recursive fashion: it first generates a background, and then conditioned on the background generates a foreground along with a shape (mask) and a pose (affine transformation) that together define how the background and foreground should be composed to obtain a complete image. Conditioned on this composite image, a second foreground and an associated shape and pose are generated, and so on. As a byproduct in the course of recursive image generation, our approach generates some object-shape foreground-background masks in a completely unsupervised way, without access to any object masks for training. Note that decomposing a scene into foreground-background layers is a classical ill-posed problem in computer vision. By explicitly factorizing appearance and transformation, LR-GAN encodes natural priors about the images that the same foreground can be 'pasted' to the different backgrounds, under different affine transformations. According to the experiments, the absence of these priors result in degenerate foreground-background decompositions, and thus also degenerate final composite images.  <ref type="bibr" target="#b8">(Welinder et al., 2010)</ref>. It generates images in two timesteps. At the first timestep, it generates background images, while generates foreground images, masks and transformations at the second timestep. Then, they are composed to obtain the final images. From top left to bottom right (row major), the blocks are real images, generated background images, foreground images, foreground masks, carved foreground images, carved and transformed foreground images, final composite images, and their nearest neighbor real images in the training set. Note that the model is trained in a completely unsupervised manner.</p><p>We mainly evaluate our approach on four datasets: MNIST-ONE (one digit) and MNIST-TWO (two digits) synthesized from <ref type="bibr">MNIST (LeCun et al., 1998)</ref>, <ref type="bibr">CIFAR-10 (Krizhevsky &amp; Hinton, 2009</ref>) and CUB-200 <ref type="bibr" target="#b8">(Welinder et al., 2010)</ref>. We show qualitatively (via samples) and quantitatively (via evaluation metrics and human studies on Amazon Mechanical Turk) that LR-GAN generates images that globally look natural and contain clear background and object structures in them that are realistic and recognizable by humans as semantic entities. An experimental snapshot on CUB-200 is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We also find that LR-GAN generates foreground objects that are contextually relevant to the backgrounds (e.g., horses on grass, airplanes in skies, ships in water, cars on streets, etc.). For quantitative comparison, besides existing metrics in the literature, we propose two new quantitative metrics to evaluate the quality of generated images. The proposed metrics are derived from the sufficient conditions for the closeness between generated image distribution and real image distribution, and thus supplement existing metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Early work in parametric texture synthesis was based on a set of hand-crafted features <ref type="bibr">(Portilla &amp; Simoncelli, 2000)</ref>. Recent improvements in image generation using deep neural networks mainly fall into one of the two stochastic models: variational autoencoders (VAEs) <ref type="bibr">(Kingma et al., 2016)</ref> and generative adversarial networks (GANs) <ref type="bibr">(Goodfellow et al., 2014)</ref>. VAEs pair a top-down probabilistic generative network with a bottom up recognition network for amortized probabilistic inference. Two networks are jointly trained to maximize a variational lower bound on the data likelihood. GANs consist of a generator and a discriminator in a minmax game with the generator aiming to fool the discriminator with its samples with the latter aiming to not get fooled.</p><p>Sequential models have been pivotal for improved image generation using variational autoencoders: <ref type="bibr">DRAW (Gregor et al., 2015)</ref> uses attention based recurrence conditioning on the canvas drawn so far. In <ref type="bibr" target="#b3">Eslami et al. (2016)</ref>, a recurrent generative model that draws one object at a time to the canvas was used as the decoder in VAE. These methods are yet to show scalability to natural images. Early compelling results using GANs used sequential coarse-to-fine multiscale generation and classconditioning <ref type="bibr" target="#b2">(Denton et al., 2015)</ref>. Since then, improved training schemes <ref type="bibr">(Salimans et al., 2016)</ref> and better convolutional structure <ref type="bibr">(Radford et al., 2015)</ref> have improved the generation results using GANs. <ref type="bibr">PixelRNN (van den Oord et al., 2016)</ref> is also recently proposed to sequentially generates a pixel at a time, along the two spatial dimensions.</p><p>In this paper, we combine the merits of sequential generation with the flexibility of GANs. Our model for sequential generation imbibes a recursive structure that more naturally mimics image composition by inferring three components: appearance, shape, and pose. One closely related work combining recursive structure with GAN is that of <ref type="bibr">Im et al. (2016)</ref> but it does not explicitly model object composition and follows a similar paradigm as by <ref type="bibr">Gregor et al. (2015)</ref>. Another closely related work is that of <ref type="bibr">Kwak &amp; Zhang (2016)</ref>. It combines recursive structure and alpha blending. However, our work differs in three main ways: (1) We explicitly use a generator for modeling the foreground poses. That provides significant advantage for natural images, as shown by our ablation studies; (2) Our shape generator is separate from the appearance generator. This factored representation allows more flexibility in the generated scenes; (3) Our recursive framework generates subsequent objects conditioned on the current and previous hidden vectors, and previously generated object. This allows for explicit contextual modeling among generated elements in the scene. See <ref type="figure" target="#fig_0">Fig. 17</ref> for contextually relevant foregrounds generated for the same background, or <ref type="figure" target="#fig_5">Fig. 6</ref> for meaningful placement of two MNIST digits relative to each.</p><p>Models that provide supervision to image generation using conditioning variables have also been proposed: Style/Structure GANs <ref type="bibr" target="#b7">(Wang &amp; Gupta, 2016)</ref> learns separate generative models for style and structure that are then composed to obtain final images. In <ref type="bibr">Reed et al. (2016a)</ref>, GAN based image generation is conditioned on text and the region in the image where the text manifests, specified during training via keypoints or bounding boxes. While not the focus of our work, the model proposed in this paper can be easily extended to take into account these forms of supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GENERATIVE ADVERSARIAL NETWORKS</head><p>Generative Adversarial Networks (GANs) consist of a generator G and a discriminator D that are simultaneously trained with competing goals: The generator G is trained to generate samples that can 'fool' a discriminator D, while the discriminator is trained to classify its inputs as either real (coming from the training dataset) or fake (coming from the samples of G). This competition leads to a minmax formulation with a value function:</p><formula xml:id="formula_0">min θ G max θ D E x∼p data (x) [log(D(x; θ D ))] + E z∼pz(z) [log(1 − D(G(z; θ G ); θ D ))] ,<label>(1)</label></formula><p>where z is a random vector from a standard multivariate Gaussian or a uniform distribution p z (z), G(z; θ G ) maps z to the data space, D(x) is the probability that x is real estimated by D. The advantage of the GANs formulation is that it lacks an explicit loss function and instead uses the discriminator to optimize the generative model. The discriminator, in turn, only cares whether the sample it receives is on the data manifold, and not whether it exactly matches a particular training example (as opposed to losses such as MSE). Hence, the discriminator provides a gradient signal only when the generated samples do not lie on the data manifold so that the generator can readjust its parameters accordingly. This form of training enables learning the data manifold of the training set and not just optimizing to reconstruct the dataset, as in autoencoder and its variants.</p><p>While the GANs framework is largely agnostic to the choice of G and D, it is clear that generative models with the 'right' inductive biases will be more effective in learning from the gradient information <ref type="bibr" target="#b2">(Denton et al., 2015;</ref><ref type="bibr">Im et al., 2016;</ref><ref type="bibr">Gregor et al., 2015;</ref><ref type="bibr">Reed et al., 2016a;</ref><ref type="bibr" target="#b9">Yan et al., 2015)</ref>. With this motivation, we propose a generator that models image generation via a recurrent process -in each time step of the recurrence, an object with its own appearance and shape is generated and warped according to a generated pose to compose an image in layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LAYERED STRUCTURE OF IMAGE</head><p>An image taken of our 3D world typically contains a layered structure. One way of representing an image layer is by its appearance and shape. As an example, an image x with two layers, foreground f and background b may be factorized as:</p><formula xml:id="formula_1">x = f m + b (1 − m),<label>(2)</label></formula><p>where m is the mask depicting the shapes of image layers, and the element wise multiplication operator. Some existing methods assume the access to the shape of the object either during training <ref type="bibr">(Isola &amp; Liu, 2013)</ref> or both at train and test time <ref type="bibr">(Reed et al., 2016a;</ref><ref type="bibr" target="#b9">Yan et al., 2015)</ref>. Representing images in layered structure is even straightforward for video with moving objects <ref type="bibr" target="#b1">(Darrell &amp; Pentland, 1991;</ref><ref type="bibr" target="#b6">Wang &amp; Adelson, 1994;</ref><ref type="bibr">Kannan et al., 2005)</ref>. <ref type="bibr" target="#b5">Vondrick et al. (2016)</ref> generates videos by separately generating a fixed background and moving foregrounds. A similar way of generating single image can be found in <ref type="bibr">Kwak &amp; Zhang (2016)</ref>.</p><p>Another way is modeling the layered structure with object appearance and pose as:</p><formula xml:id="formula_2">x = ST (f , a) + b,<label>(3)</label></formula><p>where f and b are foreground and background, respectively; a is the affine transformation; ST is the spatial transformation operator. Several works fall into this group <ref type="bibr">(Roux et al., 2011;</ref><ref type="bibr">Huang &amp; Murphy, 2015;</ref><ref type="bibr" target="#b3">Eslami et al., 2016)</ref>. In <ref type="bibr">Huang &amp; Murphy (2015)</ref>, images are decomposed into layers of objects with specific poses in a variational autoencoder framework, while the number of objects (i.e., layers) is adaptively estimated in <ref type="bibr" target="#b3">Eslami et al. (2016)</ref>.</p><p>To contrast with these works, LR-GAN uses a layered composition, and the foreground layers simultaneously model all three dominant factors of variation: appearance f , shape m and pose a. We will elaborate it in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LAYERED RECURSIVE GAN (LR-GAN)</head><p>The basic structure of LR-GAN is similar to GAN: it consists of a discriminator and a generator that are simultaneously trained using the minmax formulation of GAN, as described in §.3.1. The key innovation of our work is the layered recursive generator, which is what we describe in this section.</p><p>The generator in LR-GAN is recursive in that the image is constructed recursively using a recurrent network. Layered in that each recursive step composes an object layer that is 'pasted' on the image generated so far. Object layer at timestep t is parameterized by the following three constituents -'canonical' appearance f t , shape (or mask) m t , and pose (or affine transformation) a t for warping the object before pasting in the image composition. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the architecture of the LR-GAN with the generator architecture unrolled for generating background x 0 ( . = x b ) and foreground x 1 and x 2 . At each time step t, the generator composes the next image x t via the following recursive computation:</p><formula xml:id="formula_3">x t = ST (m t , a t ) affine transformed mask ST (f t , a t ) affine transformed appearance + (1 − ST (m t , a t )) x t−1 pasting on image composed so far , ∀t ∈ [1, T ] (4)</formula><p>where ST ( , a t ) is a spatial transformation operator that outputs the affine transformed version of with a t indicating parameters of the affine transformation.</p><p>Since our proposed model has an explicit transformation variable a t that is used to warp the object, it can learn a canonical object representation that can be re-used to generate scenes where the object occurs as mere transformations of it, such as different scales or rotations. By factorizing the appearance, shape and pose, the object generator can focus on separately capturing regularities in these three factors that constitute an object. We will demonstrate in our experiments that removing these factorizations from the model leads to its spending capacity in variability that may not solely be about the object in Section 5.5 and 5.6. <ref type="figure" target="#fig_1">Fig. 2</ref> shows our LR-GAN architecture in detail -we use different shapes to indicate different kinds of layers (convolutional, fractional convolutional, (non)linear, etc), as indicated by the legend. Our model consists of two main pieces -a background generator G b and a foreground generator G f . G b and G f do not share parameters with each other. G b computation happens only once, while G f is recurrent over time, i.e., all object generators share the same parameters. In the following, we will introduce each module and connections between them. object layers (Eqn. 4). The 'bottom' connections are constructed by a LSTM on the noise vectors z 0 , z 1 , z 2 . Intuitively, this noise-vector-LSTM provides information to the foreground generator about what else has been generated in past. Besides, when generating multiple objects, we use a pooling layer P c f and a fully-connected layer E c f to extract the information from previous generated object response map. By this way, the model is able to 'see' previously generated objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DETAILS OF GENERATOR ARCHITECTURE</head><p>Background Generator. The background generator G b is purposely kept simple. It takes the hidden state of noise-vector-LSTM h 0 l as the input and passes it to a number of fractional convolutional layers (also called 'deconvolution' layer in some papers) to generate images at its end. The output of background generator x b will be used as the canvas for the following generated foregrounds.</p><p>Foreground Generator. The foreground generator G f is used to generate an object with appearance and shape. Correspondingly, G f consists of three sub-modules, G c f , which is a common 'trunk' whose outputs are shared by G i f and G m f . G i f is used to generate the foreground appearance f t , while G m f generates the mask m t for the foreground. All three sub-modules consists of one or more fractional convolutional layers combined with batch-normalization and nonlinear layers. The generated foreground appearance and mask have the same spatial size as the background. The top of G m f is a sigmoid layer in order to generate one channel mask whose values range in (0, 1). Spatial Transformer. To spatially transform foreground objects, we need to estimate the transformation matrix. As in Jaderberg et al. <ref type="formula" target="#formula_0">(2015)</ref>, we predict the affine transformation matrix with a linear layer T f that has six-dimensional outputs. Then based on the predicted transformation matrix, we use a grid generator G g to generate the corresponding sampling coordinates in the input for each location at the output. The generated foreground appearance and mask share the same transformation matrix, and thus the same sampling grid. Given the grid, the sampler S will simultaneously sample the f t and m t to obtainf t andm t , respectively. Different from Jaderberg et al. <ref type="formula" target="#formula_0">(2015)</ref>, our sampler here normally performs downsampling, since the the foreground typically has smaller size than the background. Pixels inf t andm t that are from outside the extent of f t and m t are set to zero. Finally,f t andm t are sent to the compositor C which combines the canvas x t−1 andf t through layered composition with blending weights given bym t (Eqn. 4).</p><p>Pseudo-code for our approach and detailed model configuration are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NEW EVALUATION METRICS</head><p>Several metrics have been proposed to evaluate GANs, such as Gaussian parzen window <ref type="bibr">(Goodfellow et al., 2014)</ref>, Generative Adversarial Metric (GAM) <ref type="bibr">(Im et al., 2016)</ref> and Inception Score <ref type="bibr">(Salimans et al., 2016)</ref>. The common goal is to measure the similarity between the generated data distribution P g (x) = G(z; θ z ) and the real data distribution P (x). Most recently, Inception Score has been used in several works <ref type="bibr">(Salimans et al., 2016;</ref><ref type="bibr" target="#b10">Zhao et al., 2016)</ref>. However, it is an assymetric metric and could be easily fooled by generating centers of data modes.</p><p>In addition to these metrics, we present two new metrics based on the following intuition -a sufficient (but not necessary) condition for closeness of P g (x) and P (x) is closeness of P g (x|y) and P (x|y), i.e., distributions of generated data and real data conditioned on all possible variables of interest y, e.g., category label. One way to obtain this variable of interest y is via human annotation. Specifically, given the data sampled from P g (x) and P (x), we ask people to label the category of the samples according to some rules. Note that such human annotation is often easier than comparing samples from the two distributions (e.g., because there is no 1:1 correspondence between samples to conduct forced-choice tests).</p><p>After the annotations, we need to verify whether the two distributions are similar in each category. Clearly, directly comparing the distributions P g (x|y) and P (x|y) may be as difficult as comparing P g (x) and P (x). Fortunately, we can use Bayes rule and alternatively compare P g (y|x) and P (y|x), which is a much easier task. In this case, we can simply train a discriminative model on the samples from P g (x) and P (x) together with the human annotations about categories of these samples. With a slight abuse of notation, we use P g (y|x) and P (y|x) to denote probability outputs from these two classifiers (trained on generated samples vs trained on real samples). We can then use these two classifiers to compute the following two evaluation metrics:</p><p>Adversarial Accuracy: Computes the classification accuracies achieved by these two classifiers on a validation set, which can be the training set or another set of real images sampled from P (x). If P g (x) is close to P (x), we expect to see similar accuracies.</p><p>Adversarial Divergence: Computes the KL divergence between P g (y|x) and P (y|x). The lower the adversarial divergence, the closer two distributions are. The low bound for this metric is exactly zero, which means P g (y|x) = P (y|x) for all samples in the validation set.</p><p>As discussed above, we need human efforts to label the real and generated samples. Fortunately, we can further simplify this. Based on the labels given on training data, we split the training data into categories, and train one generator for each category. With all these generators, we generate samples of all categories. This strategy will be used in our experiments on the datasets with labels given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>We conduct qualitative and quantitative evaluations on three datasets: 1) MNIST <ref type="bibr">(LeCun et al., 1998)</ref>; 2) CIFAR-10 (Krizhevsky &amp; Hinton, 2009); 3) CUB-200 <ref type="bibr" target="#b8">(Welinder et al., 2010)</ref>. To add variability to the MNIST images, we randomly scale (factor of 0.8 to 1.2) and rotate (− π 4 to π 4 ) the digits and then stitch them to 48 × 48 uniform backgrounds with random grayscale value between <ref type="bibr">[0,</ref><ref type="bibr">200]</ref>. Images are then rescaled back to 32 × 32. Each image thus has a different background grayscale value and a different transformed digit as foreground. We rename this sythensized dataset as MNIST-ONE (single digit on a gray background). We also synthesize a dataset MNIST-TWO containing two digits on a grayscale background. We randomly select two images of digits and perform similar transformations as described above, and put one on the left and the other on the right side of a 78 × 78 gray background. We resize the whole image to 64 × 64.</p><p>We develop LR-GAN based on open source code 1 . We assume the number of objects is known. Therefore, for MNIST-ONE, MNIST-TWO, CIFAR-10, and CUB-200, our model has two, three, two, and two timesteps, respectively. Since the size of foreground object should be smaller than that of canvas, we set the minimal allowed scale 2 in affine transforamtion to be 1.2 for all datasets except for MNIST-TWO, which is set to 2 (objects are smaller in MNIST-TWO). In LR-GAN, the  background generator and foreground generator have similar architectures. One difference is that the number of channels in the background generator is half of the one in the foreground generator. We compare our results to that of <ref type="bibr">DCGAN (Radford et al., 2015)</ref>. Note that LR-GAN without LSTM at the first timestep corresponds exactly to the DCGAN. This allows us to run controlled experiments. In both generator and discriminator, all the (fractional) convolutional layers have 4 × 4 filter size with stride 2. As a result, the number of layers in the generator and discriminator automatically adapt to the size of training images. Please see the Appendix (Section 6.2) for details about the configurations. We use three metrics for quantitative evaluation, including Inception Score (Salimans et al., 2016) and the proposed Adversarial Accuracy, Adversarial Divergence. Note that we report two versions of Inception Score. One is based on the pre-trained Inception net, and the other one is based on the pre-trained classifier on the target datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">QUALITATIVE RESULTS</head><p>In <ref type="figure" target="#fig_2">Fig. 3 and 4</ref>, we show the generated samples for CIFAR-10 and CUB-200, respectively. MNIST results are shown in the next subsection. As we can see from the images, the compositional nature of our model results in the images being free of blending artifacts between backgrounds and foregrounds. For CIFAR-10, we can see the horses and cars with clear shapes. For CUB-200, the bird shapes tend to be even sharper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MNIST-ONE AND MNIST-TWO</head><p>We now report the results on MNIST-ONE and MNIST-TWO. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the generation results of our model on MNIST-ONE. As we can see, our model generates the background and the foreground in separate timestep, and can disentagle the foreground digits from background nearly perfectly. Though initial values of the mask randomly distribute in the range of (0, 1), after training, the masks are nearly binary and accurately carve out the digits from the generated foreground. More results on MNIST-ONE (including human studies) can be found in the Appendix (Section 6.3). <ref type="figure" target="#fig_5">Fig. 6</ref> shows the generation results for MNIST-TWO. Similarly, the model is also able to generate background and the two foreground objects separately. The foreground generator tends to generate a single digit at each timestep. Meanwhile, it captures the context information from the previous time steps. When the first digit is placed to the left side, the second one tends to be placed on the right side, and vice versa.  , the image blocks are real images, generated background images, foreground images and masks at the second timestep, composite images at the second time step, generated foreground images and masks at the third timestep and the final composite images, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">CUB-200</head><p>We study the effectiveness of our model trained on the CUB-200 bird dataset. In <ref type="figure" target="#fig_0">Fig. 1</ref>, we have shown a random set of generated images, along with the intermediate generation results of the model. While being completely unsupervised, the model, for a large fraction of the samples, is able to  successfully disentangle the foreground and the background. This is evident from the generated bird-like masks.</p><p>We do a comparative study based on Amazon Mechanical Turk (AMT) between DCGAN and LR-GAN to quantify relative visual quality of the generated images. We first generated 1000 samples from both the models. Then, we performed perfect matching between the two image sets using the Hungarian algorithm on L2 norm distance in the pixel space. This resulted in 1000 image pairs. Some examplar pairs are shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. For each image pair, 9 judges are asked to choose the one that is more realistic. Based on majority voting, we find that our generated images are selected 68.4% times, compared with 31.6% times for DCGAN. This demonstrates that our model has generated more realistic images than DCGAN. We can attribute this difference to our model's ability to generate foreground separately from the background, enabling stronger edge cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">CIFAR-10</head><p>We now qualitatively and quantitatively evaluate our model on CIFAR-10, which contains multiple object categories and also various backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of image generation quality:</head><p>We conduct AMT studies to compare the fidelity of image generation. Towards this goal, we generate 1000 images from DCGAN and LR-GAN, respectively. We ask 5 judges to label each image to either belong to one of the 10 categories or as 'non recognizable' or 'recognizable but not belonging to the listed categories'. We then assign each image a quality level between [0,5] that captures the number of judges that agree with the majority choice. <ref type="figure" target="#fig_7">Fig. 8</ref> shows the images generated by both approaches, ordered by increasing quality level. We merge images at quality level 0 (all judges said non-recognizable) and 1 together, and similarly images at level 4 and 5. Visually, the generated samples by our model have clearer boundaries and object structures. We also computed the fraction of non-recognizable images: Our model had a 10% absolute drop in non-recognizability rate (67.3% for ours vs. 77.7% for DCGAN). For reference, 11.4% of real CIFAR images were categorized as non-recognizable. <ref type="figure" target="#fig_8">Fig. 9</ref> shows more generated (intermediate) results of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative evaluation on generators:</head><p>We evaluate the generators based on three metrics: 1) Inception Score; 2) Adversarial Accuracy; 3) Adversarial Divergence. To obtain a classifier model for evaluation, we remove the top layer in the discriminator used in our model, and then append two fully connected layers on the top of it. We train this classifier using the training samples of CIFAR-10 based on the annotations. Following Salimans et al. <ref type="formula" target="#formula_0">(2016)</ref>, we generated 50,000 images   based on DCGAN and LR-GAN, repsectively. We compute two types of Inception Scores. The standard Inception Score is based on the Inception net as in <ref type="bibr">Salimans et al. (2016)</ref>, and the contextual Inception Score is based on our trained classifier model. To distinguish, we denote the standard one as 'Inception Score † ', and the contextual one as 'Inception Score † † '. To obtain the Adversarial Accuracy and Adversarial Divergence scores, we train one generator on each of 10 categories for DCGAN and LR-GAN, respectively. Then, we use these generators to generate samples of different categories. Given these generated samples, we train the classifiers for DCGAN and LR-GAN separately. Along with the classifier trained on the real samples, we compute the Adversarial Accuracy and Adversarial Divergence on the real training samples. In <ref type="table" target="#tab_0">Table 1</ref>, we report the Inception Scores, Adversarial Accuracy and Adversarial Divergence for comparison. We can see that our model outperforms DCGAN across the board. To point out, we obtan different Inception Scores based on different classifier models, which indicates that the Inception Score varies with different models.</p><p>Quantitative evaluation on discriminators: We evaluate the discriminator as an extractor for deep representations. Specifically, we use the output of the last convolutional layer in the discriminator as features. We perform a 1-NN classification on the test set given the full training set. Cosine similarity is used as the metric. On the test set, our model achieves 62.09%±0.01% compared to DCGAN's 56.05%±0.02%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual generation:</head><p>We also show the efficacy of our approach to generate diverse foregrounds conditioned on fixed background. The results in <ref type="figure" target="#fig_0">Fig. 17</ref> in Appendix showcase that the foreground generator generates objects that are compatible with the background. This indicates that the model has captured contextual dependencies between the image layers.</p><p>Category specific models: The objects in CIFAR-10 exhibit huge variability in shapes. That can partly explain why some of the generated shapes are not as compelling in <ref type="figure" target="#fig_8">Fig. 9</ref>. To test this hypothesis, we reuse the generators trained for each of 10 categories used in our metrics to obtain the generation results. <ref type="figure" target="#fig_0">Fig. 10</ref> shows results for categories 'horse', 'frog' and 'cat'. We can see that the model is now able to generate object-specific appearances and shapes, similar in vein to our results on the CUB-200 dataset.  <ref type="figure" target="#fig_0">Fig. 11</ref> shows results from an ablated model without affine transformations in the foreground layers, and compares the results with the full model that does include these transformations. We note that one significant problem emerges that the decompositions are degenerate, in the sense that the model is unable to break the symmetry between foreground and background layers, often generating object appearances in the model's background layer and vice versa. For CUB-200, the final generated images have some blendings between foregrounds and backgrounds. This is particularly the case for those images without bird-shape masks. For CIFAR-10, a number of generated masks are inverted. In this case, the background images are carved out as the foreground objects. The foreground generator takes almost all the duty to generate the final images, which make it harder to generate images as clear as the model with transformation. From these comparisons, we qualitatively demonstrate the importance of modeling transformations in the foreground generation process. Another merit of using transformation is that the intermediate outputs of the model are more interpretable and faciliate to the downstreaming tasks, such as scene paring, which is demonstrated in Section 6.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">IMPORTANCE OF TRANSFORMATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">IMPORTANCE OF SHAPES</head><p>We perform another ablation study by removing the mask generator to understand the importance of modeling object shapes. In this case, the generated foreground is simply pasted on top of the generated background after being transformed. There is no alpha blending between the foregrounds and backgrounds. The generation results for three datasets, MNIST-ONE, CUB-200, CIFAR-10 are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. As we can see, though the model works well for the generation of MNIST-ONE, it fails to generate reasonable images across the other datasets. Particularly, the training does not even converge for CUB-200. Based on these results, we qualitatively demonstrate that mask generator in our model is fairly important to obtain plausible results, especially for realistic images.  We conduct human studies on generation results on MNIST-ONE. Specifically, we generate 1,000 images using both LR-GAN and DCGAN. As references, we also include 1000 real images. Then we ask the users on AMT to label each image to be one of the digits (0-9). We also provide them an option 'non recognizable' in case the generated image does not seem to contain a digit. Each image was judged by 5 unique workers. Similar to CIFAR-10, if an image is recognized to be the same digit by all 5 users, it is assigned to quality level 5. If it is not recognizable according to all users, it is assigned to quality level 0. <ref type="figure" target="#fig_0">Fig. 13 (left)</ref> shows the number of images assigned to all six quality levels. Compared to DCGAN, our model generated more samples with high quality levels.</p><p>As expected, the real images have many samples with high quality levels. In <ref type="figure" target="#fig_0">Fig. 13 (right)</ref>, we show the number of images that are recognized to each digit category (0-9). For qualitative comparison, we show examplar images at each quality level in <ref type="figure" target="#fig_0">Fig. 14.</ref> From left to right, the quality level increases from 0 to 5. As expected, the images with higher quality level are more clear.</p><p>For quantitative evaluation, we use the same way as for CIFAR-10. The classifier model used for contextual Inception Score is trained based on the training set. We generate 60,000 samples based on DCGAN and LR-GAN for evaluation, respectively. To obtain the Adversarial Accuracy and Adversarial Divergence, we first train 10 generators for 10 digit categories separately, and then use the generated samples to train the classifier. As shown in <ref type="table" target="#tab_2">Table 3</ref>, our model has higher scores than DCGAN on both standard and contextual Inception Score. Also, our model has a slightly higher   adversarial accuracy, and lower adversarial divergence than DCGAN. We find that the all three image sets have low standard Inception Scores. This is mainly because the Inception net is trained on ImageNet, which has a very different data distribution from the MNIST dataset. Based on this, we argue that the standard Inception Score is not suitable for some image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">MORE RESULTS ON CUB-200</head><p>In this experiment, we reduce the minimal allowed object scale to 1.1, which allows the model to generate larger foreground objects. The results are shown in <ref type="figure" target="#fig_0">Fig. 15</ref>. Similar to the results when the constraint is 1.2, the crisp bird-like masks are generated automatically by our model.    learns where to place the generated faces so that the whole image looks natural. For comparison, please refer to <ref type="bibr">(Kwak &amp; Zhang, 2016</ref>) which does not model the transformation. We can find the generation results degrade much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">STATISTICS ON TRANSFORMATION MATRICES</head><p>In this part, we analyze the statistics on the transformation matrices generated by our model for different datasets, including MNIST-ONE, CUB-200, CIFAR-10 and LFW. We used affine transformation in our model. So there are 6 parameters, scaling in the x coordinate (s x ), scaling in the y coordinate (s y ), translation in the x coordinate (t x ), translation in the y coordinate (t y ), rotation in the x coordinate (r x ) and rotation in the y coordinate (r y ). In <ref type="figure" target="#fig_1">Fig. 20</ref>, we show the histograms on different parameters for different datasets.These histograms show that the model produces non-trivial varied scaling, translation and rotation on all datasets. For different datasets, the learned transformation have different patterns. We hypothesize that this is mainly determined by the configurations of objects in the images. For example, on MNIST-ONE, all six parameters have some fluctuations since the synthetic dataset contains digits randomly placed at different locations. For the other three datasets, the scalings converge to single value since the object sizes do not vary much, and the variations on rotation and translation suffice to generate realistic images. Specifically, we can find the generator largely relies on the translation on x coordinate for generating CUB-200. This makes sense since birds in the images have similar scales, orientations but various horizontal locations. For CIFAR-10, since there are 10 different object categories, the configurations are more diverse, hence the generator uses all parameters for generation except for the scaling. For LFW, since faces have similar configurations, the learned transformations have less fluctuation as well. As a result, we can see that LR-GAN indeed models the transformations on the foreground to generate images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">CONDITIONAL IMAGE GENERATION</head><p>Considering our model can generate object-like masks (shapes) for images, we conducted an experiment to evaluate whether our model can be potentially used for image segmentation and object detection. We make some changes to the model. For the background generator, the input is a real image instead of a random vector. Then the image is passed through an encoder to extract the hidden features, which replaces the random vector z 0 and are fed to the background generator. For the foreground generator, we subtract the image generated by the background generator from the input image to obtain a residual image. Then this residual image is fed to the same encoder to get the hidden features, which are used as the input for foreground generator. In our conditional model, we want to reconstruct the image, so we add a reconstruction loss along with the adversarial loss. We train this conditional model on CIFAR-10. The (intermediate) outputs of the model is shown in <ref type="figure" target="#fig_0">Fig. 21</ref>. Interestingly, the model successfully learned to decompose the input images into background and foreground. The background generator tends to do an image inpainting by generating a complete background without object, while the foreground generator works as a segmentation model to get object mask from the input image.</p><p>Similarly, we also run the conditional LR-GAN on LFW dataset. As we can see in <ref type="figure" target="#fig_1">Fig. 22</ref>, the foreground generator automatically and consistently learned to generate the face regions, even though there are large portion of background in the input images. In other words, the conditional LR-GAN successfully learned to detection faces in images. We suspect this success is due to that it has low cost for the generator to generate similar images, and thus converge to the case that the first generator generate background, and the second generator generate face images.</p><p>Based on these experiments, we argue that our model can be possibly used for image segmentation and object detection in a generative and unsupervised manner. One future work would be verifying this by applying it to high-resolution and more complicate datasets.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Generation results of our model on CUB-200</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Temporal Connections. LR-GAN has two kinds of temporal connections -informally speaking, one on 'top' and one on 'bottom'. The 'top' connections perform the act of sequentially 'pasting' LR-GAN architecture unfolded to three timesteps. It mainly consists of one background generator, one foreground generator, temporal connections and one discriminator. The meaning of each component is explained in the legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Generated images on CIFAR-10 based on our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Generated images on CUB-200 based on our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Generation results of our model on MNIST-ONE. From left to right, the image blocks are real images, generated background images, generated foreground images, generated masks and final composite images, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Generation results of our model on MNIST-TWO. From top left to bottom right (row major)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Matched pairs of generated images based on DCGAN and LR-GAN, respectivedly. The odd columns are generated by DCGAN, and the even columns are generated by LR-GAN. These are paired according to the perfect matching based on Hungarian algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative comparison on CIFAR-10. Top three rows are images generated by DCGAN; Bottom three rows are by LR-GAN. From left to right, the blocks display generated images with increasing quality level as determined by human studies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Generation results of our model on CIFAR-10. From left to right, the blocks are: generated background images, foreground images, foreground masks, foreground images carved out by masks, carved foregrounds after spatial transformation, final composite images and nearest neighbor training images to the generated images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Category specific generation results of our model on CIFAR-10 categories of horse, frog, and cat (top to bottom). The blocks from left to right are: generated background images, foreground images, foreground masks, foreground images carved out by masks, carved foregrounds after spatial transformation and final composite images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Generation results from an ablated LR-GAN model without affine transformations. From top to bottom, the block rows correspond to different datasets: MNIST-ONE, CUB-200, CIFAR-10. From left to right, the blocks show generated background images, foreground images, foreground masks, and final composite images. For comparison, the rightmost column block shows final generated images from a non-ablated model with affine transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Generation results from an ablated LR-GAN model without mask generator. The block rows correspond to different datasets (from top to bottom: MNIST-ONE, CUB-200, CIFAR-10). From left to right, the blocks show generated background images, foreground images, transformed foreground images, and final composite images. For comparison, the rightmost column block shows final generated images from a non-ablated model with mask generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>•</head><label></label><figDesc>MNIST-ONE: G b : (256)4c-(128)4c2s-(64)4c2s-(3)4c2s; G c f : (512)4c-(256)4c2s-(128)4c2s; G i f : (3)4c2s; G m f : (1)4c2s; D: (64)4c2s-(128)4c2s-(256)4c2s-(256)4p4s-1 • MNIST-TWO: G b : (256)4c-(128)4c2s-(64)4c2s-(32)4c2s-(3)4c2s; G c f : (512)4c-(256)4c2s-(128)4c2s-(64)4c2s; G i f : (3)4c2s; G m f : (1)4c2s;D: (64)4c2s-(128)4c2s-(256)4c2s-(512)4c2s-(512)4p4s-1 • CUB-200: G b : (512)4c-(256)4c2s-(128)4c2s-(64)4c2s-(3)4c2s; G c f : (1024)4c-(512)4c2s-(256)4c2s-(128)4c2s; G i f : (3)4c2s; G m f : (1)4c2s;D: (128)4c2s-(256)4c2s-(512)4c2s-(1024)4c2s-(1024)4p4s-1 • CIFAR-10: G b : (256)4c-(128)4c2s-(64)4c2s-(3)4c2s; G c f : (512)4c-(256)4c2s-(128)4c2s; G i f : (3)4c2s; G m f : (1)4c2s D: (64)4c2s-(128)4c2s-(256)4c2s-(256)4p4s-1 6.3 RESULTS ON MNIST-ONE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Statistics of annotations in human studies on MNIST-ONE. Left: distribution of quality level; Right: distribution of recognized digit categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Qualitative comparison on MNIST-ONE. Top three rows are samples generated by DC-GAN. Bottom three rows are samples generated by LR-GAN. The quality level increases from left to right as determined via human studies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Generation results of our model on CUB-200 when setting minimal allowed scale to 1.1. From left to right, the blocks show the generated background images, foreground images, foreground masks, foreground images carved out by masks, carved foreground images after spatial transformation. The sixth and seventh blocks are final composite images and the nearest neighbor real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 :</head><label>17</label><figDesc>Walking in the latent foreground space by fixing backgrounds in our model on CIFAR-10. From left to right, the blocks are: generated background images, foreground images, foreground masks, foreground images carved out by masks, carved out foreground images after spatial transformation, and final composite images. Each row has the same background, but different foregrounds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 :</head><label>18</label><figDesc>Statistics of annotations in human studies on CIFAR-10. Left to right: word cloud for real images, images generated by DCGAN, images generated by LR-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 :</head><label>19</label><figDesc>Generation results of our model on LFW. From left to right, the blocks are: generated background images, foreground images, foreground masks, carved out foreground images after spatial transformation, and final composite images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 20 :</head><label>20</label><figDesc>Histograms of transformation parameters learnt in our model for different datasets. From left to right, the datasets are: MNIST-ONE, CUB-200, CIFAR-10 and LFW. From top to bottom, they are scaling s x , s y , translation t x , t y , and rotation r x , r y in x and y coordinate, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 21 :</head><label>21</label><figDesc>Conditional generation results of our model on CIFAR-10. From left to right, the blocks are: real images, generated background images, foreground images, foreground masks, foreground images carved out by masks, carved foreground images after spatial transformation, and final composite (reconstructed) images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 22 :</head><label>22</label><figDesc>Conditional generation results of our model on LFW, displayed with the same layout to Fig. 21.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison between DCGAN and LR-GAN on CIFAR-10. Evaluate using the pre-trained Inception net as Salimans et al. (2016) † † Evaluate using the supervisedly trained classifier based on the discriminator in LR-GAN.</figDesc><table><row><cell>Training Data</cell><cell>Real Images</cell><cell>DCGAN</cell><cell>Ours</cell></row><row><cell>Inception Score  †</cell><cell>11.18±0.18</cell><cell>6.64±0.14</cell><cell>7.17±0.07</cell></row><row><cell>Inception Score  † †</cell><cell>7.23±0.09</cell><cell>5.69±0.07</cell><cell>6.11±0.06</cell></row><row><cell>Adversarial Accuracy</cell><cell cols="2">83.33±0.08 37.81±0.02</cell><cell>44.22 ±0.08</cell></row><row><cell>Adversarial Divergence</cell><cell>0</cell><cell>7.58±0.04</cell><cell>5.57±0.06</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>†</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Information and model configurations on different datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">MNIST-ONE MNIST-TWO</cell><cell>CIFAR-10</cell><cell>CUB-200</cell></row><row><cell>Image Size</cell><cell>32</cell><cell>64</cell><cell>32</cell><cell>64</cell></row><row><cell>#Images</cell><cell>60,000</cell><cell>60,000</cell><cell>50,000</cell><cell>5,994</cell></row><row><cell>#Timesteps</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>2</cell></row><row><cell cols="5">#Parameters 5.25M/4.11M 7.53M/6.33M 5.26M/4.11M 27.3M/6.34M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison on MNIST-ONE. Evaluate using the pre-trained Inception net as Salimans et al. (2016) † † Evaluate using the supervisedly trained classifier based on the discriminator in LR-GAN.</figDesc><table><row><cell>Training Data</cell><cell>Real Images</cell><cell>DCGAN</cell><cell>Ours</cell></row><row><cell>Inception Score  †</cell><cell>1.83±0.01</cell><cell>2.03±0.01</cell><cell>2.06±0.01</cell></row><row><cell>Inception Score  † †</cell><cell>9.15±0.04</cell><cell>6.42±0.03</cell><cell>7.15±0.04</cell></row><row><cell>Adversarial Accuracy</cell><cell cols="2">95.22 ± 0.25 26.12 ± 0.07</cell><cell>26.61 ± 0.06</cell></row><row><cell>Adversarial Divergence Score</cell><cell>0</cell><cell>8.47 ± 0.03</cell><cell>8.39 ± 0.04</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>†</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/soumith/dcgan.torch 2 Scale corresponds to the size of the target canvas with respect to the object -the larger the scale, the larger the canvas, and the smaller the relative size of the object in the canvas. 1 means the same size as the canvas.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1 Stochastic Layered Recursive Image Generation 1: z 0 ∼ N (0, I) 2: </p><p>predict shared represenation embedding 18:</p><p>x t−1 19: end for 6.2 MODEL CONFIGURATIONS <ref type="table">Table 2</ref> lists the information and model configuration for different datasets. The dimensions of random vectors and hidden vectors are all set to 100. We also compare the number of parameters in DCGAN and LR-GAN. The numbers before '/' are our model, after '/' are DCGAN. Based on the same notation used in <ref type="bibr" target="#b10">(Zhao et al., 2016)</ref>, the architectures for the different datasets are:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03657</idno>
		<title level="m">Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust estimation of a multi-layered motion representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Visual Motion</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1603.08575</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1601.06759</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02612</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representing moving images with layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05631</idno>
		<title level="m">Generative image modeling using style and structure adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1512.00570</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">1 illustrates the generative process in our model. g( ) evaluates the function g at . • is a composition operator that composes its operands so that f • g(</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Algo</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>= f (g(</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">16, we show more results on CIFAR-10 when setting minimal allowed object scale to 1.1. The rightmost column block also shows the training images that are closest to the generated images (cosine similarity in pixel space)</title>
		<imprint/>
	</monogr>
	<note>We can see our model does not memorize the training data. Figure 16: Generation results of our model on CIFAR-10. with minimal allowed scale be 1.1, From left to right, the layout is same to Fig. 15</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Note that our model has two or more inputs. So we can walk along any of them or their combination. In Fig. 17, we generate multiple foregrounds for the same fixed generated background. We find that our model consistently generates contextually compatible foregrounds. For example, for the grass-like backgrounds, the foreground generator generates horses and deer</title>
		<imprint/>
	</monogr>
	<note>Similar to DCGAN, we also show results by walking in the latent space. and airplane-like objects for the blue sky</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Besides asking people to select a name from a list for an image, we also conducted another human study where we ask people to use one word (free-form) to describe the main object in the image. Each image was &apos;named&apos; by 5 unique people. We generate word clouds for real images, images generated by DCGAN and LR-GAN, as shown in Fig</title>
	</analytic>
	<monogr>
		<title level="m">As we mentioned above, we conducted human studies on CIFAR-10</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Different from previous works which work on cropped and aligned faces, we directly generate the original images which contains a large portion of backgrounds. This configuration helps to verify the efficiency of LR-GAN to model the object appearance, shape and pose. In Fig. 19, we show the (intermediate) generation results of LR-GAN. Surprisingly, without any supervisions, the model generated background and faces in separate steps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Results On Lfw Face Dataset We</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LFW dataset</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>and the generated masks accurately depict face shapes. Moreover, the model</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
