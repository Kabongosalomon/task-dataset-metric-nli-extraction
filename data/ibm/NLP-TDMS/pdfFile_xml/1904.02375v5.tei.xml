<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONVPOINT: CONTINUOUS CONVOLUTIONS FOR POINT CLOUD PROCESSING PREPRINT. ACCEPTED AT COMPUTER &amp; GRAPHICS, SEE FINAL VERSION IN JOURNAL ISSUE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ONERA</orgName>
								<orgName type="institution" key="instit2">Université Paris-Saclay</orgName>
								<address>
									<postCode>FR-91123</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CONVPOINT: CONTINUOUS CONVOLUTIONS FOR POINT CLOUD PROCESSING PREPRINT. ACCEPTED AT COMPUTER &amp; GRAPHICS, SEE FINAL VERSION IN JOURNAL ISSUE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point clouds are unstructured and unordered data, as opposed to images. Thus, most machine learning approach developed for image cannot be directly transferred to point clouds. In this paper, we propose a generalization of discrete convolutional neural networks (CNNs) in order to deal with point clouds by replacing discrete kernels by continuous ones. This formulation is simple, allows arbitrary point cloud sizes and can easily be used for designing neural networks similarly to 2D CNNs. We present experimental results with various architectures, highlighting the flexibility of the proposed approach. We obtain competitive results compared to the state-of-the-art on shape classification, part segmentation and semantic segmentation for large-scale point clouds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Point clouds are widely used in varied domains such as historical heritage preservation and autonomous driving. They can be either directly generated using active sensors such as LI-DARs or RGB-Depth sensors, or an intermediary product of photogrammetry.</p><p>These point sets are sparse samplings of the underlying surface of scenes or objects. With the exception of structured acquisitions (e.g., with LIDARs), point clouds are generally unordered and without spatial structure; they cannot be sampled on a regular grid and processed as image pixels. Moreover, the points may or may not hold colorimetric features. Thus, point-cloud dedicated processing must be able to deal only with the relative positions of the points.</p><p>Due to these considerations, methods developed for image processing cannot be used directly on point clouds. In particular, convolutional neural networks (CNNs) have reached state-ofthe-art performance in many image processing tasks. They use discrete convolutions which make extensive use the grid structure of the data, which does not exist with point clouds.</p><p>Two common approaches to circumvent this problem are: first, to project the points into a space suitable for discrete convolutions, e.g., voxels; second, to reformulate the CNNs to take into account point clouds' unstructured nature. In this paper, we adopt the second approach and propose a generalization of CNNs for point clouds. The main contributions of this paper are two-fold.</p><p>First, we introduce a continuous convolution formulation designed for unstructured data. The continuous convolution is a simple and straightforward extension of the discrete one.</p><p>Second, we show that this continuous convolution can be used to build neural networks similarly to its image processing counterpart. We design neural networks using this convolution and a hierarchical data representation structure based on a search tree.</p><p>We show that our framework, ConvPoint, which extends our work presented in <ref type="bibr" target="#b10">[11]</ref>, can be applied to various classification and segmentation tasks, including large scale indoor and outdoor semantic segmentation. For each task, we show that ConvPoint is competitive with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1904.02375v5 [cs.CV] 19 Feb 2020</head><p>The paper is organized as follows: Section 2 presents the related work, Section 3 describes the continuous convolutional formulation, Section 4 is dedicated to the spatial representation of the data, and Section 5 presents the convolution as a layer. Finally, Section 6 shows experiments on different datasets for classification and semantic segmentation of point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Point cloud processing is a widely discussed topic. We focus here on machine learning techniques for point cloud classification or local attribute estimation.</p><p>Most of the early methods use handcrafted features defined using a point and its neighborhood, or a local estimate of the underlying surface around the point. These features describe specific properties of the shape and are designed to be invariant to rigid or non rigid transformations of the shape <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>. Then, classical machine learning algorithms are trained using these descriptors.</p><p>In the last years, the release of large annotated point cloud databases has allowed the development of deep neural network methods that can learn both descriptors and decision functions.</p><p>The direct adaptation of CNNs developed for image processing is to use 3D convolutions. Methods like <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b54">55]</ref> apply 3D convolutions on a voxel grid. Even though recent hardware advances enable the use of these networks on larger scenes, they are still time consuming and require a relatively low voxel resolution, which may result in a loss of information and undesirable bias due to grid axis alignment. In order to avoid these drawbacks, <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> use sparse convolutional kernels and <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b50">51]</ref> scan the 3D space to focus computation where objects are located.</p><p>A second class of algorithms avoids 3D convolutions by creating 2D representations of the shape, applying 2D CNNs and projecting the results back to 3D. This approach has been used for object classification <ref type="bibr" target="#b46">[47]</ref>, jointly with voxels <ref type="bibr" target="#b38">[39]</ref>, and for semantic segmentation <ref type="bibr" target="#b11">[12]</ref>. One of the main issues of using multi-view frameworks is to design an efficient and robust strategy for choosing viewpoints.</p><p>The previous methods are based on 2D or 3D CNNs, creating the need to structure the data (3D grid or 2D image) in order to process it. Recently, work has focused on developing deep learning architectures for non-Euclidean data. This work, referred to as geometric deep learning <ref type="bibr" target="#b12">[13]</ref>, operates on manifolds, graphs, or directly on point clouds.</p><p>Neural networks on graphs were pioneered in <ref type="bibr" target="#b41">[42]</ref>. Since then, several methods have been developed using the spectral domain from the graph Laplacian <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b57">58]</ref> or polynomials <ref type="bibr" target="#b17">[18]</ref> as well as gated recurrent units for message passing <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>The first extension of CNNs to manifolds is Geodesic CNN <ref type="bibr" target="#b33">[34]</ref>, which redefines usual layers such as convolution or max pooling. It is applied to local mesh patches in a polar representation to learn invariant shape features for shape correspondences. In <ref type="bibr" target="#b9">[10]</ref>, the representation is improved with anisotropic diffusion kernels and the resulting method is not bound to triangular meshes anymore. More recently, MoNet <ref type="bibr" target="#b35">[36]</ref> offers a unified formulation of CNNs on manifolds which included <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">34]</ref> and reaches state-of-the-art performance on shape correspondence benchmarks.</p><p>These methods have proved to be very efficient but they usually operate on graphs or meshes for surface analysis. However, building a mesh from raw point clouds is a very difficult task and requires in practice priors regarding the surface to be reconstructed <ref type="bibr" target="#b7">[8]</ref>.</p><p>A fourth class of machine learning approaches processes directly the raw point clouds; the method proposed in this paper belongs to this category.</p><p>One of the recent breakthroughs in dealing with unstructured data is PointNet <ref type="bibr" target="#b37">[38]</ref>. The key idea is to construct a transfer function invariant by permutation of the inputs features, obtained by using the order invariant max pooling function. The coordinates of the points are given as input and geometric operations (affine transformations) are obtained with small auxiliary networks. However, it fails to capture local structures. Its improvement, PointNet++ <ref type="bibr" target="#b39">[40]</ref>, uses a cascade of PointNet networks from local scale to global scale.</p><p>Convolutional neural layers are widely used in machine learning for image processing and more generally for data sampled on a regular grid (such as 3D voxels). However, the use of CNNs when data is missing or not sampled on a regular grid is not straightforward. Several works have studied the generalization of such powerful schemes to such cases.</p><p>To avoid problems linked to discrete kernels and sparse data. <ref type="bibr" target="#b16">[17]</ref> introduces deformable convolutional kernels able to adapt to the recognition task. In <ref type="bibr" target="#b45">[46]</ref>, the authors adapt <ref type="bibr" target="#b16">[17]</ref> to deal with point clouds. The input signal is interpolated on the convolutional kernel, the convolution is applied, and the output is interpolated back to input shape. The kernel elements' locations are optimized like in <ref type="bibr" target="#b16">[17]</ref> and the input points are weighted according to their distance to kernel elements, like in <ref type="bibr" target="#b45">[46]</ref>. However, unlike in <ref type="bibr" target="#b45">[46]</ref>, the our approach is not dependent of a convolution kernel designed on a grid.</p><p>In <ref type="bibr" target="#b28">[29]</ref>, a χ-transform is applied on the point coordinates to create geometrical features to be combined with the input point features. This is a major difference relative to our approach: as in <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b28">[29]</ref> makes use of the input geometry as features. We want the geometry to behave as in discrete convolutions, i.e. weighting the relation between kernel and input. In other words, the geometric aspects are defined in the network structure (convolution strides, pooling layers) and point coordinates (e.g. pixels indices for images) are not an input of the network.</p><p>In <ref type="bibr" target="#b51">[52]</ref>, the authors propose a convolution layer defined as a weighted sum of the input features, the weights being computed by a multi-layer perceptron (MLP). Our approach has points in common with <ref type="bibr" target="#b51">[52]</ref> both in concept and implementation, but differs in two ways. First, our approach computes a dense weighting function that takes into account the whole kernel. Second, as in <ref type="bibr" target="#b48">[49]</ref>, the derived kernel is an explicit set of points associated with weights. However, whereas <ref type="bibr" target="#b48">[49]</ref> uses an explicit RBF Gaussian function to correlate input and kernel, we propose to learn this kernel to input relation function with a multi-layer perceptron (MLP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convolution for point processing</head><p>We build our continuous convolutional layer by adapting the discrete convolution formulation used for grid-sampled data such as images.</p><p>Notations. In the following sections, d is the dimension of the spatial domain (e.g., 3 for 3D point clouds) and n is the dimension of the features domain (i.e., the dimension of the input features of the convolutional layer). a, b is an integer sequence from a to b with step 1. The cardinality of a set S is denoted by |S|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discrete convolutions</head><p>Let K = {w i , ∈ R n , i ∈ 1, |K| } be the kernel and X = {x i , ∈ R n , i ∈ 1, |X| } be the input. In discrete convolutions, K and X have the same cardinality.</p><p>We first consider the case where the elements of K and X range over the same locations (e.g., grid coordinates on an image).</p><p>Given a bias β, the output y is:</p><formula xml:id="formula_0">y = β + |K| i=1 w i x i = β + |K| i=1 |X| j=1 w i x j 1(i, j)<label>(1)</label></formula><p>where 1(., .) is the indicator function such that 1(a, b) = 1 if a = b and 0 otherwise. This expresses a one-to-one relation between kernel elements and input elements.</p><p>We now reformulate the previous equation by making explicit the underlying order of sets K and X. Let's consider</p><formula xml:id="formula_1">K = {(c, w)} (resp. X = {(p, x)}) where c i ∈ R d (resp. p j ∈ R d )</formula><p>is the spatial location of the kernel element i (resp. j is the spatial location of the j-th point in the input). For convolutions on images, c and p denote pixel coordinates in the considered patch. The output is then:</p><formula xml:id="formula_2">y = β + |X| j=1 |K| i=1 w i x j 1(c i , p j )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutions on point sets</head><p>In this study, we consider that elements of X may have any spatial locations, i.e., we do not assume a grid or other structure on X. In practice, using equation <ref type="formula" target="#formula_2">(2)</ref> on such an X would result in 1(c, p) to be zero (almost) all the time as the probability for a random p to match exactly c is zero. Thus, using the indicator function is too restrictive to be useful when processing point clouds.</p><p>We need a more general function φ to establish a relation between the points {p} and the kernel elements {c}. We define φ as a geometrical weighting function that distributes the input x onto the kernel.</p><p>φ must be invariant to permutations of the input points (it is necessary since point clouds are unordered). This is achieved if φ is independently applied on each point p. It can also be function of the set kernel points, i.e., {c}.</p><p>Moreover, to be invariant to a global translation applied on both the kernel and the input cloud, we used relative coordinates with respect to kernel elements, i.e., we apply the φ function The φ function is then a function such that:</p><formula xml:id="formula_3">φ : R d × (R d ) |K| −→ R (3) (p, {c}) → φ({p − c})<label>(4)</label></formula><p>Finally, the convolution operation for point sets is defined by:</p><formula xml:id="formula_4">y = β + 1 |X| |X| j=1 |K| i=1 w i x j φ({p j − c})<label>(5)</label></formula><p>where we added a normalization according to the input set size for robustness to variation in input size.</p><p>In this formulation, the spatial domain and the feature domain are mixed similarly to the discrete formulation. Unlike Point-Net <ref type="bibr" target="#b37">[38]</ref> and PointNet++ <ref type="bibr" target="#b39">[40]</ref>, spatial coordinates are not input features. This formulation is closer to <ref type="bibr" target="#b51">[52]</ref> where the authors estimate directly the weights of the input features, i.e., the product w i φ({p j − c}), mixing estimation in the feature space and the geometrical space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Weighting function</head><p>In practice designing by hand such a φ function is not easy. Intuitively, it would decrease with the norm of p − c. As in <ref type="bibr" target="#b48">[49]</ref>, Gaussian functions could be used for that, but it would require handcrafted parameters which are difficult to tune. Instead, we choose to learn this function with a simple MLP. Such an approach does not make any specific assumption about the behavior of the function φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Convolution</head><p>The convolution operation is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. For a kernel {(c, w)}, the spatial part {c} and feature part {w} are processed separately. The black arrows and boxes are for operations in the features space, green ones are for operations in the point cloud space. Please note that removing the green operations corresponds to the discrete convolution.</p><p>Moreover, as in the case of discrete convolutions, our convolutional layer is made of several kernels (corresponding to the number of output channel). The output of the convolution is thus a vector y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Parameters and optimization on kernel element location</head><p>To set the location of the kernel elements, we randomly sample the locations c ∈ {c} in the unit sphere and consider them as parameters. As φ is differentiable with respect to its input, {c} can be optimized.</p><p>At training time, both parameters {w} and {c} of K as well as the MLP parameters are optimized using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Properties</head><p>Permutation invariance. As stated in <ref type="bibr" target="#b37">[38]</ref>, operators on point clouds must be invariant unser the permutation of points. In the general case, the points are not ordered. In our formulation, y is a sum over the input points. Thus, permutations of the points have no effect on the results.</p><p>Translation invariance. As the geometric relations between the points and the kernel elements are relative, i.e., ({p − c}), applying a global translation to the point cloud does not change the results.</p><p>Insensitivity to the scale of the input point cloud. Many point clouds, such as photogrammetric point clouds, have no metric scale information. Moreover the scale may vary from one point cloud to another inside a dataset. In order to make the convolution robust to the input scale, the input geometric points {p} are normalized to fit in the unit ball.</p><p>Reduced sensibility to the size of input point cloud. Dividing y by |X| makes the output less sensitive to input size. For instance, using "2X" as input does not change the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hierarchical representation and neighborhood computation</head><p>Let P be an input point cloud. The convolution as described in the previous section is a local operation on a subset X of P . It operates a projection of P on an output point cloud {q} and computes the features {y} associated with each point of</p><formula xml:id="formula_5">Q = {q, y}.</formula><p>Depending on the cardinality of Q, we have three possible behaviors for the convolution layer:</p><p>(a) |P | = |Q|. In this case the cardinality of the output is the same as the intput. In the discrete framework it corresponds to the convolution without stride. Computation of {q} {q} can either be given as an input, or computed from {p}. For the second case, a possible strategy is to randomly pick points in {p} to generate {q}. However, it is possible to pick several times the same point and some points of {p} may not be in the neighborhoods of the points of {q}. An alternative is proposed in <ref type="bibr" target="#b39">[40]</ref> using a furthest-point sampling strategy. This is very efficient and ensures a spatially uniform sampling, however it requires to compute all mutual distances in the point cloud.</p><p>We propose an intermediate solution.</p><p>For each point, we memorize how many times it has been selected. We pick the next point in the set of points with the lower number of selection. Each time a point q is selected, its score is increased by 100.</p><p>The score of the points in its neighborhood are increased by 1. The points of {q} are iteratively picked until the specified number of points is reached. Using a higher score for the points in {q} ensures that they will not be chosen anymore, except if all points have been selected once.</p><p>Neighborhood computation All k-nearest neighbor search are computed using a kd-tree built with {p}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Convolutional layer</head><p>The convolutional layer is presented on <ref type="figure" target="#fig_4">Fig. 3</ref>. It is composed of the two previously described operations (point selection and the convolution itself). The inputs are P and optionally {q}. If {q} is not provided, it is selected as a subset of P following the procedure described in the previous section. First, for each point of {q}, local neighborhoods in P are computed using a k-d tree. Then, for each of these subsets, we apply the convolution operation, creating the output features. Finally, the output Q is the union of the pairs {(q, y)}.</p><p>Parameters The parameters of the convolutional layers are very similar to discrete convolution parameters in the most deep learning frameworks.</p><p>-Number of output channels (C): it is the number of convolutional kernels used in the layer. It defines the output feature dimension, i.e., the dimension of y. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>The following section is dedicated to experiments and comparison with state-of-the-art methods. As the spatial structure generation (selection of the output point cloud for each layer) is a stochastic process, two runs through the network may lead to different outputs. In the following, we aggregate the results of multiple runs by averaging the outputs. The number of runs is then referred to as the number of spatial samplings. In the  folowing tables, it correspond to the number between parentheses (for classification and part segmentation). The influence of this number is discussed in section 6.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Classification</head><p>The first experiments are dedicated to points cloud classification. We experimented on both 2D and 3D point cloud datasets.</p><p>Network The network is described in <ref type="figure" target="#fig_5">Fig. 4</ref>. It is composed of five convolutions that progressively reduce the point cloud size to one single point, while increasing the number of channels. The features associated to this point are the inputs of a linear layer. This architecture is very similar to the ones that can be used for image processing (e.g., LeNet <ref type="bibr" target="#b26">[27]</ref>).</p><p>2D classification: MNIST The 2D experiment is done on the MNIST dataset. This is a dataset for the classification of gray scale handwritten digits. The point cloud P = {(p, x)} is built from the images, using pixel coordinates as point coordinates and, thus, is sampled on a grid. We build two variants of the dataset: first, point clouds are built with the whole image and the features associated with each point is the grey level  Methods OA</p><p>Image-based methods LeNet <ref type="bibr" target="#b26">[27]</ref> 99.20 NiN <ref type="bibr" target="#b31">[32]</ref> 99.53</p><p>Point-based methods PointNet++ <ref type="bibr" target="#b39">[40]</ref> 99.49 PointCNN <ref type="bibr" target="#b28">[29]</ref> 99.54 Ours -Gray levels <ref type="formula" target="#formula_0">(16)</ref>  Results are presented in table 1(a). We compare with both image CNNs (LeNet <ref type="bibr" target="#b26">[27]</ref> and Network in Network <ref type="bibr" target="#b31">[32]</ref>) and point-based methods (PointNet++ <ref type="bibr" target="#b39">[40]</ref> and PointCNN <ref type="bibr" target="#b28">[29]</ref>). Scores, averaged over 16 spatial samplings, are competitive with other methods. More interestingly, we do not observe a great difference between the two variants (grayscale points or black points only). In the Gray levels experiment (whole image), the framework is able to learn from the color value only as the points do not hold shape-related information. On the contrary, in the Black points only, it learns from geometry only, which is a common case for point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D classification: ModelNet40</head><p>We also experimented on 3D classification on the ModelNet40 dataset. This dataset is a set of meshes from 40 various classes (planes, cars, chairs, tables...). We generated point clouds by randomly sampling points on the triangular faces of the meshes. In our experiments, we use an input size of either 1024 or 2048 points for training. Table 1(b) presents the results. As for 2D classification, we are competitive with the state of the art concerning point-based classification approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Segmentation network</head><p>The segmentation network is presented on <ref type="figure" target="#fig_7">Fig. 5</ref>. It has an encoder-decoder structure, similar to U-Net, a stack of convolu- Note: striped convolution (layer 0, 1 and 12) are only in semantic segmentation network. tions that reduces the cardinality of the point cloud as an encoder and a symmetrical stack of convolution as a decoder with skip connections. In the decoder, the points used for upsampling are the same as the points in the encoder at the corresponding layer. Following the U-Net architerture, the features from the encoder and from the decoder are concatenated at the input of the convolutional layers of the decoder. Finally, the last layer is a point-wise linear layer used to generate an output dimension corresponding to the number of classes.</p><p>The network comes in two variants, i.e., with two different numbers of layers. The part segmentation network (plain colors, in <ref type="figure" target="#fig_7">Fig. 5</ref>) is used with ShapeNet for part segmentation. The second network, used for large-scale segmentation, is the same network with three added convolutions (hatched layers in <ref type="figure" target="#fig_7">Fig. 5</ref>). It is a larger network; its only purpose is to deal with larger input point clouds.</p><p>For both versions of the network, we add a dropout layer between the last convolution and the linear layer. At training time, the probability of an element to be set to zero is 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Part segmentation</head><p>Given a point cloud, the part segmentation task is to recognize the different constitutive parts of the shape. In practice, it is a semantic segmentation at shape level. We use the Shapenet <ref type="bibr" target="#b56">[57]</ref> dataset. It is composed of 16680 models belonging to 16 shape categories and split in train/test sets. Each category is annotated with 2 to 6 part labels, totalling 50 part classes. As in <ref type="bibr" target="#b28">[29]</ref>, we consider the part annotation problem as a 50-class semantic segmentation problem. We use a cross-entropy loss for each category. The scores are computed at shape level.</p><p>We use the semantic segmentation network from <ref type="figure" target="#fig_7">Fig. 5</ref>. The provided point clouds have various sizes, we randomly select 2500 points (possibly with duplication if the point cloud size is lower than 2500) and predict the labels for each input point. The points do not have color features so we set the input features to one (this is the same as for the MNIST experiment with black points only). As all points may not have been selected for labeling, the class of an unlabeled point is given by the label of its nearest neighbor.</p><p>The results are presented in table 2. The scores are the mean class intersection over union (mcIoU) and instance average intersection over union (mIoU). Similarly to classification, we aggregate the scores of multiple runs through the network. Our framework is also competitive with the state-of-the-art methods as we rank among the top five methods for both mcIoU and mIou.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Semantic segmentation</head><p>We now consider semantic segmentation for large-scale point clouds. As opposed to part segmentation, point clouds are now sampled in multi-object scenes outdoors and indoors.</p><p>Datasets We use three datasets, one with indoor scenes, two with outdoors acquisitions.</p><p>The Stanford 2D-3D-Semantics dataset <ref type="bibr" target="#b1">[2]</ref> (S3DIS) is an indoor point cloud dataset for semantic segmentation. It is composed of six scenes, each corresponding to an office building floor. The points are labeled according to 13 classes: 6 building elements classes (floor, ceiling...), 6 office equipment classes (tables, chairs...) and a "stuff" class regrouping all the small equipment (computers, screens...) and rare items. For each scene considered as a test set, we train on the other five.</p><p>The Semantic8 <ref type="bibr" target="#b20">[21]</ref> outdoor dataset is composed of 30 ground lidar scenes: 15 for training and 15 for evaluation. Each scene is generated from one lidar acquisition and the point cloud size ranges from 16 million to 430 million points. 8 classes are manually annotated.</p><p>Finally, the Paris-Lille 3D dataset (NPM3D) <ref type="bibr" target="#b40">[41]</ref> has been acquired using a Mobile Laser System. The training set contains four scenes taken from the two cites, totalizing 38 million points. The 3 tests scenes, with 10 million points each, were acquired on two other cities. The annotations correspond to 10 coarse classes from buildings to pedestrian.</p><p>For both Semantic8 and NPM3D, test labels are unknown and evaluated on an online server.</p><p>Learning and prediction strategies As the scenes may be large, up to hundred of millions of points, the whole point clouds cannot be directly given as input to the network.</p><p>At training time, we randomly select points in the considered point cloud, and extract all the points in an infinite vertical column centered on this point, the column section is 2 meters wide for indoor scenes and 8 meters wide for outdoor scenes <ref type="figure">(Fig. 6</ref>).</p><p>During testing, we compute a 2D occupancy pixel map with "pixel" size 0.1 meters for indoor scenes and 0.5 meters for outdoor scenes by projecting vertically on the horizontal plane. Then, we considered each occupied cell as a center for a column (same size as for training).</p><p>For each column, we randomly select 8192 points which we feed as input to the network. Finally, the output scores are aggregated (summed) at point level and points not seen by the network receive the label of their nearest neighbor.</p><p>Improving efficiency with fusion of two networks learned on different modalities First, the objective is to evaluate the influence of the color information as an input feature.</p><p>We trained two networks, one with color information (RGB) and one without (NoColor However, according to the scores, the intuition is only partly verified. As an example in table 4(a), the NoColor model is much more efficient than the RGB model on the column class, mainly due to color confusion as walls and columns have most of the time the same color. To our understanding, when color is provided, the stochastic optimization process follows the easiest way to reach a training optimum, thus, giving too much importance to color information. In contrast, the NoColor generates different features, based only on the geometry, and is more discriminating on some classes. Now looking at table 4(b), we observe similar performances for both models. As these models use different inputs, they likely focus on different features of the scene. It should thus be possible to exploit this complementarity to improve the scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model fusion</head><p>To show the interest of fusing models, we chose to use a residual fusion module <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>. This approach have proven to produce good results for networks with different input modalities. Moreover, one advantage is that the two segmentation networks are first trained separately, the fusion module being trained afterward. This training process makes it possible to first, reuse the geometry only model if the RGB information is not available and second, to train with limited resources (see implementation details in section 7).</p><p>The fusion module is presented in <ref type="figure" target="#fig_9">Fig. 7</ref>. The features before the fully-connected layer are concatenated, becoming the input of two convolutions and a point-wise linear layer. The outputs of both segmentation networks and the residual module are then added to form the final output. At training time, we add a dropout layer between the last convolution and the linear layer.  In table 4(a) and (b), the results of segmentation with fusion is reported at line ConvPoint -fusion. As expected, the fusion increases the segmentation scores with respect to RGB and NoColor alone.</p><p>On the S3DIS dataset (table 4(a)), the fusion module obtains a better score on 10 out 13 categories and the average intersection over union is increased by 3.5%. It is also interesting to note that, on categories for which the fusion is not better than with one single modality or the other, the score is close to the best mono-mode model. In other words, the fusion often improves the performance and in any case does not degrade it.</p><p>On the Semantic8 dataset (table 4(b)), we observe the same behavior. The gain is particularly high on the artefact class, which is one of the most difficult class: both mono-mode models reach 43-44% while the fusion model reaches 61%. It validates the fact that both RGB and NoColor models learn different features and that the fusion module is not only a sum of activations, but can select the best of both modalities.</p><p>For comparison with official benchmarks, we use the fusion model.</p><p>Large-scale datasets: comparison with the state of the art <ref type="table" target="#tab_6">Table 4</ref> presents also, for comparison, the scores obtained by other methods in the literature. Our approach is competitive with the state of the art on the three datasets.</p><p>On S3DIS, we place second behind KPConv <ref type="bibr" target="#b48">[49]</ref> in term of average intersection over union (mIoU), while being first on several categories. It can be noted that approaches sharing concepts with ours, such as PCCN <ref type="bibr" target="#b51">[52]</ref> or PointCNN <ref type="bibr" target="#b28">[29]</ref>, do not perform as well as ours.</p><p>On Semantic8, we report the state of the benchmark leaderboard at the time of article writing (for entries that are not anonymous). PointNet++ has two entries in the benchmark, we only reported the best one. Our convolutional network for segmentation places first before Superpoint Graph (SPG) <ref type="bibr" target="#b25">[26]</ref> and SnapNet <ref type="bibr" target="#b11">[12]</ref>. It differs greatly from SPG, which relies on a pre-segmentation of the point cloud, and SnapNet, which uses a 2D segmentation  network on virtual pictures of the scene to produce segments. We surpass the PointNet++ by 13% on the average IoU. We perform particularly well on car and artifacts detection where other methods, except for SPG get relatively low results.</p><p>Finally on NPM3D Paris-Lille dataset (table 4(c)), we also report the official benchmark at the time the paper was written. Based on the average IoU, we place second surpassed only by KPConv <ref type="bibr" target="#b48">[49]</ref>. Our approach is the best or second best for 6 out of 9 categories. The second place is explained mostly by the relatively low score on pedestrian and trash cans. These are particularly difficult classes, due to their variability and the low number of instances in the train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Emprical properties of the convolutional layer.</head><p>Filter visualization <ref type="figure" target="#fig_10">Fig. 8</ref> presents a visualization of some characteristics of the first convolutional layer of the 2D classification model trained on MNIST. <ref type="figure" target="#fig_10">Fig. 8(a)</ref> shows the weighting function associated with four of the sixty-four kernel elements of the first convolutional layer.</p><p>These weights are the output of the MLP function. As expected, their nature varies a lot depending on the kernel element, underlying different regions of interest for each kernel element. <ref type="figure" target="#fig_2">Fig. 8(b)</ref> shows the resulting convolutional filters. These are computed using the previously presented weighting functions, multiplied by the weights of each kernel element (w) and summed over the kernel elements. As with discrete CNNs, we observe that the network has learned various filters, with different orientations and shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Influence of random selection of output points</head><p>The strategy for selecting points of the output {q} is stochastic at each layer, i.e., for a fixed input, two runs through the same layer may lead to two different {q}'s. Therefore, the outputs {y}'s may also be different, and so may be the predicted labels.</p><p>To increase robustness, we aggregate several outputs computed with the same network and the same input point cloud. This is referred to as the number of sampling (from 1 to 16) in table 3. We observe an improvement of the performances with the number of spatial sampling. In practice, we only use up This procedure shares similarities with the approaches used for image processing for test set augmentation such as image crops <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b47">48]</ref>. The main difference resides in the fact that the output variation is not a result of input variation but is inherent to the network, i.e., the network is not deterministic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Robustness to point cloud size and neighborhood size</head><p>In order to evaluate the robustness to test conditions that are different from the training ones, we propose two experiments. As stated in section 3, the definition of the convolutional layer does not require a fixed input size. However for a gain in performance and time, we trained the networks with minibatches, fixing the input size at training. We evaluate the influence of the input size at inference on the performance of a model trained with fix input size. Results are presented in <ref type="figure" target="#fig_11">Fig. 9</ref>   Note: scores are computed with a 16-element spatial structure.</p><p>The model was trained with the default configuration.</p><p>Second, in our formulation, the neighborhood size |X| for each convolution layer remains a variable parameter after training, i.e., it is possible to change the neighborhood size at test time. It is the reason why, in equation <ref type="formula" target="#formula_4">(5)</ref>, the normalizing weight 1/|X| (average with respect to input size) has been added to be robust to neighborhood size variation. In addition to the robustness provided by averaging, the variation of k somehow simulates a density variation of the points for the layer. We evaluate the robustness to such variations in table 5, on the classification dataset ModelNet40, with a single model trained with the default configuration (see <ref type="figure" target="#fig_5">Fig. 4</ref>). We report the impact of k on the first layer (table 5(a)) and on the fourth layer (table 5(b)). As expected, even though the best score is reached for the default k, the layer is robust to a high variation of k values: the overall accuracy loss is lower than 2% when k is 2 times larger or smaller. A particularly interesting feature is that the first layer, which extract local features, is more robust to a decreasing k, making the features more local than a increasing k. It is the opposite for the fourth layer: global features are more robust to an increase of k than a decreasing k.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Computation times and implementation details</head><p>In our experiments, the convolutional pipeline was implemented using Pytorch <ref type="bibr" target="#b0">[1]</ref>. The neighborhood computation are done with NanoFLANN <ref type="bibr" target="#b8">[9]</ref> and parallelized over CPU cores using OpenMP <ref type="bibr" target="#b15">[16]</ref>. <ref type="table" target="#tab_9">Table 6</ref> presents the computation times. We consider two hardware configurations: a desktop workstation (Config. 1) and a low-end configuration, i.e., a gaming laptop (Config. 2). For instance, Config. 2 could correspond to a hardware specification for embedded devices.  <ref type="bibr" target="#b28">[29]</ref> which was reported as the fastest in <ref type="bibr" target="#b28">[29]</ref> among other methods. We used the code version available one the official PointCNN repository at the time of submission, and used the recommended network architecture. For both framework, we ran experiments on the Config. 1 computer. On the ModelNet40 classification dataset, our model is about 30% faster than PointCNN for training, but inference times are similar. The difference is more significant on the ShapeNet segmentation dataset. For a batch size of 4, our segmentation framework is more than 5 times faster for training, and 3 times faster at test time.</p><p>Moreover, we also show that our ConvPoint is more memory efficient than the implementation of PointCNN. The "-" symbol in <ref type="table" target="#tab_9">table 6</ref> indicates batch sizes / numbers of points configurations that exceed the GPU memory. For example, we can train the classification model with 2048 points and batch size of 128, which is not possible with PointCNN (on an Nvidia GTX 1070 GPU). The same goes on ShapeNet where we can train with a batch size of 64 while PointCNN already uses too much GPU memory with batch size 8.</p><p>In table 6(b), we report timings for the large-scale segmentation network and the fusion architecture, with a point cloud size of 8192. Note that for training this network we used NVidia GTX 1080Ti GPU. These timings represent the inference time only (neighborhood computation and convolutions), not data loading. We first observe that even the fusion architecture (two segmentation networks and a fusion module) can be run on the small configuration. Second, as for CNN with images, we benefit from using batches which reduce per point cloud computation time. Finally, our implementation is efficient given that for the segmentation network, we are able to process from 100,000 points (Config. 2) to 200,000 points (Config. 1) per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and limitations</head><p>Convolutional layer First, our convolution design is agnostic to the object scales, due to neighborhood normalization to the unit ball. It is of interest for non metric data such as CAD models or photogrammetric point clouds where scales are not always available. On the contrary, in metric scans, object sizes are valuable information (e.g., humans have almost all the time similar sizes) but removing the normalization would cause the kernel and the input points to have different volumes.</p><p>Second, an alternative is to use a fixed-radius neighborhood instead of a fix number of neighbors. As pointed out in <ref type="bibr" target="#b48">[49]</ref>, the resulting features would be more related to geometry and less to sampling. However, the actual code optimization to speed up computation such as batch training would be inapplicable due to a variable number of neighbors in a batch.</p><p>Input features Another perspective is to explore the use of precomputed features as inputs. In this study, we only use raw data for network inputs: RGB colors when available, or all features set to 1 otherwise. In the future, we will work on feeding the networks with extra features such as normals or curvatures.</p><p>Network architecture Finally, we proposed two networks architectures that are widely inspired from computer vision models. It would be interesting to explore further variations of network architectures. As our formulation generalizes the discrete convolution, it is possible to transpose more CNN architectures, such as residual networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper, we presented a new CNN framework for point cloud processing. The proposed formulation is a generalization of the discrete convolution for sparse and unstructured data. It is flexible and computationally efficient, which it makes it possible to build various network architectures for classification, part segmentation and large-scale semantic segmentation. Through several experiments on various benchmarks, real and simulated, we have shown that our method is efficient and at state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Segmentation results on datasets (a) NPM3D and (b) Semantic8. Point cloud colored according height (top), laser intensity or RGB colors (middle) and predicted segmentation label (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Convolution operation. Black arrows and boxes with black frames are features operations, green ones are spatial operations. on {p j − c i }, i ∈ 1, |K| , the set of relative positions of p j relatively to kernel elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( b )</head><label>b</label><figDesc>|P | &gt; |Q|. This includes the particular case of {p} ⊃ {q}. The convolution operates a spatial size reduction. The parallel with the discrete framework is a convolution with stride bigger than one. (c) |P | &lt; |Q|. This includes the particular case of {p} ⊂ {q}. The convolutional layer produces an upsampled version of the input. It is similar to the up-convolutional layer in discrete pipelines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>-</head><label></label><figDesc>Size of the output point cloud (|Q|): it is the number of points that are passed to the next layer. -Kernel size (|K|): it is the number of kernel elements used for the convolution computation. -Neighborhood size (k): it is the number of points in {p} to consider for each point in {q}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Convolutional layer, composed of two steps: the spatial structure computation (selecting {q} if needed by computing the local neighborhoods of each q) and the convolution operation itself on each neighborhood, as defined in figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Classification network. It is composed of five convolution layer (blue blocks) with progressive point cloud size reduction (|Q|) and a final fully connected layer (green block). For all layers |K| = 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Segmentation networks. It is made of an encoder (progressive reduction of the point cloud size) followed by a decoder (to get back to the original point cloud size). Skip connections (black arrows) allow information to flow directly from encoder to decoder: they provide point locations for decoder's convolutions and encoder's features are concatenated with decoder's ones at corresponding scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Segmentation networks: residual fusion. The input of the fusion module is the concatenation of the features collected before the fully-connected layer in networks 1 and 2. Its output is added as a correction to the sum of the outputs of networks 1 and 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Weighting function and filters for the first convolution layer of the classification model trained of MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Influence of a varying number of input points at training time versus at test time on ModelNet40. Each colored curve shows the performance of a function of model trained with a given input point cloud size when tested on different numbers of input points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Shape classification. Overall accuracy (OA %) and class average accuracy (AA, %).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Ours -1024 pts (16) 91.8 88.5 Ours -2048 pts (16) 92.5 89.6 ({x} = {grey level}); second, only the black points are considered ({x} = {1}).</figDesc><table><row><cell></cell><cell>Methods</cell><cell>OA AA</cell></row><row><cell></cell><cell>Mesh or voxels</cell><cell></cell></row><row><cell></cell><cell>Subvolume [39]</cell><cell>89.2</cell></row><row><cell></cell><cell>MVCNN [47]</cell><cell>90.1</cell></row><row><cell></cell><cell>Points</cell><cell></cell></row><row><cell></cell><cell>DGCNN [54]</cell><cell>92.2 90.2</cell></row><row><cell></cell><cell>PointNet [38]</cell><cell>89.2 86.2</cell></row><row><cell>99.62</cell><cell>PointNet++ [40]</cell><cell>90.7</cell></row><row><cell>Ours -Black points (16) 99.49</cell><cell>PointCNN [29]</cell><cell>92.2 88.1</cell></row><row><cell></cell><cell>KPConv [49]</cell><cell>92.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>: ShapeNet</cell></row><row><cell>Method</cell><cell>mcIoU</cell><cell>mIoU</cell></row><row><cell>SyncSpecCNN [58]</cell><cell>82.0</cell><cell>84.7</cell></row><row><cell>Pd-Network [25]</cell><cell>82.7</cell><cell>85.5</cell></row><row><cell>3DmFV-Net [7]</cell><cell>81.0</cell><cell>84.3</cell></row><row><cell>PointNet [38]</cell><cell>80.4</cell><cell>83.7</cell></row><row><cell>PointNet++ [40]</cell><cell>81.9</cell><cell>85.1</cell></row><row><cell>SubSparseCN [19]</cell><cell>83.3</cell><cell>86.0</cell></row><row><cell>SPLATNet [46]</cell><cell>83.7</cell><cell>85.4</cell></row><row><cell>SpiderCNN [56]</cell><cell>81.7</cell><cell>85.3</cell></row><row><cell>SO-Net [28]</cell><cell>81.0</cell><cell>84.9</cell></row><row><cell>PCNN [4]</cell><cell>81.8</cell><cell>85.1</cell></row><row><cell>KCNet [43]</cell><cell>82.2</cell><cell>83.7</cell></row><row><cell>SpecGCN [50]</cell><cell>-</cell><cell>85.4</cell></row><row><cell>RSNet [23]</cell><cell>81.4</cell><cell>84.9</cell></row><row><cell>DGCNN [54]</cell><cell>82.3</cell><cell>85.1</cell></row><row><cell>SGPN [53]</cell><cell>82.8</cell><cell>85.8</cell></row><row><cell>PointCNN [29]</cell><cell>84.6</cell><cell>86.1</cell></row><row><cell>KPConv [49]</cell><cell>85.1</cell><cell>86.4</cell></row><row><cell>ConvPoint (16)</cell><cell>83.4</cell><cell>85.8</cell></row><row><cell>rank</cell><cell>4</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Influence of the spatial sampling number, ShapeNet dataset.</figDesc><table><row><cell>Spatial samplings</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell></row><row><cell>mIoU</cell><cell>83.9</cell><cell>84.8</cell><cell>85.4</cell><cell>85.7</cell><cell>85.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Large scene semantic segmentation benchmarks. score , second best and third best . (a) S3DIS Method OA mAcc mIoU ceil. floor wall beam col. wind. door chair table book. sofa board clut. Pointnet [38] 78.5 66.2 47.6 88.0 88.7 69.3 42.4 23.1 47.5 51.6 42.0 54.1 38.2 9.6 29.4 35.2</figDesc><table><row><cell cols="10">Best RSNet [23] -66.5 56.5 92.5 92.8 78.6 32.8 34.4 51.6 68.1 60.1 59.7 50.2 16.4 44.9 52.0</cell></row><row><cell>PCCN [52]</cell><cell>-</cell><cell cols="8">67.0 58.3 92.3 96.2 75.9 0.27 6.0 69.5 63.5 65.6 66.9 68.9 47.3 59.1 46.2</cell></row><row><cell>SPGraph [26]</cell><cell cols="9">85.5 73.0 62.1 89.9 95.1 76.4 62.8 47.1 55.3 68.4 73.5 69.2 63.2 45.9 8.7 52.9</cell></row><row><cell>PointCNN [29]</cell><cell cols="9">88.1 75.6 65.4 94.8 97.3 75.8 63.3 51.7 58.4 57.2 71.6 69.1 39.1 61.2 52.2 58.6</cell></row><row><cell>KPConv [49]</cell><cell>-</cell><cell cols="8">79.1 70.6 93.6 92.4 83.1 63.9 54.3 66.1 76.6 57.8 64.0 69.3 74.9 61.3 60.3</cell></row><row><cell>ConvPoint -RGB</cell><cell>87.9</cell><cell>-</cell><cell cols="7">64.7 95.1 97.7 80.0 44.7 17.7 62.9 67.8 74.5 70.5 61.0 47.6 57.3 63.5</cell></row><row><cell cols="2">ConvPoint -NoColor 85.2</cell><cell>-</cell><cell cols="7">62.6 92.8 94.2 76.7 43.0 43.8 51.2 63.1 71.0 68.9 61.3 56.7 36.8 54.7</cell></row><row><cell>ConvPoint -Fusion</cell><cell>88.8</cell><cell>-</cell><cell cols="7">68.2 95.0 97.3 81.7 47.1 34.6 63.2 73.2 75.3 71.8 64.9 59.2 57.6 65.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Semantic8</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="9">mIoU OA Man made Natural High veg. Low veg. Buildings Hard scape Artefacts Cars</cell></row><row><cell>TML-PC [37]</cell><cell cols="3">0.391 0.745</cell><cell>0.804</cell><cell>0.661</cell><cell>0.423</cell><cell>0.412</cell><cell>0.647</cell><cell>0.124</cell><cell>0.000 0.058</cell></row><row><cell>TMLC-MS [22]</cell><cell cols="3">0.494 0.850</cell><cell>0.911</cell><cell>0.695</cell><cell>0.328</cell><cell>0.216</cell><cell>0.876</cell><cell>0.259</cell><cell>0.113 0.553</cell></row><row><cell>PointNet++ [40]</cell><cell cols="3">0.631 0.857</cell><cell>0.819</cell><cell>0.781</cell><cell>0.643</cell><cell>0.517</cell><cell>0.759</cell><cell>0.364</cell><cell>0.437 0.726</cell></row><row><cell>SnapNet [12]</cell><cell cols="3">0.674 0.910</cell><cell>0.896</cell><cell>0.795</cell><cell>0.748</cell><cell>0.561</cell><cell>0.909</cell><cell>0.365</cell><cell>0.343 0.772</cell></row><row><cell>SPGraph [26]</cell><cell cols="3">0.762 0.929</cell><cell>0.915</cell><cell>0.756</cell><cell>0.783</cell><cell>0.717</cell><cell>0.944</cell><cell>0.568</cell><cell>0.529 0.884</cell></row><row><cell>ConvPoint -RGB</cell><cell cols="3">0.750 0.938</cell><cell>0.934</cell><cell>0.847</cell><cell>0.758</cell><cell>0.706</cell><cell>0.950</cell><cell>0.474</cell><cell>0.432 0.902</cell></row><row><cell cols="4">ConvPoint -NoColor 0.726 0.927</cell><cell>0.918</cell><cell>0.788</cell><cell>0.748</cell><cell>0.646</cell><cell>0.962</cell><cell>0.451</cell><cell>0.442 0.856</cell></row><row><cell>ConvPoint -Fusion</cell><cell cols="3">0.765 0.934</cell><cell>0.921</cell><cell>0.806</cell><cell>0.760</cell><cell>0.719</cell><cell>0.956</cell><cell>0.473</cell><cell>0.611 0.877</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) NPM3D</cell><cell></cell><cell></cell></row><row><cell>Name</cell><cell></cell><cell cols="8">mIoU Ground Building Pole Bollard Trash can Barrier Pedestrian Car Natural</cell></row><row><cell cols="2">MSRRNetCRF</cell><cell>65.8</cell><cell></cell><cell>99.0</cell><cell cols="2">98.2 45.8 15.5</cell><cell>64.8</cell><cell>54.8</cell><cell>29.9</cell><cell>95.0 89.5</cell></row><row><cell>EdConvE</cell><cell></cell><cell>66.4</cell><cell></cell><cell>99.2</cell><cell cols="2">91.0 41.3 50.8</cell><cell>65.9</cell><cell>38.2</cell><cell>49.9</cell><cell>77.8 83.8</cell></row><row><cell>RFMSSF</cell><cell></cell><cell>56.3</cell><cell></cell><cell>99.3</cell><cell cols="2">88.6 47.8 67.3</cell><cell>2.3</cell><cell>27.1</cell><cell>20.6</cell><cell>74.8 78.8</cell></row><row><cell>MS3DVS</cell><cell></cell><cell>66.9</cell><cell></cell><cell>99.0</cell><cell cols="2">94.8 52.4 38.1</cell><cell>36.0</cell><cell>49.3</cell><cell>52.6</cell><cell>91.3 88.6</cell></row><row><cell>HDGCN</cell><cell></cell><cell>68.3</cell><cell></cell><cell>99.4</cell><cell cols="2">93.0 67.7 75.7</cell><cell>25.7</cell><cell>44.7</cell><cell>37.1</cell><cell>81.9 89.6</cell></row><row><cell cols="2">KP-FCNN [49]</cell><cell>82.0</cell><cell></cell><cell>99.5</cell><cell cols="2">94.0 71.3 83.1</cell><cell>78.7</cell><cell>47.7</cell><cell>78.2</cell><cell>94.4 91.4</cell></row><row><cell cols="2">ConvPoint -Fusion</cell><cell>75.9</cell><cell></cell><cell>99.5</cell><cell cols="2">95.1 71.6 88.7</cell><cell>46.7</cell><cell>52.9</cell><cell>53.5</cell><cell>89.4 85.4</cell></row><row><cell cols="6">to 16 samplings because a larger number does not significantly</cell><cell></cell><cell></cell><cell></cell></row><row><cell>improve the scores.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>for the ModelNet40 classification dataset. Each curve (from blue to red) is an instance of the classification network, trained with 16, 32, . . . , 2048 input points. The black dots are the scores for each model at their native input size. The dashed curve describe a theoretical model performing as well as the best model for each input size. Please note that the horizontal scale is a log scale and that each step is doubling the number of input points. A first observation is that almost each model performs the best at its native input size and that very few points are needed on ModelNet40 to reach decent performances: with 32 points, the performance already reaches to 85%. Besides, the larger the training size is, the more robust to size variation the model becomes. While the model trained on 32 points see its performance drop by 25% with ±50% points, the model trained with 2048 points still reaches 82% (a drop of 10%) with only 512 points (4 times less).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>ModelNet40 classification scores with a variation of neighborhood sizes k at test time.</figDesc><table><row><cell>(a)</cell><cell cols="4">First layer (default k = 32)</cell></row><row><cell></cell><cell>/2</cell><cell>/1.5</cell><cell>×1</cell><cell>×1.5</cell><cell>×2</cell></row><row><cell></cell><cell>91.0</cell><cell>92.2</cell><cell>92.5</cell><cell>91.6</cell><cell>90.2</cell></row><row><cell>(b)</cell><cell cols="4">4 th layer (default k = 16)</cell></row><row><cell></cell><cell>/2</cell><cell>/1.5</cell><cell>×1</cell><cell>×1.5</cell><cell>×2</cell></row><row><cell></cell><cell>90.9</cell><cell>91.2</cell><cell>92.5</cell><cell>91.9</cell><cell>91.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Training and inference timings. (a) Comparison with PointCNN [29] on ModelNet40 and ShapeNet, computed with Config. 1 Comparison between large segmentation network and fusion architecture, computed on the S3DIS dataset. Timings are given in milliseconds.</figDesc><table><row><cell>Dataset Point Batch num. size ModelNet40 1024 32 128 2048 32 128 ShapeNet 2048 4 8</cell><cell>PointCNN Training Test (epoch) 58 s 8 s 51 s 7 s 80 s 11 s -1320 s 120 s -</cell><cell cols="2">ConvPoint Training Test (epoch) 42 s 7 s 35 s 7 s 53 s 9 s 45 s 8 s 255 s 41 s 185 s 37 s</cell><cell>(b) Batch size Segmentation network 1 Config. 1 93 Config. 2 155 Fusion architecture Config. 1 230 Config. 2 418</cell><cell>2 69 95 170 282</cell><cell>4 46 89 110 232</cell><cell>8 40 81 103 -</cell><cell>16 39 -101 -</cell><cell>32 38 ---</cell></row><row><cell>64</cell><cell>-</cell><cell>116 s</cell><cell>29 s</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Config. 1: Middle-end configuration. Intel Xeon E5-1620 v3, 3.50 GHz, 8 cores, 8 threads + Nvidia GTX 1070 8Gb</cell></row><row><cell cols="10">Config. 2: Low-end configuration. Intel Core i7-5500U+, 2.40GHz, 2 cores, 4 threads + Nvidia GTX 960M 2Gb</cell></row><row><cell cols="9">The symbol "-" corresponds to setups (batch size and number of points) that exceed GPU memory.</cell></row></table><note>Number of input points (at test)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 (</head><label>6</label><figDesc></figDesc><table /><note>a) is a comparison with PointCNN</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Soumith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<title level="m">Point Convolutional Neural Networks by Extension Operators</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The wave kernel signature: A quantum mechanical approach to shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schlickewei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1626" to="1633" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic Segmentation of Earth Observation Data Using Multimodal and Multi-scale Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">3D Point Cloud Classification and Segmentation using 3D Modified Fisher Vector Representation for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08241</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of surface reconstruction from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Seversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="301" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">nanoflann: a C++ header-only fork of FLANN, a library for nearest neighbor (NN) with kd-trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rai</surname></persName>
		</author>
		<ptr target="https://github.com/jlblancoc/nanoflann" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalizing discrete convolutions for unstructured point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval (3DOR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scale-invariant heat kernel signatures for non-rigid shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1704" to="1711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs. pages http-openreview</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">OpenMP: An Industry-Standard API for Shared-Memory Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Menon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="1998-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold Sparse Convolutional Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">net: A new large scale point cloud classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeantic3d</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, volume IV-1-W1</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast semantic segmentation of 3D point clouds with strongly varying density. ISPRS Annals of Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent Slice Networks for 3D Segmentation of Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3D scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kdnetworks for the recognition of 3D point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution On X-Transformed Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">FPNN: Field probing neural networks for 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Shape classification using the innerdistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="286" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Voxnet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mind the gap: modeling local and global context in (road) networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Montoya-Zegarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladickỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Paris-lille-3d: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="545" to="557" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08275</idno>
		<title level="m">SPLATNet: Sparse Lattice Networks for Point Cloud Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08889</idno>
		<title level="m">Kpconv: Flexible and deformable convolution for point clouds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Voting for Voting in Online Point Cloud Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><forename type="middle">M A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">SGPN: Similarity group proposal network for 3D point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">3D shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3D shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2282" to="2290" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
