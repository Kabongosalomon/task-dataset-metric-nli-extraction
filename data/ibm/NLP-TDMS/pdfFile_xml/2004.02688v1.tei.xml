<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Light3DPose: Real-time Multi-Person 3D Pose Estimation from Multiple Views</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Elmi</surname></persName>
							<email>alessio@checkoutfree.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Checkout Technologies s.r.l</orgName>
								<address>
									<postCode>20100</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Mazzini</surname></persName>
							<email>davide@checkoutfree.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Checkout Technologies s.r.l</orgName>
								<address>
									<postCode>20100</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Tortella</surname></persName>
							<email>pietro@checkoutfree.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Checkout Technologies s.r.l</orgName>
								<address>
									<postCode>20100</postCode>
									<settlement>Milan</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Light3DPose: Real-time Multi-Person 3D Pose Estimation from Multiple Views</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach to perform 3D pose estimation of multiple people from a few calibrated camera views. Our architecture, leveraging the recently proposed unprojection layer, aggregates feature-maps from a 2D pose estimator backbone into a comprehensive representation of the 3D scene. Such intermediate representation is then elaborated by a fully-convolutional volumetric network and a decoding stage to extract 3D skeletons with sub-voxel accuracy. Our method achieves state of the art MPJPE on the CMU Panoptic dataset using a few unseen views and obtains competitive results even with a single input view. We also assess the transfer learning capabilities of the model by testing it against the publicly available Shelf dataset obtaining good performance metrics. The proposed method is inherently efficient: as a pure bottom-up approach, it is computationally independent of the number of people in the scene. Furthermore, even though the computational burden of the 2D part scales linearly with the number of input views, the overall architecture is able to exploit a very lightweight 2D backbone which is orders of magnitude faster than the volumetric counterpart, resulting in fast inference time. The system can run at 6 FPS, processing up to 10 camera views on a single 1080Ti GPU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Multi-person 3D pose estimation is a complex problem, with many applications in different fields of computer vision, like people tracking or augmented reality. This problem is usually tackled with a two steps approach. At first, every view is processed independently in order to produce a set of 2D poses -or possibly, some intermediate feature representation. In this stage, all the achievements in 2D pose estimation field can be exploited (see <ref type="bibr" target="#b0">[1]</ref> for a survey). Next, these poses have to be matched across views and eventually triangulated, in order to produce a final estimate of the 3D scene. Usually, occlusions between people -or even self-occlusions -are the main difficulties to deal with: crowded scenes and complex poses produce noisy 2D detections, which are hard to filter out or recover in the matching and triangulation phase.</p><p>Hence, the idea of creating a system which is able to handle occlusions in a global way, and that is not affected by the limitations brought by single-view inferences. Inspired by <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b3">[4]</ref>, we developed a multi-person 3D reconstruction system, which takes a set of images capturing the scene from different views and outputs a set of 3D pose reconstructions in a global reference frame. Its main building block is a fully convolutional neural network, where low-level features of the input views are unprojected, fused and transformed r Equal contribution. Authors order determined by random function. in order to produce a 3D representation of the probabilistic space. Following the general bottom-up approach of pose estimation, we extended the notion of part affinity field in three dimensions, making the pose reconstruction from density maps quick and agile. By doing this, we avoided all the limitations of the top-down strategies, where scalability is penalized as the number of people grows, and where interperson occlusions and self-occlusions cannot be encoded -and recovered -by the network in a global way. On the contrary, thanks to the huge variety of pose configurations available in the CMU Panoptic dataset and with clever augmentation strategies about view-points, we could prove that our system is not affected by those limitations: our system can exploit activations and "shadows" in the feature space to estimate occlusions. Moreover, it does not depend on sophisticated algorithms of detection-view assignment, and it does not pay the computational burden of adding more views and subjects in the reconstruction process. Furthermore, we found that our system can produce good results even with just a single view suggesting that this approach can be further investigated also for monocular depth estimation tasks with multiple poses. We conducted several experiments, which show the feasibility of our work and compare it to the other state-of-the-art approaches.</p><p>Our main contributions are the following:</p><p>• as far as we know this is the first complete bottomup approach adopted in this context. In particular, it is capable of handling crowded scenes with good accuracy results and computational time. • We show that even a very light backbone can produce good results. This implies that adding more views is almost computationally free.  <ref type="figure">Fig. 2</ref>. An overall view of the complete processing pipeline. 2D pose backbone replicas process each view separately. Feature maps are then aggragated by the Unprojection layer into a 3D input representation of the scene. A volumetric network produces an output representation. A further decoding produces the final 3D pose estimations.</p><p>• We introduce 3D-data augmentation policies that greatly enhance the number of samples seen by the volumetric network. • Our post-processing strategy leads to a sub-voxel localization, overcoming the issue of a quantized 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Multi-view, multi-person 3D pose estimation tries to fuse the achievements coming from 2D pose estimation, structure from motion and monocular depth estimation research fields. All of them are very well studied and pretty active topics nowadays.</p><p>Pose estimation from a single image is usually tackled following one of these two main strategies: bottom-up or top-down approaches. The former try to infer all key-points (i.e. parts) and/or limbs simultaneously and aggregate them eventually, using specific post-process logic. These methods can claim higher speed over their competitors since neural inference is done only once. At the same time, they usually have to deal with down-scaled feature maps, which limit the accuracy in terms of localization. In this group, we cannot omit <ref type="bibr" target="#b3">[4]</ref>. Inspired by the work of <ref type="bibr" target="#b4">[5]</ref>, they introduced the notion of part-affinity fields. Their work has been extended by <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> leading to a better part association, and by <ref type="bibr" target="#b7">[8]</ref>, where stronger descriptors led to a finer sub-pixel resolution. Other insights on the resolution issue were provided by <ref type="bibr" target="#b8">[9]</ref>, with heatmap encoding/decoding refinements. On the contrary, top-down approaches <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, possibly combined with multi-scale strategies <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, rely on object detectors to identify humans in the scene, then a single-person neural inference is performed for each of them. These techniques generally outperform their bottom-up competitors on public challenges, while suffering in scalability with increasing number of subjects. Some hybrid approaches have emerged as well <ref type="bibr" target="#b14">[15]</ref>. Finally, we want to mention some attempts <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> to reduce the computational burden of pose estimation networks.</p><p>Three-dimensional pose estimation has emerged following two different tracks. The first one aims to recover the third dimension from a monocular view <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. These methods usually start from 2D pose estimations, and lift them in a second stage in order to obtain their depth. In particular, they all deal with single-pose scenarios. We mention two attempts to extend this task to a multiple poses: Moon et al. <ref type="bibr" target="#b24">[25]</ref> adopted a top-down strategy; Rogez et al. <ref type="bibr" target="#b25">[26]</ref> introduced pose proposals (from anchorposes) in the spirit of the Faster R-CNN approach. The second research track takes advantage of multiple views and claims to reconstruct 3D poses in a global reference frame. Sometimes this is the initial step of detection-to-track pipelines, like in <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, where temporal evolution can be exploited in order to refine predictions. Multiple-view pose reconstruction may focus on single <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b1">[2]</ref> or multiple poses <ref type="bibr" target="#b32">[33]</ref>, and they can exploit geometrical constraints <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, in pair with visual features <ref type="bibr" target="#b35">[36]</ref>. In particular, we highlight two works where multi-view projections have been combined with deep learning. <ref type="bibr" target="#b31">[32]</ref> exploited the epipolar geometry in order to refine 2D pose estimation model, and consequently improve the final single pose 3D reconstruction. <ref type="bibr" target="#b1">[2]</ref> showed that 2D features of each view can be fused and processed into a volumetric representation, which is analyzed to achieve a neat 3D reconstruction of the pose -again -for a single subject. However, to the best of our knowledge there is not any attempt to extend this approach to a multi-person scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>We call Light3DPose our system. In this section, we outline the architecture of Light3DPose, followed by a detailed explanation of all its components.</p><p>We are given a detection space S with fixed boundaries and a set of fixed setup cameras {C i } i=1,...,Nc whose intrinsic and extrinsic parameters are known. In particular, the projections P i : S → F i are known, where F i denotes the frame of the camera C i . The cameras are synchronized, so for any time t we have a set of images I t 1 , . . . , I t Nc , one for each camera. We will assume the time fixed throughout the paper, and omit the superscript t.</p><p>The input of Light3DPose is a set of pairs {I i , C νi } i=1,...,m where I i is an image and C νi is one of the setup cameras. The number of input pairs m is variable and can range from 1 to N c . In Section V we study both from the performance and computational sides the impact of the number of input views.</p><p>The output of Light3DPose is a set of 3D human poses {A 1 , . . . , A k }, with k an arbitrary number. A 3D human pose A i is a list (a l i ) l∈pose layout of joints. Each joint is a pair composed of a point in the space S and a label identifying the joint type, but when no confusion arises we identify the joint with the underlying point in S. The joint type ranges in a pose layout named CMU14, described in Section IV-C.</p><p>The internal pipeline of Light3DPose is composed of three main stages (see <ref type="figure">Figure 2</ref>):</p><p>• a 2D Views Processing stage which returns a 2D feature map for each camera; • an Unprojection layer <ref type="bibr" target="#b1">[2]</ref> which aggregates the information coming from all the 2D views into a 3D features space representation; • a Volumetric Processing that process the aggregated 3D representation and produces the output; and each of these stages are composed of a different number of modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 2D Views Processing</head><p>This processing stage takes as input one image I and produces a 2D activation R(B(I)). When Light3DPose processes a set of pairs {(I i , C i )}, each I i is fed independently to the 2D Views processing stage. The different 2D View Processing stages share the same weight.</p><p>The stage is composed of two modules: a 2D Pose Backbone followed by a Reduction module.</p><p>1) 2D Backbone: The input to the 2D Backbone module is an image I, and the output is a 2D feature map B(I). The 2D backbone is a MobileNet V1 <ref type="bibr" target="#b36">[37]</ref> with some modifications from <ref type="bibr" target="#b15">[16]</ref> on the latest layers. The stride of conv4 2dw has been removed and all succeeding convolutions have been set to dilation 2. This operation makes the network global stride to be 16 instead of 32 which is common for classification networks. We used weights pretrained on COCO dataset from <ref type="bibr" target="#b15">[16]</ref>.</p><p>2) Reduction Module: Input to the reduction module is the 2D feature map B(I), and the output is a 2D feature map R(B(I). The purpose of this module is to project the feature space produced by the 2D Backbone to a lower-dimensional feature space. This module is crucial in order to encode the information of the backbone into a lighter feature map, to maintain the computations performed by the Volumetric Network feasible. Our Reduction Module is essentially a residual module composed of three depth-wise convolutions + ReLUs. We borrow this architecture from <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unprojection</head><p>This processing stage represents the contact point between the 2D feature maps and the 3D model of the scene, collecting the result of the 2D Processing stage into a 3D feature map representation. This is the only stage of Light3DPose that uses the calibration parameters of the cameras, and it has no trainable parameters.</p><p>Fix integer numbers Q x , Q y , Q z , and a positive float value Q size . Construct a cube C ⊆ S composed of Q x × Q y × Q z voxels with edge of length Q size . In C one has the integer coordinate system (i x , i y , i z ) corresponding to the index of the voxels of C, and we denote by ι : C → S the embedding.</p><p>The input to this stage is a set of pairs</p><formula xml:id="formula_0">{(R i , C νi )} i=1,.</formula><p>..,m , where each R i is the output of one of the 2D View Processing modules, and C νi is one of the setup cameras.</p><p>The output of the unprojection stage is a 3D feature map</p><formula xml:id="formula_1">U ι with shape Q x × Q y × Q z × N f eats , where N f eats is the number of channels of the 2D feature map R.</formula><p>To compute the value of the j-th feature of the voxel U ι (i x , i y , i z ) we use the formula:</p><formula xml:id="formula_2">U ι (i x , i y , i z ) j = 1 m · m i=1 R i (P νi (ι(i x , i y , i z ))) j<label>(1)</label></formula><p>where recall that P i denotes the projection associated with the camera C i , and by R i (u, v) j we denote the j-th channel of the 2D feature map R i at the point with frame coordinates (u, v). This layer is a generalization of the Unprojection introduced in <ref type="bibr" target="#b1">[2]</ref> where a cube is built around each person. It can be efficiently implemented using vectorized operations and a differentiable sampling operator <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Volumetric Processing</head><p>Input to this stage is the 3D feature map U output of the Unprojection layer.</p><p>The output of this stage is a set of 3D human poses</p><formula xml:id="formula_3">{A 1 , . . . , A k }.</formula><p>The stage is composed of three modules:</p><p>• the Volumetric Network, • the Sub-voxel Joint Detection • the Skeleton Decoder. The approach is similar to OpenPose <ref type="bibr" target="#b3">[4]</ref>: the neural part of the network is trained to predict a Gaussian centered on each joint; the network should also predict a set of Part Affinity Fields (PAFs) that are used by the decoder to efficiently build the skeletons. Our method directly predicts 3D poses, thus the main differences between our volumetric processing part and OpenPose are in the use of a different neural architecture to handle 3D volumes data, and an adaptation to the 3D setting of the decoding of the output of the Volumetric network. Moreover, we introduce a Sub-voxel Peak Detector module to increase the accuracy of the joints predictions.</p><p>1) Volumetric Network: This is the trainable neural part of the volumetric processing. The purpose is to predict a set of 3D Gaussians centered on every joint and a set of 3D PAFs for the skeleton reconstruction.</p><p>The input to this module is the 3D activation U output of the unprojection layer with shape Q</p><formula xml:id="formula_4">x × Q y × Q z × N f eats .</formula><p>Output of this module is a 3D activation V with shape Q x × Q y × Q z × N gt , where N gt = N joints + 3 · N P AF , where N joints is the number of joints of the pose layout, and N P AF is the number of PAF. This output can also be seen as a pair of collections V = ((H l ) l∈pose layout , (V s ) s∈P AF s ) where each H l is a 3D feature map corresponding to a heatmap and each V s is a collection of 3 (one per each of to the 3-dimensional directions of the vector) 3D features map corresponding to a vectormap.</p><p>We adopt a V2V network from <ref type="bibr" target="#b38">[39]</ref>, but we set the minimum number of channels of the earliest and latest layers to 64 in wherever layer the original network has 32 channels. We name this modified V2V network: V2V64. We also experimented with 32 and 96 channels architectures. Results are reported in Section V.</p><p>The output V of the module is then confronted with the ground-truth with an appropriate loss function, which is used to perform the training of Light3DPose. The dataset labels are lists of poses of persons in the 3D space. The procedure to create ground-truth heatmaps and vectormaps is a generalization to 3D space of the one in <ref type="bibr" target="#b3">[4]</ref>, so we omit the details. We opted to use a SmoothL1 loss function and to weight equally the loss coming from the heatmap and the vectormap. We experimented different loss functions and weights between heatmap and vectormap, the results are reported in V.</p><p>2) Sub-voxel Joint Detector: Several state-of-the-art works on single-person pose estimation are based on a variation of the Integral Regression Framework <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b13">[14]</ref> which represents the unifying approach between heatmap and regressionbased methods. The Integral Pose Regression framework assumes that the point to be localized follows a unimodal distribution. This is not the case of multiple poses scenarios, where more than one peak need to be estimated. We present an alternative formulation of such framework which, under the correct assumptions, can be used in a multi-person setup.</p><p>The sub-voxel joint detector module takes as input one heatmap H ouput of the Volumetric network, and outputs a list of peaks S(H) = {p i }. The module is applied to each joint heatmap {H l }, obtaining a set of peak for each joint type {{p l i } i=1,...,n l } l . In order to simplify the notation, we discuss the 1D case, but operators can be intuitively extended to 2D or 3D. Given a learned heatmap H, for each spatial location x the values H(x) represent the probability of such location of being a joint. We fix a neighbour function N : C → subsets(C) that associates to each point a neighbor of it (typically, an interval of a given radius centered at x). Define the non-local maxima suppression P : C → {0, 1} via the formula :</p><formula xml:id="formula_5">P(x) = δ max x∈N (x) H(x) = H(x)<label>(2)</label></formula><p>where δ is a Dirac function. P(x) = 1 if and only if x is a maximum of H |N (x) . Define the pixel-peaks as</p><formula xml:id="formula_6">R = {x ∈ C | P (x) = 1}</formula><p>For each x ∈R, define the localized heatmap</p><formula xml:id="formula_7">L x H = 1 x∈N (x) H(x) · H |N (x)</formula><p>Finally, define the sub-pixel peaks as The assumption we rely on is that for every x, in the neighbour N (x) there should be at most one local maximum. In general, this assumption holds if the radius is small enough w.r.t. the quantization constant Q size . In practice, we obtain good results by choosing N (x) to be a 1 or 2 voxels radius interval centered at x, see Section V.</p><formula xml:id="formula_8">S(H) =    x∈N (x)x · (L x H)(x) | x ∈R   </formula><p>3) Skeletons decoder: This module takes as input the peaks {S(H l )} l∈pose layout of the sub-pixel joint detection and the vectormaps {V s } s∈P AF s output of the Volumetric Network and outputs a list of 3D poses. Our algorithm is a direct extension of the one proposed by OpenPose <ref type="bibr" target="#b3">[4]</ref>, with the only difference that line integrals are computed over threedimensional vector fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>A. Datasets 1) CMU Panoptic dataset <ref type="bibr" target="#b2">[3]</ref>: it consists of 31 Full-HD and 480 VGA video streams from synchronized cameras at 29.97 FPS; various scenes (65 sequences with multiple people, social interactions, and a wide range of actions) for a total duration of 5.5 hours. The dataset includes robustly labeled 3D poses, computed using all the camera views. This dataset is perhaps the most complete, open and free to use dataset available for the task of 3D pose estimation. However,  considering that they released annotations quite recently, most works in literature use it only for qualitative evaluations <ref type="bibr" target="#b35">[36]</ref> or for single-person pose detection <ref type="bibr" target="#b1">[2]</ref> discarding multi-person scenes. To the best of our knowledge only <ref type="bibr" target="#b32">[33]</ref> makes use of CMU Panoptic dataset to train and evaluate multi-person 3D pose estimation. We adopt the same subset of scenes and the same train/val/test split of CMU Panoptic used in <ref type="bibr" target="#b32">[33]</ref>: 20 scenes (343k images) of which 10, 4 and 6 scenes for training, validation and test respectively. Only HD cameras are used with data frame rates downsampled to 2 FPS. Since one of our concerns is to assess the cross-view generalization of our model, we split the dataset by scene and by view. Val and test splits use cameras 2, 13, 16, 18 while the train split uses all (or a subset of) the remaining 27 cameras. This is the same camera split used by <ref type="bibr" target="#b1">[2]</ref>. We name this dataset: PanopticD2D.</p><p>2) Shelf <ref type="bibr" target="#b33">[34]</ref>: we adopt this dataset to evaluate the ability of our model to transfer to a completely unseen setup. It consists of a single scene of four people disassembling a shelf at a close range. Video streams are from five calibrated cameras. The dataset includes 3D annotated groundtruth skeletons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metrics</head><p>We employed two commonly used metrics that capture different types of errors in models prediction:</p><p>• MPJPE: Mean Per Joint Precision Error. Given a pair of skeletons, MPJPE is defined as the average of the square distance of the predicted joints from the corresponding ground-truth joints. • PCP: Percentage of Correct estimated Parts. We implemented this metric according to <ref type="bibr" target="#b35">[36]</ref>. A body part is correct if the average distance of the two joints is less than a threshold from the corresponding groundtruth joints locations. The threshold is computed as the 50% of the length of the groundtruth body part. Before computing these metrics we associate for each scene the predicted skeletons to the groundtruth skeletons using linear assignment.</p><p>C. Implementation details 1) Pose Layout: We used a simplified pose layout of 14 keypoints. Apart from the canonical 12 parts of arms and legs, we only added neck and nose. Sometimes, a layout conversion was needed across different datasets and labeling standards. Moreover, we defined 13 PAFs; starting from the neck, a treestructure along arms, legs and nose has been defined. In our setup, increasing excessively the number of joints or PAFs would not make sense due to the limitations of our quantized space.</p><p>2) Skeleton Decoding: Parameters have been found by performing a grid search on Panoptic D2D validation set. Eventually, we opted for an interpolation over a region of size 5 × 5 × 5 voxels. Then, all local maxima with a score lower than 0.3 are discarded; every PAF where the linear integral is on average lower than 0.2 is also removed. Finally, only candidate poses with more than 7 keypoints are retained.</p><p>3) 3D space quantization: we set the size of the quantization voxel to 7.5 cm. This allows us to maintain a quantization of 64 × 64 × 32 voxels on Panoptic dataset to efficiently cover the whole scene of approximately 5 × 5 × 2.5 meters, the last dimension being the vertical axis. 4) Training recipe: Models have been trained with Adam optimizer. We set the initial learning rate to 0.002 and used a step decay policy of 0.3 every 50 epochs. All models have been trained for 200 epochs with a batch size of 8. We implemented the architecture in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ABLATION ANALYSIS A. 3D Augmentation</head><p>We applied 3D data augmentation techniques to the 3D feature space between the Unprojection layer and the Volumetric network. In particular, we implemented the followings:</p><p>1) Random cube embedding: During the training, we consider C ⊆ S to be strictly smaller, and to be randomly embedded. This corresponds to take a random crop of the 3D crop of the scene to be considered for the parameters update.</p><p>From the volumetric network point of view, this reflects into a data augmentation strategy, since moving the cube inside S corresponds to a change of the observed scene and a change in the extrinsic parameters of the cameras.</p><p>We set C to have 32 × 32 × 32 voxels, and we change the embedding at the start of each epoch.</p><p>2) Random rotation: we implement rotations along the vertical axis of 90 • , 180 • , 270 • , to allow a fast implementation. One should take care of the fact that rotation of the 3D space is not reflected into images transformation, so when a rotation is applied we cut the back-propagation graph just before the unprojection layer. In our specific architecture, this sparse  back-prop signal does not drastically affect the training since the only trainable part before the volumetric network is the Reduction layer which has a limited number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture</head><p>In <ref type="table" target="#tab_1">Table I</ref> we reported the results of different experiments to evaluate the contribution of our architectural choices.</p><p>1) Number of volumetric features: it refers to the channels of the volumetric input: it involves the 2D feature maps, the input/output of the unprojection and the volumetric network. For 32 features we used the original V2V network whereas for 64 and 96 we modified it as described in Section III-C1. Models with 64 and 96 channels achieve similar MPJPE and PCP values but 64 is an obvious choice for being computationally lighter.</p><p>2) Loss: we run experiments with different loss types and weighted differently the heatmap and vectormap losses. By evaluating separately PAFs and Peaks quality we noticed that good peaks have a stronger impact in the final metrics than good PAFs, thus we weighted more the Peak part of the loss. Results seem to suggest that the task of predicting good peaks should be tackled with a more elaborate approach than simply differentiate loss weights.</p><p>3) Sub-voxel refinement: by activating it we achieve a lower MPJPE. It has almost no effect on PCP since it improves the sub-voxel localization but does not reduce false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Study on the number of input views</head><p>These experiments have a two-fold goal. On one side, we wanted to understand better the impact on the accuracy of a short/large number of views in the training pool; on the other hand, we wanted to check how well our augmentation strategies could compensate/emulate unseen angles. In <ref type="figure" target="#fig_1">Figure  3</ref> we reported four experiments where we varied the number of views and the number of simultaneous angles used on each training inference. In particular, they show that even a few cameras can produce mildly good results; also, after a certain number, adding more views gives unnoticeable improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. COMPARISON WITH STATE-OF-THE-ART</head><p>In <ref type="table" target="#tab_1">Table II</ref> we report a comparison between our method and the results in <ref type="bibr" target="#b32">[33]</ref> (ACTOR and ORACLE). We remark that the task that authors of <ref type="bibr" target="#b32">[33]</ref> are trying to solve is different from ours. They train an agent to find what are the best views to use to triangulate that particular scene. We consider it to be a good baseline even if the core task of <ref type="bibr" target="#b32">[33]</ref> is not the triangulation algorithm itself. We select 4 fixed validation views and we never train on those. Since some recordings have fewer views available, it turned out that only 36.2% of the test set has 4 views, 31.3% has 3 and 32.5% has just two angles available. The evaluation metric is the MPJPE expressed in cm. The MPJPE of our method is more than 3 times lower compared to ACTOR with 4 views and on average lower than the Oracle.</p><p>We also run our model on the Shelf dataset in order to test it in a completely new environment with unseen views, camera parameters, sensors and every other detail that can bias the evaluation. Results are reported in <ref type="table" target="#tab_1">Table III</ref>. Our method obtains good results even if not on-par with the work by Dong et al. <ref type="bibr" target="#b35">[36]</ref>. However, their approach is much slower being based on top-down 2D backbones. We detail a speed comparison between the two methods in Section VI-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inference speed</head><p>Being a pure bottom-up approach, our method can scale well when increasing the number of views and subjects. Even though our complexity is O(n) in the number of views and O(n 2 ) in the number of people, adding more cameras affects only the Backbone, Reduction and Unprojection modules, which are a small fraction of the cumulative computation OpenPose <ref type="bibr" target="#b15">[16]</ref> (same 2D backbone weights as ours) and iterative greedy matching <ref type="bibr" target="#b34">[35]</ref>. Right: direct 3D pose estimation with our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Direct 3D Pose Estimation (Our Model)</head><p>Single View Input <ref type="figure">Fig. 6</ref>. Direct 3D pose predictions by our model from a single camera view.</p><p>On the frame we projected in red the groundtruth, in white our predictions. In the 3D plot: predictions in color, groundtruth in dashed-black. The network "hallucinates" straight legs of non visible body parts relying on a strong learned prior.</p><p>burden (e.g. for 10 views they take all together only 45 ms, see <ref type="figure">Figure 4</ref>). On the other hand, post-processing the CNN output costs even less; Cao et al. <ref type="bibr" target="#b3">[4]</ref>, implemented an optimized version which takes 0.58 ms for a 9 people image. For reference we can compare our method with the one presented in <ref type="bibr" target="#b35">[36]</ref>, see <ref type="table" target="#tab_1">Table III</ref>. Their approach starts with a person detector <ref type="bibr" target="#b42">[43]</ref>, which takes around 10 ms per view. Then, each detection is forwarded to two branches, of which the 2D pose estimation <ref type="bibr" target="#b10">[11]</ref> is most expensive (we measured 67 ms). From here, the final 3D pose inference takes around 80 ms. We can estimate that a 5 views scenario with 5 people will take (10 * 5) + (67 * 5) + 80 ≈ 465ms, which is about 3.2 times our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative results</head><p>In <ref type="figure" target="#fig_2">figure 5</ref> we show a comparison between our model which performs direct 3D estimations and the result of the geometric triangulation using the 2D skeletons predicted by Lightweight OpenPose <ref type="bibr" target="#b15">[16]</ref> and the iterative greedy matching by <ref type="bibr" target="#b34">[35]</ref>. Notice that our 2D backbone has exactly the same weights as the backbone of <ref type="bibr" target="#b15">[16]</ref> since we do not train nor finetune such part of the network. This highlights the power of estimating directly 3D poses: our volumetric architecture can learn strong pose priors and implicitly discards false detections. By exploiting the 3D representation of the space, it is less prone to occlusion-related errors and it can better deal with crowded scenes. This behavior is even more evident in <ref type="figure">Figure 6</ref> where our method correctly predicts all 3D poses from a monocular view. In particular, notice that even the legs of the blue skeleton are predicted even if they are not visible from that particular view. (View and scene from the validation set). We suppose that the model hallucinates straight up legs since most of the people in Panoptic D2D training set are standing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We present a method for multi-person human pose estimation from calibrated views. Our neural architecture is able to predict 3D pose representations directly from raw camera views. To the best of our knowledge, this is the first attempt to tackle such a task in a completely bottom-up fashion. The proposed method exhibits good computational scalability properties: in particular, it is essentially independent of the number of people in the scene. Moreover, it scales linearly with the number of input views.</p><p>Conducted experiments show state-of-the-art performance on the Panoptic D2D dataset as well as a good generalization on the unseen Shelf dataset. We hope that our work can open new research lines and new scenarios. The method visibly benefits from a wide variety of configurations of people, cameras, and environments during training. Simple 3D data augmentation techniques have been explored and proven effective in enhancing the performance; however, larger datasets, both real and synthetic, could significantly increase the model capabilities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>This work proposes a fast and scalable approach for multi-person 3D pose estimation. Feature representations extracted from each views are aggregated and exploited to perform unified triangulation and pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Accuracy vs number of views. 1to4-pool4-cross view: training with 1 to 4 simultaneous views from a pool of 4; 1to4-pool10-cross view: 1 to 4 from a pool of 10; 1to10-pool20-cross view: 1 to 10 from a pool of 20; 1to10-all views: 1 to 10 from all available views. Configurations 1-2-3 are cross-view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Left: geometric triangulation using 2D poses from Lightweight</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDIES ON PANOPTICD2D VALIDATION SET FOR DIFFERENT ASPECTS OF OUR ARCHITECTURE. 3D AUGMENTATIONS, LOSS TYPE AND RATIO BETWEEN HEATMAP LOSS AND VECTORMAP LOSS WEIGHTS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PCP</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MPJPE (cm)</cell><cell>Head</cell><cell>Torso</cell><cell>Up Arm</cell><cell>Lo Arm</cell><cell>Up Leg</cell><cell>Lo Leg</cell><cell>Avg</cell></row><row><cell>Cube Rotation</cell><cell></cell><cell></cell><cell cols="3">3D Augmentations</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">8.236 99.1 99.3 87.8 65.4 96.9 88.3 89.2</cell></row><row><cell></cell><cell cols="8">4.598 99.6 99.7 98.5 90.1 99.3 98.5 97.7</cell></row><row><cell></cell><cell cols="8">5.350 99.6 99.7 98.6 91.1 99.0 94.9 97.3</cell></row><row><cell></cell><cell cols="8">3.859 99.7 99.7 99.5 95.6 99.3 98.8 98.8</cell></row><row><cell></cell><cell></cell><cell cols="6">Number of Volumetric Features</cell></row><row><cell>32</cell><cell cols="8">4.760 99.6 99.7 97.1 78.9 99.5 98.6 95.9</cell></row><row><cell>64</cell><cell cols="8">3.859 99.7 99.7 99.5 95.6 99.3 98.8 98.8</cell></row><row><cell>96</cell><cell cols="8">3.975 99.7 99.7 99.5 96.2 99.3 98.7 98.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Loss Type</cell><cell></cell><cell></cell></row><row><cell>L1</cell><cell cols="8">4.106 99.6 99.7 99.2 96.2 99.0 98.0 98.7</cell></row><row><cell>L2</cell><cell cols="8">4.125 99.6 99.7 99.5 96.6 99.4 98.9 99.0</cell></row><row><cell>SmoothL1</cell><cell cols="8">3.859 99.7 99.7 99.5 95.6 99.3 98.8 98.8</cell></row><row><cell></cell><cell></cell><cell cols="6">Heatmap / Vectormap Loss Ratio</cell></row><row><cell>1</cell><cell cols="8">3.859 99.7 99.7 99.5 95.6 99.3 98.8 98.8</cell></row><row><cell>3</cell><cell cols="8">4.074 99.7 99.7 99.1 96.6 99.5 98.6 98.9</cell></row><row><cell>10</cell><cell cols="8">3.935 99.7 99.7 98.0 90.9 99.5 98.8 97.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Sub-voxel refinement</cell><cell></cell></row><row><cell></cell><cell cols="8">4.899 99.7 99.7 99.4 94.9 99.3 98.8 98.6</cell></row><row><cell></cell><cell cols="8">3.859 99.7 99.7 99.5 95.6 99.3 98.8 98.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II METHODS</head><label>II</label><figDesc>COMPARISON ON PANOPTIC D2D TEST SET. MPJPE AND PCP METRICS FOR SCENES WITH SINGLE PERSON AND MULTIPLE PEOPLE.</figDesc><table><row><cell></cell><cell>MPJPE (cm)</cell><cell></cell><cell>PCP</cell></row><row><cell>Model</cell><cell>single multi</cell><cell>avg</cell><cell>avg</cell></row><row><cell>ACTOR [33] (2 views)*</cell><cell cols="2">17.21 50.24 33.72</cell><cell>-</cell></row><row><cell>ACTOR (4 views)*</cell><cell cols="2">8.19 20.10 14.14</cell><cell>-</cell></row><row><cell>ACTOR (10 views)*</cell><cell cols="2">6.13 12.21 9.17</cell><cell>-</cell></row><row><cell>Oracle [33] (using GT to select cameras)*</cell><cell cols="2">4.24 9.19 6.71</cell><cell>-</cell></row><row><cell>Ours (1 unseen view)</cell><cell cols="3">10.34 9.32 9.43 80.8</cell></row><row><cell cols="4">Ours (2 to 4 unseen views depending on scene) 5.30 4.09 4.22 98.2</cell></row><row><cell>Ours (10 views, from training view pool)</cell><cell cols="3">3.50 3.56 3.55 98.6</cell></row><row><cell cols="4">*ACTOR: number in brackets refers to maximum number of views to choose</cell></row><row><cell cols="4">from. Oracle means: best views to triangulate are selected using groundtruth.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>COMPARISON ON SHELF DATASET. METRIC IS PCP.</figDesc><table><row><cell></cell><cell cols="2">Model</cell><cell></cell><cell></cell><cell cols="7">Actor 1 Actor 2 Actor 3 Avg Speed(s)</cell></row><row><cell></cell><cell cols="6">Belagiannis et al. [34] 66.1</cell><cell>65.0</cell><cell cols="3">83.2 71.4</cell><cell>-</cell></row><row><cell></cell><cell cols="6">Belagiannis et al. [40] 75.0</cell><cell>67.0</cell><cell cols="3">86.0 76.0</cell><cell>-</cell></row><row><cell></cell><cell cols="6">Belagiannis et al. [41] 75.3</cell><cell>69.7</cell><cell cols="3">87.6 77.5</cell><cell>-</cell></row><row><cell></cell><cell cols="3">Ershadi et al. [42]</cell><cell></cell><cell cols="2">93.3</cell><cell>75.9</cell><cell cols="3">94.8 88.0</cell><cell>-</cell></row><row><cell></cell><cell cols="3">Dong et al. [36]</cell><cell></cell><cell cols="2">98.8</cell><cell>94.1</cell><cell cols="3">97.8 96.9</cell><cell>.465</cell></row><row><cell></cell><cell cols="2">Ours</cell><cell></cell><cell></cell><cell cols="2">94.3</cell><cell>78.4</cell><cell cols="3">96.8 89.8</cell><cell>.146</cell></row><row><cell></cell><cell>0.16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inference time (seconds)</cell><cell>0.00 0.02 0.04 0.06 0.08 0.10 0.12</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell cols="3">5 Number of Views 6</cell><cell>7</cell><cell>8</cell><cell>9 Volumetric Unprojection Reduction Backbone</cell><cell>10</cell></row><row><cell cols="12">Fig. 4. Adding more views increases computational time by a linear factor.</cell></row><row><cell cols="12">However, only few modules are affected by this growth. The main CNN block</cell></row><row><cell cols="12">(in red) has a O(1) complexity, both in the number of views and people.</cell></row><row><cell cols="12">Inference time is measured on a single NVIDIA GeForce GTX 1080Ti.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Igor Moiseev and all the Checkout Technologies team for the fruitful discussions and their friendly support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning based 2d human pose estimation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Science and Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="663" to="676" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7718" to="7727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3334" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Associative embedding: End-toend learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="713" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="417" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Real-time 2d multi-person pose estimation on cpu: Lightweight openpose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Osokin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12004</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2500" to="2509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d human pose estimation with 2d marginal heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1477" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="905" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-person 3d pose estimation and tracking in sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Synergetic reconstruction from 2d pose and 3d motion for wide-space multi-person video motion capture in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ikegami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05613</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6988" to="6997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domes to drones: Selfsupervised active triangulation for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Iterative greedy matching for 3d human pose tracking from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="537" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7792" to="7801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern Recognition</title>
		<meeting>the IEEE conference on computer vision and pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple human pose estimation with temporally consistent 3d pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="742" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1929" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multiple human 3d pose estimation from multiview images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ershadi-Nasab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="15" to="573" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Lighthead r-cnn: In defense of two-stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07264</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
