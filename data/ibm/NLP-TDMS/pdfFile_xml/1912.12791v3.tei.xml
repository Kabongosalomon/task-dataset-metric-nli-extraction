<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object as Hotspots: An Anchor-Free 3D Object Detection Approach via Firing of Hotspots</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Semiconductor, Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Semiconductor, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Semiconductor, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Object as Hotspots: An Anchor-Free 3D Object Detection Approach via Firing of Hotspots</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate 3D object detection in LiDAR based point clouds suffers from the challenges of data sparsity and irregularities. Existing methods strive to organize the points regularly, e.g. voxelize, pass them through a designed 2D/3D neural network, and then define object-level anchors that predict offsets of 3D bounding boxes using collective evidences from all the points on the objects of interest. Contrary to the state-of-the-art anchor-based methods, based on the very nature of data sparsity, we observe that even points on an individual object part are informative about semantic information of the object. We thus argue in this paper for an approach opposite to existing methods using object-level anchors. Inspired by compositional models, which represent an object as parts and their spatial relations, we propose to represent an object as composition of its interior non-empty voxels, termed hotspots, and the spatial relations of hotspots. This gives rise to the representation of Object as Hotspots (OHS). Based on OHS, we further propose an anchor-free detection head with a novel ground truth assignment strategy that deals with interobject point-sparsity imbalance to prevent the network from biasing towards objects with more points. Experimental results show that our proposed method works remarkably well on objects with a small number of points. Notably, our approach ranked 1 st on KITTI 3D Detection Benchmark for cyclist and pedestrian detection, and achieved state-of-theart performance on NuScenes 3D Detection Benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Great success has been witnessed in 2D detection recently thanks to the evolution of CNNs. However, extend-* Work done while interning at Samsung. † Corresponding author: lin1.sun@samsung.com ing 2D detection methods to LiDAR based 3D detection is not trivial because point clouds have very different properties from those of RGB images. Point clouds are irregular, so <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b3">3]</ref> have converted the point clouds to regular grids by subdividing points into voxels and process them using 2D/3D CNNs. Another unique property and challenge of LiDAR point clouds is the sparseness. LiDAR points lie on the objects' surfaces and meanwhile due to occlusion, selfocclusion, reflection or bad weather conditions, very limited quantity of points can be captured by LiDAR.</p><p>Inspired by compositional part-based models <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8]</ref>, which have shown robustness when classifying partially occluded 2D objects and for detecting partially occluded object parts <ref type="bibr" target="#b9">[9]</ref>, we propose to detect objects in LiDAR point clouds by representing them as composition of their interior non-empty voxels. We define the non-empty voxles which contain points within the objects as spots. Furthermore, to encourage the most discriminative features to be learned, we select a small subset of spots in each object as hotspots, thus introducing the concept of hotspots. The selection criteria are elaborated in Sec. 3.2. Technically, during training, hotspots are spots assigned with positive labels; during inference hotspots are activated by the network with high confidences.</p><p>Compositional models represent objects in terms of object parts and their corresponding spatial relations. For example, it can not be an actual dog if a dog's tail is found on the head of the dog. We observe the ground truth box implicitly provides relative spatial information between hotspots and therefore propose a spatial relation encoding to reinforce the inherent spatial relations between hotspots.</p><p>We further realize that our hotspot selection can address an inter-object point-sparsity imbalance issue caused by different object sizes, different distances to the sensor, different occlusion/truncation levels, and reflective surfaces etc. A large number of points are captured on large objects 1 arXiv:1912.12791v3 [cs.CV] <ref type="bibr" target="#b13">13</ref> Oct 2020 or nearby objects to the sensor while much fewer points are collected for small objects and occluded ones. In the KITTI training dataset, the number of points in annotated bounding boxes ranges from 4874 to 1. We categorize this issue as feature imbalance: objects with more points tend to have rich and redundant features for predicting semantic classes and localization while those with few points have few features to learn from.</p><p>The concept of hotspots along with their spatial relations gives rise to a novel representation of Object as Hotspots (OHS). Based on OHS, we design an OHS detection head with a hotspot assignment strategy that deals with interobject point-sparsity imbalance by selecting a limited number of hotspots and balancing positive examples in different objects. This strategy encourages the network to learn from limited but the most discriminative features from each object and prevents a bias towards objects with more points.</p><p>Our concept of OHS is more compatible with anchorfree detectors. Anchor-based detectors assign ground truth to anchors which match the ground truth bounding boxes with IoUs above certain thresholds. This strategy is objectholistic and cannot discriminate different parts of the objects while anchor-free detectors usually predict heatmaps and assign ground truth to individual points inside objects. However, it's nontrivial to design an anchor-free detector. Without the help of human-defined anchor sizes, bounding box regression becomes difficult. We identify the challenge as regression target imbalance due to scale variance and therefore adopt soft argmin from stereo vision <ref type="bibr" target="#b10">[10]</ref> to regress bounding boxes. We show the effectiveness of soft argmin in handling regression target imbalance in our algorithm.</p><p>The main contributions of proposed method can be summarized as follows:</p><p>• We propose a novel representation, termed Object as HotSpots (OHS) to compositionally model objects from LiDAR point clouds as hotspots with spatial relations between them.</p><p>• We propose a unique hotspot assignment strategy to address inter-object point-sparsity imbalance and adopt soft argmin to address the regression target imbalance in anchor-free detectors.</p><p>• Our approach shows robust performance for objects with very few points. The proposed method sets the new state-of-the-art on Nuscene dataset and KITTI test dataset for cyclist and pedestrian detection. Our approach achieves real-time speed with 25 FPS on KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0.1">Anchor-Free Detectors for RGB Images</head><p>Anchor-free detectors for RGB images represent objects as points. Our concept of object as hotspots is closely related to this spirit. ExtremeNet <ref type="bibr" target="#b11">[11]</ref> generates the bounding boxes by detecting top-most, left-most, bottom-most, right-most, and center points of the objects. CornerNet <ref type="bibr" target="#b12">[12]</ref> detects a pair of corners as keypoints to form the bounding boxes. Zhou et al <ref type="bibr" target="#b13">[13]</ref> focuses on box centers, while Cen-terNet <ref type="bibr" target="#b14">[14]</ref> regards both box centers and corners as keypoints. FCOS <ref type="bibr" target="#b15">[15]</ref> and FSAF <ref type="bibr" target="#b16">[16]</ref> detect objects by dense points inside the bounding boxes. The difference between these detectors and our OHS is, ours also takes advantage of the unique property of LiDAR point clouds. We adaptively assign hotspots according to different point-sparsity within each bounding box, which can be obtained from annotations. Whereas in RGB images CNNs tend to learn from texture information <ref type="bibr" target="#b17">[17]</ref>, from which it is hard to measure how rich the features are in each object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0.2">Anchor-Free Detectors for Point Clouds</head><p>Some algorithms without anchors are proposed for indoors scenes. SGPN <ref type="bibr" target="#b18">[18]</ref> segments instances by semantic segmentation and learning a similarity matrix to group points together. This method is not scalable since the size of similarity matrix grows quadratically with the number of points. 3D-BoNet <ref type="bibr" target="#b19">[19]</ref> learns bounding boxes to provide a boundary for points from different instances. Unfortunately, both methods will fail when only partial point clouds have been observed, which is common in LiDAR point clouds. PIXOR <ref type="bibr" target="#b3">[3]</ref> and LaserNet <ref type="bibr" target="#b20">[20]</ref> project LiDAR points into bird's eye view (BEV) or range view and use standard 2D CNNs to produce bounding boxes in BEV. Note that we do not count VoteNet <ref type="bibr" target="#b21">[21]</ref> and Point-RCNN <ref type="bibr" target="#b22">[22]</ref> as anchor-free methods due to usage of anchor sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0.3">Efforts Addressing Regression Target Imbalance</head><p>The bounding box centers and sizes appear in different scales. Some objects have relatively large sizes while others do not. The scale variances in target values give rise to the scale variances in gradients. Small values tend to have smaller gradients and have less impact during training. Regression target imbalance is a great challenge for anchorfree detectors. Anchor-free detectors <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b23">23]</ref> became popular after Feature Pyramid Networks (FPN) <ref type="bibr" target="#b24">[24]</ref> was proposed to handle objects of different sizes. Complimentary to FPNs, anchor-based detectors <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref> rely on anchor locations and sizes to serve as normalization factors to guarantee that regression targets are mostly small values around zero. Multiple sizes and aspect ratios are hand-designed to capture the multi-modal distribution of bounding box sizes. Anchor-free detectors can be regarded as anchor-based detectors with one anchor of unit size at each location and thus anchor-free detectors don't enjoy the normalizing effect of different anchor sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Object as Hotspots</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hotspot Definition</head><p>We represent an object as composition of hotspots. Spots are defined as non-empty voxels which have points and overlap with objects. Only a subset of spots are assigned as hotspots and used for training, to mitigate the imbalance of number of points and the effect of missing or occluded part of objects. Hotspots are responsible for aggregating minimal and the most discriminative features of an object for background/foreground or inter-class classification. In training, hotspots are assigned by ground truth; in inference, hotspots are predicted by the network.</p><p>Intuitively the hotspots should satisfy three properties: 1) they should compose distinguishable parts of the objects in order to capture discriminative features; 2) they should be shared among objects of the same category so that common features can be learned from the same category; 3) they should be minimal so that when only a small number of Li-DAR points are scanned in an object, hotspots still contain essential information to predict semantic information and localization, i.e. hotspots should be robust to objects with a small number of points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hotspot Selection &amp; Assignment</head><p>Hotspot selection &amp; assignment is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref> (a). Unlike previous anchor-free detectors <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b13">13]</ref>, which densely assign positive samples inside objects, we only select a subset of spots on objects as hotspots. We assign hotspots to the output feature map of the backbone network. After passing through the backbone network, a neuron on the feature map can be mapped to a super voxel in input point cloud space. We denote a voxel corresponding to a neuron on the output feature map as V n , where n indexes a neuron.</p><p>The annotations do not tell which parts are distinguishable, but we can infer them from the ground truth bounding boxes B gt . We assume V n is an interior voxel of the object if inside B gt . Then we consider V n as a spot if it's both nonempty and inside B gt . We choose hotspots as nearest spots to the object center based on two motivations: 1) Points away from the object center are less reliable compared to those near the object centers, i.e., they are more vulnerable to the change of view angle. 2) As stated in FCOS <ref type="bibr" target="#b15">[15]</ref>, locations closer to object centers tend to provide more accurate localization.</p><p>We choose at most M nearest spots as hotspots in each object. M is an adaptive number determined by M = C V ol , where C is a hyperparameter we choose and V ol is the volume of the bounding box. Because relatively large objects tend to have more points and richer features, we use M to further suppress the number of hotspots in these objects. If the number of spots in an object is less than M , we assign all spots as hotspots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">HotSpot Network</head><p>Based on OHS, we architect the Hotspot Network (HotSpotNet) for LiDAR point clouds. HotSpotNet consists of a 3D feature extractor and Object-as-Hotspots (OHS) head. OHS head has three subnets for hotspot classification, box regression and spatial relation encoder.</p><p>The overall architecture of our proposed HotSpotNet is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The input LiDAR point clouds are voxelized into cuboid-shape voxels. The input voxels pass through the 3D CNN to generate the feature maps. The three subnets will guide the supervision and generate the predicted 3D bounding boxes. Hotspot assignment happens at the last convolutional feature maps of the backbone. The details of network architecture and the three subnets for supervision are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object-as-Hotspots Head</head><p>Our OHS head network consists of three subnets: 1) a hotspot classification subnet that predicts the likelihood of class categories; 2) a box regression subnet that regresses the center locations, dimensions and orientations of the 3D boxes. 3) a spatial relation encoder for hotspots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Hotspot Classification</head><p>The classification module is a convolutional layer with K heatmaps each corresponding to one category. The hotspots are labeled as ones. The targets for all the non-hotspots are zeros. We apply a gradient mask so that gradients for non-hotspots inside the ground truth bounding boxes are set to zero. That means they are ignored during training and do not contribute to back-propagation. Binary classification is applied to hotspots and non-hotspots. Focal loss <ref type="bibr" target="#b27">[27]</ref> is applied at the end,</p><formula xml:id="formula_0">L cls = K k=1 α(1 − p k ) γ log(p k )<label>(1)</label></formula><p>where,</p><formula xml:id="formula_1">p k = p , hotspots (1 − p) , non-hotspots</formula><p>p is the output probability, and K is the number of categories. The total classification loss is averaged over the total number of hotspots and non-hotspots, excluding the non-hotspots within ground truth bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Box Regression</head><p>The bounding box regression only happens on hotspots. For each hotspot, an eight-dimensional vector [d x , d y , z, log(l), log(w), log(h), cos(r), sin(r)] is regressed to represent the object in LiDAR point clouds. d x , d y are the axis-aligned deviations from the hotspot to the object centroid. The hotspot centroid in BEV can be obtained by:</p><formula xml:id="formula_2">[x h , y h ] = ( j + 0.5 L (x max − x min ) + x min , i + 0.5 W (y max − y min ) + y min ),<label>(2)</label></formula><p>where i, j is the spatial index of its corresponding neuron on the feature map with size W × L, and [x min , x max ], [y min , y max ] are the ranges for x, y when we voxelize all the points. As discussed in Sec. 2.0.3, anchor-free detectors suffer from regression target imbalance. Instead of introducing FPN, i.e. extra layers and computational overhead to our network, we tackle regression target imbalance by carefully designing the targets: 1) We regress log(l), log(w), log(h) instead of their original values because log scales down the absolute values; 2) We regress cos(r), sin(r) instead of r directly because they are constrained in [−1, 1] instead of the original angle value in [−π, π]; 3) We use soft argmin <ref type="bibr" target="#b10">[10]</ref> to help regress d x , d y and z. To regress a point location in a segment ranging from a to b by soft argmin, we divide the segment into N bins, each bin accounting for a length of b−a N . The target location can be represented as</p><formula xml:id="formula_3">t = Σ N i (S i C i ),</formula><p>where S i represents the softmax score of the i th bin and C i is the center location of the i th bin. Soft argmin is widely used in stereo vision to predict disparity in sub-pixel resolution. We notice soft argmin can address regression target imbalance by turning the regression into classification problem and avoiding regressing absolute values.</p><p>Smooth L1 loss <ref type="bibr" target="#b26">[26]</ref> is adopted for regressing these bounding box targets and the regression loss is only computed over hotspots. Only selected non-empty voxels on objects are assigned as hotspots. Previous anchor-free detectors <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref> densely assign locations inside objects as positive samples. Middle: spatial relation encoding: we divide the object bounding box in BEV into quadrants by the orientation (front-facing direction) and its perpendicular direction. Quadrants I, II, III, and IV are color-coded with green, blue, purple and orange respectively in the illustration. Right: illustration of how points of a vehicle are classified into different quadrants, with the same set of color-coding as the middle figure.</p><formula xml:id="formula_4">L loc (x) = 0.5x 2 , |x| &lt; 1 |x| − 0.5 , otherwise<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Spatial Relation Encoder</head><p>Inspired by compositional models, we incorporate hotspot spatial relations to our HotSpotNet. Since convolution is translation-invariant, it's hard for a CNN to learn spatial relations without any supervision. Therefore, we explore the implicit spatial relation from annotations. We observe that most target objects for autonomous driving can be considered as rigid objects (e.g. cars), so the relative locations of hotspots to object centers do not change, which can be determined with the help of bounding box centers and orientations. We thus categorize the relative hotspot location to the object center on BEV into a one-hot vector representing quadrants, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> Middle&amp;Right. We train hotspot spatial relation encoder as quadrant classification with binary cross-entropy loss and we compute the loss only for hotspots.</p><formula xml:id="formula_5">L q = 3 i=0 −[q i log(p i ) + (1 − q i ) log(1 − p i )] (4)</formula><p>where i indexes the quadrant, q i is the target and p i the predicted likelihood falling into the specific quadrant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning and Inference</head><p>The final loss for our proposed HotSpotNet is the weighted sum of losses from three branches:</p><formula xml:id="formula_6">L = δL cls + βL loc + ζL q<label>(5)</label></formula><p>Where, δ, β and ζ are the weights to balance the classification, box regression and spatial relation encoder loss.</p><p>During inference, if the corresponding largest entry value of the K-dimensional vector of the classification heatmaps is above the threshold, we consider the location as hotspot firing for the corresponding object. Since one instance might have multiple hotspots, we further use Non-Maximum Supression (NMS) with the Intersection Over Union (IOU) threshold to pick the most confident hotspot for each object. The spatial relation encoder does not contribute to inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we summarize the dataset in Sec. 5.1 and present the implementation details of our proposed HotSpotNet in 5.2. We evaluate our method on KITTI 3D detection Benchmark <ref type="bibr" target="#b29">[29]</ref> in Sec. 5.3 and NuScenes 3D detection dataset <ref type="bibr" target="#b30">[30]</ref> in Sec. 5.4. We also analyze the advantages of HotSpotNet in Sec. 5.5.2 and present ablation studies in Sec. 5.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation</head><p>KITTI Dataset KITTI has 7,481 annotated LiDAR point clouds for training with 3D bounding boxes for classes such as cars, pedestrians and cyclists. It also provides 7,518 LiDAR point clouds for testing. In the rest of paper, all the ablation studies are conducted on the common train/val split, i.e. 3712 LiDAR point clouds for training and 3769 LiDAR point clouds for validation. To further compare the results with other approaches on KITTI 3D detection benchmark, we randomly split the KITTI annotated data into 4 : 1 for training and validation and report the performance on KITTI test dataset. Following the official KITTI evaluation protocol, average precision (AP) based on 40 points is applied for evaluation. The IoU threshold is 0.7 for cars and 0.5 for pedestrians and cyclists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NuScenes Dataset</head><p>The dataset contains 1, 000 scenes, including 700 scenes for training, 150 scenes for validation and 150 scenes for test. 40, 000 frames are annotated in total, including 10 object categories. The mean average precision (mAP) is calculated based on the distance threshold (i.e. 0.5m, 1.0m, 2.0m and 4.0m). Additionally, a new metric, nuScenes detection score (NDS) <ref type="bibr" target="#b30">[30]</ref>, is introduced as a weighted sum of mAP and precision on box location, scale, orientation, velocity and attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Backbone Network In experiments on KITTI, we adopt the same backbone as used by SECOND <ref type="bibr" target="#b31">[31]</ref>.  <ref type="bibr" target="#b34">[34]</ref> backbones and report each performance. We set voxel size as (0.1, 0.1, 0.16)m for ResNet backbone and (0.2, 0.2)m for PP backbone. For each hotspot, we also set (log l, log w, log h) as outputs of soft argmin to handle the size variances for 10 object categories.</p><p>Object-as-Hotspots Head Since the output feature map of the backbone network is consolidated to BEV, in this paper we assign hotspots in BEV as well. Our OHS head consists of a shared 3 × 3 convolution layer with stride 1. We use a 1 × 1 convolution layer followed by sigmoid to predict confidence for hotspots. For regression, we apply several 1 × 1 convolution layers to different regressed values. Two 1 × 1 convolution layers are stacked to predict soft argmin for (d x , d y ) and z. Additional two 1 × 1 convolution layers to predict the dimensions and rotation. We set the range [−4, 4] with 16 bins for d x , d y and 16 bins for z, with the same vertical range as the input point cloud. We set C = 64 to assign hotspots. For hotspot spatial relation encoder, we use another a 1 × 1 convolution layer with softmax for cross-entropy classification. We set γ = 2.0 and α = 0.25 for focal loss. For KITTI, the loss weights are set as δ = β = ζ = 1. For NuScenes we set δ = 1 and β = ζ = 0.25.</p><p>Training and Inference For KITTI, we train the entire network end-to-end with adamW <ref type="bibr" target="#b35">[35]</ref> optimizer and onecycle policy <ref type="bibr" target="#b36">[36]</ref> with LR max 2.25e −3 , division factor 10, momentum ranges from 0.95 to 0.85 and weight decay 0.01. We train the network with batch size 8 for 150 epochs. During testing, we keep 100 proposals after filtering the confidence lower than 0.3, and then apply the rotated NMS with IOU threshold 0.01 to remove redundant boxes.</p><p>For NuScenes, we set LR max as 0.001. We train the network with batch size 48 for 20 epochs. During testing, we keep 80 proposals after filtering the confidence lower than 0.1, and IOU threshold for rotated NMS is 0.02.</p><p>Data Augmentation Following SECOND <ref type="bibr" target="#b31">[31]</ref>, for KITTI, we apply random flipping, global scaling, global rotation, rotation and translation on individual objects, and GT database sampling. For NuScenes, we adopt same augmentation strategies as in CBGS <ref type="bibr" target="#b32">[32]</ref> except we add random flipping along x axis and attach GT objects from the annotated frames. Half of points from GT database are randomly dropped and GT boxes containing fewer than five points are abandoned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiment results on KITTI benchmark</head><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, we evaluate our method on the KITTI test dataset. For fair comparison, we also show the performance of our implemented SECOND <ref type="bibr" target="#b31">[31]</ref> with same voxel size as ours, represented by HR-SECOND in the table. For the 3D object detection benchmark, solely LiDAR-based, our proposed HotSpotNet outperforms all published LiDAR-based, one-stage detectors on cars, cyclists and pedestrians of all difficulty levels. In particular, by the time of submission our method ranks 1st among all published methods on KITTI test set for cyclist and pedestrian detection. HotSpotNet shows its advantages on objects with a small number of points. The results demonstrate the success of representing objects as hotspots. Our one-stage approach also beats some classic 3D two-stage detectors for car detection, including those fusing LiDAR and RGB images information. Still, our proposed OHS detection head is complimentary to architecture design in terms of better feature extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Inference Speed</head><p>The inference speed of HotSpotNet is 25FPS, tested on KITTI dataset with a Titan V100. We compare inference speed with other approaches in <ref type="table" target="#tab_1">Table 1</ref>. We achieve significant performance gain while maintaining the speed as our baseline SECOND <ref type="bibr" target="#b31">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experiment results on NuScenes dataset</head><p>We present results on NuScenes validation set ( <ref type="table" target="#tab_2">Table 2</ref>) and test set ( <ref type="table" target="#tab_4">Table 3</ref>). We reproduced the baseline CBGS <ref type="bibr" target="#b32">[32]</ref> based on implementation from CenterPoint <ref type="bibr" target="#b48">[48]</ref> without double-flip testing. Our reproduced mAPs are much higher than the results presented in the original CBGS paper. As shown in <ref type="table" target="#tab_2">Table 2</ref>, our HotSpotNet outperforms CBGS by 1.8 and 3.2 in mAP for the PointPillars and ResNet backbone respectively. In <ref type="table" target="#tab_4">Table 3</ref>  outperforms all detectors on the NuScenes 3D Detection benchmark using a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Analysis</head><p>We argue that our approach advances in preventing the network from biasing towards objects with more points without compromising performance on these objects. We analyze the effect of different number of hotspots and performance on objects with different number of points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Different Number of Hotspots</head><p>In Sec. 3.2, we set M = C V ol as the maximum number of hotspots in each object during training. Here we present the performances with different C values: 32, 64, 128, 256, Inf , where Inf means we assign all spots as hotspots. The results are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We can see that generally the larger C is, the higher performance in detecting cars. We only perceive a significant drop when C = 32 and the overall performance in detecting cars is not sensitive to different values of C. The performance in detecting cyclists reaches its peak when C = 128. The lower the C value, the better performance in detecting pedestrians. The performance of detecting pedestrians does not change much when C ≤ 64. To balance the performance on all classes and prevent over-fitting on one class, we choose C = 64 in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Performance on objects with different number of points</head><p>Comparison between SECOND <ref type="bibr" target="#b31">[31]</ref> and our approach for objects with different number of points is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Our approach is consistently better to detect objects with different number of points and less likely to miss objects even with a small number of points. Notably, the relative gain of our approach compared to SECOND increases as the number of points decreases, showing our approach is more robust to sparse objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Effect of different target assignment strategies</head><p>We show the effect of our hotspot assignment strategy in <ref type="table" target="#tab_6">Table 4</ref>. We present three types of target assignment strategy for hotspot while keeping all other settings the same.   Dense means we assign all voxels (empty and non-empty) inside objects as hotspots while ignoring voxels around ground truth bounding box boundaries. 2) We assign all non-empty spots as hotspots, corresponding to C = inf in <ref type="table" target="#tab_6">Table 4</ref>. The maximum number of hotspots in each object is M = C V ol as explained in Sec. 3.2. 3) We set C = 64 in our approach to adaptively limit the number of hotspots in each objects. For reference, we also include our baseline, SEC-OND <ref type="bibr" target="#b31">[31]</ref>. The results show that ours (Dense) and ours (C = inf) have similar performances. When considering pedestrian detection ours (C = inf) is slightly better than ours (Dense). Compared to SECOND, they are both better in car and cyclist detection, especially in the hard cases, but worse in pedestrian detection. The inter-object pointsparsity imbalance makes the pedestrian category hard to train. After balancing the number of hotspots over all objects, ours (C = 64) outperforms all other target assignment strategies by a large margin in both cyclist and pedestrian detection, while the performance for cars barely changes. This justifies our motivation to force the network to learn the minimal and most discriminative features for each objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Effect of spatial relation encoder</head><p>To prove the effectiveness of our hotspot spatial encoder, we show the results of our HotSpotNet with and without spatial relation encoder on KITTI validation split for cars in <ref type="table" target="#tab_10">Table   7</ref>. We can see that when our algorithm is trained with the spatial relation encoder, the overall performance is boosted. Especially, the great improvement can be observed in hard cases for cyclists and pedestrians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.3">Effect of soft argmin</head><p>We show the importance of soft argmin in <ref type="table" target="#tab_9">Table 6</ref>. We perceive improvements by using soft argmin instead of the raw values. Particularly on small objects, e.g. cyclists and pedestrians, soft argmin considerably improves the performance by avoiding regression on absolute values with different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.4">Effects of different spatial relation encodings</head><p>Besides the quadrant partition presented in the main paper, we present four more types of encodings as shown in <ref type="figure" target="#fig_4">Fig.  5</ref>. We supervise our network using different spatial encoding targets: 1) classifying hotspot location into left or right part of the object; 2) classifying hotspot location into front or back of the object; 3) classifying hotspot location into quadrants of the objects; 4) classifying hotspot location into eight directions of the objects; 5) directly regressing deviation to the object center. Deviation is two decimals denoting the relative deviations from the center along the box width and length, and ranges within [−0.5, 0.5] because we normalize the values by the box width and length.    Thus, (0, 0) is the center of the box and the four corners are (−0.5, −0.5), (−0.5, 0.5), (0.5, −0.5) and (0.5, 0.5). The performance of our approach without any spatial relation encoding is presented using 'Ours w/o directions'. The performances of integrating different encodings into our approach are listed in <ref type="table" target="#tab_10">Table 7</ref>. Generally, too coarse, e.g. two partitions, left&amp;right or front&amp;back, or too sophisticated, e.g. eight directions, hotspot-object encoding relations does not help the regression. By contrast, quadrant partition can improve the performance. We argue that quadrant partition encodes the coarse spatial location of the hotspots which helps the final accurate localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Qualitative Visualization</head><p>Previously, we introduce the concept of hotspots and their assignment methods. Do we really learn the hotspots and what do they look like? We trace our detection bounding boxes results back to original fired voxels and visualize them in <ref type="figure" target="#fig_5">Fig. 6</ref>    Interestingly, all the fired hotspots sit at the front corner of the car. It shows that the front corner may be the most distinctive 'part' for detecting/representing a car.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a novel representation, Object-as-Hotspots and an anchor-free detection head with its unique target assignment strategy to tackle inter-object point-sparsity imbalance. Spatial relation encoding as quadrants strengthens features of hotspots and further boosts accurate 3D localization. Extensive experiments show that our approach is effective and robust to sparse point clouds. Meanwhile we address regression target imbalance by carefully designing regression targets, among which soft argmin is applied. We believe our work sheds light on rethinking 3D object representations and understanding characteristics of point clouds and corresponding challenges. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Outline of HotSpotNet. The point cloud is (a) voxelized, and passed through the (b) backbone network to produce 3D feature maps. These feature maps go through (c) a shared convolution layer, pass into three modules to perform (d) Hotspot Classification and (e) 3D Bounding Box regression (f) Spatial Relation Encoder to train the network, and (g) selected hotspots are assigned as positive labels to (d) Hotspot Classification. During inference only (d) Hotspot Classification and (e) 3D Bounding Box Regression are performed to obtain hotspots and bounding boxes respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left: illustration of hotspot selection &amp; assignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Performances with different C values on KITTI val. The horizontal axis also shows the number of active hotspots on average with different C values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Recall of detecting objects with different number of points on KITTI val.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Different hotspot-object spatial relation encodings in local object coordinate system. (a) classifying the hotspot location into left or right part of object bounding box. (b) classifying the hotspot location into front or back of object bounding box. (c) classifying the hotspot location into quadrants of object bounding box. (d) classifying the hotspot location into 8 directions of object bounding box. (e) regressing the hotspot location relative to the object center. The object center is the origin. The farther away from the center, the higher the absolute degree of deviation (0.5). The values are normalized by the sizes of the bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Hotspots visualization. (a) is the original LiDAR point clouds visualization in BEV. All the cars in (a) with active hotspots (colored in red) are visualized in (b). Better viewed in color and zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>.34 55.93 42.60 18.53 24.27 17.31 13.96 17.60 77.47 57.73 48.36 61.22 44.37 39.48 33.69 .32 84.78 68.70 60.82 75.83 53.67 35.52 45.31 33.14 PointPillars[34] L One 62 74.31 82.58 68.99 58.65 77.10 51.92 41.92 51.45 Performance of 3D object detection on KITTI test set. "L", "I" and "L+I" indicates the method uses LiDAR point clouds, RGB images and fusion of two modalities, respectively. FPS stands for frame per second. Bold numbers denotes the best results for single-modal one-stage detectors. Blue numbers are results for best-performing detectors. ResNet 84.0 56.2 67.4 38.0 20.7 82.6 66.2 49.7 65.8 64.3 59.5 66.0</figDesc><table><row><cell>, our approach</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>3D object detection mAP on NuScenes val set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>59.9 18.7 19.4 18.0 11.6 69.4 29.8 14.2 44.6 38.3 31.6 49.7</figDesc><table><row><cell>Method</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>trailer</cell><cell>constr-uction vehicle</cell><cell>pede-strian</cell><cell>motor-cycle</cell><cell>bike</cell><cell>traffic cone</cell><cell>barr-ier</cell><cell>mAP</cell><cell>NDS</cell></row><row><cell>SARPNET [49]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointPillars [34]</cell><cell cols="4">68.4 23.0 28.2 23.4</cell><cell>4.1</cell><cell cols="2">59.7 27.4</cell><cell cols="5">1.1 30.8 38.9 30.5 45.3</cell></row><row><cell>WYSIWYG [50]</cell><cell cols="4">79.1 30.4 46.6 40.1</cell><cell>7.1</cell><cell cols="2">65.0 18.2</cell><cell cols="5">0.1 28.8 34.7 35.0 41.9</cell></row><row><cell>CBGS [32]</cell><cell cols="12">81.1 48.5 54.9 42.9 10.5 80.1 51.5 22.3 70.9 65.7 52.8 63.3</cell></row><row><cell cols="13">HotSpotNet-ResNet (Ours) 83.1 50.9 56.4 53.3 23.0 81.3 63.5 36.6 73.0 71.6 59.3 66.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>3D detection mAP on the NuScenes test set</figDesc><table><row><cell></cell><cell>90 92</cell><cell>Car</cell><cell></cell><cell>85 90</cell><cell>Cyclist</cell><cell></cell><cell>70.0 72.5</cell><cell>Pedestrian</cell><cell>Easy Moderate Hard</cell></row><row><cell>3D AP(%)</cell><cell>80 82 84 86 88</cell><cell>Easy Moderate Hard</cell><cell>3D AP(%)</cell><cell>65 70 75 80</cell><cell>Easy Moderate Hard</cell><cell>3D AP(%)</cell><cell>57.5 60.0 62.5 65.0 67.5</cell></row><row><cell></cell><cell cols="2">32/12.1 64/17.2 128/25.2 256/38.0 inf/39.3 C / #active hotspots</cell><cell></cell><cell cols="2">32/20.4 64/21.0 128/23.1 256/23.1 inf/23.1 C / #active hotspots</cell><cell></cell><cell cols="2">32/14.2 64/14.2 128/14.2 256/14.2 inf/14.2 C / #active hotspots</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>81.96 90.95 77.24 61.62 80.13 57.77 64.19 69.14 57.99 Ours (Dense) 82.2 91.09 79.69 66.45 85.85 62.16 62.82 68.88 55.78 Ours (C = inf) 82.93 91.98 80.46 67.66 86.41</figDesc><table><row><cell>Method</cell><cell>3D Detection on Car Mod Easy Hard</cell><cell cols="2">3D Detection on Cyclist 3D Detection on Pedestrian Mod Easy Hard Mod Easy Hard</cell></row><row><cell cols="3">SECOND [31] 63.5</cell><cell>62.08 68.22</cell><cell>56.64</cell></row><row><cell cols="3">Ours (C = 64) 82.75 91.87 80.22 72.55 88.22 68.08</cell><cell>65.9 72.23</cell><cell>60.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Effect of different target assignment strategy. Dense: assigning both empty and non-empty voxels inside objects as hotspots; C=inf: assigning all spots as hotspots; C=64: assigning limited number of spots as hotspots.</figDesc><table><row><cell>[512, 1024] [256, 512] [128, 256] number of points</cell><cell>[64, 128]</cell><cell>[0, 64]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>. Here we visualize some samples with cars from validation dataset, all the fired hotspots are marked /o quadrant 82.27 91.75 79.96 69.31 89.48 65.04 65.45 72.77 58.36 Ours w quadrant 82.75 91.87 80.22 72.55 88.22 68.08 65.9 72.23 60.06 Diff ↑ 0.48 ↑ 0.12 ↑ 0.26 ↑ 3.24 ↓ −1.24 ↑ 3.04 ↑ 0.45 ↓ −0.54 ↑ 1.7</figDesc><table><row><cell>Method</cell><cell>3D Detection on Car Mod Easy Hard</cell><cell>3D Detection on Cyclist Mod Easy Hard</cell><cell>3D Detection on Pedestrian Mod Easy Hard</cell></row><row><cell>Ours w</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Effect of quadrants as spatial relation encoding. /o soft argmin 82.31 91.53 79.88 68.65 88.11 64.36 63.7 67.62 57.15 Ours w/ soft argmin 82.75 91.87 80.22 72.55 88.22 68.08 65.9 72.23 60.06 Diff ↑ 0.44 ↑ 0.34 ↑ 0.34 ↑ 3.9 ↑ 0.11 ↑ 3.72 ↑ 2.9 ↑ 4.59 ↑ 2.91</figDesc><table><row><cell>Method</cell><cell>3D Detection on Car Mod Easy Hard</cell><cell>3D Detection on Cyclist 3D Detection on Pedestrian Mod Easy Hard Mod Easy Hard</cell></row><row><cell>Ours w</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Performance of soft argmin on (x, y, z) coordination.</figDesc><table><row><cell>Method</cell><cell>3D Detection on Car Mod Easy Hard</cell><cell>3D Detection on Cyclist Mod Easy Hard</cell><cell>3D Detection on Pedestrian Mod Easy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Effects of different hotspot spatial relation encodings. red. (a) presents the original LiDAR point clouds in BEV and (b) shows all the point clouds from the detected cars.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>We thank Dr. XYZ , Ernest Cheung (Samsung), Gweltaz Lever (Samsung), and Chenxu Luo (Johns Hopkins University and Samsung) for useful discussions that greatly improved the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">on Pedestrian Mod Easy Hard Mod Easy Hard Mod Easy Hard</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context and hierarchy in a probabilistic image model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised structure learning: Hierarchical recursive composition, suspicious coincidence and competitive exclusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Leo Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoda</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning a hierarchical compositional shape vocabulary for multi-class object representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Boben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of dictionaries of hierarchical compositional models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenze</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Wieczorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonali</forename><surname>Parbhoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Morel-Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06171</idno>
		<title level="m">Greedy structure learning of hierarchical compositional models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepvoting: A robust and explainable deep network for semantic part detection under partial occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01355</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00621</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning object bounding boxes for 3d instance segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01140</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lasernet: An efficient probabilistic 3d object detector for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">K</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11027</idno>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Class-balanced grouping and sampling for point cloud 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09492</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Superconvergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">1100612</biblScope>
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complex-yolo: An euler-regionproposal for real-time 3d object detection on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Amende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst-Michael</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03851</idno>
		<title level="m">Iou loss for 2d/3d object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Voxelfpn: multi-scale voxel feature aggregation in 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05286</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IROS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10471</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11275</idno>
		<title level="m">Center-based 3d object detection and tracking</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sarpnet: Shape attention regional proposal network for lidar-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page" from="53" to="63" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ziglar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04986</idno>
		<title level="m">What you see is what you get: Exploiting visibility for 3d object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
