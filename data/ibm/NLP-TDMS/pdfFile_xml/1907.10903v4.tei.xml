<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DROPEDGE: TOWARDS DEEP GRAPH CONVOLU- TIONAL NETWORKS ON NODE CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<email>yu.rong@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<email>hwenbing@126.com</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
							<email>tingyangxu@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<email>jzhuang@uta.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DROPEDGE: TOWARDS DEEP GRAPH CONVOLU- TIONAL NETWORKS ON NODE CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over-fitting and over-smoothing are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on https://github.com/DropEdge/DropEdge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Convolutional Networks (GCNs), which exploit message passing or equivalently certain neighborhood aggregation function to extract high-level features from a node as well as its neighborhoods, have boosted the state-of-the-arts for a variety of tasks on graphs, such as node classification <ref type="bibr" target="#b0">(Bhagat et al., 2011;</ref><ref type="bibr" target="#b13">Zhang et al., 2018)</ref>, social recommendation <ref type="bibr" target="#b6">(Freeman, 2000;</ref><ref type="bibr" target="#b26">Perozzi et al., 2014)</ref>, and link prediction <ref type="bibr" target="#b20">(Liben-Nowell &amp; Kleinberg, 2007)</ref> to name some. In other words, GCNs have been becoming one of the most crucial tools for graph representation learning. Yet, when we revisit typical GCNs on node classification <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>, they are usually shallow (e.g. the number of the layers is 2 1 ). Inspired from the success of deep CNNs on image classification, several attempts have been proposed to explore how to build deep GCNs towards node classification <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b18">Li et al., 2018a;</ref><ref type="bibr">Xu et al., 2018a;</ref>; nevertheless, none of them delivers sufficiently expressive architecture. The motivation of this paper is to analyze the very factors that impede deeper GCNs to perform promisingly, and develop method to address them.</p><p>We begin by investigating two factors: over-fitting and over-smoothing. Over-fitting comes from the case when we utilize an over-parametric model to fit a distribution with limited training data, where the model we learn fits the training data very well but generalizes poorly to the testing data. It does exist if we apply a deep GCN on small graphs (see 4-layer GCN on Cora in <ref type="figure" target="#fig_0">Figure 1</ref>). Over-smoothing, towards the other extreme, makes training a very deep GCN difficult. As first introduced by <ref type="bibr" target="#b18">Li et al. (2018a)</ref>  other, such that, if extremely we go with an infinite number of layers, all nodes' representations will converge to a stationary point, making them unrelated to the input features and leading to vanishing gradients. We call this phenomenon as over-smoothing of node features. To illustrate its influence, we have conducted an example experiment with 8-layer GCN in <ref type="figure" target="#fig_0">Figure 1</ref>, in which the training of such a deep GCN is observed to converge poorly.</p><p>Both of the above two issues can be alleviated, using the proposed method, DropEdge. The term "DropEdge" refers to randomly dropping out certain rate of edges of the input graph for each training time. There are several benefits in applying DropEdge for the GCN training (see the experimental improvements by DropEdge in <ref type="figure" target="#fig_0">Figure 1</ref>). First, DropEdge can be considered as a data augmentation technique. By DropEdge, we are actually generating different random deformed copies of the original graph; as such, we augment the randomness and the diversity of the input data, thus better capable of preventing over-fitting. Second, DropEdge can also be treated as a message passing reducer. In GCNs, the message passing between adjacent nodes is conducted along edge paths. Removing certain edges is making node connections more sparse, and hence avoiding over-smoothing to some extent when GCN goes very deep. Indeed, as we will draw theoretically in this paper, DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it.</p><p>We are also aware that the dense connections employed by JKNet <ref type="bibr">(Xu et al., 2018a)</ref> are another kind of tools that can potentially prevent over-smoothing. In its formulation, JKNet densely connects each hidden layer to the top one, hence the feature mappings in lower layers that are hardly affected by over-smoothing are still maintained. Interestingly and promisingly, we find that the performance of JKNet can be promoted further if it is utilized along with our DropEdge. Actually, our DropEdge-as a flexible and general technique-is able to enhance the performance of various popular backbone networks on several benchmarks, including <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>, ResGCN <ref type="bibr">), JKNet (Xu et al., 2018a</ref>, and GraphSAGE <ref type="bibr" target="#b8">(Hamilton et al., 2017)</ref>. We provide detailed evaluations in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>GCNs Inspired by the huge success of CNNs in computer vision, a large number of methods come redefining the notion of convolution on graphs under the umbrella of GCNs. The first prominent research on GCNs is presented in <ref type="bibr" target="#b1">Bruna et al. (2013)</ref>, which develops graph convolution based on spectral graph theory. Later, Kipf &amp; Welling (2017); <ref type="bibr" target="#b3">Defferrard et al. (2016)</ref>; <ref type="bibr" target="#b10">Henaff et al. (2015)</ref>; <ref type="bibr" target="#b19">Li et al. (2018b)</ref>; <ref type="bibr" target="#b16">Levie et al. (2017)</ref> apply improvements, extensions, and approximations on spectralbased GCNs. To address the scalability issue of spectral-based GCNs on large graphs, spatial-based GCNs have been rapidly developed <ref type="bibr" target="#b8">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b23">Niepert et al., 2016;</ref><ref type="bibr" target="#b7">Gao et al., 2018)</ref>. These methods directly perform convolution in the graph domain by aggregating the information from neighbor nodes. Recently, several sampling-based methods have been proposed for fast graph representation learning, including the node-wise sampling methods <ref type="bibr" target="#b8">(Hamilton et al., 2017)</ref>, the layer-wise approach <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> and its layer-dependent variant <ref type="bibr" target="#b13">(Huang et al., 2018)</ref>. Specifically, GAT <ref type="bibr" target="#b29">(Velickovic et al., 2018)</ref> has discussed applying dropout on edge attentions. While it actually is a post-conducted version of DropEdge before attention computation, the relation to over-smoothing is never explored in <ref type="bibr" target="#b29">Velickovic et al. (2018)</ref>. In our paper, however, we have formally presented the formulation of DropEdge and provided rigorous theoretical justification of its benefit in alleviating over-smoothing. We also carried out extensive experiments by imposing DropEdge on several popular backbones. One additional point is that we further conduct adjacency normalization after dropping edges, which, even simple, is able to make it much easier to converge during training and reduce gradient vanish as the number of layers grows.</p><p>Deep GCNs Despite the fruitful progress, most previous works only focus on shallow GCNs while the deeper extension is seldom discussed. The attempt for building deep GCNs is dated back to the GCN paper <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>, where the residual mechanism is applied; unexpectedly, as shown in their experiments, residual GCNs still perform worse when the depth is 3 and beyond. The authors in <ref type="bibr" target="#b18">Li et al. (2018a)</ref> first point out the main difficulty in constructing deep networks lying in over-smoothing, but unfortunately, they never propose any method to address it. The follow-up study <ref type="bibr" target="#b15">(Klicpera et al., 2019)</ref> solves over-smoothing by using personalized PageRank that additionally involves the rooted node into the message passing loop; however, the accuracy is still observed to decrease when the depth increases from 2. JKNet (Xu et al., 2018a) employs dense connections for multi-hop message passing which is compatible with DropEdge for formulating deep GCNs. <ref type="bibr" target="#b24">Oono &amp; Suzuki (2019)</ref> theoretically prove that the node features of deep GCNs will converge to a subspace and incur information loss. It generalizes the conclusion in <ref type="bibr" target="#b18">Li et al. (2018a)</ref> by further considering the ReLu function and convolution filters. Our interpretations on why DropEdge can impede over-smoothing is based on the concepts proposed by <ref type="bibr" target="#b24">Oono &amp; Suzuki (2019)</ref>. A recent method  has incorporated residual layers, dense connections and dilated convolutions into GCNs to facilitate the development of deep architectures. Nevertheless, this model is targeted on graph-level classification (i.e. point cloud segmentation), where the data points are graphs and naturally disconnected from each other. In our task for node classification, the samples are nodes and they all couple with each other, thus the over-smoothing issue is more demanded to be addressed. By leveraging DropEdge, we are able to relieve over-smoothing, and derive more enhanced deep GCNs on node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NOTATIONS AND PRELIMINARIES</head><p>Notations. Let G = (V, E) represent the input graph of size N with nodes v i ∈ V and edges</p><formula xml:id="formula_0">(v i , v j ) ∈ E.</formula><p>The node features are denoted as X = {x 1 , · · · , x N } ∈ R N ×C , and the adjacency matrix is defined as A ∈ R N ×N which associates each edge (v i , v j ) with its element A ij . The node degrees are given by d = {d 1 , · · · , d N } where d i computes the sum of edge weights connected to node i. We define D as the degree matrix whose diagonal elements are obtained from d.</p><p>GCN is originally developed by <ref type="bibr" target="#b14">Kipf &amp; Welling (2017)</ref>. The feed forward propagation in GCN is recursively conducted as</p><formula xml:id="formula_1">H (l+1) = σ Â H (l) W (l) ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">H (l+1) = {h (l+1) 1 , · · · , h (l+1) N</formula><p>} are the hidden vectors of the l-th layer with h (l) i as the hidden feature for node i;Â =D −1/2 (A + I)D −1/2 is the re-normalization of the adjacency matrix, and D is the corresponding degree matrix of A + I; σ(·) is a nonlinear function, i.e. the ReLu function; and W (l) ∈ R C l ×C l−1 is the filter matrix in the l-th layer with C l refers to the size of l-th hidden layer. We denote one-layer GCN computed by Equation 1 as Graph Convolutional Layer (GCL) in what follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OUR METHOD: DROPEDGE</head><p>This section first introduces the methodology of the DropEdge technique as well as its layer-wise variant where the adjacency matrix for each GCN layer is perturbed individually. We also explain how the proposed DropEdge can prevent over-fitting and over-smoothing in generic GCNs. Particularly for over-smoothing, we provide its mathematical definition and theoretical derivations on showing the benefits of DropEdge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">METHODOLOGY</head><p>At each training epoch, the DropEdge technique drops out a certain rate of edges of the input graph by random. Formally, it randomly enforces V p non-zero elements of the adjacency matrix A to be zeros, where V is the total number of edges and p is the dropping rate. If we denote the resulting adjacency matrix as A drop , then its relation with A becomes</p><formula xml:id="formula_3">A drop = A − A ,<label>(2)</label></formula><p>where A is a sparse matrix expanded by a random subset of size V p from original edges E. Following the idea of <ref type="bibr" target="#b14">Kipf &amp; Welling (2017)</ref>, we also perform the re-normalization trick on A drop , leading tô A drop . We replaceÂ withÂ drop in Equation 1 for propagation and training. When validation and testing, DropEdge is not utilized.</p><p>Preventing over-fitting. DropEdge produces varying perturbations of the graph connections. As a result, it generates different random deformations of the input data and can be regarded as a data augmentation skill for graphs. To explain why this is valid, we provide an intuitive understanding here. The key in GCNs is to aggregate neighbors' information for each node, which can be understood as a weighted sum of the neighbor features (the weights are associated with the edges). From the perspective of neighbor aggregation, DropEdge enables a random subset aggregation instead of the full aggregation during GNN training. Statistically, DropEdge only changes the expectation of the neighbor aggregation up to a multiplier p, if we drop edges with probability p. This multiplier will be actually removed after weights normalization, which is often the case in practice. Therefore, DropEdge does not change the expectation of neighbor aggregation and is an unbiased data augmentation technique for GNN training, similar to typical image augmentation skills (e.g. rotation, cropping and flapping) that are capable of hindering over-fitting in training CNNs. We will provide experimental validations in § 5.1.</p><p>Layer-Wise DropEdge. The above formulation of DropEdge is one-shot with all layers sharing the same perturbed adjacency matrix. Indeed, we can perform DropEdge for each individual layer. Specifically, we obtainÂ drop . Such layer-wise version brings in more randomness and deformations of the original data, and we will experimentally compare its performance with the original DropEdge in § 5.2.</p><p>Over-smoothing is another obstacle of training deep GCNs, and we will detail how DropEdge can address it to some extent in the next section. For simplicity, the following derivations assume all GCLs share the same perturbed adjacency matrix, and we will leave the discussion on layer-wise DropEdge for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TOWARDS PREVENTING OVER-SMOOTHING</head><p>By its original definition in <ref type="bibr" target="#b18">Li et al. (2018a)</ref>, the over-smoothing phenomenon implies that the node features will converge to a fixed point as the network depth increases. This unwanted convergence restricts the output of deep GCNs to be only relevant to the graph topology but independent to the input node features, which as a matter of course incurs detriment of the expressive power of GCNs. <ref type="bibr" target="#b24">Oono &amp; Suzuki (2019)</ref> has generalized the idea in <ref type="bibr" target="#b18">Li et al. (2018a)</ref> by taking both the non-linearity (i.e. the ReLu function) and the convolution filters into account; they explain over-smoothing as convergence to a subspace rather than convergence to a fixed point. This paper will use the concept of subspace by <ref type="bibr" target="#b24">Oono &amp; Suzuki (2019)</ref> for more generality.</p><p>We first provide several relevant definitions that facilitate our later presentations.</p><formula xml:id="formula_4">Definition 1 (subspace). Let M := {EC|C ∈ R M ×C } be an M -dimensional subspace in R N ×C , where E ∈ R N ×M is orthogonal, i.e. E T E = I M , and M ≤ N .</formula><p>Definition 2 ( -smoothing). We call the -smoothing of node features happens for a GCN, if all its hidden vectors H (l) beyond a certain layer L have a distance no larger than ( &gt; 0) with respect to a subspace M that is independent to the input features, namely,</p><formula xml:id="formula_5">d M (H (l) ) &lt; , ∀l ≥ L,<label>(3)</label></formula><p>where d M (·) computes the distance between the input matrix and the subspace M. 3 Definition 3 (the -smoothing layer). Given the subspace M and , we call the minimal value of the layers that satisfy Equation 3 as the -smoothing layer, that is, l * (M, ) := min l {d M (H (l) ) &lt; }.</p><p>Since conducting analysis exactly based on the -smoothing layer is difficult, we instead define the relaxed -smoothing layer which is proved to be an upper bound of l * .</p><p>Definition 4 (the relaxed -smoothing layer). Given the subspace M and , we calll(M, ) =</p><formula xml:id="formula_6">log( /d M (X)) log sλ</formula><p>as the relaxed smoothing layer, where, · computes the ceil of the input, s is the supremum of the filters' singular values over all layers, and λ is the second largest eigenvalue ofÂ. Besides, we havel ≥ l * 4 .</p><p>According to the conclusions by the authors in <ref type="bibr" target="#b24">Oono &amp; Suzuki (2019)</ref>, a sufficiently deep GCN will certainly suffer from the -smoothing issue for any small value of under some mild conditions (the details are included in the supplementary material). Note that they only prove the existence of -smoothing in deep GCN without developing any method to address it.</p><p>Here, we will demonstrate that adopting DropEdge alleviates the -smoothing issue in two aspects: 1. By reducing node connections, DropEdge is proved to slow down the convergence of over-smoothing; in other words, the value of the relaxed -smoothing layer will only increase if using DropEdge. 2.</p><p>The gap between the dimensions of the original space and the converging subspace, i.e. N − M measures the amount of information loss; larger gap means more severe information loss. As shown by our derivations, DropEdge is able to increase the dimension of the converging subspace, thus capable of reducing information loss.</p><p>We summarize our conclusions as follows. Theorem 1. We denote the original graph as G and the one after dropping certain edges out as G . Given a small value of , we assume G and G will encounter the -smoothing issue with regard to subspaces M and M , respectively. Then, either of the following inequalities holds after sufficient edges removed.</p><p>• The relaxed smoothing layer only increases:l(M, ) ≤l(M , );</p><p>• The information loss is decreased:</p><formula xml:id="formula_7">N − dim(M) &gt; N − dim(M ).</formula><p>The proof of Theorem 1 is based on the derivations in <ref type="bibr" target="#b24">Oono &amp; Suzuki (2019)</ref> as well as the concept of mixing time that has been studied in the random walk theory <ref type="bibr" target="#b21">(Lovász et al., 1993)</ref>. We provide the full details in the supplementary material. Theorem 1 tells that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. In this way, DropEdge enables us to train deep GCNs more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DISCUSSIONS</head><p>This sections contrasts the difference between DropEdge and other related concepts including Dropout, DropNode, and Graph Sparsification.</p><p>DropEdge vs. Dropout The Dropout trick <ref type="bibr" target="#b11">(Hinton et al., 2012)</ref> is trying to perturb the feature matrix by randomly setting feature dimensions to be zeros, which may reduce the effect of over-fitting but is of no help to preventing over-smoothing since it does not make any change of the adjacency matrix. As a reference, DropEdge can be regarded as a generation of Dropout from dropping feature dimensions to dropping edges, which mitigates both over-fitting and over-smoothing. In fact, the impacts of Dropout and DropEdge are complementary to each other, and their compatibility will be shown in the experiments.</p><p>DropEdge vs. DropNode Another related vein belongs to the kind of node sampling based methods, including GraphSAGE <ref type="bibr" target="#b8">(Hamilton et al., 2017)</ref>, FastGCN <ref type="bibr" target="#b2">(Chen et al., 2018)</ref>, and AS-GCN <ref type="bibr" target="#b13">(Huang et al., 2018)</ref>. We name this category of approaches as DropNode. For its original motivation, DropNode samples sub-graphs for mini-batch training, and it can also be treated as a specific form of dropping edges since the edges connected to the dropping nodes are also removed. However, the effect of DropNode on dropping edges is node-oriented and indirect. By contrast, DropEdge is edge-oriented, and it is possible to preserve all node features for the training (if they can be fitted into the memory at once), exhibiting more flexibility. Further, to maintain desired performance, the sampling strategies in current DropNode methods are usually inefficient, for example, GraphSAGE suffering from the exponentially-growing layer size, and AS-GCN requiring the sampling to be conducted recursively layer by layer. Our DropEdge, however, neither increases the layer size as the depth grows nor demands the recursive progress because the sampling of all edges are parallel.</p><p>DropEdge vs. Graph-Sparsification Graph-Sparsification <ref type="bibr" target="#b4">(Eppstein et al., 1997)</ref> is an old research topic in the graph domain. Its optimization goal is removing unnecessary edges for graph compressing while keeping almost all information of the input graph. This is clearly district to the purpose of DropEdge where no optimization objective is needed. Specifically, DropEdge will remove the edges of the input graph by random at each training time, whereas Graph-Sparsification resorts to a tedious optimization method to determine which edges to be deleted, and once those edges are discarded the output graph keeps unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Datasets Joining the previous works' practice, we focus on four benchmark datasets varying in graph size and feature type: (1) classifying the research topic of papers in three citation datasets: Cora, Citeseer and Pubmed <ref type="bibr" target="#b27">(Sen et al., 2008)</ref>; (2) predicting which community different posts belong to in the Reddit social network <ref type="bibr" target="#b8">(Hamilton et al., 2017)</ref>. Note that the tasks in Cora, Citeseer and Pubmed are transductive underlying all node features are accessible during training, while the task in Reddit is inductive meaning the testing nodes are unseen for training. We apply the full-supervised training fashion used in <ref type="bibr" target="#b13">Huang et al. (2018)</ref> and <ref type="bibr" target="#b2">Chen et al. (2018)</ref> on all datasets in our experiments. The statics of all datasets are listed in the supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CAN DROPEDGE GENERALLY IMPROVE THE PERFORMANCE OF DEEP GCNS?</head><p>In this section, we are interested in if applying DropEdge can promote the performance of current popular GCNs (especially their deep architectures) on node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementations</head><p>We consider five backbones: GCN (Kipf &amp; Welling, 2017), ResGCN <ref type="bibr" target="#b9">(He et al., 2016;</ref>, JKNet (Xu et al., 2018a), IncepGCN 5 and GraphSAGE <ref type="bibr" target="#b8">(Hamilton et al., 2017)</ref> with varying depth from 2 to 64. <ref type="bibr">6</ref> Since different structure exhibits different training dynamics on different dataset, to enable more robust comparisons, we perform random hyper-parameter search for each model, and report the case giving the best accuracy on validation set of each benchmark. The searching space of hyper-parameters and more details are provided in <ref type="table" target="#tab_7">Table 4</ref> in the supplementary material. Regarding the same architecture w or w/o DropEdge, we apply the same set of hyperparameters except the drop rate p for fair evaluation.</p><p>Overall Results <ref type="table" target="#tab_0">Table 1</ref> summaries the results on all datasets. We only report the performance of the model with 2/8/32 layers here due to the space limit, and provide the accuracy under other different depths in the supplementary material. It's observed that DropEdge consistently improves the  <ref type="table" target="#tab_1">Table 2</ref>; for the SOTA methods, we reuse the results reported in <ref type="bibr" target="#b13">Huang et al. (2018)</ref>. We have these findings: (1) Clearly, our DropEdge obtains significant enhancement against SOTAs; particularly on Reddit, the best accuracy by our method is 97.02%, and it is better than the previous best by AS-GCN (96.27%), which is regarded as a remarkable boost considering the challenge on this benchmark.</p><p>(2) For most models with DropEdge, the best accuracy is obtained under the depth beyond 2, which again verifies the impact of DropEdge on formulating deep networks.</p><p>(3) As mentioned in § 4.3, FastGCN, AS-GCN and GraphSAGE are considered as the DropNode extensions of GCNs. The DropEdge based approaches outperform the DropNode based variants as shown in <ref type="table" target="#tab_1">Table 2</ref>, which somehow confirms the effectiveness of DropEdge. Actually, employing DropEdge upon the DropNode methods further delivers promising enhancement, which can be checked by revisiting the increase by DropEdge for GraphSAGE in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">HOW DOES DROPEDGE HELP?</head><p>This section continues a more in-depth analysis on DropEdge and attempts to figure out why it works. Due to the space limit, we only provide the results on Cora, and defer the evaluations on other datasets to the supplementary material.</p><p>Note that this section mainly focuses on analyzing DropEdge and its variants, without the concern with pushing state-of-the-art results. So, we do not perform delicate hyper-parameter selection. We employ GCN as the backbone in this section. Here, GCN-n denotes GCN of depth n. The hidden dimension, learning rate and weight decay are fixed to 256, 0.005 and 0.0005, receptively. The    <ref type="formula" target="#formula_8">(4)</ref> random seed is fixed. We train all models with 200 epochs. Unless otherwise mentioned, we do not utilize the "withloop" and "withbn" operation (see their definitions in <ref type="table" target="#tab_7">Table 4</ref> in the appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">ON PREVENTING OVER-SMOOTHING</head><p>As discussed in § 4.2, the over-smoothing issue exists when the top-layer outputs of GCN converge to a subspace and become unrelated to the input features with the increase in depth. Since we are unable to derive the converging subspace explicitly, we measure the degree of over-smoothing by instead computing the difference between the output of the current layer and that of the previous one. We adopt the Euclidean distance for the difference computation. Lower distance means more serious over-smoothing. Experiments are conducted on GCN-8. <ref type="figure" target="#fig_4">Figure 3 (a)</ref> shows the distances of different intermediate layers (from 2 to 6) under different edge dropping rates (0 and 0.8). Clearly, over-smoothing becomes more serious in GCN as the layer grows, which is consistent with our conjecture. Conversely, the model with DropEdge (p = 0.8) reveals higher distance and slower convergent speed than that without DropEdge (p = 0), implying the importance of DropEdge to alleviating over-smoothing. We are also interested in how the oversmoothing will act after training. For this purpose, we display the results after 150-epoch training in <ref type="figure" target="#fig_4">Figure 3 (b)</ref>. For GCN without DropEdge, the difference between outputs of the 5-th and 6-th layers is equal to 0, indicating that the hidden features have converged to a certain stationary point. On the contrary, GCN with DropEdge performs promisingly, as the distance does not vanish to zero when    <ref type="figure">Figure 4</ref> the number of layers grows; it probably has successfully learned meaningful node representations after training, which could also be validated by the training loss in <ref type="figure" target="#fig_4">Figure 3 (c</ref> Here, we provide the experimental evaluation on assessing its effect. As observed from <ref type="figure">Figure 4b</ref>, the LW DropEdge achieves lower training loss than the original version, whereas the validation value between two models is comparable. It implies that LW DropEdge can facilitate the training further than original DropEdge. However, we prefer to use DropEdge other than the LW variant so as to not only avoid the risk of over-fitting but also reduces computational complexity since LW DropEdge demands to sample each layer and spends more time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have presented DropEdge, a novel and efficient technique to facilitate the development of deep Graph Convolutional Networks (GCNs). By dropping out a certain rate of edges by random, DropEdge includes more diversity into the input data to prevent over-fitting, and reduces message passing in graph convolution to alleviate over-smoothing. Considerable experiments on Cora, Citeseer, Pubmed and Reddit have verified that DropEdge can generally and consistently promote the performance of current popular GCNs, such as GCN, ResGCN, JKNet, IncepGCN, and GraphSAGE. It is expected that our research will open up a new venue on a more in-depth exploration of deep GCNs for broader potential applications.</p><p>A APPENDIX: PROOF OF THEOREM 1</p><p>To prove Theorem 1, we need to borrow the following definitions and corollaries from <ref type="bibr" target="#b24">Oono &amp; Suzuki (2019)</ref>. First, we denote the maximum singular value of W l by s l and set s := sup l∈N+ s l . We assume that W l of all layers are initialized so that s ≤ 1. Second, we denote the distance that induced as the Frobenius norm from X to M by d M (X) := inf Y ∈M ||X − Y || F . Then, we recall Corollary 3 and Proposition 1 in <ref type="bibr" target="#b24">Oono &amp; Suzuki (2019)</ref> as Corollary 1 below.</p><p>Corollary 1. Let λ 1 ≤ · · · ≤ λ N be the eigenvalues ofÂ, sorted in ascending order. Suppose the multiplicity of the largest eigenvalue λ N is M (≤ N ), i.e., λ N −M &lt; λ N −M +1 = · · · = λ N and the second largest eigenvalue is defined as</p><formula xml:id="formula_8">λ := N −M max n=1 |λ n | &lt; |λ N |.<label>(4)</label></formula><p>Let E to be the eigenspace associated with λ N −M +1 , · · · , λ N . Then we have λ &lt; λ N = 1, and</p><formula xml:id="formula_9">d M (H (l) ) ≤ s l λd M (H (l−1) ),<label>(5)</label></formula><p>where M := {EC|C ∈ R M ×C }. Besides, s l λ &lt; 1, implying that the output of the l-th layer of GCN on G exponentially approaches M.</p><p>We also need to adopt some concepts from <ref type="bibr" target="#b21">Lovász et al. (1993)</ref> in proving Theorem 1. Consider the graph G as an electrical network, where each edge represents an unit resistance. Then the effective resistance, R st from node s to node t is defined as the total resistance between node s and t. According to Corollary 3.3 and Theorem 4.1 (i) in <ref type="bibr" target="#b21">Lovász et al. (1993)</ref>, we can build the connection between λ and R st for each connected component via commute time as the following inequality.</p><formula xml:id="formula_10">λ ≥ 1 − 1 R st ( 1 d s + 1 d t ).<label>(6)</label></formula><p>Prior to proving Theorem 1, we first derive the lemma below. Lemma 2. The -smoothing happens whenever the layer number satisfies</p><formula xml:id="formula_11">l ≥l = log d M (X) log(sλ) ,<label>(7)</label></formula><p>where · computes the ceil of the input. It meansl ≥ l * .</p><p>Proof. We start our proof from Inequality 5, leading to</p><formula xml:id="formula_12">d M (H (l) ) ≤ s l λd M (H (l−1) ) ≤ ( l i=1 s i )λ l d M (X) ≤ s l λ l d M (X)</formula><p>When it reaches -smoothing, the following inequality should be satisfied as</p><formula xml:id="formula_13">d M (H (l) ) ≤ s l λ l d M (X) &lt; , ⇒ l log sλ &lt; log d M (X) .<label>(8)</label></formula><p>Since 0 ≤ sλ &lt; 1, then log sλ &lt; 0. Therefore, the Inequality 8 becomes</p><formula xml:id="formula_14">l &gt; log d M (X) log sλ .<label>(9)</label></formula><p>Clearly, we havel ≥ l * since l * is defined as the minimal layer that satisfies -smoothing. The proof is concluded. Now, we prove Theorem 1.</p><p>Proof. Our proof relies basically on the connection between λ and R st in Equation <ref type="formula" target="#formula_10">(6)</ref>. We recall Corollary 4.3 in <ref type="bibr" target="#b21">Lovász et al. (1993)</ref> that removing any edge from G can only increase any R st , then according to <ref type="formula" target="#formula_10">(6)</ref>, the lower bound of λ only increases if the removing edge is not connected to either s or t (i.e. the degree d s and d t keep unchanged). Since there must exist a node pair satisfying R st = ∞ after sufficient edges (except self-loops) are removed from one connected component of G, we have the infinite case λ = 1 given in Equation <ref type="formula" target="#formula_10">(6)</ref> that both 1/d s and 1/d t are consistently bounded by a finite number,i.e. 1. It implies λ does increase before it reaches λ = 1. Asl is positively related to λ (see the right side of Equation <ref type="formula" target="#formula_11">(7)</ref> where log(sλ) &lt;0), we have proved the first part of Theorem 1, i.e.,l(M, ) ≤l(M , ) after removing sufficient edges.</p><p>When there happens R st = ∞, the connected component is disconnected into two parts, which leads to the increment of the dimension of M by 1 and proves the second part of Theorem 1. i.e., the information loss is decreased:</p><formula xml:id="formula_15">N − dim(M) &gt; N − dim(M ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B APPENDIX: MORE DETAILS IN EXPERIMENTS B.1 DATASETS STATISTICS</head><p>Datasets The statistics of all datasets are summarized in <ref type="table" target="#tab_4">Table 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 MODELS AND BACKBONES</head><p>Backbones Other than the multi-layer GCN, we replace the CNN layer with graph convolution layer to implement three popular backbones recasted from image classification. They are residual network (ResGCN) <ref type="bibr" target="#b9">(He et al., 2016;</ref>, inception network (IncepGCN) <ref type="bibr" target="#b28">(Szegedy et al., 2016)</ref> and dense network (JKNet) <ref type="bibr" target="#b12">(Huang et al., 2017;</ref><ref type="bibr">Xu et al., 2018b)</ref>. <ref type="figure" target="#fig_6">Figure 5</ref> shows the detailed architectures of four backbones. Furthermore, we employ one input GCL and one output GCL on these four backbones. Therefore, the layers in ResGCN, JKNet and InceptGCN are at least 3 layers. All backbones are implemented in Pytorch <ref type="bibr" target="#b25">(Paszke et al., 2017)</ref>. For GraphSAGE, we utilize the Pytorch version implemented by DGL <ref type="bibr" target="#b30">(Wang et al., 2019)</ref>. Self Feature Modeling We also implement a variant of graph convolution layer with self feature modeling <ref type="bibr" target="#b5">(Fout et al., 2017)</ref>:</p><formula xml:id="formula_16">H (l+1) = σ Â H (l) W (l) + H (l) W (l) self ,<label>(10)</label></formula><p>where W</p><formula xml:id="formula_17">(l) self ∈ R C l ×C l−1 .</formula><p>Hyper-parameter Optimization We adopt the Adam optimizer for model training. To ensure the re-productivity of the results, the seeds of the random numbers of all experiments are set to the same. We fix the number of training epoch to 400 for all datasets. All experiments are conducted on a NVIDIA Tesla P40 GPU with 24GB memory.</p><p>Given a model with n ∈ {2, 4, 8, 16, 32, 64} layers, the hidden dimension is 128 and we conduct a random search strategy to optimize the other hyper-parameter for each backbone in § 5.1. The decryptions of hyper-parameters are summarized in <ref type="table" target="#tab_7">Table 4</ref>. <ref type="table" target="#tab_8">Table 5</ref> depicts the types of the normalized adjacency matrix that are selectable in the "normalization" hyper-parameter. For GraphSAGE, the aggregation type like GCN, MAX, MEAN, or LSTM is a hyper-parameter as well.</p><p>For each model, we try 200 different hyper-parameter combinations via random search and select the best test accuracy as the result.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>and further explained in Wu et al. (2019); Xu et al. (2018a); Klicpera et al. (2019), graph convolutions essentially push representations of adjacent nodes mixed with each Performance of Multi-layer GCNs on Cora. We implement 4-layer GCN w and w/o DropEdge (in orange), 8-layer GCN w and w/o DropEdge (in blue) 2 . GCN-4 gets stuck in the over-fitting issue attaining low training error but high validation error; the training of GCN-8 fails to converge satisfactorily due to over-smoothing. By applying DropEdge, both GCN-4 and GCN-8 work well for both training and validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>drop by independently computing Equation 2 for each l-th layer. Different layer could have different adjacency matrixÂ(l)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>Figure 2 Table 2: Accuracy (%) comparisons with SOTAs. The number in parenthesis denotes the network depth for the models with DropEdge. Transductive Inductive Cora Citeseer Pubmed Reddit GCN 86.64 79.34 90.22 95.68 FastGCN 85.00 77.60 88.00 93.70 ASGCN 87.44 79.66 90.60 96.27 GraphSAGE 82.20 71.40 87.10 94.32 GCN+DropEdge 87.60(4) 79.20(4) 91.30(4) 96.71(4) ResGCN+DropEdge 87.00(4) 79.40(16) 91.10(32) 96.48(16) JKNet+DropEdge 88.00(16) 80.20(8) 91.60(64) 97.02(8) IncepGCN+DropEdge 88.20(8) 80.50(8) 91.60(4) 96.87(8) GraphSAGE+DropEdge 88.10(4) 80.00(2) 91.70(8) 96.54(4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Analysis on over-smoothing. Smaller distance means more serious over-smoothing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>The illustration of four backbones. GCL indicates graph convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>The validation loss on different backbones w and w/o DropEdge. GCN-n denotes PlainGCN of depth n; similar denotation follows for other backbones. B.4 THE ABLATION STUDY ON CITESEER Figure 7a shows the ablation study of Dropout vs. DropEdge and Figure 4b depicts a comparison between the proposed DropEdge and the layer-wise DropEdge on Citeseer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>I) −1/2 (A + I)(D + I) −1/2 Augmented Normalized Adjacency with Self-loop BingGeNormAdj I + (D + I) −1/2 (A + I)(D + I) −1/2 Augmented Random Walk AugRWalk (D + I) −1 (A + I)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Testing accuracy (%) comparisons on different backbones w and w/o DropEdge.</figDesc><table><row><cell></cell><cell></cell><cell>2 layers</cell><cell></cell><cell>8 layers</cell><cell></cell><cell>32 layers</cell><cell></cell></row><row><cell>Dataset</cell><cell>Backbone</cell><cell cols="6">Orignal DropEdge Orignal DropEdge Orignal DropEdge</cell></row><row><cell></cell><cell>GCN</cell><cell>86.10</cell><cell>86.50</cell><cell>78.70</cell><cell>85.80</cell><cell>71.60</cell><cell>74.60</cell></row><row><cell></cell><cell>ResGCN</cell><cell>-</cell><cell>-</cell><cell>85.40</cell><cell>86.90</cell><cell>85.10</cell><cell>86.80</cell></row><row><cell>Cora</cell><cell>JKNet IncepGCN</cell><cell>--</cell><cell>--</cell><cell>86.70 86.70</cell><cell>87.80 88.20</cell><cell>87.10 87.40</cell><cell>87.60 87.70</cell></row><row><cell></cell><cell>GraphSAGE</cell><cell>87.80</cell><cell>88.10</cell><cell>84.30</cell><cell>87.10</cell><cell>31.90</cell><cell>32.20</cell></row><row><cell></cell><cell>GCN</cell><cell>75.90</cell><cell>78.70</cell><cell>74.60</cell><cell>77.20</cell><cell>59.20</cell><cell>61.40</cell></row><row><cell></cell><cell>ResGCN</cell><cell>-</cell><cell>-</cell><cell>77.80</cell><cell>78.80</cell><cell>74.40</cell><cell>77.90</cell></row><row><cell>Citeseer</cell><cell>JKNet IncepGCN</cell><cell>--</cell><cell>--</cell><cell>79.20 79.60</cell><cell>80.20 80.50</cell><cell>71.70 72.60</cell><cell>80.00 80.30</cell></row><row><cell></cell><cell>GraphSAGE</cell><cell>78.40</cell><cell>80.00</cell><cell>74.10</cell><cell>77.10</cell><cell>37.00</cell><cell>53.60</cell></row><row><cell></cell><cell>GCN</cell><cell>90.20</cell><cell>91.20</cell><cell>90.10</cell><cell>90.90</cell><cell>84.60</cell><cell>86.20</cell></row><row><cell></cell><cell>ResGCN</cell><cell>-</cell><cell>-</cell><cell>89.60</cell><cell>90.50</cell><cell>90.20</cell><cell>91.10</cell></row><row><cell>Pubmed</cell><cell>JKNet IncepGCN</cell><cell>--</cell><cell>--</cell><cell>90.60 90.20</cell><cell>91.20 91.50</cell><cell>89.20 OOM</cell><cell>91.30 90.50</cell></row><row><cell></cell><cell>GraphSAGE</cell><cell>90.10</cell><cell>90.70</cell><cell>90.20</cell><cell>91.70</cell><cell>41.30</cell><cell>47.90</cell></row><row><cell></cell><cell>GCN</cell><cell>96.11</cell><cell>96.13</cell><cell>96.17</cell><cell>96.48</cell><cell>45.55</cell><cell>50.51</cell></row><row><cell></cell><cell>ResGCN</cell><cell>-</cell><cell>-</cell><cell>96.37</cell><cell>96.46</cell><cell>93.93</cell><cell>94.27</cell></row><row><cell>Reddit</cell><cell>JKNet IncepGCN</cell><cell>--</cell><cell>--</cell><cell>96.82 96.43</cell><cell>97.02 96.87</cell><cell>OOM OOM</cell><cell>OOM OOM</cell></row><row><cell></cell><cell>GraphSAGE</cell><cell>96.22</cell><cell>96.28</cell><cell>96.38</cell><cell>96.42</cell><cell>96.43</cell><cell>96.47</cell></row></table><note>testing accuracy for all cases. The improvement is more clearly depicted in Figure 2a, where we have computed the average absolute improvement over all backbones by DropEdge on each dataset under different numbers of layers. On Citeseer, for example, DropEdge yields further improvement for deeper architecture; it gains 0.9% average improvement for the model with 2 layers while achieving a remarkable 13.5% increase for the model with 64 layers. In addition, the validation losses of all 4-layer models on Cora are shown in Figure 2b. The curves along the training epoch are dramatically pulled down after applying DropEdge, which also explains the effect of DropEdge on alleviating over-fitting. Another valuable observation in Table 1 is that the 32-layer IncepGCN without DropEdge incurs the Out-Of-Memory (OOM) issue while the model with DropEdge survives, showing the advantage of DropEdge to save memory consuming by making the adjacency matrix sparse. Comparison with SOTAs We select the best performance for each backbone with DropEdge, and contrast them with existing State of the Arts (SOTA), including GCN, FastGCN, AS-GCN and GraphSAGE in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) comparisons with SOTAs. The number in parenthesis denotes the network depth for the models with DropEdge.</figDesc><table><row><cell></cell><cell></cell><cell>Transductive</cell><cell></cell><cell>Inductive</cell></row><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Reddit</cell></row><row><cell>GCN</cell><cell>86.64</cell><cell>79.34</cell><cell>90.22</cell><cell>95.68</cell></row><row><cell>FastGCN</cell><cell>85.00</cell><cell>77.60</cell><cell>88.00</cell><cell>93.70</cell></row><row><cell>ASGCN</cell><cell>87.44</cell><cell>79.66</cell><cell>90.60</cell><cell>96.27</cell></row><row><cell>GraphSAGE</cell><cell>82.20</cell><cell>71.40</cell><cell>87.10</cell><cell>94.32</cell></row><row><cell>GCN+DropEdge</cell><cell>87.60(4)</cell><cell>79.20(4)</cell><cell>91.30(4)</cell><cell>96.71(4)</cell></row><row><cell>ResGCN+DropEdge</cell><cell cols="4">87.00(4) 79.40(16) 91.10(32) 96.48(16)</cell></row><row><cell>JKNet+DropEdge</cell><cell>88.00(16)</cell><cell cols="2">80.20(8) 91.60(64)</cell><cell>97.02(8)</cell></row><row><cell>IncepGCN+DropEdge</cell><cell>88.20(8)</cell><cell>80.50(8)</cell><cell>91.60(4)</cell><cell>96.87(8)</cell></row><row><cell>GraphSAGE+DropEdge</cell><cell>88.10(4)</cell><cell>80.00(2)</cell><cell>91.70(8)</cell><cell>96.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>ON COMPATIBILITY WITH DROPOUT § 4.3 has discussed the difference between DropEdge and Dropout. Hence, we conduct an ablation study on GCN-4, and the validation losses are demonstrated inFigure 4a. It reads that while both Dropout and DropEdge are able to facilitate the training of GCN, the improvement by DropEdge is more significant, and if we adopt them concurrently, the loss is decreased further, indicating the compatibility of DropEdge with Dropout.</figDesc><table><row><cell>).</cell></row><row><cell>5.2.2</cell></row></table><note>5.2.3 ON LAYER-WISE DROPEDGE § 4.1 has descried the Layer-Wise (LW) extension of DropEdge.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Dataset Statistics</figDesc><table><row><cell cols="2">Datasets Nodes</cell><cell>Edges</cell><cell cols="4">Classes Features Traing/Validation/Testing Type</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>1,208/500/1,000</cell><cell>Transductive</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>1,812/500/1,000</cell><cell>Transductive</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>3</cell><cell>500</cell><cell>18,217/500/1,000</cell><cell>Transductive</cell></row><row><cell>Reddit</cell><cell cols="2">232,965 11,606,919</cell><cell>41</cell><cell>602</cell><cell>152,410/23,699/55,334</cell><cell>Inductive</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>summaries the hyper-parameters of each backbone with the best accuracy on different datasets and their best accuracy are reported inTable 2. B.3 THE VALIDATION LOSS ON DIFFERENT BACKBONES W AND W/O DROPEDGE.Figure 6 depicts the additional results of validation loss on different backbones w and w/o DropEdge.</figDesc><table><row><cell>Validation Loss</cell><cell>0.6 0.7 0.8 0.9 1.0 1.1 1.2</cell><cell>Cora ResGCN-6 GCN-6 InceptGCN-6 JKNet-6 ResGCN-6+DropEdge GCN-6+DropEdge InceptGCN-6+DropEdge JKNet-6+DropEdge</cell><cell>Validation Loss</cell><cell>1.2 1.4 1.6 1.8 2.0</cell><cell>Citeseer ResGCN-4 GCN-4 InceptGCN-4 JKNet-4 ResGCN-4+DropEdge GCN-4+DropEdge InceptGCN-4+DropEdge JKNet-4+DropEdge</cell><cell>Validation Loss</cell><cell>1.2 1.4 1.6 1.8 2.0</cell><cell>Citeseer ResGCN-6 GCN-6 InceptGCN-6 JKNet-6 ResGCN-6+DropEdge GCN-6+DropEdge InceptGCN-6+DropEdge JKNet-6+DropEdge</cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell>0 25 50 75 100 125 150 175 200 Epoch</cell><cell></cell><cell>0.8</cell><cell>0 25 50 75 100 125 150 175 200 Epoch</cell><cell></cell><cell>0.8</cell><cell>Epoch 0 25 50 75 100 125 150 175 200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>: Hyper-parameter Description</cell></row><row><cell cols="2">Hyper-parameter Description</cell></row><row><cell>lr</cell><cell>learning rate</cell></row><row><cell>weight-decay</cell><cell>L2 regulation weight</cell></row><row><cell cols="2">sampling-percent edge preserving percent (1 − p)</cell></row><row><cell>dropout</cell><cell>dropout rate</cell></row><row><cell>normalization</cell><cell>the propagation models (Kipf &amp; Welling, 2017)</cell></row><row><cell>withloop</cell><cell>using self feature modeling</cell></row><row><cell>withbn</cell><cell>using batch normalization</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>The normalization / propagation models</figDesc><table><row><cell>Description</cell><cell>Notation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>The hyper-parameters of best accuracy for each backbone on all datasets.</figDesc><table><row><cell>Dataset</cell><cell>Backbone</cell><cell cols="2">nlayers Acc.</cell><cell>Hyper-parameters</cell></row><row><cell></cell><cell>GCN</cell><cell>4</cell><cell cols="2">0.876 lr:0.010, weight-decay:5e-3, sampling-percent:0.7, dropout:0.8, nor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>malization:FirstOrderGCN</cell></row><row><cell></cell><cell>ResGCN</cell><cell>4</cell><cell cols="2">0.87 lr:0.001, weight-decay:1e-5, sampling-percent:0.1, dropout:0.5, nor-</cell></row><row><cell>Cora</cell><cell>JKNet</cell><cell>16</cell><cell cols="2">malization:FirstOrderGCN 0.88 lr:0.008, weight-decay:5e-4, sampling-percent:0.2, dropout:0.8, nor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>malization:AugNormAdj</cell></row><row><cell></cell><cell>IncepGCN</cell><cell>8</cell><cell cols="2">0.882 lr:0.010, weight-decay:1e-3, sampling-percent:0.05, dropout:0.5,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>normalization:AugNormAdj</cell></row><row><cell></cell><cell>GraphSage</cell><cell>4</cell><cell cols="2">0.881 lr:0.010, weight-decay:5e-4, sampling-percent:0.4, dropout:0.5, ag-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>gregator:mean</cell></row><row><cell></cell><cell>GCN</cell><cell>4</cell><cell cols="2">0.792 lr:0.009, weight-decay:1e-3, sampling-percent:0.05, dropout:0.8,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>normalization:BingGeNormAdj, withloop, withbn</cell></row><row><cell></cell><cell>ResGCN</cell><cell>16</cell><cell cols="2">0.794 lr:0.001, weight-decay:5e-3, sampling-percent:0.5, dropout:0.3, nor-</cell></row><row><cell>Citeseer</cell><cell>JKNet</cell><cell>8</cell><cell cols="2">malization:BingGeNormAdj, withloop 0.802 lr:0.004, weight-decay:5e-5, sampling-percent:0.6, dropout:0.3, nor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>malization:AugNormAdj, withloop</cell></row><row><cell></cell><cell>IncepGCN</cell><cell>8</cell><cell cols="2">0.805 lr:0.002, weight-decay:5e-3, sampling-percent:0.2, dropout:0.5, nor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>malization:BingGeNormAdj, withloop</cell></row><row><cell></cell><cell>GraphSage</cell><cell>2</cell><cell cols="2">0.8 lr:0.001, weight-decay:1e-4, sampling-percent:0.1, dropout:0.5, ag-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>gregator:mean</cell></row><row><cell></cell><cell>GCN</cell><cell>4</cell><cell cols="2">0.913 lr:0.010, weight-decay:1e-3, sampling-percent:0.3, dropout:0.5, nor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>malization:BingGeNormAdj, withloop, withbn</cell></row><row><cell></cell><cell>ResGCN</cell><cell>32</cell><cell cols="2">0.911 lr:0.003, weight-decay:5e-5, sampling-percent:0.7, dropout:0.8, nor-</cell></row><row><cell>Pubmed</cell><cell>JKNet</cell><cell>64</cell><cell cols="2">malization:AugNormAdj, withloop, withbn 0.916 lr:0.005, weight-decay:1e-4, sampling-percent:0.5, dropout:0.8, nor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>malization:AugNormAdj, withloop,withbn</cell></row><row><cell></cell><cell>IncepGCN</cell><cell>4</cell><cell cols="2">0.916 lr:0.002, weight-decay:1e-5, sampling-percent:0.5, dropout:0.8, nor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>malization:BingGeNormAdj, withloop, withbn</cell></row><row><cell></cell><cell>GraphSage</cell><cell>8</cell><cell cols="2">0.917 lr:0.007, weight-decay:1e-4, sampling-percent:0.8, dropout:0.3, ag-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>gregator:mean</cell></row><row><cell></cell><cell>GCN</cell><cell cols="3">4 0.9671 lr:0.005, weight-decay:1e-4, sampling-percent:0.6, dropout:0.5, nor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>malization:AugRWalk, withloop</cell></row><row><cell></cell><cell>ResGCN</cell><cell cols="3">16 0.9648 lr:0.009, weight-decay:1e-5, sampling-percent:0.2, dropout:0.5, nor-</cell></row><row><cell>Reddit</cell><cell>JKNet</cell><cell cols="3">malization:BingGeNormAdj, withbn 8 0.9702 lr:0.010, weight-decay:5e-5, sampling-percent:0.6, dropout:0.5, nor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>malization:BingGeNormAdj, withloop,withbn</cell></row><row><cell></cell><cell>IncepGCN</cell><cell cols="3">8 0.9687 lr:0.008, weight-decay:1e-4, sampling-percent:0.4, dropout:0.5, nor-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>malization:FirstOrderGCN, withbn</cell></row><row><cell></cell><cell>GraphSAGE</cell><cell cols="3">4 0.9654 lr:0.005, weight-decay:5e-5, sampling-percent:0.2, dropout:0.3, ag-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>gregator:mean</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Accuracy (%) comparisons on different backbones with and without DropEdge</figDesc><table><row><cell>2 4 8 16 32 64</cell><cell>Orignal DropEdge Orignal DropEdge Orignal DropEdge Orignal DropEdge Orignal DropEdge Orignal DropEdge</cell><cell>86.10 86.50 85.50 87.60 78.70 85.80 82.10 84.30 71.60 74.60 52.00 53.20</cell><cell>--86.00 87.00 85.40 86.90 85.30 86.90 85.10 86.80 79.80 84.80</cell><cell>--86.90 87.70 86.70 87.80 86.20 88.00 87.10 87.60 86.30 87.90</cell><cell>--85.60 87.90 86.70 88.20 87.10 87.70 87.40 87.70 85.30 88.20</cell><cell>87.80 88.10 87.10 88.10 84.30 87.10 84.10 84.50 31.90 32.20 31.90 31.90</cell><cell>75.90 78.70 76.70 79.20 74.60 77.20 65.20 76.80 59.20 61.40 44.60 45.60</cell><cell>--78.90 78.80 77.80 78.80 78.20 79.40 74.40 77.90 21.20 75.30</cell><cell>--79.10 80.20 79.20 80.20 78.80 80.10 71.70 80.00 76.70 80.00</cell><cell>--79.50 79.90 79.60 80.50 78.50 80.20 72.60 80.30 79.00 79.90</cell><cell>78.40 80.00 77.30 79.20 74.10 77.10 72.90 74.50 37.00 53.60 16.90 25.10</cell><cell>90.20 91.20 88.70 91.30 90.10 90.90 88.10 90.30 84.60 86.20 79.70 79.00</cell><cell>--90.70 90.70 89.60 90.50 89.60 91.00 90.20 91.10 87.90 90.20</cell><cell>--90.50 91.30 90.60 91.20 89.90 91.50 89.20 91.30 90.60 91.60</cell><cell>--89.90 91.60 90.20 91.50 90.80 91.30 OOM 90.50 OOM 90.00</cell><cell>90.10 90.70 89.40 91.20 90.20 91.70 83.50 87.80 41.30 47.90 40.70 62.30</cell><cell>96.11 96.13 96.62 96.71 96.17 96.48 67.11 90.54 45.55 50.51 --</cell><cell>--96.13 96.33 96.37 96.46 96.34 96.48 93.93 94.27 --</cell><cell>--96.54 96.75 96.82 97.02 OOM 96.78 OOM OOM --</cell><cell>--96.48 96.77 96.43 96.87 OOM OOM OOM OOM --</cell><cell>96.22 96.28 96.45 96.54 96.38 96.42 96.15 96.18 96.43 96.47 --</cell></row><row><cell></cell><cell>Backbone</cell><cell>GCN</cell><cell>ResGCN</cell><cell>JKNet</cell><cell>IncepGCN</cell><cell>GraphSAGE</cell><cell>GCN</cell><cell>ResGCN</cell><cell>JKNet</cell><cell>IncepGCN</cell><cell>GraphSAGE</cell><cell>GCN</cell><cell>ResGCN</cell><cell>JKNet</cell><cell>IncepGCN</cell><cell>GraphSAGE</cell><cell>GCN</cell><cell>ResGCN</cell><cell>JKNet</cell><cell>IncepGCN</cell><cell>GraphSAGE</cell></row><row><cell></cell><cell>Dataset</cell><cell></cell><cell></cell><cell cols="2">Cora</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Citeseer</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pubmed</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Reddit</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">To check the efficacy of DropEdge more clearly, here we have removed bias in all GCN layers, while for the experiments in § 5, the bias are kept.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The definition of dM(·) is provided in the supplementary material. 4 All detailed definitions and proofs are provided in the appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The formulation is given in the appendix.6  For Reddit, the maximum depth is 32 considering the memory bottleneck.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Node classification in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smriti</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social network data analytics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="115" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sparsification-a technique for speeding up dynamic graph algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eppstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zvi</forename><surname>Galil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><forename type="middle">F</forename><surname>Italiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Nissenzweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="669" to="696" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basir</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visualizing social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of social structure</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Random walks on graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">László</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorics, Paul erdos is eighty</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On asymptotic behaviors of graph cnns from dynamical systems perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.01315" />
	</analytic>
	<monogr>
		<title level="m">Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
