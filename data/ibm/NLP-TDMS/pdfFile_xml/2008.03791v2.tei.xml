<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JOURNAL OF IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 1 Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE, Caifeng</roleName><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Shan</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">JOURNAL OF IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 1 Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Action Recognition</term>
					<term>Skeleton</term>
					<term>Activation Map</term>
					<term>Graph Convolutional Network</term>
					<term>Occlusion</term>
					<term>Jittering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current methods for skeleton-based human action recognition usually work with complete skeletons. However, in real scenarios, it is inevitable to capture incomplete or noisy skeletons, which could significantly deteriorate the performance of current methods when some informative joints are occluded or disturbed. To improve the robustness of action recognition models, a multi-stream graph convolutional network (GCN) is proposed to explore sufficient discriminative features spreading over all skeleton joints, so that the distributed redundant representation reduces the sensitivity of the action models to nonstandard skeletons. Concretely, the backbone GCN is extended by a series of ordered streams which is responsible for learning discriminative features from the joints less activated by preceding streams. Here, the activation degrees of skeleton joints of each GCN stream are measured by the class activation maps (CAM), and only the information from the unactivated joints will be passed to the next stream, by which rich features over all active joints are obtained. Thus, the proposed method is termed richly activated GCN (RA-GCN). Compared to the state-of-the-art (SOTA) methods, the RA-GCN achieves comparable performance on the standard NTU RGB+D 60 and 120 datasets. More crucially, on the synthetic occlusion and jittering datasets, the performance deterioration due to the occluded and disturbed joints can be significantly alleviated by utilizing the proposed RA-GCN. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human action recognition has achieved promising progress in recent computer vision researches and plays an increasingly crucial role in many potential applications, such as video surveillance, human-computer interaction, video retrieval and so on <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. The main purpose of action recognition is to classify human actions from motion data which can be Caifeng Shan is with the College of Electrical Engineering and Automation, Shandong University of Science and Technology (SDUST), Qingdao 266590, China, and also with the Artificial Intelligence Research, Chinese Academy of Sciences (CAS-AIR), Beijing 100190, China. (Email: caifeng.shan@gmail.com) <ref type="bibr" target="#b0">1</ref> The codes and pretrained models of the preposed RA-GCN are available at http://github.com/yfsong0709/RA-GCNv2. captured as RGB videos <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, depth maps <ref type="bibr" target="#b5">[6]</ref>, infrared images <ref type="bibr" target="#b6">[7]</ref> and 3D skeleton sequences <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>Traditional action recognition is dominated by RGB videobased methods. These methods usually consider RGB videos as temporal sequences of image frames, and use sequential models such as recurrent neural network (RNN) to exploit temporal information from all the feature maps extracted by convolution neural networks (CNN) for each frame <ref type="bibr" target="#b3">[4]</ref>. On the other hand, many researchers utilize 3D CNN to derive useful information directly from the videos <ref type="bibr" target="#b4">[5]</ref>, which has obtained a comparable performance with the former methods. Nevertheless, both of these two categories extract spatial structure information from 2D RGB frames, while the spatial configurations of actors are absolutely presented in 3D space. Thus, these RGB-based methods will lose some crucial information due to the intrinsic weakness. Moreover, the RGB videos often contain complex background and illumination variations, which leads to significant performance degradation in practice.</p><p>Compared to RGB videos, skeleton-based human action recognition methods reflect a growing prospect, due to its superiority in background adaptability, robustness to light variations and less computational cost. Skeleton data is composed of 2D/3D coordinates of multiple skeleton joints in motion sequences, which can be either collected by multimodal sensors such as Kinect or directly estimated from 2D images by pose estimation methods <ref type="bibr" target="#b10">[11]</ref>. Current methods usually deal with skeleton data in two ways. One is to connect these joints into a global vector, then model temporal information by using RNNbased methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The other way is to treat or expand temporal sequences of joints into 2D images, then utilize CNN-based methods to recognize actions <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. However, it is difficult to utilize the spatial structure information among skeleton joints effectively with both the RNN and CNN methods, though many researchers propose additional constraints or dedicated network structures to model the spatial structure of skeleton joints. Recently, graph neural networks (GNN), which can explicitly incorporate graphical structure information into the learning of neural network, have made great progress in many fields <ref type="bibr" target="#b17">[18]</ref>. Yan et al. <ref type="bibr" target="#b9">[10]</ref> firstly propose a spatial temporal graph convolutional network (ST-GCN) to capture the patterns embedded in the spatial configuration as well as the temporal dynamics in skeleton sequences, which achieves a significant improvement in action recognition.</p><p>However, current skeleton-based action recognition models still lack of robustness to the noisy or incomplete skeleton data captured in real scenarios. For example, the subjects may be self-occluded by pose variations or occluded by other contextual objects. <ref type="figure">Fig.1</ref> displays some examples of occluded actions due to other persons in the scenes. The noisy data will deteriorate the performance of the models heavily. Therefore, how to enhance the robustness of skeleton-based action recognition models is still an urgent and challenging problem.</p><p>To against various degradations, ensemble learning has proved to be an effective strategy <ref type="bibr" target="#b18">[19]</ref>, which induces multiple classifiers based on the same or distinct predictive classifiers so that the integration of these individual classifiers could enhance the robustness of recognition models. Inspired by the success of ensemble learning, in this paper, we propose a GCN-based multi-stream model, which aims to learn rich discriminative features from skeleton motion sequences, and thereby improve the robustness of the proposed model. The purpose of each stream in our approach is to explore a group of discriminative features over the skeleton joints unactivated by previous streams. The learnt redundant but complementary features over all skeleton joints provide an effective strategy to handle the noisy or incomplete skeleton data. For example, when we recognize the action throwing, the most discriminative joints are located on the two arms at the moment of object leaving the hands in the process of throwing, while the body swaying as well as the contextual sub-actions of hands can also be used to infer the action of throwing.</p><p>In order to distinguish the most informative joints for each stream, we introduce a successful technique named class activation maps (CAM) <ref type="bibr" target="#b19">[20]</ref> into our model, which initially aims to visualize the activation heatmap in the final CNN layer responsible for visual classification. The activation maps obtained by previous GCN streams are accumulated as a mask matrix to inform the new stream about which joints have been already activated. Then, the new stream will be forced to explore new discriminative features from unactivated joints. Therefore, the proposed method is called richly activated GCN (RA-GCN), where the richly discovered and complementary features will improve the robustness of the model to nonstandard skeletons. To the best of our knowledge, this is the first time to employ the CAM technique to enhance the model robustness by expanding the activated skeleton joints, which alleviates the problems of occlusion and jittering in skeletonbased action recognition.</p><p>To validate the advantages of the proposed methods, besides the traditional skeleton action datasets, the NTU RGB+D 60 <ref type="bibr" target="#b20">[21]</ref> &amp; 120 <ref type="bibr" target="#b21">[22]</ref> datasets, we also build four synthetic occlusion datasets, where the joints in the NTU 60 and 120 datasets are partially occluded over both spatial and temporal dimensions, and two synthetic jittering datasets, where some randomly selected joints are disturbed by Gaussian noises. More details of these datasets can be found in Section IV-A. Our experiments on these new datasets demonstrate that the proposed RA-GCN significantly alleviates the performance deterioration in the case of incomplete or noisy skeleton data.</p><p>This work is an extension of an earlier and preliminary version presented in <ref type="bibr" target="#b22">[23]</ref>. Compared to our previous work, the modifications and contributions of this paper are summarized as follows:</p><p>• In previous work, the activation masks are obtained by a Softmax function in the activation modules, which activates only a few joints for each stream. In contrast, we propose to use a normalization activation function to expand the activated scope, thus the corresponding stream will obtain a better and more interpretable activation map. • Compared to previous work, we extend the original loss function with a number of additional cross-entropy regularizations on each individual network stream, so that the features can be learnt more effectively. • The synthetic datasets are extended by more degradation operators, where the occlusion degradation is further divided into four types, including Frame, Part, Block and Random, and two synthetic jittering datasets are newly constructed. More experiments are performed to validate the effectiveness and robustness of the proposed approach in different degradation conditions. The remainder of this paper is organized as follows: Section II describes recent studies related to our work. Section III introduces several crucial components of the proposed RA-GCN. Extensive experimental results on standard and non-standard datasets are reproted in Section IV, and the conclusion of this paper is given in Section V.</p><p>II. RELATED WORK a) Skeleton-based models: To find a more effective representation of the dynamics of human actions, Johansson <ref type="bibr" target="#b23">[24]</ref> utilizes 3D skeleton sequences for action recognition, making an obvious decrease of computational cost as well as a good performance boost. Recently, with the rapid development of deep learning techniques, skeleton-based action recognition methods have attracted increasing attentions. Researchers have proposed various models to improve the performance of action recognition, which can be divided into three major categories. The first category builds the models with convolutional networks. For example, Li et al. <ref type="bibr" target="#b15">[16]</ref> propose a CNN-based co-occurrence feature learning framework, which gradually aggregates various levels of contextual information. Kim et al. <ref type="bibr" target="#b14">[15]</ref> build a temporal convolutional network to explicitly learn readily interpretable spatio-temporal representations for 3D human action recognition.   Other layers contain the same input and output channels. Both of the fifth and the eighth layers use a temporal stride 2 to reduce the sequence length. GAP and Concat are global average pooling and concatenation operation, S is the number of streams, and ⊗ and denote element-wise multiplication and subtraction, respectively. This model totally contains three steps. The input sequence x is firstly transformed into x by the data preprocessing module. Secondly, x will be sent to each stream after being filtered by a corresponding mask matrix. Finally, the output of each stream will be concatenated to obtain the final class of x.</p><p>Besides, for the second category, researchers concatenate all joints in one frame into a single vector, then use sequential models such as long short-term memory (LSTM) to explore the temporal dynamics. Du et al. <ref type="bibr" target="#b24">[25]</ref> design a hierarchical bidirectional RNN to capture rich dependencies between different human body parts. The study in <ref type="bibr" target="#b8">[9]</ref> employs a view adaptive LSTM, which enables the network itself adaptive to the most suitable observation viewpoints. Additionally, Song et al. <ref type="bibr" target="#b25">[26]</ref> firstly introduce attention modules into skeletonbased action recognition.</p><p>Both CNN-based and RNN-based methods are still limited to extract the spatial structure information among skeleton joints, where the joints of different body parts are connected as a skeleton graph. Instead, in the third category, graphbased methods can be naturally utilized to deal with the skeleton graph, which successfully captures the most informative features for various actions. Si et al. <ref type="bibr" target="#b12">[13]</ref> use GNN to model the relationships among five body parts. Yan et al. <ref type="bibr" target="#b9">[10]</ref> initially introduce GCN into skeleton-based action recognition, and produce a baseline named ST-GCN for future research. Based on the ST-GCN, many studies achieve continuous improvements on skeleton-based action recognition <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>.</p><p>b) Occlusion in human action recognition: Occlusion is a prominent challenge in human action recognition. If the skeleton joints are partially occluded, the approaches mentioned above will face a considerable decline of performance. To handle this problem, Wang et al. <ref type="bibr" target="#b29">[30]</ref> try to infer occlusion maps from a global SVM classifier, and Weinland et al. <ref type="bibr" target="#b31">[31]</ref> propose a local partitioning and hierarchical classification of the 3D Histogram of Oriented Gradients (HOG) descriptor for providing the robustness to both occlusions and view point changes. However, there are few studies addressing the problem of noisy or incomplete data in skeleton joints. In this paper, we propose an approach to exploring rich features over all joints, so as to alleviate the effects of data degradation. c) Salient Regions Exploration: Similar with our motivation, some previous studies have proposed to explore the salient regions or erase them to exploit complementary information for referring expression grounding <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref> or weakly supervised detection tasks <ref type="bibr" target="#b34">[34]</ref>. The study <ref type="bibr" target="#b32">[32]</ref> proposes a simple yet effective network to prohibit attentions from spreading to unexpected background regions, in order to promote the quality of object attention. Liu et al. <ref type="bibr" target="#b33">[33]</ref> design a novel attention-guided erasing approach to aligning various types of information crossing visual and textual modalities. Moreover, Li et al. <ref type="bibr" target="#b34">[34]</ref> provide a framework to dynamically erase the focused area according to the on-line attention maps. However, previous salient regions exploration methods mainly concentrate on object detection or localization tasks in images, while in this work, we exploit the complementary attended skeleton joints for alleviating the occlusion or jittering problems, which is still not considered in previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL ARCHITECTURE</head><p>In order to enhance the robustness of action recognition models, we propose the RA-GCN to explore sufficient discriminative features from all skeleton joints. The proposed RA-GCN constructs a multi-stream network, where each stream is responsible for extracting features from a group of activated joints. In this way, when the joints activated by the first stream are occluded, the model can also discover discriminative information from the other streams. The overview of RA-GCN is presented in <ref type="figure" target="#fig_2">Fig.2</ref>. Suppose that V is the number of joints in one skeleton and T is the number of frames in one sequence, the size of input data x is C in ×T ×V , where C in = 3 denotes the 3D coordinates of each joint. Note that different skeletons in a multi-agent action are treated as different samples.</p><p>The proposed method consists of three main steps. Firstly, in the preprocessing module, for extracting more informative features, the input data x is transformed into x , which is subsequently sent to all the GCN streams. Secondly, for each stream, the skeleton joints in x will be filtered by the element-wise product with a mask matrix, which records the currently unactivated joints. These joints are distinguished by accumulating the activated maps calculated by the activation modules of preceding streams. Here, the mask matrix of each stream is initialized to an all-one matrix with the same shape as x . After the masking operation, the input data of each stream only contains the joints unactivated by the preceding streams, and passes through a baseline network to obtain a feature representation based on the incomplete skeleton joints. Finally, the features of all streams are concatenated in the output module, and a fully connected layer with Softmax activation function is used to obtain the final class of input x. These three steps will be discussed in details in next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Preprocessing</head><p>Usually, some actions such as taking off the shoes and wearing the shoes are extremely similar with only spatial features. To tackle this problem, conventional RGB-based methods introduce a sophisticated technique named Optical Flow <ref type="bibr" target="#b35">[35]</ref> into their models, for depicting the motion features exactly. Besides, the work in <ref type="bibr" target="#b27">[28]</ref> argues that relative coordinates of joints are usually more informative than absolute coordinates. Inspired by this, geometric features such as relative coordinates and motion features such as temporal displacements are applied in our models to increase the discriminative information for action recognition. Therefore, the input data need to be preprocessed before distributing it to all the GCN streams.</p><p>The relative coordinates can be recognized as the differencė x r between all joints and the center joint (middle spine) in each frame, which can be seen in <ref type="figure" target="#fig_3">Fig.3</ref>(a). In this way, all joints are transformed to the relative coordinates, which is more robust to the changing position. Besides, for extracting more informative motion features, we computeẋ t by x t+1 − x t , where x t means the feature map of the t th frame, which is shown in <ref type="figure" target="#fig_3">Fig.3(b)</ref>. Then, x will be obtained by concatenating x,ẋ r andẋ t .</p><p>B. Richly Activated GCN 1) Baseline Model: The baseline of our method is the ST-GCN <ref type="bibr" target="#b9">[10]</ref>, which is composed of ten graph convolutional layers. Yan et al. <ref type="bibr" target="#b9">[10]</ref> formulate spatial graph convolutional operation as follows:</p><formula xml:id="formula_0">f out (v ti ) = vtj ∈B(vti) 1 Z ti (v tj ) f in (v tj ) · w(l ti (v tj )),<label>(1)</label></formula><p>where f in and f out are the input and output feature maps respectively, v ti denotes the i th joint at the t th frame, which can also be regarded as the root joint in this procedure, B(v ti ) is the neighbor set of v ti , the normalizing term Z ti is added to balance the contributions of different neighbors, w(·) is a weight function implemented by several 1×1 Conv layers and l ti (·) means a label function. There are three label functions in <ref type="bibr" target="#b9">[10]</ref>, but we only choose the distance-based label function in our method, which defines l ti (v tj ) = d(v ti , v tj ).</p><p>That means the neighbor set B(v ti ) is divided into several subsets, according to the graph distance between v tj and the root joint v ti . For example, if the joint v tj directly connects with the root joint v ti , then d(v ti , v tj ) = 1. The joints with the same distance will form a subset and share a learnable weight function w(·). To implement the spatial graph convolution with the adjacency matrix A, Eq.1 is transformed into:</p><formula xml:id="formula_1">f out = D d=0 W d f in (Λ − 1 2 d A d Λ − 1 2 d ⊗ M d ),<label>(2)</label></formula><p>where D is the predefined maximum distance, A d denotes the adjacency matrix for distance d,</p><formula xml:id="formula_2">Λ ii d = k A ik d + α is the normalized diagonal matrix, A ik</formula><p>d denotes the element of the i th row and k th column of A d and α is set to a small value, e.g., 10 −4 , to avoid the empty rows in Λ d . For each adjacency matrix, we accompany it with a learnable matrix M d , which expresses the importance of all edges in one skeleton.</p><p>After the spatial graph convolutional block, a 1 × L convolutional layer is used to extract temporal information of the feature map f out , where L is the temporal window size. Both spatial and temporal convolutional blocks are followed with a BatchNorm layer and a ReLU layer, and the total ST-GCN layer contains a residual connection. Besides, a dropout layer with the drop probability of 0.5 is added between every spatial convolutional block and temporal convolutional block to avoid overfitting. The structure of one ST-GCN layer is shown in <ref type="figure" target="#fig_4">Fig.4</ref>. 2) Activation Module: The activation module in the RA-GCN is constructed to distinguish the activated joints of each stream, then guide the learning process of the new stream by accumulating the activated maps of preceding streams. This procedure can be mainly implemented by extending the CAM technique <ref type="bibr" target="#b19">[20]</ref> to the field of GCN. The original CAM technique is to localize class-specific image regions in CNNs, and score c is defined as the scores of all pixels for class c, where the score of each pixel is</p><formula xml:id="formula_3">score c (x, y) = k w c k f k (x, y).<label>(3)</label></formula><p>In this formulation, f k (·, ·) is the feature map before the global average pooling operation, and w c k is the weight of the k th channel for class c. In this paper, we replace the coordinate (x, y) in a feature map with the frame number t and the joint number i in a skeleton sequence, by which we are able to locate the activated joints. Here, the class c is selected as the ground truth. We use score s c to denote the score map of all joints for the true class and the s th stream. To determine which joints are activated by the corresponding stream, a predefined threshold δ is utilized, and the activation map of map s c is calculated by</p><formula xml:id="formula_4">map s c = ε( score s c max(score s c ) − δ)<label>(4)</label></formula><p>where ε(·) is the Heaviside step function and max(·) denotes the maximum function. Then, the mask matrix of the s th stream is represented as</p><formula xml:id="formula_5">mask s = ( s−1 i=1 mask i ) ⊗ (1 − map s−1 c ),<label>(5)</label></formula><p>where denotes the element-wise product of all mask matrices before the s th stream. Specially, the mask matrix of the first stream is an all-one matrix. Finally, the input of the s th stream will be obtained by</p><formula xml:id="formula_6">x s = x ⊗ mask s ,<label>(6)</label></formula><p>where x is the skeleton representation after preprocessing. Eq.5 and Eq.6 illustrate that the input of the s th stream only consists of the joints which are not activated by previous streams. Thus, the RA-GCN will explore discriminative features from all joints sufficiently.  3) Loss Function: In our previous model <ref type="bibr" target="#b22">[23]</ref>, the loss function only supervises the total network, which is extended with a number of additional losses for each individual network streams in this paper, so that the feature can be learnt more effectively. Suppose thatŷ s ∈ R C is the output of the s th stream,ŷ ∈ R C is the output of the whole model and y ∈ R C is the ground truth, where C is the number of classes. Then the loss function of the proposed RA-GCN is</p><formula xml:id="formula_7">L = −ylogŷ − S s=1 ylogŷ s<label>(7)</label></formula><p>where S is the number of streams.</p><p>IV. EXPERIMENTAL RESULTS A. Dataset a) NTU RGB+D 60 <ref type="bibr" target="#b20">[21]</ref>: This dataset is a large-scale indoor action recognition dataset, which contains 56880 video samples collected by Microsoft Kinect v2, and consists of 60 action classes performed by 40 subjects. Each video is composed of 25 joints and no more than two skeletons in one frame. The maximum frame number T is set to 300 for simplicity. The authors of this dataset recommend two benchmarks: (1) cross-subject (CS) contains 40320 samples for training and 16560 samples for evaluation, by splitting 40 subjects into two groups; (2) cross-view (CV) uses cameras 2 and 3 (37920 samples) for training and camera 1 (18960 samples) for evaluation. We follow this convention and report the top-1 recognition rate on both two benchmarks. In addition, according to <ref type="bibr" target="#b11">[12]</ref>, there are 302 wrong samples that need to be ignored during training and evaluation.</p><p>b) NTU RGB+D 120 <ref type="bibr" target="#b21">[22]</ref>: This dataset is currently the largest indoor action recognition dataset, which is an extended version of NTU 60 dataset. It contains 114480 videos and consists of 120 classes. Similarly, two benchmarks are suggested: (1) cross-subject (CSub) contains 630226 samples for training and 50922 samples for evaluation; (2)cross-setup (CSet) contains 54471 videos for training and 59477 videos for evaluation, which are separated based on the distance and height of their collectors. Note that there are 532 bad samples in this dataset which should be ignored in all experiments.</p><p>c) Occlusion dataset: To validate the robustness of our method to incomplete skeletons, we construct a synthetic occlusion dataset based on the CS benchmark of NTU 60 dataset and the CSet benchmark of NTU 120 dataset, where some joints are selected to be occluded (set to zero) over both spatial and temporal dimensions. Note that this operation is executed before data preprocessing, and all the joints related to the occluded joints (with zero energy) are ignored in data preprocessing phase. For example, if the frame x t is occluded, then the temporal displacementsẋ t = x t+1 − x t andẋ t−1 = x t − x t−1 are both set to zero. This synthetic dataset consists of four cases, which are frame occlusions, part occlusions, block occlusions and random occlusions, respectively. Part occlusion and block occlusion are both used to simulate the real scenarios that occluded by contextual objects. Frame occlusion is designed for the loss of key frames, while random occlusion is for data missing in signal transmission. Some examples on the four types of occlusions are illustrated in <ref type="figure" target="#fig_6">Fig.5</ref>. Note that all the models in occlusion experiments are trained with standard skeletons, and then tested with incomplete skeletons. d) Jittering dataset: Skeleton jittering is a common factor that has a big impact on the recognition performance. To claim the robustness of the proposed method to jittering skeletons, we propose a synthetic jittering dataset based on the CS benchmark of NTU 60 dataset and the CSet benchmark of NTU 120 dataset, where the Gaussian noise N (µ, σ 2 ) is added to some randomly selected joints to simulate the jittering joints. This jittering operation is also executed before data preprocessing module, while the data preprocessing module has no difference with that in standard setting. In this paper, two types of Gaussian noise are used, which are N (0, 0.1 2 ) and N (0, 0.05 2 ). In the bottom line of <ref type="figure" target="#fig_6">Fig.5</ref>, an example of jittering skeletons is displayed, and the red joints denote the noisy joints. Similar with the occlusion dataset, all the models of this dataset are trained with standard skeletons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>In our experiments, some hyper-parameters need to be modified. The initial learning rate is set to 0.1 and divided by 10 every 20 epochs, while the maximum number of   iterations is set to 60. The models are learnt by using the stochastic gradient descent (SGD) algorithm with a momentum 0.9 and a weight decay 10 −4 . In order to avoid overfitting, the probability of the dropout layer between the spatial and temporal blocks is selected as 0.5. The first four ST-GCN layers have 64 channels for output, while the number will be 128 and 256 for the middle three layers and the last three layers. Moreover, at the fifth and eighth layers, the temporal convolutional blocks contain a temporal stride 2, for reducing the computational cost. As to the maximum graph distance D, the temporal window size L and the mask threshold δ, we will discuss about their effects in Section IV-C. Before training a multi-stream RA-GCN, we need to pretrain a one-stream RA-GCN with preprocessed skeleton data to get the baseline model, so as to ensure that the first stream of RA-GCN is able to capture the most informative joints. Accordingly, the following streams are forced to seek for other discriminative joints. Additionally, the mask matrix of each stream is initialized to an all-one matrix. Finally, we finetune the RA-GCN model with the setting mentioned above. All the experiments are performed on two TITAN X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameters Setting</head><p>In order to train a baseline model, we firstly need to determine the value of two hyper-parameters introduced in Section III-B1, i.e., the D for the maximum distance and the L for the temporal window size. These two hyper-parameters have a great impact on our model, because they control the receptive field of the GCN blocks. To find the optimal values, we evaluate many groups of the two hyper-parameters (D ∈ {1, 2, 3} and L ∈ {3, 5, 7, 9, 11}) on the NTU 60 dataset, and some representative experimental results are given in the first part of Tab.I. It is observed that the baseline model achieves the best accuracy when D = 2 and L = 5 on the CS benchmark. As to the CV benchmark, D and L are optimally set to 3 and 9, respectively. Note that it is not always better to choose bigger D and L, since a bigger receptive field will lead to the over-smoothing problem, and eventually harm the model performance. The experimental results also demonstrate this point. From these experiments, an optimal baseline model is obtained, which will be utilized to construct the multi-stream RA-GCN.</p><p>For another hyper-parameter δ mentioned in Section III-B2, we select its value on a two-stream RA-GCN, with the hyperparameters D = 2 and L = 5. The δ decides which joints are activated, and hereby controls the number of activated joints for each stream. As seen in the bottom part of Tab.I, the model obtains the best accuracy when δ is set to 0.3 on the CS benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>The proposed method consists of several fundamental components, e.g., the data preprocessing module, the activation module and so on. In this section, we will analyze the significance of each component. All these experiments are performed by a two-stream RA-GCN with D = 2, L = 5 and δ = 0.3 on the CS benchmark, and the results are presented in the top line of Tab.II. As we remove the activation module, the accuracy of our model will drop by 1.2%. And the pretrained procedure is also important, without which the performance will have a 1.5% decline. In addition, if the Heaviside step function ε(·) and the threshold δ are replaced by a Softmax function as shown in our previous model <ref type="bibr" target="#b22">[23]</ref>, then the accuracy will drop to 85.8%. According to these experimental results, the necessity of each component in our model is validated for boosting the performance of action recognition.</p><p>Furthermore, as seen in the middle line, the data preprocessing module brings a huge improvement, without which the performance is significantly deteriorated. Concretely, the temporal displacements of raw skeletons obtain the best performance, but which are still worse than the whole feature concatenated by the original coordinates, relative coordinates, and temporal displacements. That means all components of the data preprocessing module are beneficial to our model.</p><p>The bottom line of Tab.II shows the results of the RA-GCN models with different numbers of streams. We will find that when the stream number is more than 3, the accuracy growth will be moderate, which is analyzed in Section IV-H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experimental Results on Standard Dataset</head><p>We compare the performance of RA-GCN against several previous SOTA methods on the NTU RGB+D 60 &amp; 120 datasets. The hyper-parameters are chosen as the optimal value given in Tab.I. Tab.III and Tab.IV display the experimental results of one-stream (1s, baseline), two-stream (2s), threestream (3s) RA-GCN and the other SOTAs methods, e.g., AGC-LSTM <ref type="bibr" target="#b26">[27]</ref> and so on.  a) NTU RGB+D 60: Since Yan et al. <ref type="bibr" target="#b9">[10]</ref> introduce GCN into skeleton action recognition, there have been many graph-based methods proposed recently, including our methods. Compared to them, our method is only 1.9% less than AGC-LSTM <ref type="bibr" target="#b26">[27]</ref> on the CS benchmark and 1.4% on the CV benchmark, while the two accuracy differences are 1.9% and 1.4% for PL-GCN <ref type="bibr" target="#b41">[41]</ref> or 2.1% and 2.1% for NAS-GCN <ref type="bibr" target="#b42">[42]</ref>. Although the newly public methods, e.g., PL-GCN and NAS-GCN, have better recognition accuracies than ours, their complexities in model size or training procedures are much higher than the proposed RA-GCN. Compared to ST-GCN <ref type="bibr" target="#b9">[10]</ref>, which is the first graph-based model for skeleton action recognition, our method outperforms by 5.8% and 5.3%, respectively. With respect to the SOTA methods which is not based on GCN, e.g., VA-LSTM <ref type="bibr" target="#b8">[9]</ref> and HCN <ref type="bibr" target="#b15">[16]</ref>, our method achieves a more significant superiority. The 3s RA-GCN outperforms HCN by 0.8% and 2.5%, and exceeds VA-LSTM nearly 8%. b) NTU RGB+D 120: On the currently largest indoor action recognition dataset, NTU RGB+D 120, our approach achieves 81.1% for the CSub benchmark and 82.7% for the CSet benchmark. Compared to other GCN-based methods, the proposed method boosts the performance significantly. Concretely, 3s RA-GCN outperforms GVFE+DH-TCN [50] by 2.8% and 2.9% on the two benchmarks, while the non-graph models such as TSRJI <ref type="bibr" target="#b49">[49]</ref> are far behind our model. Futhermore, for more convincing, three popular models, i.e., ST-GCN <ref type="bibr" target="#b9">[10]</ref>, SR-TSL <ref type="bibr" target="#b12">[13]</ref> and 2s-AGCN <ref type="bibr" target="#b28">[29]</ref>, are implemented by ourselves, according to their released codes. Compared with them, our model only falls behind 2s-AGCN by 1.4% and 1.5% on the two benchmarks of NTU 120 dataset.</p><p>c) Model complexity: To compare the model complexity and computational cost, we calculate the number of parameters in terms of released codes, or ask the authors for the complexities of some models. All the results shown in Tab.III and Tab.IV, from which our model contains similar amount of parameters with other SOTA methods. For some LSTMbased methods, such as SR-TSL <ref type="bibr" target="#b12">[13]</ref>, our model only contains nearly 1/3 parameters, due to the efficiency of the GCN technique. Totally, there are only 6.21 million parameters in the 3s RA-GCN, and the inference speed of this model is about 18.7 sequences per second per GPU. Note that there are usually more than 50 frames in an action sequence, thus the training and testing speeds are obviously sufficient for realtime processing.</p><p>In general, the proposed RA-GCN can achieve comparable performance with the SOTA methods, because the main purpose of RA-GCN is to discover sufficient redundant representation, while most actions can be recognized by only a few informative joints. However, when these informative joints are occluded or disturbed, the performance of traditional methods will deteriorate significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experimental Results on Occlusion Datasets</head><p>In this section, we will analyze the experimental results on the synthetic occlusion datasets based on the CS benchmark of NTU 60 dataset and the CSet benchmark of NTU 120 dataset. There are four types of occlusion given as follows:</p><p>a) Frame occlusions: This type of occlusion is constructed to simulate temporal occlusion. We randomly occlude a subsequence in first 100 frames, because the length of most samples is less than 100. The length of the subsequence is set to <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b50">50</ref>, respectively, and the experimental results are shown in Tab.V. It is observed that the proposed RA-GCN achieves a significant superiority to ST-GCN <ref type="bibr" target="#b9">[10]</ref>, SR-TSL <ref type="bibr" target="#b12">[13]</ref> and 2s-AGCN <ref type="bibr" target="#b28">[29]</ref>. Besides, the difference between 3s RA-GCN and the baseline model shows a rising trend with the increasing number of occluded frames. As to the comparison of different numbers of streams, 3s RA-GCN is a little better than the others. b) Part occlusions: The part occlusion aims at imitating the cases that some key parts of a person are occluded. The occluded parts 1, 2, 3, 4, 5 denote left arm, right arm, two hands, Compared to the baseline model, 3s RA-GCN obtains large advantages when evaluating without parts 3 and 5. Moreover, 2s RA-GCN has a similar performance with the 3s model. c) Block occlusions: Usually, the pedestrian would be occluded by contextual objects. For simulating this real scenario, we design the block occlusion experiments, which occlude the joints behind a predefined horizontal line. The height of the horizontal line is set to five ranges, and the fourth line of <ref type="figure" target="#fig_6">Fig.5</ref> shows an example of range 3. Tab.VII presents these experimental results. Similar with the above occlusion experimental results, our RA-GCN obtains the best accuracies over most of experiments compared to other models. d) Random occlusions: During the process of signal transmission, the transmitted data are prone to be lost. Thus, the purpose of random occlusion experiments is to imitate this situation. The occluded probability for every joint is set to 0.2, 0.3, 0.4, 0.5 and 0.6, respectively. The experimental  <ref type="bibr" target="#b9">[10]</ref>, SR-TSL <ref type="bibr" target="#b12">[13]</ref> and 2s-AGCN <ref type="bibr" target="#b28">[29]</ref> have a rapid performance degradation. This phenomenon is caused mainly because these conventional graph-based models require an integrated graph structure, however, which is entirely destroyed by random occlusions. e) Failure cases: Specially, we also find that when some important joints, such as right arm, are occluded, some action categories, e.g., handshaking, cannot be inferred by other joints. The proposed method will fail in such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Experimental Results on Jittering Datasets</head><p>To discuss the impact of jittering skeletons, two jittering datasets are designed by adding different Gaussian noises, and the corresponding experimental results are shown in Tab.IX and Tab.X. The jittering probability for every joint is set to 0.02, 0.04, 0.06, 0.08 and 0.10, respectively. It is clearly observed from these tables that, the 3s RA-GCN outperforms other models by a huge gap on NTU 60-based jittering dataset. Moreover, with the increase of jittering probability, the gap between 3s RA-GCN and the baseline is synchronously increasing. Therefore, it is proven that our method is robustness to the current skeletons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Results Analysis</head><p>In this section, we will analyze why our model is more robust to noisy or incomplete input data. <ref type="figure" target="#fig_8">Fig.6</ref> shows an example of the activated joints. In this figure, the top line shows the activated joints of the baseline model, which is also considered as a 1s RA-GCN. The activated joints in the middle line correspond to a 2s RA-GCN, while the bottom line presents the results of a 3s RA-GCN. For a clearer display, we select five contextual frames from the sequence to represent the action throwing, instead of a whole sequence.</p><p>From this figure, it is observed that the proposed RA-GCN successfully expands the activation map of the baseline model. The baseline model only captures the most discriminative The red points denote the activated joints, while the blue points denote the unactivated joints.</p><p>joints, e.g., the joints in two arms. In contrast, our model not only concentrates on the joints in two arms, but also activates some other discriminative joints, which play an auxiliary role in this action, such as the slightly swaying body and stepping legs. Moreover, with the increase of streams, more activated joints can be discovered accordingly. Therefore, the multistream RA-GCN still has a modest recognition performance when a few joints are occluded or disturbed. This will lead to a more robust capability of our model to data degradation than other models. In addition, it is worth to notice that more streams in the RA-GCN will not always obtain a more accurate model, because the number of discriminative joints in an action category is often limited. In this work, three streams are sufficient to discover these joints, and the experimental results in Section IV-D also demonstrates this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, to reduce the impact of noisy or incomplete skeletons in action recognition, we have proposed a novel model named RA-GCN for discovering rich features over all skeleton joints, which achieves a much better performance than the baseline model and improves the robustness of the model. With extensive experiments on the NTU RGB+D 60 &amp; 120 datasets, we verify the effectiveness and robustness of our model. For evaluating the model's performance on nonstandard skeletons, we construct various synthetic datasets composed of four types of occlusion and two types of jittering. On these synthetic datasets, the proposed RA-GCN outperforms the other SOTA methods, as well as showing a significant improvement than the baseline model. In the future, we will consider to add the attention module within our model, in order to make each stream focus more on informative joints.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work is sponsored by National Key R&amp;D Program of China (No.2016YFB1001002), National Natural Science Foundation of China (No.61525306, No.61633021, No.61721004), Shandong Provincial Key Research and Development Program (Major Scientific and Technological Innovation Project) (No.2019JZZY010119) and CAS-AIR. Yi-Fan Song, Zhang Zhang, and Liang Wang are with the School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS), Beijing 100190, China, and also with the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing 100190, China. (Email: yifan.song@cripac.ia.ac.cn, zzhang@nlpr.ia.ac.cn, wangliang@nlpr.ia.ac.cn)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The pipeline of RA-GCN with three stream networks. Each stream contains one ST-GCN. The two numbers under the ST-GCN layers are the numbers of input and output channels, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>(Best viewed in color.) The illustration of data preprocessing. (a) is relative joint coordinates. (b) is temporal displacements. The orange solid lines mean the physical connections between neighboring joints (bones) and the blue dash lines denote the differences between two joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The structure of an ST-GCN layer, where BN means the BatchNorm layer, and ⊕ denotes the element-wise summation operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>(Best viewed in color.) The demonstration of the degraded datasets generated by adding various types of noise and occlusion to the standard NTU 60 dataset. The first line is the original frames. The middle lines are the frame occlusion without frames 3 and 4, the part occlusion without left arm, the block occlusion without the joints under the red dash line and the random occlusion with an occluded probability p = 0.2, respectively. In addition, the bottom line is an example of the jittering skeletons, where the red joints denote the noisy joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>I COMPARISON OF DIFFERENT PARAMETER SETTINGS OF RA-GCN ON THETWO BENCHMARKS NTU 60 (and 2s denote the number of streams</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>(Best viewed in color.) An example of activated joints of all streams for the baseline model, the RA-GCN with two streams and three streams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2008.03791v2 [cs.CV] 26 Nov 2020</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sample 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sample 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sample 3</cell></row><row><cell>Frame 1</cell><cell>Frame 2</cell><cell>Frame 3</cell><cell>Frame 4</cell><cell>Frame 5</cell></row><row><cell cols="5">Fig. 1. Some examples of occluded actions. The person behind is partially</cell></row><row><cell cols="2">occluded by the person in front.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II COMPARISON</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">OF DIFFERENT MODEL SETTINGS ON THE CS BENCHMARK</cell></row><row><cell></cell><cell>OF NTU 60 (%)</cell><cell></cell></row><row><cell>Model</cell><cell>Setting</cell><cell>accuracy</cell></row><row><cell></cell><cell>w/o activation module</cell><cell>85.5</cell></row><row><cell>2s RA-GCN</cell><cell>w/o pretrained</cell><cell>85.2</cell></row><row><cell></cell><cell>w/ activation function in [23]</cell><cell>85.8</cell></row><row><cell></cell><cell>only raw skeleton</cell><cell>76.8</cell></row><row><cell>2s RA-GCN</cell><cell>only relative coordinates</cell><cell>73.4</cell></row><row><cell></cell><cell>only temporal displacements</cell><cell>83.1</cell></row><row><cell>1s RA-GCN</cell><cell>-</cell><cell>85.8</cell></row><row><cell>2s RA-GCN</cell><cell>-</cell><cell>86.7</cell></row><row><cell>3s RA-GCN</cell><cell>-</cell><cell>87.3</cell></row><row><cell>4s RA-GCN</cell><cell>-</cell><cell>87.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF THE SOTA METHODS ON THE TWO BENCHMARKS OF NTU 60 IN ACCURACY (%) AND MODEL SIZE (MILLION)</figDesc><table><row><cell>Model</cell><cell>Year</cell><cell>Param.</cell><cell>Data</cell><cell>CS</cell><cell>CV</cell></row><row><cell>DSSCA-SSLM [36]</cell><cell>2017</cell><cell>-</cell><cell>Both</cell><cell>74.9</cell><cell>-</cell></row><row><cell>2D-3D-Softargma [37]</cell><cell>2018</cell><cell>-</cell><cell>RGB</cell><cell>85.5</cell><cell>-</cell></row><row><cell>Glimpse Clouds [38]</cell><cell>2018</cell><cell>-</cell><cell>RGB</cell><cell cols="2">86.6 93.2</cell></row><row><cell>H-BRNN [25]</cell><cell>2015</cell><cell>-</cell><cell cols="3">Skeleton 59.1 64.0</cell></row><row><cell>VA-LSTM [9]</cell><cell>2017</cell><cell>-</cell><cell cols="3">Skeleton 79.4 87.6</cell></row><row><cell cols="2">CNN+Motion+Trans [14] 2017</cell><cell>-</cell><cell cols="3">Skeleton 83.2 89.3</cell></row><row><cell>3scale ResNet152 [39]</cell><cell>2017</cell><cell>-</cell><cell cols="3">Skeleton 85.0 92.3</cell></row><row><cell>HCN [16]</cell><cell>2018</cell><cell>-</cell><cell cols="3">Skeleton 86.5 91.1</cell></row><row><cell>ST-GCN [10]</cell><cell>2018</cell><cell>3.10</cell><cell cols="3">Skeleton 81.5 88.3</cell></row><row><cell>DPRL+GCNN [17]</cell><cell>2018</cell><cell>-</cell><cell cols="3">Skeleton 83.5 89.8</cell></row><row><cell>SR-TSL [13]</cell><cell>2018</cell><cell>19.07</cell><cell cols="3">Skeleton 84.8 92.4</cell></row><row><cell>PB-GCN [28]</cell><cell>2018</cell><cell>-</cell><cell cols="3">Skeleton 87.5 93.2</cell></row><row><cell>AS-GCN [40]</cell><cell>2019</cell><cell>6.99</cell><cell cols="3">Skeleton 86.8 94.2</cell></row><row><cell>2s-AGCN [29]</cell><cell>2019</cell><cell>6.94</cell><cell cols="3">Skeleton 88.5 95.1</cell></row><row><cell>AGC-LSTM [27]</cell><cell>2019</cell><cell>22.89  †</cell><cell cols="3">Skeleton 89.2 95.0</cell></row><row><cell>PL-GCN [41]</cell><cell>2020</cell><cell>20.70  †</cell><cell cols="3">Skeleton 89.2 95.0</cell></row><row><cell>NAS-GCN [42]</cell><cell>2020</cell><cell>6.57  †</cell><cell cols="3">Skeleton 89.4 95.7</cell></row><row><cell>preliminary version [23]</cell><cell>2019</cell><cell>6.21</cell><cell cols="3">Skeleton 85.9 93.5</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell>2020</cell><cell>2.03</cell><cell cols="3">Skeleton 85.8 93.1</cell></row><row><cell>2s RA-GCN</cell><cell>2020</cell><cell>4.13</cell><cell cols="3">Skeleton 86.7 93.4</cell></row><row><cell>3s RA-GCN</cell><cell>2020</cell><cell>6.21</cell><cell cols="3">Skeleton 87.3 93.6</cell></row><row><cell cols="3">: These results are implemented by ourselves.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>† : These results are provided by their authors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF THE SOTA METHODS ON THE TWO BENCHMARKS OF NTU 120 IN ACCURACY (%) AND MODEL SIZE (MILLION)</figDesc><table><row><cell>Model</cell><cell>Year</cell><cell>Param.</cell><cell>Data</cell><cell>CSub</cell><cell>CSet</cell></row><row><cell>Pose Evolution Map [43]</cell><cell>2017</cell><cell>-</cell><cell>Both</cell><cell>64.6</cell><cell>66.9</cell></row><row><cell>Soft RNN [44]</cell><cell>2018</cell><cell>-</cell><cell>RGB</cell><cell>36.3</cell><cell>44.9</cell></row><row><cell>PA-LSTM [21]</cell><cell>2016</cell><cell>-</cell><cell>Skeleton</cell><cell>25.5</cell><cell>26.3</cell></row><row><cell>ST-LSTM [8]</cell><cell>2016</cell><cell>-</cell><cell>Skeleton</cell><cell>55.7</cell><cell>57.9</cell></row><row><cell>2s attention LSTM [45]</cell><cell>2017</cell><cell>-</cell><cell>Skeleton</cell><cell>61.2</cell><cell>63.3</cell></row><row><cell cols="2">Skeleton Visualization [46] 2018</cell><cell>-</cell><cell>Skeleton</cell><cell>60.3</cell><cell>63.2</cell></row><row><cell>FSNet [47]</cell><cell>2018</cell><cell>-</cell><cell>Skeleton</cell><cell>59.9</cell><cell>62.4</cell></row><row><cell>SkeleMotion [48]</cell><cell>2019</cell><cell>-</cell><cell>Skeleton</cell><cell>67.7</cell><cell>66.9</cell></row><row><cell>TSRJI [49]</cell><cell>2019</cell><cell>-</cell><cell>Skeleton</cell><cell>67.9</cell><cell>62.8</cell></row><row><cell>ST-GCN [10]</cell><cell>2018</cell><cell>3.10</cell><cell>Skeleton</cell><cell>70.7</cell><cell>73.2</cell></row><row><cell>SR-TSL [13]</cell><cell>2018</cell><cell>19.07</cell><cell>Skeleton</cell><cell>74.1</cell><cell>79.9</cell></row><row><cell>2s-AGCN [29]</cell><cell>2019</cell><cell>6.94</cell><cell>Skeleton</cell><cell>82.5</cell><cell>84.2</cell></row><row><cell>AS-GCN [40]</cell><cell>2019</cell><cell>6.99</cell><cell>Skeleton</cell><cell>77.7  †</cell><cell>78.9  †</cell></row><row><cell>GVFE+DH-TCN [50]</cell><cell>2019</cell><cell>-</cell><cell>Skeleton</cell><cell>78.3</cell><cell>79.8</cell></row><row><cell>preliminary version [23]</cell><cell>2019</cell><cell>6.25</cell><cell>Skeleton</cell><cell>74.4</cell><cell>79.4</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell>2020</cell><cell>2.07</cell><cell>Skeleton</cell><cell>78.2</cell><cell>80.0</cell></row><row><cell>2s RA-GCN</cell><cell>2020</cell><cell>4.17</cell><cell>Skeleton</cell><cell>81.0</cell><cell>82.5</cell></row><row><cell>3s RA-GCN</cell><cell>2020</cell><cell>6.25</cell><cell>Skeleton</cell><cell>81.1</cell><cell>82.7</cell></row><row><cell cols="3">: These results are implemented by ourselves.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>† : These results are reported in [50].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V EXPERIMENTAL</head><label>V</label><figDesc>RESULTS (%) WITH FRAME OCCLUSION ON THE CS BENCHMARK OF NTU 60 (TOP) AND THE CSET BENCHMARK OF NTU 120 (BOTTOM) : the difference between 3s RA-GCN and the baseline model two legs and torso, respectively. As seen in Tab.VI, there is a huge gap between RA-GCN and other SOTA methods.</figDesc><table><row><cell>Frame</cell><cell></cell><cell cols="4">Number of Occluded Frames</cell><cell></cell></row><row><cell>Occlusion</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell>ST-GCN [10]</cell><cell cols="6">80.7 69.3 57.0 44.5 34.5 24.0</cell></row><row><cell>SR-TSL [13]</cell><cell cols="6">84.8 70.9 62.6 48.8 41.3 28.8</cell></row><row><cell>2s-AGCN [29]</cell><cell>88.5</cell><cell cols="5">74.8 60.8 49.7 38.2 28.0</cell></row><row><cell cols="7">preliminary version [23] 85.9 81.9 75.0 66.3 54.4 40.6</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell cols="6">85.8 81.6 72.9 61.6 47.9 34.0</cell></row><row><cell>2s RA-GCN</cell><cell cols="3">86.7 83.0 76.4</cell><cell cols="3">65.6 53.1 39.5</cell></row><row><cell>3s RA-GCN</cell><cell>87.3</cell><cell cols="3">83.9 76.4 66.3</cell><cell cols="2">53.2 38.5</cell></row><row><cell>difference*</cell><cell>1.5</cell><cell>2.3</cell><cell>3.5</cell><cell>4.7</cell><cell>5.3</cell><cell>4.5</cell></row><row><cell>ST-GCN [10]</cell><cell cols="6">73.2 60.8 48.8 38.2 27.3 17.4</cell></row><row><cell>SR-TSL [13]</cell><cell cols="6">79.9 67.4 58.8 50.4 44.7 37.1</cell></row><row><cell>2s-AGCN [29]</cell><cell>84.2</cell><cell cols="5">73.4 56.8 44.2 33.0 23.4</cell></row><row><cell cols="7">preliminary version [23] 79.4 76.2 69.8 60.4 47.4 33.0</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell cols="6">80.0 76.1 67.9 57.1 44.0 30.3</cell></row><row><cell>2s RA-GCN</cell><cell cols="5">82.5 79.1 72.4 63.3 51.2</cell><cell>36.6</cell></row><row><cell>3s RA-GCN</cell><cell cols="6">82.7 79.6 72.9 63.3 51.2 36.8</cell></row><row><cell>difference*</cell><cell>2.7</cell><cell>3.5</cell><cell>5.0</cell><cell>6.2</cell><cell>7.2</cell><cell>6.5</cell></row><row><cell cols="6">*: the difference between 3s RA-GCN and the baseline model</cell><cell></cell></row><row><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">EXPERIMENTAL RESULTS (%) WITH PART OCCLUSION ON THE CS</cell></row><row><cell cols="7">BENCHMARK OF NTU 60 (TOP) AND THE CSET BENCHMARK OF NTU</cell></row><row><cell></cell><cell cols="2">120 (BOTTOM)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Part</cell><cell></cell><cell cols="3">Occluded Part</cell><cell></cell><cell></cell></row><row><cell>Occlusion</cell><cell>None</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>ST-GCN [10]</cell><cell>80.7</cell><cell cols="5">71.4 60.5 62.6 77.4 50.2</cell></row><row><cell>SR-TSL [13]</cell><cell>84.8</cell><cell cols="5">70.6 54.3 48.6 74.3 56.2</cell></row><row><cell>2s-AGCN [29]</cell><cell>88.5</cell><cell cols="2">72.4 55.8</cell><cell>82.1</cell><cell cols="2">74.1 71.9</cell></row><row><cell>preliminary version [23]</cell><cell>85.9</cell><cell cols="5">73.4 60.4 73.5 81.8 70.6</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell>85.8</cell><cell cols="5">69.9 54.0 66.8 82.4 64.9</cell></row><row><cell>2s RA-GCN</cell><cell>86.7</cell><cell cols="2">75.9 62.1</cell><cell>69.2</cell><cell cols="2">83.3 72.8</cell></row><row><cell>3s RA-GCN</cell><cell>87.3</cell><cell cols="5">74.5 59.4 74.2 83.2 72.3</cell></row><row><cell>difference*</cell><cell>1.5</cell><cell>4.6</cell><cell>5.4</cell><cell>7.4</cell><cell>0.8</cell><cell>7.4</cell></row><row><cell>ST-GCN [10]</cell><cell>73.2</cell><cell cols="5">59.7 47.3 52.5 68.5 48.5</cell></row><row><cell>SR-TSL [13]</cell><cell>79.9</cell><cell cols="5">59.4 50.3 41.2 64.8 55.0</cell></row><row><cell>2s-AGCN [29]</cell><cell>84.2</cell><cell cols="2">62.8 46.6</cell><cell>77.8</cell><cell cols="2">67.0 60.7</cell></row><row><cell>preliminary version [23]</cell><cell>79.4</cell><cell cols="5">65.6 51.2 57.3 75.3 64.9</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell>80.0</cell><cell cols="5">64.0 49.7 50.0 74.7 60.2</cell></row><row><cell>2s RA-GCN</cell><cell>82.5</cell><cell cols="5">67.4 54.1 56.0 77.6 67.7</cell></row><row><cell>3s RA-GCN</cell><cell>82.7</cell><cell cols="2">68.5 54.9</cell><cell>57.5</cell><cell cols="2">79.0 69.9</cell></row><row><cell>difference*</cell><cell>2.7</cell><cell>4.5</cell><cell>5.2</cell><cell>7.5</cell><cell>4.3</cell><cell>9.7</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII EXPERIMENTAL</head><label>VII</label><figDesc>RESULTS (%) WITH BLOCK OCCLUSION ON THE CS BENCHMARK OF NTU 60 (TOP) AND THE CSET BENCHMARK OF NTU 120 (BOTTOM) : the difference between 3s RA-GCN and the baseline model results are displayed in Tab.VIII, from which we can see that RA-GCN extremely alleviates the performance deterioration, while ST-GCN</figDesc><table><row><cell>Block</cell><cell cols="6">Height Range of the Horizontal Line</cell></row><row><cell>Occlusion</cell><cell>None</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>ST-GCN [10]</cell><cell>80.7</cell><cell cols="5">76.4 70.1 60.7 48.4 36.1</cell></row><row><cell>SR-TSL [13]</cell><cell>84.8</cell><cell cols="5">74.9 69.3 61.1 49.4 36.9</cell></row><row><cell>2s-AGCN [29]</cell><cell>88.5</cell><cell cols="5">79.2 73.6 64.6 53.4 40.2</cell></row><row><cell>preliminary version [23]</cell><cell>85.9</cell><cell cols="5">81.6 78.6 72.5 62.3 48.2</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell>85.8</cell><cell cols="5">82.6 78.0 69.5 57.8 43.8</cell></row><row><cell>2s RA-GCN</cell><cell>86.7</cell><cell>84.4</cell><cell cols="4">81.2 74.0 62.9 48.7</cell></row><row><cell>3s RA-GCN</cell><cell>87.3</cell><cell>84.5</cell><cell cols="4">81.0 73.8 62.3 47.6</cell></row><row><cell>difference*</cell><cell>1.5</cell><cell>1.8</cell><cell>3.0</cell><cell>4.3</cell><cell>4.5</cell><cell>3.8</cell></row><row><cell>ST-GCN [10]</cell><cell>73.2</cell><cell cols="5">65.5 56.8 45.0 32.7 22.6</cell></row><row><cell>SR-TSL [13]</cell><cell>79.9</cell><cell cols="5">68.2 60.1 47.7 36.5 29.0</cell></row><row><cell>2s-AGCN [29]</cell><cell>84.2</cell><cell cols="5">71.7 65.7 56.3 44.6 31.1</cell></row><row><cell>preliminary version [23]</cell><cell>79.4</cell><cell cols="5">75.7 72.3 63.8 50.9 36.0</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell>80.0</cell><cell cols="5">74.8 70.0 61.3 48.7 34.7</cell></row><row><cell>2s RA-GCN</cell><cell>82.5</cell><cell cols="5">79.0 75.2 67.8 55.8 40.6</cell></row><row><cell>3s RA-GCN</cell><cell>82.7</cell><cell cols="5">79.7 76.2 69.1 57.3 42.2</cell></row><row><cell>difference*</cell><cell>2.7</cell><cell>4.9</cell><cell>6.2</cell><cell>7.8</cell><cell>8.6</cell><cell>7.5</cell></row><row><cell cols="6">*: the difference between 3s RA-GCN and the baseline model</cell><cell></cell></row><row><cell></cell><cell cols="2">TABLE VIII</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">EXPERIMENTAL RESULTS (%) WITH RANDOM OCCLUSION ON THE CS</cell></row><row><cell cols="7">BENCHMARK OF NTU 60 (TOP) AND THE CSET BENCHMARK OF NTU</cell></row><row><cell></cell><cell cols="3">120 (BOTTOM)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random</cell><cell></cell><cell cols="4">Occluded Probability</cell><cell></cell></row><row><cell>Occlusion</cell><cell>0</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell></row><row><cell>ST-GCN [10]</cell><cell cols="2">80.7 12.4</cell><cell>6.6</cell><cell>6.2</cell><cell>4.0</cell><cell>4.2</cell></row><row><cell>SR-TSL [13]</cell><cell cols="4">84.8 43.0 25.2 12.1</cell><cell>6.0</cell><cell>3.7</cell></row><row><cell>2s-AGCN [29]</cell><cell>88.5</cell><cell cols="3">38.5 22.8 13.4</cell><cell>8.5</cell><cell>6.1</cell></row><row><cell cols="7">preliminary version [23] 85.9 84.1 81.7 77.2 70.0 57.4</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell cols="6">85.8 82.4 77.1 72.3 63.8 49.9</cell></row><row><cell>2s RA-GCN</cell><cell cols="2">86.7 85.2</cell><cell cols="4">83.1 79.4 73.0 60.1</cell></row><row><cell>3s RA-GCN</cell><cell>87.3</cell><cell>85.4</cell><cell cols="4">82.9 78.9 71.9 61.1</cell></row><row><cell>difference*</cell><cell>1.5</cell><cell>3.0</cell><cell>5.8</cell><cell>6.6</cell><cell>8.1</cell><cell>11.2</cell></row><row><cell>ST-GCN [10]</cell><cell>73.2</cell><cell>4.1</cell><cell>2.2</cell><cell>1.9</cell><cell>1.6</cell><cell>1.3</cell></row><row><cell>SR-TSL [13]</cell><cell cols="4">79.9 44.4 27.1 10.5</cell><cell>8.8</cell><cell>5.1</cell></row><row><cell>2s-AGCN [29]</cell><cell>84.2</cell><cell>20.1</cell><cell>9.4</cell><cell>6.3</cell><cell>4.6</cell><cell>3.7</cell></row><row><cell cols="6">preliminary version [23] 79.4 77.0 73.9 70.4 62.6</cell><cell>42.1</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell cols="6">80.0 75.1 68.4 57.4 44.7 27.6</cell></row><row><cell>2s RA-GCN</cell><cell cols="2">82.5 79.7</cell><cell cols="3">76.2 71.0 62.0</cell><cell>48.7</cell></row><row><cell>3s RA-GCN</cell><cell>82.7</cell><cell>79.8</cell><cell cols="4">75.6 68.9 58.1 43.7</cell></row><row><cell>difference*</cell><cell>2.7</cell><cell>4.7</cell><cell>7.2</cell><cell cols="3">11.5 13.4 16.1</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX EXPERIMENTAL</head><label>IX</label><figDesc>RESULTS (%) WITH JITTERING SKELETONS (µ = 0, σ = 0.1) ON THE CS BENCHMARK OF NTU 60 (TOP) AND THE CSET BENCHMARK OF NTU 120 (BOTTOM) : the difference between 3s RA-GCN and the baseline modelTABLE X EXPERIMENTAL RESULTS (%) WITH JITTERING SKELETONS (µ = 0, σ = 0.05) ON THE CS BENCHMARK OF NTU 60 (TOP) AND THE CSET BENCHMARK OF NTU 120 (BOTTOM) 76.4 65.1 50.2 32.8 19.5 SR-TSL [13] 84.8 69.4 55.3 50.1 46.6 39.2 2s-AGCN [29] 88.5 78.9 79.8 76.8 72.6 60.7 preliminary version [23] 85.9 83.8 81.3 75.3 69.2 61.4 baseline (1s RA-GCN) 85.8 82.4 77.1 72.3 63.8 49.9 2s RA-GCN 86.7 83.8 77.3 71.6 61.6 58.5 3s RA-GCN 87.3 87.0 84.5 81.1 72.9 61.4 : the difference between 3s RA-GCN and the baseline model</figDesc><table><row><cell>µ = 0</cell><cell></cell><cell cols="4">Jittering Probability</cell></row><row><cell>σ = 0.1</cell><cell>0</cell><cell cols="5">0.02 0.04 0.06 0.08 0.10</cell></row><row><cell>ST-GCN [10]</cell><cell cols="5">80.7 66.4 44.1 32.7 13.3</cell><cell>7.0</cell></row><row><cell>SR-TSL [13]</cell><cell cols="6">84.8 70.4 53.2 41.0 33.9 21.4</cell></row><row><cell>2s-AGCN [29]</cell><cell>88.5</cell><cell cols="5">74.9 60.9 41.9 29.4 20.6</cell></row><row><cell cols="7">preliminary version [23] 85.9 73.2 59.8 45.3 41.6 34.5</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell cols="6">85.8 84.1 66.1 34.2 22.2 13.9</cell></row><row><cell>2s RA-GCN</cell><cell cols="6">86.7 70.0 55.3 48.2 41.5 36.4</cell></row><row><cell>3s RA-GCN</cell><cell cols="5">87.3 84.2 72.4 61.6 42.4</cell><cell>28.7</cell></row><row><cell>difference*</cell><cell>1.5</cell><cell>0.1</cell><cell>6.3</cell><cell cols="3">27.4 20.2 14.8</cell></row><row><cell>ST-GCN [10]</cell><cell cols="6">73.2 63.4 50.2 33.7 18.6 10.3</cell></row><row><cell>SR-TSL [13]</cell><cell cols="6">79.9 60.3 50.9 39.2 30.7 19.6</cell></row><row><cell>2s-AGCN [29]</cell><cell>84.2</cell><cell cols="4">42.3 37.9 35.8 31.0</cell><cell>23.7</cell></row><row><cell cols="5">preliminary version [23] 79.4 78.1 73.4 55.2</cell><cell cols="2">26.9 17.2</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell cols="4">80.0 72.2 35.9 12.6</cell><cell>7.0</cell><cell>5.4</cell></row><row><cell>2s RA-GCN</cell><cell cols="2">82.5 79.3</cell><cell cols="4">62.5 38.4 22.2 15.2</cell></row><row><cell>3s RA-GCN</cell><cell cols="5">82.7 77.8 65.7 47.9 29.5</cell><cell>20.5</cell></row><row><cell>difference*</cell><cell>2.7</cell><cell>5.6</cell><cell cols="4">29.8 35.3 22.5 15.1</cell></row><row><cell>µ = 0</cell><cell></cell><cell cols="4">Jittering Probability</cell></row><row><cell>σ = 0.05</cell><cell>0</cell><cell cols="5">0.02 0.04 0.06 0.08 0.10</cell></row><row><cell cols="2">ST-GCN [10] 80.7 difference* 1.5</cell><cell>4.6</cell><cell>7.4</cell><cell>8.8</cell><cell>9.1</cell><cell>11.5</cell></row><row><cell>ST-GCN [10]</cell><cell cols="6">73.2 70.4 64.0 59.5 44.0 32.1</cell></row><row><cell>SR-TSL [13]</cell><cell cols="6">79.9 68.2 55.4 47.9 41.3 33.6</cell></row><row><cell>2s-AGCN [29]</cell><cell>84.2</cell><cell cols="5">56.7 49.2 41.9 37.3 32.0</cell></row><row><cell cols="7">preliminary version [23] 79.4 79.0 78.7 77.1 72.3 61.2</cell></row><row><cell>baseline (1s RA-GCN)</cell><cell cols="6">80.0 79.4 71.9 46.4 25.2 14.3</cell></row><row><cell>2s RA-GCN</cell><cell cols="3">82.5 81.9 79.7</cell><cell cols="3">69.5 44.9 31.8</cell></row><row><cell>3s RA-GCN</cell><cell>82.7</cell><cell>82.0</cell><cell cols="4">79.6 74.2 62.1 48.5</cell></row><row><cell>difference*</cell><cell>2.7</cell><cell>2.6</cell><cell>7.7</cell><cell cols="3">27.8 36.9 34.2</cell></row></table><note>**</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Approaches and applications of virtual reality and gesture recognition: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sudha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sriraghav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manisha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of vision-based methods for action representation, segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust human activity recognition from depth video using spatiotemporal multi-fused features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="295" to="308" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning approach for human action recognition in infrared images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Syst. Res</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="146" to="154" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Multimedia Expo Workshop (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Int. Joint Conf</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ensemble learning for data stream analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Minku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stefanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Woźniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="132" to="156" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Richly activated graph convolutional network for action recognition with incomplete skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Part-based graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Br. Mach. Vis. Conf. (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An hog-lbp human detector with partial occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="32" to="39" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Making action recognition robust to occlusions and viewpoint changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Özuysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="635" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="549" to="559" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>NeurIPS)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving referring expression grounding with cross-modal attention-guided erasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep multimodal feature analysis for action recognition in rgb+ d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1045" to="1058" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Glimpse clouds: Human activity recognition from unstructured feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Multimedia Expo Workshop</title>
		<imprint>
			<publisher>ICMEW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Actionalstructural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Part-level graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning graph convolutional network for skeleton-based human action recognition by neural searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Early action prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2568" to="2583" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Skeletonbased human action recognition with global context-aware attention lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdiyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Skeleton-based online action prediction using scale selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Skelemotion: A new representation of skeleton joint sequences based on motion information for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Adv. Video Signal Based Surveill. (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Skeleton image representation for 3d action recognition based on tree structure and reference joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIBGRAPI Conf. Graph., Patterns Images</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Vertex feature encoding and hierarchical temporal modeling in a spatial-temporal graph convolutional network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ottersten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09745</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">And he is working toward the Ph.D. degree in the Center for Research on Intelligent Perception and Computing (CRIPAC), Institute of Automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi-Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese Academy of Sciences (CASIA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Song received his M.S. degree in Zhengzhou University</orgName>
		</respStmt>
	</monogr>
	<note>Currently, He is a student of the School of Artificial Intelligence, University of Chinese Academy and Sciences (UCAS). His research interests include computer vision, activity recognition, video surveillance, and time series analysis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
