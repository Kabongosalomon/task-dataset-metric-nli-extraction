<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object-Aware Instance Labeling for Weakly Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Kosugi</surname></persName>
							<email>kosugi@hal.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
							<email>yamasaki@hal.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
							<email>aizawa@hal.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object-Aware Instance Labeling for Weakly Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised object detection (WSOD), where a detector is trained with only image-level annotations, is attracting more and more attention. As a method to obtain a well-performing detector, the detector and the instance labels are updated iteratively. In this study, for more efficient iterative updating, we focus on the instance labeling problem, a problem of which label should be annotated to each region based on the last localization result. Instead of simply labeling the top-scoring region and its highly overlapping regions as positive and others as negative, we propose more effective instance labeling methods as follows. First, to solve the problem that regions covering only some parts of the object tend to be labeled as positive, we find regions covering the whole object focusing on the context classification loss. Second, considering the situation where the other objects contained in the image can be labeled as negative, we impose a spatial restriction on regions labeled as negative. Using these instance labeling methods, we train the detector on the PASCAL VOC 2007 and 2012 and obtain significantly improved results compared with other state-ofthe-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detectors trained on large-scale datasets with instance-level annotations (i.e., strongly supervised object detectors) have made significant progress <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> with the recent development of convolutional neural networks (CNNs), but such detailed large-scale datasets are timeconsuming and labor-intensive to collect accurately. On the other hand, image-level labels that indicate the presence of an object can be acquired easily and in large amounts because such labels take less time to annotate manually or can be collected using an image search on the Internet. In order to take advantage of readily available image-level annotations, in this study, we focus on the problem of training a detector with only image-level annotations; that is, weakly supervised object detection <ref type="bibr">(WSOD)</ref>.</p><p>As a method to obtain a well-performing detector with only image-level annotations, both a detector and instance labels are updated iteratively. Conventional methods include an alternating iterative strategy <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref>. A detector is trained on instance labels initialized based on a simple rule (e.g., supposing the object is at the center of the image <ref type="bibr" target="#b17">[18]</ref>), and the instance labels are updated using the trained detector, over and over again. Although the initial instance labels are rough and the detector trained on the initial labels has low performance, the detector and the instance labels are refined step by step through the alternate optimization. In a recent method <ref type="bibr" target="#b19">[20]</ref>, for end-to-end iterative update of detectors and instance labels, multiple instance classifiers (object detectors) have been employed. Each instance classifier is trained using the last instance classifier's localization result as supervisions. The method can reduce the training time by optimizing multiple instance classifiers end-to-end, and achieves good performance.</p><p>In order to obtain efficient iterative update, we focus on an instance labeling problem, a problem of which label should be annotated to each region based on the last localization result. The instance labeling method employed by one of the state-of-the-art methods <ref type="bibr" target="#b19">[20]</ref> is rather simple; the most confident region and its highly overlapping regions are labeled as positive, and other regions are labeled as negative or background. For more effective instance labeling, we propose two methods: Context-Aware Positive (CAP) labeling and Spatially Restricted Negative (SRN) labeling. CAP labeling is aimed at solving the problem that the most discriminative parts of the object (e.g., faces in the person class) tend to be detected rather than the whole object. We find that the classification loss of the context of the region (i.e., the outside of the region) differs depending on whether the region covers the whole object or not. Utilizing this characteristic, we replace the incomplete detected region with a region covering the whole object. In addition to CAP labeling, we develop SRN labeling to consider the negative labeling; that is, which region should be annotated as background. When an image has multiple objects of the same class, even though one object is labeled as positive, the other objects can be labeled as negative. SRN labeling solves this problem by imposing a spatial restriction on negative labeling. We show a comparison with the baseline labeling in <ref type="figure" target="#fig_0">Figure 1</ref>. In order to verify the efficacy of our method, we conducted experiments on the PASCAL VOC 2007 and 2012 datasets <ref type="bibr" target="#b5">[6]</ref>. The obtained mean Average Precision (mAP) scores are 47.6% and 43.4% respectively, which surpasses other state-of-the-art methods.</p><p>In summary, the contributions of this paper are as follows:</p><p>• We improve the WSOD method from the viewpoint of instance labeling. • We propose two methods for instance labeling. The first method is aimed at finding a region covering the whole object based on the context classification loss. The second can avoid labeling objects as negative by imposing a spatial restriction. • Experiments on the PASCAL VOC 2007 and 2012 datasets demonstrate that our method can achieve better performance than other state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>WSOD is a task where a detector is trained with only image-level annotations. Methods for WSOD can be roughly divided into three approaches: the alternating approach, end-to-end approach, and transferring approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Alternating approach</head><p>A conventional method to train a detector with only image-level annotations is an alternating approach <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref>. Song et al. <ref type="bibr" target="#b17">[18]</ref> initialized instance labels supposing the object is at the center of the image and trained the detector. The initial instance labels are rough, because the location information of the object is unavailable, and the detector trained on the initial labels has low performance. By updating the detector and the instance labels alternately, the detector and the instance labels are refined step by step.</p><p>Based on the alternating approach, other methods were developed to detect objects more accurately. Li et al. <ref type="bibr" target="#b11">[12]</ref> trained a classifier using entire images, and then selected confident class-specific region proposals using a mask-out strategy. Cinbis et al. <ref type="bibr" target="#b2">[3]</ref> developed a multi-fold learning method to solve the problem that alternating approaches are easily trapped in local optima. Jie et al. <ref type="bibr" target="#b8">[9]</ref> developed a selftaught learning method to select more reliable seed positive proposals. Our instance labeling method can be applied to these alternating approaches, but alternating approaches, which split the training process between the optimizing detector and updating instance labels, tend to get stuck in local optima and are time-consuming. Therefore, we apply our instance labeling method to an end-to-end iterative approach described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">End-to-end approach</head><p>Bilen et al. <ref type="bibr" target="#b1">[2]</ref> proposed a weakly supervised deep detection network (WSDDN) with two streams: a classification stream and a detection stream. The outputs of these two streams are combined and used to score each region. Kantorov et al. <ref type="bibr" target="#b9">[10]</ref> extended WSDDN to consider contextual information. Diba et al. <ref type="bibr" target="#b4">[5]</ref> and Wei et al. <ref type="bibr" target="#b23">[24]</ref> used semantic segmentation based on class activation map <ref type="bibr" target="#b28">[29]</ref> to discover region proposals that tightly cover the object. Tang et al. <ref type="bibr" target="#b20">[21]</ref> developed high-quality region proposals by exploiting the low-level information in CNN.</p><p>An end-to-end approach (online instance classifier refinement, OICR) that takes advantage of alternating approaches was proposed by Tang et al. <ref type="bibr" target="#b19">[20]</ref>. OICR takes WSDDN as the initial instance localization method and has multiple instance classifiers (object detectors). The first instance classifier is trained on the instance-level supervisions labeled by WSDDN, and the second instance classifier is trained using the localization result of the first in-stance classifier as supervisions. Similar to alternating approaches, the instance classifiers and the instance labels are refined iteratively. Because OICR takes less time to train and has higher performance than alternating approaches, recent methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref> employ OICR as a baseline. We also apply our instance labeling method to OICR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Transferring approach</head><p>The location information obtained by the above WSOD method can be transferred to a supervised object detector. Shen et al. <ref type="bibr" target="#b15">[16]</ref> proposed a generative adversarial learning paradigm. They introduced a discriminator and trained a one-stage detector similar to SSD <ref type="bibr" target="#b12">[13]</ref> so that the discriminator cannot distinguish the detector and OICR <ref type="bibr" target="#b19">[20]</ref> model. The trained one-stage detector achieves faster detection. Zhang et al. <ref type="bibr" target="#b27">[28]</ref> proposed pseudo labeling methods named pseudo ground-truth excavation and pseudo ground-truth adaptation. Using these methods, they generated pseudo ground truth boxes from localization result of OICR <ref type="bibr" target="#b19">[20]</ref> and trained a faster R-CNN <ref type="bibr" target="#b14">[15]</ref> model. Zhang et al. <ref type="bibr" target="#b25">[26]</ref> proposed a zigzag learning strategy, in which they developed a criterion (the Energy Accumulation Score) to automatically measure and rank localization difficulty. As the localization result of WSOD is unreliable, at first they used easy images to localize and added difficult images progressively. Instead of only using the top-scoring regions as pseudo ground truth, supervised object detectors can be trained more effectively with these transferring approaches.</p><p>We can obtain a further performance improvement combining transferring approaches and our localization result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>A goal of WSOD is to train a detector with only imagelevel annotations. As a typical method to obtain a wellperforming detector, both the detector and instance labels are updated iteratively. In order to train a detector iteratively, we have to solve the problem of which label should be annotated to each region based on the last localization result. In this study, we focus on this problem; that is, the instance labeling problem.</p><p>Among iteratively updating methods, we employ the OICR <ref type="bibr" target="#b19">[20]</ref> as a baseline. We first introduce OICR shortly.</p><p>OICR OICR includes two modules, multiple instance classification and instance refinement. In particular, an end-toend WSOD method named WSDDN <ref type="bibr" target="#b1">[2]</ref> is employed as a multiple instance classification module. WSDDN includes two streams that calculate region-wise scores in a different way based on CNN features pooled by Spatial Pyramid Pooling (SPP) <ref type="bibr" target="#b7">[8]</ref>, a classification stream, and a detection stream. The classification stream conducts a softmax operation on each region proposal for classification. The detection stream conducts a softmax operation on each class in order to estimate which region is most valuable for classification. Both output scores are combined by an elementwise product and defined as each region's detection score.</p><p>Suppose an input image is X, the image label vector is Y = [y 1 , ..., y C ], and its region proposals by Selective Search <ref type="bibr" target="#b22">[23]</ref> are {r 1 , r 2 , ..., r J }, where C denotes the number of image classes, y c = 1 or 0 denotes the image with or without object c, and J denotes the number of the region proposals. Through WSDDN, we obtain the initial proposal score matrix x 0 ∈ R C×J , where each element x 0 cj denotes region r j 's score for class c. When WSDDN is trained, the image score φ c is obtained by the sum over all proposals, φ c = J j=1 x 0 cj , and the following multi class cross entropy is minimized,</p><formula xml:id="formula_0">L b = − C c=1 {y c log φ c + (1 − y c ) log(1 − φ c )}.<label>(1)</label></formula><p>By utilizing WSDDN as an initial localization network, multiple instance classifiers are trained progressively to refine the localization result and obtain a well-performing detector. Here, let K be a number of instance classifiers and x k ∈ R (C+1)×J be an output proposal score of the k th instance classifier. Different from x 0 , x k (k ∈ {1, ..., K}) has the {C +1} th dimension for background. To train the multiple instance classifiers progressively, the ground truth label y k ∈ R (C+1)×J for the k th instance classifier is made from the last instance classifier's output x k−1 . Based on y k , each instance classifier is trained to minimize the following loss:</p><formula xml:id="formula_1">L k r = − 1 J J j=1 C+1 c=1 y k cj log x k cj .<label>(2)</label></formula><p>In OICR, instance labeling is a problem of how to generate an instance label y k from the last localization result x k−1 . Suppose an image X has class label c, they first select proposal r jc with the highest score,</p><formula xml:id="formula_2">j c = arg max j x k−1 cj ,<label>(3)</label></formula><p>and inspired by the fact that highly overlapped regions should have the same label, they formulate the following labeling algorithm,</p><formula xml:id="formula_3">y k cj = 1 if IoU(r j , r jc ) &gt; I t 0 otherwise ,<label>(4)</label></formula><p>where IoU is a function of calculating Intersection over Union (IoU) between two regions and I t is a threshold. When multiple classes satisfy IoU(r j , r jc ) &gt; I t , y k cj whose c = arg max c IoU(r j , r j c ) is 1 and the others are 0. If a region is not assigned any object classes, that is, y k cj is 0 for all c ∈ {1, ..., C}, the region is labeled as background,</p><formula xml:id="formula_4">y k (C+1)j = 1.<label>(5)</label></formula><p>However, the label generated from the last localization result is unreliable, especially at the beginning of the training. This results in the instability of the training. To solve this problem, the loss function in Eq. <ref type="formula" target="#formula_1">(2)</ref> is changed to the weighted version as follows,</p><formula xml:id="formula_5">w k j = x k−1 cjc ,<label>(6)</label></formula><formula xml:id="formula_6">L k r = − 1 J J j=1 C+1 c=1 w k j y k cj log x k cj .<label>(7)</label></formula><p>When an image has multiple classes, in Eq. <ref type="formula" target="#formula_5">(6)</ref>, c = arg max c IoU(r j , r j c ). At the beginning of the training or for a difficult image to localize, the weight w k j takes a low value and the contribution to the training becomes small.</p><p>Problem The simple instance labeling method described above has two problems. First, the most discriminative part of the object tends to be detected rather than the whole object. If r jc does not highly overlap the whole object, the progressive update is trapped in the local optima. Second, the simple instance labeling does not take into account cases where an image contains multiple objects of the same class. Even though one object is correctly labeled as positive, other objects can be incorrectly labeled as background. To solve these problems, we propose more effective instance labeling methods named CAP labeling and SRN labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">CAP Labeling</head><p>We propose CAP labeling to avoid the problem that the most discriminative part of the object tends to be detected rather than the whole object. We introduce a network that judges whether a region covers the whole object or not, and when we generate the instance labels, the top-scoring region is selected from regions covering the whole object.</p><p>In some previous methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>, mask-out strategy is used to find the whole object. If a mask-out image by a region drops the classification confidence, that region can be considered discriminative. However, we experimentally find that the mask-out by regions covering only some parts of the object can drop the classification confidence. Such a mask-out method is improper for discovery of regions covering the whole object.</p><p>In order to judge whether a region covers the whole object or not more properly, we focus on the research of Tanaka et al. <ref type="bibr" target="#b18">[19]</ref>, who deal with a problem of classification with noisy labels, where a classifier is trained with noisy labeled images. Here, noisy labeled images mean incorrectly labeled images (e.g., a dog image labeled as a cat). According to them, when a classifier is trained on noisy labeled images, the training loss differs depending on whether the data is noisy or clean. The loss tends to decrease for clean images and is hard to decrease for noisy labeled images.</p><p>We find that this characteristic can be used to judge whether a region covers the whole object or not. We focus not on the inside of the region but on the outside of the region. We call the outside of the region as the context of that region. Taking an image containing a cat as an example; when a region covers the whole cat, no cat exists in the context of the region. On the other hand, when a region does not cover the whole cat, some parts of the cat are in the context. If we label the contexts of all regions as a cat, these are noisy labeled images: when a region covers the whole cat, the context of that region is noisy and otherwise clean. By training a classifier using this data, classification loss differs depending on whether a region covers the whole cat or not.</p><p>As a simple method to train a classifier based on the context, the inside of the region is filled with the mean pixel value before the image is input into a CNN. However, this method requires CNN forwarding for each region and is time-consuming. To achieve low computational cost, we perform mask-out to the CNN feature. The CNN feature corresponding to the inside of the region is filled with zero values. Then the feature after mask-out is pooled with global average pooling (GAP) and input to a fully connected (FC) layer.</p><p>Let the output of the classifier with CNN feature maskout be p ∈ R C×J , where each element p cj denotes a probability for class c of r j 's context. The classifier is trained to minimize the standard multi class cross entropy loss with the image-level label Y,</p><formula xml:id="formula_7">L context = − 1 J J j=1 C c=1</formula><p>{y c log p cj +(1−y c ) log(1−p cj )}.</p><p>(8) If a region covers the whole object, the training loss of the context is high after training because the context of the region is noisy. In other words, the class probability p cj , whose class c is contained by the image (y c = 1), is low. On the other hand, if a region does not cover the whole object, the class probability of the context, which is clean, is high.</p><p>When we conduct instance labeling, the top-scoring region is selected from regions whose context class probabilities are low,</p><formula xml:id="formula_8">j c = arg max j x k−1 cj s.t. p cj &lt; P t ,<label>(9)</label></formula><p>where P t is a threshold. Then following the OICR method, highly overlapping regions are labeled as positive based on Eq. (4). Even though a region is covering the whole object, the training loss of the context can decrease in some cases; for example, when the context is closely related to the object (e.g., an aeroplane and sky) or when two or more objects <ref type="figure">Figure 2</ref>. Overview of our method. We label each region as positive or negative based on the last localization result, and the next instance classifier is trained on these instance labels. To label regions covering the whole object as positive, we discover such regions focusing on the context classification loss. In addition, we employ a spatial restriction to avoid labeling the other objects as negative. Although this image shows only the first instance classifier, the second and subsequent instance classifiers are trained in the same way.</p><p>are in an image. To solve this problem, we introduce Xiao et al.'s <ref type="bibr" target="#b24">[25]</ref> saliency map. Following the previous WSOD method of Wei et al. <ref type="bibr" target="#b23">[24]</ref>, we define areas whose saliency is higher than 0.06 as foreground and others as background. When the classifier is trained based on Eq. (8), background areas are filled with the mean pixel value before it is input to the classifier. When the class probability is calculated for Eq. (9), we divide foreground segments to each independent segment, select the foreground segment that has the highest IoU between the segment and the box, and fill the other areas with the mean pixel value. As a result, the object on which the box is focusing is visible and the other objects are hidden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SRN Labeling</head><p>In CAP labeling, we label regions highly overlapped by r jc as positive. If a region is not assigned any object class, that is, y k cj is 0 for all c ∈ {1, ..., C}, the region is labeled as background, y k (C+1)j = 1. This labeling has a problem: when an image has multiple objects of a specific class, even though one object is correctly labeled as positive, the other objects are labeled as background.</p><p>To solve this problem, we propose SRN labeling. This method is inspired by the fact that at a distant area from the object other objects may exist. In SRN labeling, we put a spatial restriction on regions that are trained as background by modifying the weight in Eq. (6) as follows,</p><formula xml:id="formula_9">w k j = x k−1 cjc if IoU(r j , r jc ) &gt; i t 0 otherwise ,<label>(10)</label></formula><p>where i t is a threshold that is lower than I t .</p><p>Originally, w k j is aimed at restricting the contribution of unreliable labels, such as those generated at the beginning of the training. SRN labeling is the spatial version of this: we regard labels at remote areas as unreliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Overall architecture</head><p>The overall architecture is shown in <ref type="figure">Figure 2</ref>. When training, we first train the context classifier according to the loss in Eq. (8) and calculate the context class probability p cj . Then we train the WSDDN and the multiple instance classifiers to minimize the following loss,</p><formula xml:id="formula_10">L OICR = L b + K k=1 L k r .<label>(11)</label></formula><p>For the test, we ignore the context classifier and the WS-DDN and take an average of multiple instance classifiers' output to obtain the final detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted experiments to verify the performance of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and metrics</head><p>To evaluate our method, we use the PASCAL VOC 2007 and 2012 datasets <ref type="bibr" target="#b5">[6]</ref>, which are standard benchmarks for WSOD. These datasets have 9,962 and 22,531 images, respectively, with 20 classes and are divided into train, val, and test sets. We select trainval images (5,011 for 2007 and 11,540 for 2012) to train our model with image-level annotations. We employ two metrics to evaluate our method: mean Average Precision (mAP) and Cor-    -VOC 2012 rect Localization (CorLoc). The mAP is used to test the detection performance of our model on the test dataset, and CorLoc measures the localization accuracy on the trainval dataset. Both metrics are based on the same IoU threshold between the predicted bounding boxes and ground truths, i.e., IoU &gt; 0.5.</p><formula xml:id="formula_11">OICR [20] - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_12">WSRPN [21] - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_13">OICR [20] - - - - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation</head><p>Our model is built on the VGG16 <ref type="bibr" target="#b16">[17]</ref> model. For the context classifier, a CNN feature is obtained replacing the last pooling layer and the fully connected layers with an additional convolutional layer of size 3 × 3, stride 1, pad 1 with 1024 units following Zhou et al. <ref type="bibr" target="#b28">[29]</ref>. The feature corresponding inside of the bounding box is mask-out by zero, and the feature after mask-out is pooled by GAP followed by a FC layer. For WSDDN <ref type="bibr" target="#b1">[2]</ref> and multiple instance classifiers <ref type="bibr" target="#b19">[20]</ref>, the CNN features of VGG16 are pooled by the SPP layer extracting the feature corresponding to the inside of the bounding box. During training, we first train the context classifier for 10K iterations (VOC 2007) or 20K it-erations (VOC 2012) with the learning rate 0.001. Then we train the WSDDN and the multiple instance classifiers for 70K iterations. The learning rate is linearly increased from 0 to 0.001 for the first 10K iterations and fixed to 0.001 and 0.0001 for the following 30K iterations and the last 30K iterations, respectively. The weight of the model is initialized with the one pretrained on the ImageNet <ref type="bibr" target="#b3">[4]</ref> dataset at the beginning of each training step. Newly added layers are initialized using Gaussian distributions with means of 0 and standard deviations of 0.01. Biases are initialized to 0. The momentum is set to 0.9 and the weight decay is set to 0.0005.</p><p>As a region proposal method, we employ Selective Search <ref type="bibr" target="#b22">[23]</ref>, which generates about 2,000 proposals for each image. For data augmentation, we use five scales {480, 576, 688, 864, 1200} resizing the shorter side to one of these scales, and cap the longer side to 2000 with horizontal flips for both training and testing. We set the number of instance classifiers K to 3, and the mean output of these instance classifiers is used during testing. Other parameters I t , P t , and i t are set to 0.5, 0.5, and 0.1 respectively. <ref type="figure">Figure 3</ref>. Examples of detection results using our method and the baseline (OICR <ref type="bibr" target="#b19">[20]</ref>). Red boxes denote the detection result of our method; green boxes denote the detection result of the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with the state-of-the-arts</head><p>We compared our proposed method with previous methods based on a single VGG16 model. The mAP result on VOC 2007 is shown in <ref type="table" target="#tab_0">Table 1</ref>. This result shows that our method outperforms the other methods. In particular, our method outperforms OICR [20] by 6.4%, which is the baseline of our method. This improvement is obtained by discovering regions covering the whole object and being aware of multiple objects. Although OICR has a problem that only discriminative parts of cat and dog tend to be detected, our method solves this problem, as shown by the gains of cat and dog (39.4% and 22.3% respectively). In addition, TS 2 C [24] and WSRPN <ref type="bibr" target="#b20">[21]</ref> also employ OICR as the baseline, but our method outperforms these methods. Examples of the detection results on the test dataset are shown in <ref type="figure">Figure 3</ref>. This result shows our method can effectively reduce false positive compared with OICR. The mAP result on VOC 2012 is also shown in <ref type="table" target="#tab_0">Table 1</ref>. The score of our method is higher than that of WSRPN <ref type="bibr" target="#b20">[21]</ref>, which is another state-of-the-art method. <ref type="table" target="#tab_3">Table 2</ref> shows the CorLoc result on VOC 2007 and 2012. Our method outperforms each previous state-of-the-art method.</p><p>Using our localization result, we train a Fast R-CNN <ref type="bibr" target="#b6">[7]</ref> (FRCNN) detector. The result is shown in <ref type="table" target="#tab_5">Table 3</ref>. The first to third methods employ the predicted top-scoring region as the pseudo ground truth; the fourth to the sixth methods focus on how to train a FRCNN detector effectively using the localization result. Following the former methods, we train a FRCNN detector using the top-scoring region by our method, whose result is shown as Ours + FRCNN. In addition, we apply Pseudo Ground-truth Excavation (PGE) <ref type="bibr" target="#b27">[28]</ref>, which is a previous state-of-the-art method for mining more accurate and tighter boxes instead of only one top-scoring box. As shown in <ref type="table" target="#tab_5">Table 3</ref>, our method obtains the highest score when combined with PGE for VOC 2007, and outperforms the previous methods with and without PGE for VOC 2012.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation experiments</head><p>We conduct extensive ablation experiments to analyze our method. All ablation experiments are conducted on the VOC 2007 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution of each labeling</head><p>Our method is composed of CAP labeling and SRN labeling. We investigate how much each method contributes to the improvement and show the results in <ref type="table">Table 4</ref>. Each method can improve the performance, and we can obtain even greater improvement by combining both labeling methods.</p><p>Context classification CAP labeling is based on the hypothesis that the context classification loss differs depending on how much of the object is covered by the region. Here, to verify the hypothesis, we visualize the training loss curve.</p><p>In order to divide regions according to the coverage of the object, we define the following sets,</p><formula xml:id="formula_14">S i = {(c, r j ) | 0.2(i−1) ≤ coverage(c, r j ) ≤ 0.2i, y c = 1},<label>(12)</label></formula><p>where i ∈ {1, 2, 3, 4, 5} and coverage(c, r j ) is a function calculating how much of the ground truth box of the class c object is covered by the region r j . The context classifier is trained to minimize the loss Eq. <ref type="bibr" target="#b7">(8)</ref>. To investigate the rela- tionship between the coverage of the object and the training loss, we define the following loss,</p><formula xml:id="formula_15">L i context = − 1 |S i | c,rj ∈Si log p cj .<label>(13)</label></formula><p>As we consider only the classes contained by the image, log(1 − p cj ) is not calculated. Note that the objective function is loss Eq. (8) and L i context is only used for visualization.</p><p>The change of each L i context during the training process is shown in <ref type="figure" target="#fig_1">Figure 4</ref>. The training loss of the boxes covering only some parts of the object (L 1 context , L 2 context ) decrease through the training process. On the other hand, the loss of the boxes covering most of the object (L 5 context ) does not decrease. This result shows that the context classification loss can be used to find boxes covering the whole object.</p><p>Context classification and simple mask-out In some previous methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>, a classifier is trained with imagelevel annotations, and regions whose mask-out drops the classification confidence are defined as the object. We refer to such methods as simple mask-out. Here, we compare our context classification and the simple mask-out approach.</p><p>To perform the simple mask-out, we first train a standard classifier, which has the same architecture as the context classifier except for the mask-out. Let an input image be X, the image label vector be Y = [y 1 , ..., y C ], and the class probability be p = [p 1 , ..., p C ]. The classifier is trained to minimize the following classification loss, After training, we mask-out the CNN feature corresponding to each region r j in the same way as context classification and obtain the probability p cj . In Bazzani et al. <ref type="bibr" target="#b0">[1]</ref> and Li et al. <ref type="bibr" target="#b11">[12]</ref>, mask-out is performed on the input image, but it requires forwarding for each region and is very time-  consuming. For the same CNN forwarding time, we maskout not the input image but the CNN feature.</p><p>To compare context classification and simple mask-out, we show regions whose p cj or p cj are lower than the threshold P t in <ref type="figure" target="#fig_3">Figure 5</ref>. Although with simple mask-out, the confidence of the classifier decreases when some parts of the object are covered, the confidence of our context classifier drops only when the whole object is covered. As a result, we can obtain regions covering the whole object. We train our model by replacing context classification with simple mask-out <ref type="table" target="#tab_6">(Table 5</ref>). In both metrics, CorLoc and mAP, the method using context classification achieves better performance. This result demonstrates the effectiveness of our context classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this study, we address weakly supervised object detection. As a typical method to train a detector with imagelevel annotations, the detector and the instance-level labels are updated iteratively. In order to achieve more efficient iterative updating, we focus on the instance labeling problem, a problem of which label should be annotated to each bounding box based on the last localization result. We improve instance labeling in two ways. First, we label boxes covering the whole object as positive, being aware that the context classification loss differs according to the coverage of the object. Second, we introduce a spatial restriction to avoid labeling other objects as negative. Experiments show that our method achieves significant improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of (a) baseline instance labeling and (b) our instance labeling. In the localization result, only the top-scoring region is shown (red box). Red/blue boxes in the instance labeling denote regions labeled as positive/negative. Our labeling method can label regions covering the whole object as positive focusing on the context classification loss and avoid labeling other objects as negative introducing a spatial restriction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Loss curve with different coverage of the object while training on the PASCAL VOC 2007 dataset. Through the training process, losses with low coverage decrease, while losses with high coverage do not decrease.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>L</head><label></label><figDesc>simple = − C c=1 {y c log p c + (1 − y c ) log(1 − p c )}. (14)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Discovery result of regions covering the whole object by each method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average precision (%) on PASCAL VOC 2007 and 2012 test datasets. 62.4 31.1 19.4 13.0 65.1 62.2 28.4 24.8 44.7 30.6 25.3 37.8 65.5 15.7 24.1 41.7 46.9 64.3 62.6 41.2 SGWSOD [11] 48.4 61.5 33.3 30.0 15.3 72.4 62.4 59.1 10.9 42.3 34.3 53.1 48.4 65.0 20.5 16.6 40.6 46.5 54.6 55.1 43.5 TS 2 C [24] 59.3 57.5 43.7 27.3 13.5 63.9 61.7 59.9 24.1 46.9 36.7 45.6 39.9 62.6 10.3 23.6 41.7 52.4 58.7 56.6 44.3 WSRPN [21] 57.9 70.5 37.8 5.7 21.0 66.1 69.2 59.4 3.4 57.1 57.3 35.2 64.2 68.6 32.8 28.6 50.8 49.5 41.1 30.0 45.3 Ours 61.5 64.8 43.7 26.4 17.1 67.4 62.4 67.8 25.4 51.0 33.7 47.6 51.2 65.2 19.3 24.4 44.6 54.1 65.6 59.5 47.6</figDesc><table><row><cell>method</cell><cell>aero bike bird boat bottle bus</cell><cell>car</cell><cell>cat chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mAP</cell></row><row><cell>-VOC 2007</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">OICR [20] 58.0 -VOC 2012</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>CorLoc (%) on PASCAL VOC 2007 and 2012 trainval datasets. 81.2 55.3 19.7 44.3 80.2 86.6 69.5 10.1 87.7 68.4 52.1 84.4 91.6 57.4 63.4 77.3 58.1 57.0 53.8 63.8 Teh et al. [22] 84.0 64.6 70.0 62.4 25.8 80.7 73.9 71.5 35.7 81.6 46.5 71.3 79.1 78.8 56.7 34.3 69.8 56.7 77.0 72.7 64.6 Ours 85.5 79.6 68.1 55.1 33.6 83.5 83.1 78.5 42.7 79.8 37.8 61.5 74.4 88.6 32.6 55.7 77.9 63.7 78.4 74.1 66.7</figDesc><table><row><cell>method</cell><cell>aero bike bird boat bottle bus</cell><cell>car</cell><cell>cat chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mean</cell></row><row><cell>-VOC 2007</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OICR [20]</cell><cell cols="5">81.7 80.4 48.7 49.5 32.8 81.7 85.4 40.1 40.6 79.5 35.7 33.7 60.5 88.8 21.8 57.9 76.3 59.9 75.3 81.4 60.6</cell></row><row><cell>TS 2 C [24]</cell><cell cols="5">84.2 74.1 61.3 52.1 32.1 76.7 82.9 66.6 42.3 70.6 39.5 57.0 61.2 88.4 9.3 54.6 72.2 60.0 65.0 70.3 61.0</cell></row><row><cell cols="6">SGWSOD [11] 71.0 76.5 54.9 49.7 54.1 78.0 87.4 68.8 32.4 75.2 29.5 58.0 67.3 84.5 41.5 49.0 78.1 60.3 62.8 78.9 62.9</cell></row><row><cell>WSRPN [21]</cell><cell>77.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>82.1 67.2 58.7 48.9 80.5 75.6 62.3 46.0 81.9 40.0 64.2 82.4 88.2 44.2 53.5 78.1 54.7 56.7 82.9 66.7</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.1</cell></row><row><cell cols="22">SGWSOD [11] 70.4 79.3 54.1 44.9 56.8 89.8 72.3 69.2 41.0 67.3 32.3 61.1 72.0 85.0 43.9 56.4 77.8 42.6 64.0 77.6 62.9</cell></row><row><cell>TS 2 C [24]</cell><cell cols="21">79.1 83.9 64.6 50.6 37.8 87.4 74.0 74.1 40.4 80.6 42.6 53.6 66.5 88.8 18.8 54.9 80.4 60.4 70.7 79.3 64.4</cell></row><row><cell>WSRPN [21]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64.9</cell></row><row><cell>Ours</cell><cell>86.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>mAP (%) on PASCAL VOC 2007 and 2012 test dataset by training FRCNN detectors.</figDesc><table><row><cell>method</cell><cell></cell><cell>VOC 2007</cell><cell>VOC 2012</cell></row><row><cell>OICR-Ens + FRCNN [20]</cell><cell></cell><cell>47.0</cell><cell>42.5</cell></row><row><cell>TS 2 C + FRCNN [24]</cell><cell></cell><cell>48.0</cell><cell>44.4</cell></row><row><cell>WSRPN-Ens + FRCNN [21]</cell><cell></cell><cell>50.4</cell><cell>45.7</cell></row><row><cell>ZLDN (WSDDN + FRCNN) [26]</cell><cell></cell><cell>47.6</cell><cell>42.9</cell></row><row><cell cols="2">ML-LocNet (WSDDN + FRCNN) [27]</cell><cell>49.7</cell><cell>43.6</cell></row><row><cell>PGE (OICR + FRCNN) [28]</cell><cell></cell><cell>51.7</cell><cell>47.3</cell></row><row><cell>Ours + FRCNN</cell><cell></cell><cell>51.4</cell><cell>48.1</cell></row><row><cell>PGE (Ours + FRCNN)</cell><cell></cell><cell>52.1</cell><cell>47.9</cell></row><row><cell cols="4">Table 4. Effect of each labeling method to mAP (%) on PASCAL</cell></row><row><cell>VOC 2007.</cell><cell></cell><cell></cell></row><row><cell>method</cell><cell>mAP</cell><cell cols="2">CorLoc</cell></row><row><cell>baseline (OICR [20])</cell><cell>41.2</cell><cell>60.6</cell></row><row><cell>CAP labeling</cell><cell>45.6</cell><cell>66.6</cell></row><row><cell>SRN labeling</cell><cell>45.1</cell><cell>63.4</cell></row><row><cell>CAP and SRN labeling</cell><cell>47.6</cell><cell>66.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Comparison between our method and our method whose context classification is replaced with the simple mask-out when using the PASCAL VOC 2007 dataset.</figDesc><table><row><cell>method</cell><cell>mAP</cell><cell>CorLoc</cell></row><row><cell>simple mask-out</cell><cell>47.1</cell><cell>64.9</cell></row><row><cell>context classification</cell><cell>47.6</cell><cell>66.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement A part of this research was supported by JST-CREST (JPMJCR1686) and the Grants-in-Aid for Scientific Research (19K22863).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-taught object localization with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multifold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Ramazan Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Saliency guided end-to-end learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baisheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial learning towards fast weakly supervised detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angtian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongluan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Eu Wern Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TS2C: tight box mining with surrounding segmentation context for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep salient object detection with dense connections and distraction diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maojun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TMM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3239" to="3251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zigzag learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ML-LocNet: Improving object localization with multi-view learning network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">W2F: A weakly-supervised to fully-supervised framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
