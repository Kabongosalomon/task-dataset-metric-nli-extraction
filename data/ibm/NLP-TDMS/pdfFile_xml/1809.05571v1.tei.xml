<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Models Matter, So Does Training: An Empirical Study of CNNs for Optical Flow Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
						</author>
						<title level="a" type="main">Models Matter, So Does Training: An Empirical Study of CNNs for Optical Flow Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 !</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate two crucial and closely related aspects of CNNs for optical flow estimation: models and training. First, we design a compact but effective CNN model, called PWC-Net, according to simple and well-established principles: pyramidal processing, warping, and cost volume processing. PWC-Net is 17 times smaller in size, 2 times faster in inference, and 11% more accurate on Sintel final than the recent FlowNet2 model. It is the winning entry in the optical flow competition of the robust vision challenge. Next, we experimentally analyze the sources of our performance gains. In particular, we use the same training procedure of PWC-Net to retrain FlowNetC, a sub-network of FlowNet2. The retrained FlowNetC is 56% more accurate on Sintel final than the previously trained one and even 5% more accurate than the FlowNet2 model. We further improve the training procedure and increase the accuracy of PWC-Net on Sintel by 10% and on KITTI 2012 and 2015 by 20%. Our newly trained model parameters and training protocols will be available on https://github.com/NVlabs/PWC-Net. Index Terms-Optical flow, pyramid, warping, cost volume, and convolutional neural network (CNN). ! â€¢ D. Sun and J. Kautz are with NVIDIA,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Models matter. Since the seminal work of AlexNet <ref type="bibr" target="#b0">[1]</ref> demonstrated the advantages of deep models over shallow ones on the ImageNet challenge <ref type="bibr" target="#b1">[2]</ref>, many novel deep convolutional neural network (CNN) <ref type="bibr" target="#b2">[3]</ref> models have been proposed and have significantly improved in performance, such as VGG <ref type="bibr" target="#b3">[4]</ref>, Inception <ref type="bibr" target="#b4">[5]</ref>, ResNet <ref type="bibr" target="#b5">[6]</ref>, and DenseNet <ref type="bibr" target="#b6">[7]</ref>. Fast, scalable, and end-to-end trainable CNNs have significantly advanced the field of computer vision in recent years, and particularly high-level vision problems.</p><p>Inspired by the successes of deep learning in high-level vision tasks, Dosovitskiy et al. <ref type="bibr" target="#b7">[8]</ref> propose two CNN models for optical flow, i.e., FlowNetS and FlowNetC, and introduce a paradigm shift to this fundamental low/middle-level vision problem. Their work shows the feasibility of directly estimating optical flow from raw images using a generic U-Net CNN architecture <ref type="bibr" target="#b8">[9]</ref>. Although their performance is below the state of the art, FlowNetS and FlowNetC are the best among their contemporary real-time methods.</p><p>Recently, Ilg et al. <ref type="bibr" target="#b9">[10]</ref> stacked one FlowNetC and several FlowNetS networks into a large model, called FlowNet2, which performs on par with state-of-the-art methods but runs much faster ( <ref type="figure" target="#fig_0">Fig. 1</ref>). However, large models are more prone to the over-fitting problem, and as a result, the subnetworks of FlowNet2 have to be trained sequentially. Furthermore, FlowNet2 requires a memory footprint of 640MB and is not well-suited for mobile and embedded devices.</p><p>SpyNet <ref type="bibr" target="#b10">[11]</ref> addresses the model size issue by combining deep learning with two classical optical flow estimation principles. SpyNet uses a spatial pyramid network and warps the second image toward the first one using the initial flow. The motion between the first and warped images is usually small. Thus SpyNet only needs a small network to estimate the motion from these two images. SpyNet performs on par with FlowNetC but below FlowNetS and FlowNet2. The reported results by FlowNet2 and SpyNet show a clear trade-off between accuracy and model size.</p><p>Is it possible to both increase the accuracy and reduce the size of a CNN model for optical flow estimation? In principle, the trade-off between model size and accuracy imposes a fundamental limit for general machine learning algorithms. However, we find that combining domain knowledge with deep learning can achieve both goals simultaneously for the particular problem of optical flow estimation.</p><p>SpyNet shows the potential of combining classical principles with CNNs. However, we argue that its performance gap with FlowNetS and FlowNet2 is due to the partial use of classical principles. First, traditional optical flow methods often pre-process the raw images to extract features that are invariant to shadows or lighting changes <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Further, in the special case of stereo matching, a cost volume is a more discriminative representation of the disparity (1D flow) than raw images or features <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. While constructing a full cost volume is computationally prohibitive for real-time optical flow estimation <ref type="bibr" target="#b16">[17]</ref>, our work constructs a "partial" cost volume by limiting the search range at each pyramid level. Linking different pyramid levels using a warping operation allows the estimation of large displacement flow.</p><p>Our network, called PWC-Net, has been designed to make full use of these simple and well-established principles. It makes significant improvements in model size and accuracy over existing CNN models for optical flow <ref type="figure" target="#fig_0">(Fig. 1)</ref>. PWC-Net is about 17 times smaller in size and 2 times faster in inferencing than FlowNet2. It is also easier to train than SpyNet and FlowNet2 and runs at about <ref type="bibr" target="#b34">35</ref>  However, it is imprecise or even misleading to conclude that the performance gains of PWC-Net come only from the new network architecture, because training matters as well. If trained improperly, a good model may perform poorly. CNNs were introduced in the late 80's <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>, but it took decades to figure out the details to train deep CNNs properly, such as dropout, ReLU units, batch normalization, and data augmentation <ref type="bibr" target="#b0">[1]</ref>. For optical flow, Dosovitskiy et al. <ref type="bibr" target="#b7">[8]</ref> report that FlowNetS outperforms FlowNetC. Ilg et al. <ref type="bibr" target="#b9">[10]</ref> show that using more iterations and dataset scheduling results in improved performance for both FlowNetS and FlowNetC. In particular, FlowNetC performs better than FlowNetS with the new training procedure. For PWC-Net, we have used the same network architecture in the first version of our arXiv paper published in Sep. 2017, the second (CVPR) version in Nov. 2017, and the current one. The improvements over previous versions result solely from better training procedures.</p><p>In some sense, a straight forward comparison between PWC-Net and previous models is unfair, because the models have been trained differently. The potential of some models may be unfulfilled due to less than optimal training procedures. To fairly compare models, we retrain FlowNetC and FlowNetS, the sub-networks of FlwoNet2, using the same training protocol as PWC-Net. We observe significant performance improvements: the retrained FlowNetC is about 56% more accurate on Sintel final than the previously trained one, although still 8% less accurate than PWC-Net. A somewhat surprising result is that the retrained FlowNetC is about 5% more accurate on Sintel final compared to the published FlowNet2 model, which has a much larger capacity. The last comparison clearly shows that better training procedures may be more effective at improving the performance of a basic model than increasing the model size, because larger models are usually harder to train 1 . The results show a complicated interplay between models 1. We cannot directly apply our training procedure to FlowNet2. and training, which requires careful experimental designs to identify the sources of performance gains.</p><p>In this paper, we further improve the training procedures for PWC-Net. Specifically, adding KITTI and HD1K data during fine-tuning improves the average end-point error (EPE) of PWC-Net on Sintel final by 10% to 4.60, which is better than all published methods. Fixing an I/O bug, which incorrectly loaded 22% of the training data, leads to a âˆ¼20% improvement on KITTI 2012 and 2015. At the time of writing, PWC-Net has the second lowest outlier percentage in non-occluded regions on KITTI 2015, surpassed only by a recent scene flow method that uses stereo input and semantic information <ref type="bibr" target="#b18">[19]</ref>.</p><p>To summarize, we make the following contributions:</p><p>â€¢ We present a compact and effective CNN model for optical flow based on well-established principles. It performs robustly across four major flow benchmarks and is the winning entry in the optical flow category of the robust vision challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>We compare FlowNetS, FlowNetC and PWC-Net trained using the same training procedure. On Sintel final, the retrained FlowNetC is about 5% more accurate than the reported FlowNet2, which uses FlowNetC as a sub-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢</head><p>We further improve the training procedures for Sintel and fix an I/O bug for KITTI, both resulting in significant performance gain for the same PWC-Net network architecture. The newly trained model parameters and training protocols will be available on https://github.com/NVlabs/PWC-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK</head><p>Variational approach. Horn and Schunck <ref type="bibr" target="#b19">[20]</ref> pioneer the variational approach to optical flow by coupling the brightness constancy and spatial smoothness assumptions using an energy function. Black and Anandan <ref type="bibr" target="#b20">[21]</ref> introduce a robust framework to deal with outliers, i.e., brightness inconstancy and spatial discontinuities. As it is computationally impractical to perform a full search, a coarse-tofine, warping-based approach is often adopted <ref type="bibr" target="#b21">[22]</ref>. Brox et al. <ref type="bibr" target="#b22">[23]</ref> theoretically justify the warping-based estimation process. The variational approach is the most popular framework for optical flow. However, it requires solving complex optimization problems and is computationally expensive for real-time applications. Sun et al. <ref type="bibr" target="#b23">[24]</ref> review the models, optimization, and implementation details for methods derived from Horn and Schunck. One surprising finding is that the original Horn and Schunck objective, when optimized using modern techniques and implementation details, performs competitively against contemporary state of the art. Thus, it is critical to separate the contributions from the objective and the optimization. Our work shows that it is also critical to separate the contributions from the CNN models and the training procedures.</p><p>One conundrum for the coarse-to-fine approach is small and fast moving objects that disappear at coarse levels. To address this issue, Brox and Malik <ref type="bibr" target="#b24">[25]</ref> embed feature matching into the variational framework, which is further improved by follow-up methods <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. In particular, the EpicFlow method <ref type="bibr" target="#b27">[28]</ref> can effectively interpolate sparse matches to dense optical flow and is widely used as a postprocessing method <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Zweig and Wolf <ref type="bibr" target="#b33">[34]</ref> use CNNs for sparse-to-dense interpolation and obtain consistent improvement over EpicFlow.</p><p>Most top-performing methods use CNNs as a component in their system. For example, DCFlow <ref type="bibr" target="#b16">[17]</ref>, the best published method on MPI Sintel final pass so far, learns CNN features to construct a full cost volume and uses sophisticated post-processing techniques, including EpicFlow, to estimate the optical flow. The next-best method, Flow-FieldsCNN <ref type="bibr" target="#b29">[30]</ref>, learns CNN features for sparse matching and densifies the matches by EpicFlow. The third-best method, MRFlow <ref type="bibr" target="#b34">[35]</ref> uses a CNN to classify a scene into rigid and non-rigid regions and estimates the geometry and camera motion for rigid regions using a plane + parallax formulation. However, none of them are real-time or endto-end trainable.</p><p>Early work on learning optical flow. There is a long history of learning optical flow before the deep learning era. Simoncelli and Adelson <ref type="bibr" target="#b35">[36]</ref> study the data matching errors for optical flow. Freeman et al. <ref type="bibr" target="#b36">[37]</ref> learn parameters of an MRF model for image motion using synthetic blob world examples. Roth and Black <ref type="bibr" target="#b37">[38]</ref> study the spatial statistics of optical flow using sequences generated from depth maps. Sun et al. <ref type="bibr" target="#b38">[39]</ref> learn a full model for optical flow, but the learning has been limited to a few training sequences <ref type="bibr" target="#b11">[12]</ref>. Li and Huttenlocker <ref type="bibr" target="#b39">[40]</ref> use stochastic optimization to tune the parameters for the Black and Anandan method <ref type="bibr" target="#b20">[21]</ref>, but the number of parameters learned is limited. Wulff and Black <ref type="bibr" target="#b40">[41]</ref> learn PCA motion basis of optical flow estimated by GPUFlow <ref type="bibr" target="#b41">[42]</ref> on real movies. Their method is fast but produces over-smoothed flow. Recent work on learning optical flow. Inspired by the success of CNNs on high-level vision tasks <ref type="bibr" target="#b0">[1]</ref>, Dosovitskiy et al. <ref type="bibr" target="#b7">[8]</ref> construct two CNN networks, FlowNetS and FlowNetC, for estimating optical flow based on the U-Net denoising autoencoder <ref type="bibr" target="#b8">[9]</ref>. The networks are pre-trained on a large synthetic FlyingChairs dataset but can surprisingly capture the motion of fast moving objects on the Sintel dataset. The raw output of the network, however, contains large errors in smooth background regions and requires variational refinement <ref type="bibr" target="#b24">[25]</ref>. Mayer et al. <ref type="bibr" target="#b42">[43]</ref> apply the FlowNet architecture to disparity and scene flow estimation. Ilg et al. <ref type="bibr" target="#b9">[10]</ref> stack several basic FlowNet models into a large one, i.e., FlowNet2, which performs on par with state of the art on the Sintel benchmark. Ranjan and Black <ref type="bibr" target="#b10">[11]</ref> develop a compact spatial pyramid network, called SpyNet. SpyNet achieves similar performance as the FlowNetC model on the Sintel benchmark, which is good but not state-of-the-art.</p><p>Another interesting line of research takes the unsupervised learning approach. Memisevic and Hinton <ref type="bibr" target="#b43">[44]</ref> propose the gated restricted Boltzmann machine to learn image transformations in an unsupervised way. Long et al. <ref type="bibr" target="#b44">[45]</ref> learn CNN models for optical flow by interpolating frames. Yu et al. <ref type="bibr" target="#b45">[46]</ref> train models to minimize a loss term that combines a data constancy term with a spatial smoothness term. While inferior to supervised approaches on datasets with labeled training data, existing unsupervised methods can be used to (pre-)train CNN models on unlabeled data <ref type="bibr" target="#b46">[47]</ref>.</p><p>Cost volume. A cost volume stores the data matching costs for associating a pixel with its corresponding pixels at the next frame <ref type="bibr" target="#b13">[14]</ref>. Its computation and processing are standard components for stereo matching, a special case of optical flow. Recent methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b30">[31]</ref> investigate cost volume processing for optical flow. All build the full cost volume at a single scale, which is both computationally expensive and memory intensive. By contrast, our work shows that constructing a partial cost volume at multiple pyramid levels leads to both effective and efficient models.</p><p>Datasets. Unlike many other vision tasks, it is extremely difficult to obtain ground truth optical flow on real-world sequences. Early work on optical flow mainly relies on synthetic datasets <ref type="bibr" target="#b47">[48]</ref>, e.g., the famous "Yosemite". Methods may over-fit to the synthetic data and do not perform well on real data <ref type="bibr" target="#b48">[49]</ref>. Baker et al. <ref type="bibr" target="#b11">[12]</ref> capture real sequences under both ambient and UV lights in a controlled lab environment to obtain ground truth, but the approach does not work for outdoor scenes. Liu et al. <ref type="bibr" target="#b48">[49]</ref> use human annotations to obtain ground truth motion for natural video sequences, but the labeling process is time-consuming.</p><p>KITTI and Sintel are currently the most challenging and widely-used benchmarks for optical flow. The KITTI benchmark is targeted for autonomous driving applications and its semi-dense ground truth is collected using LIDAR <ref type="bibr" target="#b49">[50]</ref>. The 2012 set only consists of static scenes. The 2015 set is extended to dynamic scenes via human annotations and more challenging to existing methods because of the large motion, severe illumination changes, and occlusions <ref type="bibr" target="#b50">[51]</ref>. The Sintel benchmark <ref type="bibr" target="#b51">[52]</ref> is created using the open source graphics movie "Sintel" with two passes, clean and final. The final pass contains strong atmospheric effects, motion blur, and camera noise, which cause severe problems to existing methods. All published, top-performing methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b34">[35]</ref> rely heavily on traditional techniques. PWC-Net is the first fully end-to-end CNN model that outperforms all published methods on both the KITTI 2015 and Sintel final pass benchmarks.</p><p>CNN models for dense prediction tasks in vision. The denoising autoencoder <ref type="bibr" target="#b52">[53]</ref> has been commonly used for dense prediction tasks in computer vision, especially with skip connections <ref type="bibr" target="#b8">[9]</ref> between the encoder and decoder. Recent work <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> shows that dilated convolution layers can better exploit contextual information and refine details for semantic segmentation. Here we use dilated convolutions to integrate contextual information for optical flow and obtain moderate performance improvement. The DenseNet architecture <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b55">[56]</ref> directly connects each layer to every other layer in a feedforward fashion and has been shown to be more accurate and easier to train than traditional CNN layers in image classification tasks. We test this idea for dense optical flow prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>We start from an overview of the network architecture of PWC-Net, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. PWC-Net first builds a feature pyramid from the two input images. At the top level of the pyramid, PWC-Net constructs a cost volume by comparing features of a pixel in the first image with corresponding features in the second image. As the top level is of small spatial resolution, we can construct the cost volume using a small search range. The cost volume and features of the first image are then fed to a CNN to estimate the flow at the top level. PWC-Net then upsamples and rescales the estimated flow to the next level. At the second to top level, PWC-Net warps features of the second image toward the first using the upsampled flow, and then constructs a cost volume using features of the first image and the warped features. As warping compensates the large motion, we can still use a small search range to construct the cost volume. The cost volume, features of the first image, and the upsampeld flow are fed to a CNN to estimate flow at the current level, which is then upsampled to the next (third) level. The process repeats until the desired level.</p><p>As PWC-Net has been designed using classical principles from optical flow, it is informative to compare the key components of PWC-Net with the traditional coarse-to-fine approaches <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> in <ref type="figure">Figure 3</ref>. First, as raw images are variant to shadows and lighting changes <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, we replace the fixed image pyramid with learnable feature pyramids. Second, we take the warping operation from the traditional approach as a layer in our network to estimate large motion. Third, as the cost volume is a more discriminative representation of the optical flow than raw images, our network has a layer to construct the cost volume, which is then processed by CNN layers to estimate the flow. The warping and cost volume layers have no learnable parameters and reduce the model size. Finally, a common practice by the traditional methods is to post-process the optical flow using contextual information, such as median filtering <ref type="bibr" target="#b56">[57]</ref> and bilateral filtering <ref type="bibr" target="#b57">[58]</ref>. Thus, PWC-Net uses a context network to exploit contextual information to refine the optical flow. Compared with energy minimization, the CNN models are computationally more efficient.</p><p>Next, we will explain the main ideas for each component, including pyramid feature extractor, optical flow estimator, and context networks. Please refer to the appendix for details of the networks.</p><p>Feature pyramid extractor. Given two input images I 1 and I 2 , we generate L-level pyramids of feature representations, with the bottom (zeroth) level being the input images, i.e., c 0 t = I t . To generate feature representation at the lth layer, c l t , we use layers of convolutional filters to downsample the features at the l âˆ’1th pyramid level, c lâˆ’1 t , by a factor of 2. From the first to the sixth levels, the number of feature channels are respectively 16, 32, 64, 96, 128, and 192.</p><p>Warping layer. At the lth level, we first upsample by a factor of 2 and rescale the estimated flow from the l+1th level, w l+1 , to the current level. We then warp features of the second image toward the first image using the upsampled flow:</p><formula xml:id="formula_0">c l w (x) = c l 2 x + 2 Ã— up 2 (w l+1 )(x) ,<label>(1)</label></formula><p>where x is the pixel index and up 2 denote the Ã—2 upsampling operator. We use bilinear interpolation to implement the warping operation and compute the gradients to the input CNN features and flow for backpropagation according to <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b58">[59]</ref>. For non-translational motion, warping can compensate for some geometric distortions and put image patches at the right scale. Note that there is no upsampled <ref type="figure">Fig. 3</ref>. Traditional coarse-to-fine approach vs. PWC-Net. Left: Image pyramid and refinement at one pyramid level by the energy minimization approach <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Right: Feature pyramid and refinement at one pyramid level by PWC-Net. PWC-Net warps features of the second image using the upsampled flow, computes a cost volume, and process the cost volume using CNNs. Both post-processing and context network are optional in each system. The arrows indicate the direction of flow estimation and pyramids are constructed in the opposite direction. Please refer to the text for details about the network.</p><p>flow at the top pyramid level and the warped features are the same as features of the second image, i.e., c L w = c L 2 . Cost volume layer. Next, we use the features to construct a cost volume that stores the matching costs for associating a pixel with its corresponding pixels at the next frame <ref type="bibr" target="#b13">[14]</ref>. We define the matching cost as the correlation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref> between features of the first image and warped features of the second image:</p><formula xml:id="formula_1">cv l (x 1 , x 2 ) = 1 N c l 1 (x 1 ) T c l w (x 2 ),<label>(2)</label></formula><p>where T is the transpose operator and N is the length of the column vector c l 1 (x 1 ). For an L-level pyramid setting, we only need to compute a partial cost volume with a limited range of d pixels, i.e., |x 1 âˆ’ x 2 | âˆž â‰¤ d. A one-pixel motion at the top level corresponds to 2 Lâˆ’1 pixels at the full resolution images. Thus we can set d to be small. The dimension of the 3D cost volume is d 2 Ã—H l Ã—W l , where H l and W l denote the height and width of the lth pyramid level, respectively.</p><p>Optical flow estimator. It is a multi-layer CNN. Its input are the cost volume, features of the first image, and upsampled optical flow and its output is the flow w l at the lth level. The numbers of feature channels at each convolutional layers are respectively 128, 128, 96, 64, and 32, which are kept fixed at all pyramid levels. The estimators at different levels have their own parameters instead of sharing the same parameters. This estimation process is repeated until the desired level, l 0 .</p><p>The estimator architecture can be enhanced with DenseNet connections <ref type="bibr" target="#b6">[7]</ref>. The inputs to every convolutional layer are the output of and the input to its previous layer. DenseNet has more direct connections than traditional layers and leads to significant improvement in image classification. We test this idea for dense flow prediction.</p><p>Context network. Traditional flow methods often use contextual information to post-process the flow. Thus we employ a sub-network, called the context network, to effectively enlarge the receptive field size of each output unit at the desired pyramid level. It takes the estimated flow and features of the second last layer from the optical flow estimator and outputs a refined flow,Åµ l0 Î˜ (x). The context network is a feed-forward CNN and its design is based on dilated convolutions <ref type="bibr" target="#b54">[55]</ref>. It consists of 7 convolutional layers. The spatial kernel for each convolutional layer is 3Ã—3. These layers have different dilation constants. A convolutional layer with a dilation constant k means that an input unit to a filter in the layer are k-unit apart from the other input units to the filter in the layer, both in vertical and horizontal directions. Convolutional layers with large dilation constants enlarge the receptive field of each output unit without incurring a large computational burden. From bottom to top, the dilation constants are 1, 2, 4, 8, 16, 1, and 1.</p><p>Training loss. Let Î˜ be the set of all the learnable parameters in our final network, which includes the feature pyramid extractor and the optical flow estimators at different pyramid levels (the warping and cost volume layers have no learnable parameters). Let w l Î˜ denote the flow field at the lth pyramid level predicted by the network, and w l GT the corresponding supervision signal. We use the same multi-scale training loss proposed in FlowNet <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_2">L(Î˜) = L l=l0 Î± l x |w l Î˜ (x)âˆ’w l GT (x)| 2 +Î³|Î˜| 2 2 ,<label>(3)</label></formula><p>where | Â· | 2 computes the L2 norm of a vector and the second term regularizes parameters of the model. Note that if the context network is used at the l 0 th level, w l0 Î˜ will be replaced by the output of the context network,Åµ l0 Î˜ (x). For fine-tuning, we use the following robust training loss:</p><formula xml:id="formula_3">L(Î˜) = L l=l0 Î± l x |w l Î˜ (x)âˆ’w l GT (x)|+ q +Î³|Î˜| 2 2<label>(4)</label></formula><p>where | Â· | denotes the L1 norm, q &lt; 1 gives less penalty to outliers, and is a small constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>Implementation details. The weights in the training loss <ref type="formula" target="#formula_2">(3)</ref> are set to be Î± 6 = 0.32, Î± 5 = 0.08, Î± 4 = 0.02, Î± 3 = 0.01, and Î± 2 = 0.005. The trade-off weight Î³ is set to be 0.0004. We scale the ground truth flow by 20 and downsample it to obtain the supervision signals at different levels. Note that we do not further scale the supervision signal at each level, the same as <ref type="bibr" target="#b7">[8]</ref>. As a result, we need to scale the upsampled flow at each pyramid level for the warping layer.</p><p>For example, at the second level, we scale the upsampled flow from the third level by a factor of 5 (= 20/4) before warping features of the second image. We use a 7-level pyramid (L = 6), consisting of 6 levels of CNN features and the input images as the bottom level. We set the desired level l 0 to be 2, i.e., our model outputs a quarter resolution optical flow and uses bilinear interpolation to obtain the full-resolution optical flow. We use a search range of 4 pixels to compute the cost volume at each level. We first train the models using the FlyingChairs dataset in Caffe <ref type="bibr" target="#b59">[60]</ref> using the S long learning rate schedule introduced in <ref type="bibr" target="#b9">[10]</ref>, i.e., starting from 0.0001 and reducing the learning rate by half at 0.4M, 0.6M, 0.8M, and 1M iterations. The data augmentation scheme is the same as that in <ref type="bibr" target="#b9">[10]</ref>. We crop 448 Ã— 384 patches during data augmentation and use a batch size of 8. We then fine-tune the models on the FlyingThings3D dataset using the S f ine schedule <ref type="bibr" target="#b9">[10]</ref> while excluding image pairs with extreme motion (magnitude larger than 1000 pixels). The cropped image size is 768Ã—384 and the batch size is 4. Finally, we fine-tune the models using the Sintel and KITTI training sets and will explain the details below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">MPI Sintel.</head><p>When fine-tuning on Sintel, we crop 768 Ã— 384 image patches, add horizontal flip, and do not add additive Gaussian noise during data augmentation. The batch size is 4.</p><p>We use the robust loss function in Eq. (4) with = 0.01 and q = 0.4. We disrupt the learning rate, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, which empirically improves both the training and test performance. We test two schemes of fine-tuning. The first one, PWC-Net-ft, uses the clean and final passes of the Sintel training data throughout the fine-tuning process. The second one, PWC-Net-ft-final, uses only the final pass for the second half of fine-tuning. We test the second scheme because DCFlow learns the features using only the final pass of the training data. Thus we test the performance of PWC-Net when the final pass of the training data is given more weight. We refer to the latter scheme as our training protocol I, which we will use later to train other models.</p><p>PWC-Net has lower average end-point error (EPE) than many recent methods on the final pass of the MPI-Sintel benchmark ( <ref type="table">Table 1)</ref>. Further, PWC-Net is the fastest among all the top-performing methods <ref type="figure" target="#fig_0">(Fig. 1</ref>). We can further reduce the running time by dropping the DenseNet connections. The resulting PWC-Net-small model is about 5% less accurate but 40% faster than PWC-Net.</p><p>PWC-Net is less accurate than traditional approaches on the clean pass. Traditional methods often use image edges to refine motion boundaries, because the two are perfectly aligned in the clean pass. However, image edges in the final pass are corrupted by motion blur, atmospheric changes, and noise. Thus, the final pass is more realistic and challenging. The results on the final and clean sets suggest that PWC-Net may be better suited for real images, where the image edges are often corrupted.</p><p>PWC-Net has higher errors on the training set but lower errors on the test set than FlowNet2, suggesting that PWC-Net may have a more appropriate capacity for this task. <ref type="table">Table 2</ref> summarizes errors in different regions. PWC-Net performs relatively better in regions with large motion and away from the motion boundaries, probably because it has been trained using only data with large motion. <ref type="figure" target="#fig_5">Figure 5</ref> shows the visual results of different variants of PWC-Net on the training and test sets of MPI Sintel. PWC-Net can recover sharp motion boundaries but may fail on small and rapidly moving objects, such as the left arm in "Market 5".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">KITTI.</head><p>When fine-tuning on KITTI, we crop 896 Ã— 320 image patches and reduce the amount of rotation, zoom, and squeeze during data augmentation. The batch size is 4 too. The large patches can capture the large motion in the KITTI dataset. Since the ground truth is semi-dense, we upsample the predicted flow at the quarter resolution to compare with the scaled ground truth at the full resolution. We exclude the invalid pixels in computing the loss function.</p><p>The CVPR version of PWC-Net outperforms many recent two-frame optical flow methods on the 2015 set, as shown in <ref type="table" target="#tab_7">Table 3</ref>. On the 2012 set, PWC-Net is inferior to SDF that assumes a rigidity constraint for the background. Although the rigidity assumption works well on the static scenes in the 2012 set, PWC-Net outperforms SDF in the 2015 set which mainly consists of dynamic scenes and is more challenging. The visual results in <ref type="figure" target="#fig_6">Fig. 6</ref> qualitatively demonstrate the benefits of using the context network, DenseNet connections, and fine-tuning, respectively. In par-          An I/O bug. We use the Caffe code <ref type="bibr" target="#b7">[8]</ref> to make all the image pairs and flow fields into a single LMDB file for training. The code requires that all the input images are of the same resolution. The size of the first 156 sequences of KITTI 2015 is 375 Ã— 1242, but the last 44 are of different resolutions, including 370Ã—1224, 374Ã—1238, and 376Ã—1241. The Caffe code cannot read the last 44 sequences properly, as shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. As a results, PWC-Net has been trained using only 156 "good" sequences and 44 "bad" ones. As a remedy, we crop all the sequences to the size of 370 Ã— 1224, because there is no reliable way to resize the sparse ground truth. Re-training with the correct 200 sequences leads to about 20% improvement on the test set of KITTI 2012 (Fl-Noc 4.22% â†’ 3.41%) and 2015 (Fl-all 9.60% â†’ 7.90%). At the time of writing, PWC-Net is ranked second in nonoccluded regions among all methods on KITTI 2015. It is surpassed by only one recent scene flow method that uses <ref type="bibr">TABLE 1</ref> Average EPE results on MPI Sintel set. "-ft" means fine-tuning on the MPI Sintel training set and the numbers in the parenthesis are results on the data the methods have been fine-tuned on. ft-final gives more weight to the final pass during fine-tuning. FlowNetC2 has been trained using the same procedure as PWC-Net-ft-final. stereo input and semantic information <ref type="bibr" target="#b18">[19]</ref> (Fl-all scores: 5.07% vs 4.69%) and more accurate than other scene flow methods, such as another recent one that also uses semantic information <ref type="bibr" target="#b63">[64]</ref>. Note that scene flow methods can use the estimated depth and the camera motion to predict the flow of out-of-boundary pixels and thus tend to have better accuracy in all regions.  images and flow fields to 768 Ã— 320. For the mixed datasets, we use more iterations and learning rate disruptions, as shown in <ref type="figure">Fig. 8</ref>. <ref type="figure">Fig. 8</ref>. Learning rate schedule for fine-tuning using data from Sintel, KITTI, and HD1K. For this mixed dataset, we use more iterations and learning rate disruptions than the learning rate schedule in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test) First frame (test)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft PWC-Net-KITTI-ft</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Robust Vision Challenge</head><p>Using mixed datasets increases the test error on KITTI 2015 (F-all 9.60% â†’ 11.63%) but reduces the test error on MPI Sintel final (AEPE 5.04% â†’ 4.9). There is a larger mismatch between the training and test data of Sintel than those of KITTI 2015. Thus, using more diverse datasets reduces the over-fitting errors on Sintel. We further use a batch size of 4, with 2 image pairs from Sintel, 1 from KITTI, and 1 from HD1K respectively, which is our training procotol II. It results in a further performance gain, i.e., PWC-Net+ in <ref type="table">Table 2</ref>.</p><p>The Middlebury images are of lower resolution and we upsample them so that the larger of the width and height of the upsampled image is around 1000 pixels. PWC-Net ROB has similar performance as the Classic+NL method (avg. training EPE 0.24 vs. 0.22; avg. test EPE 0.33 vs 0.32).</p><p>PWC-Net ROB is ranked first on the HD1K benchmark <ref type="bibr" target="#b64">[65]</ref>, which consists of real-world images corrupted by rain, glare, and windshield wipers etc.. The 2560 Ã— 1080 res- olution images causes out-of-memory issue on an NVIDIA Pascal TitanX GPU with 12GB memory and requires an NVIDIA Volta 100 GPU with 16GB memory. <ref type="figure">Figure 9</ref> shows some visual results on Middlebury and HD1K test set. Despite minor artifacts, PWC-Net ROB performs robustly across these benchmarks using the same set of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Experiments</head><p>Feature pyramid extractor. PWC-Net uses a two-layer CNN to extract features at each pyramid level. <ref type="table" target="#tab_10">Table 5a</ref> summarizes the results of two variants that use one layer (â†“) and three layers (â†‘) respectively. A larger-capacity feature pyramid extractor leads to consistently better results on both the training and validation datasets. Replacing the feature pyramids with image pyramids results in about 40% loss in accuracy, confirming the benefits of learning features. To further understand the effect of the pyramids, we test feature pyramids with different levels, as shown in <ref type="table" target="#tab_10">Table 5c</ref>. Using 5-level pyramids leads to consistently worse results. Using 6-level pyramids has better performance than the default 7-level pyramids when trained on FlyingChairs, but the two have close performance after fine-tuning using FlyingThings3D. One possible reason is that the cropping size for FlyingChairs (448 Ã— 384) is too small for the 7-level pyramids. The size of the top level is 7 Ã— 6, too small for a search range of 4 pixels. By contrast, the cropping size for FlyingThings3D (768 Ã— 384) is better suited for the 7-levelpyramids.</p><p>Optical flow estimator. PWC-Net uses a five-layer CNN in the optical flow estimator at each level. <ref type="table" target="#tab_10">Table 5d</ref> shows the results by two variants that use four layer (â†“) and seven layers (â†‘) respectively. A larger-capacity optical flow estimator leads to better performance. However, we observe in our experiments that a deeper optical flow estimator might get stuck at poor local minima, which can be detected by checking the validation errors after a few thousand iterations and fixed by running from a different random initialization.</p><p>Removing the context network results in larger errors on both the training and validation sets <ref type="table" target="#tab_10">(Table 5e</ref>). Removing the DenseNet connections results in higher training error but lower validation errors when the model is trained on FlyingChairs. However, after the model is fine-tuned on FlyingThings3D, DenseNet leads to lower errors.</p><p>We also test a residual version of the optical flow estimator, which estimates a flow increment and adds it to the initial flow to obtain the refined flow. As shown in <ref type="table" target="#tab_10">Table 5h</ref>, this residual version slightly improves the performance.</p><p>Cost volume. We test the search range to compute the cost volume, shown in <ref type="table" target="#tab_10">Table 5b</ref>. Removing the cost volume results in consistent worse results. A larger range leads to lower training error. However, all three settings have similar performance on Sintel, because a range of 2 at every level can already deal with a motion up to 200 pixels at the input resolution. A larger range has lower EPE on KITTI, likely because the images from the KITTI dataset have larger displacements than those from Sintel. A smaller range, however, seems to force the network to ignore pixels with extremely large motion and focus more on small-motion pixels, thereby achieving lower Fl-all scores.  Warping. Warping allows for estimating a small optical flow (increment) at each pyramid level to deal with a large optical flow. Removing the warping layers results in a significant loss of accuracy <ref type="table" target="#tab_10">(Table 5g</ref>). Without the warping layer, PWC-Net still produces reasonable results, because the default search range of 4 to compute the cost volume is large enough to capture the motion of most sequences at the low-resolution pyramid levels.</p><p>Independent Runs. To test the robustness to the initializations, we train PWC-Net with different runs. These independent runs have almost the same training error but some minor differences in performance on the validation sets, as shown in <ref type="table" target="#tab_10">Table 5f</ref>.</p><p>Dataset scheduling. We also train PWC-Net using different dataset scheduling schemes, as shown in <ref type="table" target="#tab_10">Table 5</ref>. Sequentially training on FlyingChairs, FlyingThings3D, and Sintel gradually improves the performance, consistent with the observations in <ref type="bibr" target="#b9">[10]</ref>. Directly training using the test data leads to good "over-fitting" results, but the trained model does not perform as well on other datasets.</p><p>Model size and running time. <ref type="table" target="#tab_12">Table 6</ref> summarizes the The timings have been obtained on the same desktop with an NVIDIA Pascal TitanX GPU. For more precise timing, we exclude the reading and writing time when benchmarking the forward and backward inference time. PWC-Net is about 2 times faster in forward inference and at least 3 times faster in training than FlowNet2.   <ref type="figure" target="#fig_0">Fig. 10</ref>. Training procedure matters. FlowNetC and FlowNetC+ use the same network architecture but have been trained differently. FlowNetC+ has been trained using our procedure and generates results with finer details and fewer artifacts than the previously trained FlowNetC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Matters</head><p>We used the same architecture in the first <ref type="bibr" target="#b65">[66]</ref> and second (CVPR) versions of our arXiv paper but observed an about 10% improvement on the final pass of the Sintel test set. The performance improvement results solely from changes in the training procedures, including performing horizontal flips, not adding additive Gaussian noise, and disrupting the learning rate. One questions arises: how do other models perform using the same training procedure as PWC-Net? To better understand the effects of models and training and fairly compare with existing methods, we re-train the FlowNetS and FlowNetC models using exactly the same training procedure as PWC-Net, including the robust training loss function. We name the retrained model as FlowNetS+ and FlowNetC+ respectively and evaluate them using the test set of Sintel, as summarized in <ref type="table">Table 1</ref>. <ref type="figure" target="#fig_0">Figure 10</ref> shows the visual results FlowNetC trained using different training protocols. The results by FlowNetC+ have fewer artifacts and are more piece-wise smooth than the reviously trained FlowNetC. As shown in <ref type="table">Table 1</ref>, FlowNetC+ is about 8% less accurate on Sintel final and 3 times larger in model size than PWC-Net , which demonstrates the benefit of the new network architecture under the same training procedure.</p><p>To our surprise, FlowNetC+ is about 5% more accurate than the published FlowNet2 model on the final pass, because FlowNet2 uses FlowNetC as a sub-network. We should note that this is not a fair comparison for FlowNet2, because we are unable to apply the same training protocol to the FlowNet2 model, which requires sequential training of several sub-networks. It is expected that a more careful, customized training schemes would improve the performance of FlowNet2.</p><p>It is often assumed or taken for granted that the results published by authors represent the best possible performance of a method. However, our results show that we should carefully evaluate published methods to "identify the source of empirical gains" <ref type="bibr" target="#b66">[67]</ref>. When we observe improvements over previous models, it is critical to analyze whether the gain results from the model or the training procedure. It would be informative to evaluate models trained in the same way or compare training procedures using the same model. To enable fair comparisons and further innovations, we will make our training protocols available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COMPARISON WITH CLOSELY-RELATED WORK</head><p>As the field is changing rapidly, it is informative to do a detailed discussion of the closely-related work. Both FlowNet2 <ref type="bibr" target="#b9">[10]</ref> and SpyNet <ref type="bibr" target="#b10">[11]</ref> have been designed using principles from stereo and optical flow. However, the architecture of PWC-Net has significant differences.</p><p>SpyNet uses image pyramids while PWC-Net learns feature pyramids. FlowNet2 uses three-level feature pyramids in the first module of its whole network, i.e., FlowNetC. By contrast, PWC-Net uses much deeper feature pyramids. As analyzed in the ablation study, using deeper feature pyramids usually leads to better performance. Both SpyNet and FlowNet2 warp the input images, while PWC-Net warps the features, which enables the information to propagate throughout the whole feature pyramids.</p><p>SpyNet feeds CNNs with images, while PWC-Net feeds a cost volume. As the cost volume is a more discriminative representation of the search space for optical flow, the learning task for CNNs becomes easier. FlowNet2/FlowNetC constructs the cost volume at a single resolution with a large search range. However, using features at a fixed resolution may not be effective at resolving the well-known "aperture problem" <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>. By contrast, PWC-Net constructs multi-resolution cost volume and reduces the computation using a small search range.</p><p>Regarding performance, PWC-Net outperforms SpyNet by a significant margin. Additionally, SpyNet has been trained sequentially, while PWC-Net can be trained endto-end from scratch. FlowNet2 achieves impressive performance by stacking several basic models into a large-capacity model. The much smaller PWC-Net obtains similar or better performance by embedding classical principles into the network architecture. It would be interesting to use PWC-Net as a building block to design large networks.</p><p>Two recent papers also incorporate domain knowledge of flow into the CNN architectures. LiteFlowNet <ref type="bibr" target="#b70">[71]</ref> uses similar ideas as PWC-Net, including feature pyramids, warping features, and constructing a cost volume with a limited search range at multiple resolutions. LiteFlowNet furthers incorporates a flow regularization layer to deal with outliers using a feature-driven local convolutions. However, LiteFlowNet requires sequential (stage-wise) training. The CVPR final version of LiteFlowNet (March. 2018) is about 8% less accurate on Sintel final than the first arXiv version of PWC-Net <ref type="bibr" target="#b65">[66]</ref> published in Sep. 2017 (avg. EPE 6.09 vs. 5.63). In an updated arXiv version <ref type="bibr" target="#b71">[72]</ref> published in May 2018, LiteFlowNet uses similar data augmentation schemes as the CVPR final version of PWC-Net, e.g., not adding Gaussian noise, horizontal flipping (image mirroring), and reducing the spatial data augmentation for KITTI. With these changes, LiteFlowNet reports performance close to PWC-Net on Sintel final (avg. EPE: 5.33 vs 5.04) and KITTI 2015 (F-all: 9.38% vs. 9.60%). This further confirms the importance of training in obtaining top performance.</p><p>Another paper, TVNet <ref type="bibr" target="#b72">[73]</ref>, subsumes a specific optical flow solver, the TV-L1 method <ref type="bibr" target="#b73">[74]</ref>, and is initialized by unfolding its optimization iterations as neural layers. TVNet is used to learn rich and task-specific patterns and obtains excellent performance on activity classification. The readers are urged to read these papers to better understand the similarities and differences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We have developed a compact but effective CNN model for optical flow estimation using simple and well-established principles: pyramidal processing, warping, and cost volume processing. Combining deep learning with domain knowledge not only reduces the model size but also improves the performance. PWC-Net is about 17 times smaller in size, 2 times faster in inference, easier to train, and 11% more accurate on Sintel final than the recent FlowNet2 model. It performs robustly across four different benchmarks using the same set of parameters and is the winning entry in the optical flow competition of the robust vision challenge.</p><p>We have also shown that the performance gains of PWC-Net result from both the new model architecture and the training procedures. Retrained using our procedures, FlowNetC is even 5% more accurate on Sintel final than the published FlowNet2, which uses FlowNetC as a subnetwork. We have further improved the training procedures, which increase the accuracy of PWC-Net on Sintel by 10% and on KITTI 2012 and 2015 by 20%. The results show the complicated interplay between models and training and call for careful experimental designs to identify the sources of empirical gains. To enable comparison and further innovations, we will make the retrained models and training protocols available on https://github.com/NVlabs/PWC-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>In this appendix, we proivde more details about PWC-Net. <ref type="figure" target="#fig_0">Figure 11</ref> shows the architecture for the 7-level feature pyramid extractor network used in our experiment. Note that the bottom level consists of the original input images. <ref type="figure" target="#fig_0">Figure 12</ref> shows the optical flow estimator network at pyramid level 2. The optical flow estimator networks at other levels have the same structure except for the top level, which does not have the upsampled optical flow and directly computes cost volume using features of the first and second images. <ref type="figure" target="#fig_0">Figure 13</ref> shows the context network that is adopted only at pyramid level 2. <ref type="figure" target="#fig_0">Fig. 11</ref>. The feature pyramid extractor network. The first image (t = 1) and the second image (t = 2) are encoded using the same Siamese network. Each convolution is followed by a leaky ReLU unit. The convolutional layer and the Ã—2 downsampling layer at each level is implemented using a single convolutional layer with a stride of 2. c l t denotes extracted features of image t at level l; <ref type="figure" target="#fig_0">Fig. 12</ref>. The optical flow estimator network at pyramid level 2. Each convolutional layer is followed by a leaky ReLU unit except the last (light green) one that outputs the optical flow. <ref type="figure" target="#fig_0">Fig. 13</ref>. The context network at pyramid level 2. Each convolutional layer is followed by a leaky ReLU unit except the last (light green) one that outputs the optical flow. The last number in each convolutional layer denotes the dilation constant.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>frames per second (fps) on Sintel resolution (1024 Ã— 436) images. It is the winning entry in the optical flow category of the first robust vision challenge. arXiv:1809.05571v1 [cs.CV] 14 Sep 2018 Left: PWC-Net outperforms all published methods on the MPI Sintel final pass benchmark in both accuracy and running time. Right: compared with previous end-to-end CNN models for flow, PWC-Net reaches the best balance between accuracy and size. The comparisons among PWC-Net, FlowNetS+, and FlowNetC+ show the improvements brought by the network architectures; all have been trained using the same training protocols. The comparisons, FlowNetS vs. FlowNetS+, FlowNetC vs. FlowNetC+, and PWC-Net vs. PWC-Net+, show the improvements brought by training protocols. Both models and training matter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Network architecture of PWC-Net. We only show the flow estimation modules at the top two levels. For the rest of the pyramidal levels, the flow estimation modules have the same structure as the second to top level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Top: learning rate schedule for fine-tuning (the step values for the first 10 5 iterations were provided by Eddy Ilg). Bottom: average endpoint error (EPE) on the final pass of the Sintel training set. We disrupt the learning rate for a better minimum, which has better accuracy in both the training and the test sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Frame</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net-Sintel-ft PWC-Net-Sintel-ft PWC-Net-Sintel-ft PWC-Net-Sintel-ft PWC-Net--Sintel-ft PWC-Net-Sintel-ft PWC-Net-Sintel-ft PWC-Net-Sintel-ft PWC-Net-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>PWC-Net PWC-Net PWC-Net PWC-Net PWC-Net-Sintel-ft PWC-Net-Sintel-ft PWC-Net-Sintel-ft PWC-Net-Sintel-ft PWC-Net--Sintel-ft PWC-Net-Sintel-ft PWC-Net-Sintel-ft PWC-Net-Sintel-ft PWC-Net-Sintel-ft Results on Sintel training and test sets. Context network, DenseNet connections, and fine-tuning all improve the results. Small and rapidly moving objects, e.g., the left arm in "Market 5", are still challenging to the pyramid-based PWC-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Results on KITTI 2015 training and test sets. Fine-tuning fixes large regions of errors and recovers sharp motion boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Improperly read images and flow fields due to an I/O bug.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>First</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Frame</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame</head><label></label><figDesc></figDesc><table><row><cell>Second frame</cell><cell>W/o context W/o context W/o context W/o context W/o context W/o context W/o context W/o context W/o context W/o context W/o context W/o context W/o context W/o context W/o context W/o context W/o context</cell></row><row><cell>W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet W/o DenseNet</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ticular, fine-tuning fixes large regions of errors in the test set, demonstrating the benefit of learning when the training and test data share similar statistics.</figDesc><table><row><cell>First frame First frame First frame First frame First frame First frame First frame First frame First frame First frame First frame First frame First frame First frame First frame First frame First frame</cell><cell>Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth Ground truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3</head><label>3</label><figDesc>Results on the KITTI dataset. "-ft" means fine-tuning on the KITTI training set and the numbers in the parenthesis are results on the data the methods have been fine-tuned on.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">KITTI 2012</cell><cell></cell><cell cols="2">KITTI 2015</cell></row><row><cell>Methods</cell><cell cols="5">AEPE AEPE Fl-Noc AEPE Fl-all</cell><cell>Fl-all</cell></row><row><cell></cell><cell cols="2">train test</cell><cell>test</cell><cell cols="2">train train</cell><cell>test</cell></row><row><cell>EpicFlow [28]</cell><cell>-</cell><cell cols="2">3.8 7.88%</cell><cell>-</cell><cell>-</cell><cell>26.29 %</cell></row><row><cell>FullFlow [31]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>23.37 %</cell></row><row><cell>CPM-flow [32]</cell><cell>-</cell><cell cols="2">3.2 5.79%</cell><cell>-</cell><cell>-</cell><cell>22.40 %</cell></row><row><cell>PatchBatch [61]</cell><cell>-</cell><cell cols="2">3.3 5.29%</cell><cell>-</cell><cell>-</cell><cell>21.07%</cell></row><row><cell>FlowFields [62]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>19.80%</cell></row><row><cell>MRFlow [35]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">14.09 % 12.19 %</cell></row><row><cell>DCFlow [17]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">15.09 % 14.83 %</cell></row><row><cell>SDF [29]</cell><cell>-</cell><cell cols="2">2.3 3.80%</cell><cell>-</cell><cell>-</cell><cell>11.01 %</cell></row><row><cell>MirrorFlow [63]</cell><cell>-</cell><cell cols="2">2.6 4.38%</cell><cell>-</cell><cell cols="2">9.93% 10.29%</cell></row><row><cell>SpyNet-ft [11]</cell><cell cols="3">(4.13) 4.7 12.31%</cell><cell>-</cell><cell>-</cell><cell>35.07%</cell></row><row><cell>FlowNet2 [10]</cell><cell>4.09</cell><cell>-</cell><cell>-</cell><cell cols="2">10.06 30.37%</cell><cell>-</cell></row><row><cell>FlowNet2-ft [10]</cell><cell cols="6">(1.28) 1.8 4.82% (2.30) (8.61%) 10.41 %</cell></row><row><cell cols="3">LiteFlowNet-CVPR (1.26) 1.7</cell><cell>-</cell><cell cols="3">(2.16) (8.16%) 10.24 %</cell></row><row><cell cols="7">LiteFlowNet-arXiv (1.05) 1.6 3.27% (1.62) (5.58%) 9.38 %</cell></row><row><cell>PWC-Net</cell><cell>4.14</cell><cell>-</cell><cell>-</cell><cell cols="2">10.35 33.67%</cell><cell>-</cell></row><row><cell cols="7">PWC-Net-ft-CVPR (1.45) 1.7 4.22% (2.16) (9.80%) 9.60%</cell></row><row><cell>PWC-Net-ft</cell><cell cols="6">(1.08) 1.5 3.41% (1.45) (7.59%) 7.90%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4</head><label>4</label><figDesc>Ablation experiments. Unless explicitly stated, the models have been trained on the FlyingChairs dataset.</figDesc><table><row><cell></cell><cell>Chairs</cell><cell cols="6">Sintel Sintel Clean Final AEPE Fl-all AEPE Fl-all KITTI 2012 KITTI 2015</cell><cell></cell><cell>Max. Disp.</cell><cell cols="2">Chairs</cell><cell>Sintel Sintel Clean Final AEPE Fl-all AEPE Fl-all KITTI 2012 KITTI 2015</cell></row><row><cell cols="2">Full model 2.00</cell><cell></cell><cell>3.33</cell><cell>4.59</cell><cell cols="3">5.14 28.67% 13.20 41.79%</cell><cell></cell><cell>0</cell><cell>2.13</cell><cell></cell><cell>3.66</cell><cell>5.09</cell><cell>5.25 29.82% 13.85 43.52%</cell></row><row><cell>Feature â†‘</cell><cell>1.92</cell><cell></cell><cell>3.03</cell><cell>4.17</cell><cell cols="3">4.57 26.73% 11.64 39.80%</cell><cell></cell><cell>2</cell><cell>2.09</cell><cell></cell><cell>3.30</cell><cell>4.50</cell><cell>5.26 25.99% 13.67 38.99%</cell></row><row><cell>Feature â†“</cell><cell>2.18</cell><cell></cell><cell>3.36</cell><cell>4.56</cell><cell cols="3">5.75 30.79% 14.05 44.92%</cell><cell></cell><cell cols="2">Full model (4) 2.00</cell><cell></cell><cell>3.33</cell><cell>4.59</cell><cell>5.14 28.67% 13.20 41.79%</cell></row><row><cell>Image</cell><cell>2.95</cell><cell></cell><cell>4.42</cell><cell>5.58</cell><cell cols="3">7.28 31.25% 16.29 45.13%</cell><cell></cell><cell>6</cell><cell>1.97</cell><cell></cell><cell>3.31</cell><cell>4.60</cell><cell>4.96 27.05% 12.97 40.94%</cell></row><row><cell cols="8">(a) Larger-capacity feature pyramid extractor has better performance.</cell><cell cols="5">(b) Cost volume. Removing the cost volume (0) results in moderate</cell></row><row><cell cols="8">Learning features leads to significantly better results than fixed image</cell><cell cols="5">performance loss. PWC-Net can handle large motion using a small search</cell></row><row><cell>pyramids.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">range to compute the cost volume.</cell></row><row><cell></cell><cell cols="7">Trained on FlyingChairs Fine-tuned on FlyingThings Chairs Clean Final Chairs Clean Final</cell><cell></cell><cell></cell><cell cols="3">Chairs</cell><cell>Sintel Sintel Clean Final AEPE Fl-all AEPE Fl-all KITTI 2012 KITTI 2015</cell></row><row><cell>5-level</cell><cell cols="2">2.13</cell><cell>3.28</cell><cell>4.52</cell><cell>2.62</cell><cell>2.98</cell><cell>4.29</cell><cell></cell><cell>Full model</cell><cell cols="2">2.00</cell><cell>3.33</cell><cell>4.59</cell><cell>5.14 28.67% 13.20 41.79%</cell></row><row><cell>6-level</cell><cell cols="2">1.95</cell><cell>2.96</cell><cell>4.32</cell><cell>2.28</cell><cell>2.50</cell><cell>3.97</cell><cell></cell><cell cols="3">Estimator â†‘ 1.92</cell><cell>3.09</cell><cell>4.50</cell><cell>4.64 25.34% 12.25 39.18%</cell></row><row><cell cols="3">Full model (7) 2.00</cell><cell>3.33</cell><cell>4.59</cell><cell>2.30</cell><cell>2.55</cell><cell>3.93</cell><cell></cell><cell cols="3">Estimator â†“ 2.01</cell><cell>3.37</cell><cell>4.58</cell><cell>4.82 26.35% 12.83 40.53%</cell></row><row><cell cols="8">(c) More feature pyramid levels help after fine-tuning on FlyingThings.</cell><cell></cell><cell cols="4">(d) Larger-capacity optical flow estimator has better performance.</cell></row><row><cell></cell><cell cols="7">Trained on FlyingChairs Fine-tuned on FlyingThings Chairs Clean Final Chairs Clean Final</cell><cell></cell><cell cols="2">Chairs</cell><cell cols="2">Sintel Sintel Clean Final AEPE Fl-all AEPE Fl-all KITTI 2012 KITTI 2015</cell></row><row><cell>Full model</cell><cell>2.00</cell><cell></cell><cell>3.33</cell><cell>4.59</cell><cell>2.34</cell><cell>2.60</cell><cell>3.95</cell><cell></cell><cell>1st run</cell><cell>2.00</cell><cell></cell><cell>3.33</cell><cell>4.59</cell><cell>5.14 28.67% 13.20 41.79%</cell></row><row><cell cols="2">No DenseNet 2.06</cell><cell></cell><cell>3.09</cell><cell>4.37</cell><cell>2.48</cell><cell>2.83</cell><cell>4.08</cell><cell></cell><cell cols="2">2nd run 2.00</cell><cell></cell><cell>3.23</cell><cell>4.36</cell><cell>4.70 25.52% 12.57 39.06%</cell></row><row><cell>No Context</cell><cell>2.23</cell><cell></cell><cell>3.47</cell><cell>4.74</cell><cell>2.55</cell><cell>2.75</cell><cell>4.13</cell><cell></cell><cell>3rd run</cell><cell>2.00</cell><cell></cell><cell>3.33</cell><cell>4.65</cell><cell>4.81 27.12% 13.10 40.84%</cell></row><row><cell cols="9">(e) Context network consistently helps; DenseNet helps after fine-tuning</cell><cell cols="4">(f) Independent runs with different initializations lead to minor</cell></row><row><cell cols="2">on FlyingThings.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">performance differences.</cell></row><row><cell></cell><cell>Chairs</cell><cell></cell><cell cols="5">Sintel Sintel Clean Final AEPE Fl-all AEPE Fl-all KITTI 2012 KITTI 2015</cell><cell></cell><cell cols="2">Chairs</cell><cell cols="2">Sintel Sintel Clean Final AEPE Fl-all AEPE Fl-all KITTI 2012 KITTI 2015</cell></row><row><cell>Full model</cell><cell>2.00</cell><cell></cell><cell>3.33</cell><cell>4.59</cell><cell cols="3">5.14 28.67% 13.20 41.79%</cell><cell></cell><cell cols="2">Full model 2.00</cell><cell></cell><cell>3.33</cell><cell>4.59</cell><cell>5.14 28.67% 13.20 41.79%</cell></row><row><cell cols="2">No warping 2.17</cell><cell></cell><cell>3.79</cell><cell>5.30</cell><cell cols="3">5.80 32.73% 13.74 44.87%</cell><cell></cell><cell>Residual</cell><cell>1.96</cell><cell></cell><cell>3.14</cell><cell>4.43</cell><cell>4.87 27.74% 12.58 41.16%</cell></row><row><cell cols="8">(g) Warping layer is a critical component for the performance.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(h) Residual connections in the optical flow estimator are helpful.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 First frame of HD1K sequence 19 Second</head><label></label><figDesc>Results on Middlebury and HD1K test sets. PWC-Net ROB has not been trained using the training data of Middlebury but performs reasonably well on the test set. It cannot recover the fine motion details of the twigs in Grove though. PWC-Net ROB has reasonable results in the regions occluded by the windshield wipers in sequence 02 of the HD1K test set.</figDesc><table><row><cell>First frame of Teddy</cell><cell>Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame</cell><cell>PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB</cell></row><row><cell>First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove First frame of Grove</cell><cell>Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame</cell><cell>PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB</cell></row><row><cell>First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02 First frame of HD1K sequence 02</cell><cell>Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame</cell><cell>PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB</cell></row><row><cell></cell><cell>frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame Second frame</cell><cell>PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB PWC-Net ROB</cell></row><row><cell>Fig. 9.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5 Training dataset schedule leads</head><label>5</label><figDesc>to better local minima. () indicates results on the dataset the method has been trained on.</figDesc><table><row><cell>Data</cell><cell cols="4">Chairs Sintel (AEPE) KITTI 2012 AEPE Clean Final AEPE Fl-all AEPE Fl-all KITTI 2015</cell></row><row><cell>Chairs</cell><cell cols="2">(2.00) 3.33</cell><cell>4.59</cell><cell>5.14 28.67% 13.20 41.79%</cell></row><row><cell>Chairs-Things</cell><cell>2.30</cell><cell>2.55</cell><cell>3.93</cell><cell>4.14 21.38% 10.35 33.67%</cell></row><row><cell cols="5">Chairs-Things-Sintel 2.56 (1.70) (2.21) 2.94 12.70% 8.15 24.35%</cell></row><row><cell>Sintel</cell><cell cols="4">3.69 (1.86) (2.31) 3.68 16.65% 10.52 30.49%</cell></row><row><cell cols="5">model size for different CNN models. PWC-Net has about</cell></row><row><cell cols="5">17 times fewer parameters than FlowNet2. PWC-Net-small</cell></row><row><cell cols="5">further reduces this by an additional 2 times via dropping</cell></row><row><cell cols="5">DenseNet connections and is more suitable for memory-</cell></row><row><cell cols="2">limited applications.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>FlowNetC+ FlowNetC+ FlowNetC+ Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market 1 Frame 11 of Market</head><label></label><figDesc></figDesc><table><row><cell>3 FlowNetC+ FlowNetC+ 1 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 Frame 16 FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 Frame 12 FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+ FlowNetC+</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 6 Model size and running time.</head><label>6</label><figDesc>PWC-Net-small drops DenseNet connections. For training, the lower bound of 14 days for FlowNet2 is obtained by 6(FlowNetC) + 2Ã—4 (FlowNetS). The inference time is for 1024 Ã— 448 resolution images.</figDesc><table><row><cell>Methods</cell><cell cols="6">FlowNetS FlowNetC FlowNet2 SpyNet PWC-Net PWC-Net-small</cell></row><row><cell cols="2">#parameters (M) 38.67</cell><cell>39.17</cell><cell>162.49</cell><cell>1.2</cell><cell>8.75</cell><cell>4.08</cell></row><row><cell cols="2">Parameter Ratio 23.80%</cell><cell>24.11%</cell><cell>100%</cell><cell cols="2">0.74% 5.38%</cell><cell>2.51%</cell></row><row><cell>Memory (MB)</cell><cell>154.5</cell><cell>156.4</cell><cell>638.5</cell><cell>9.7</cell><cell>41.1</cell><cell>22.9</cell></row><row><cell>Memory Ratio</cell><cell>24.20%</cell><cell>24.49%</cell><cell>100%</cell><cell cols="2">1.52% 6.44%</cell><cell>3.59%</cell></row><row><cell>Training (days)</cell><cell>4</cell><cell>6</cell><cell>&gt;14</cell><cell>-</cell><cell>4.8</cell><cell>4.1</cell></row><row><cell>Forward (ms)</cell><cell>11.40</cell><cell>21.69</cell><cell>84.80</cell><cell>-</cell><cell>28.56</cell><cell>20.76</cell></row><row><cell>Backward (ms)</cell><cell>16.71</cell><cell>48.67</cell><cell>78.96</cell><cell>-</cell><cell>44.37</cell><cell>28.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 7 Comparison of network architectures.</head><label>7</label><figDesc></figDesc><table><row><cell>Principles</cell><cell>FlowNetC</cell><cell>FlowNet2</cell><cell>SpyNet</cell><cell>PWC-Net</cell></row><row><cell>Pyramid</cell><cell>3-level</cell><cell>3-level</cell><cell>Image</cell><cell>6-level</cell></row><row><cell>Warping</cell><cell>-</cell><cell>Image</cell><cell>Image</cell><cell>Feature</cell></row><row><cell>Cost volume</cell><cell cols="2">single-level single-level large range large range</cell><cell>-</cell><cell>multi-level small range</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Zhile Ren and Jinwei Gu for porting the Caffe code to PyTorch, Eddy Ilg for clarifying details about the FlowNet2 paper, Ming-Hsuan Yang for helpful suggestions, Michael Pellauer for proofreading, github users for clarifying questions, and the anonymous reviewers at ICCV'17 and CVPR'18 for constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust computation of optical flow in a multi-scale differential framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="81" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>2016. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bounding boxes, segmentations and object coordinates: How important is recognition for 3D scene flow estimation in autonomous driving scenarios?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lucas/Kanade meets Horn/Schunck: combining local and global optic flow methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>SchnÃ¶rr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: Descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep-Flow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting semantic information and deep matching for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CNN-based patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Full flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient coarse-to-fine patchmatch for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">S2f: Slow-to-fast interpolator flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interponet, a brain inspired neural network for optical flow dense interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optical flow in mostly rigid scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Probability distributions of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning lowlevel vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">T</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the spatial statistics of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning for optical flow using stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient sparse-to-dense optical flow estimation using a learned basis and layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Anisotropic Huber-L1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Trobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>HÃ¤usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of image transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for optical flow with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Performance of optical flow techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beauchemin</surname></persName>
		</author>
		<idno>1994. 4</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Humanassisted motion annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An improved algorithm for TV-L1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dagstuhl Motion Workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Bilateral filtering-based optical flow estimation with occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isnardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">PatchBatch: A batch augmented loss for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gadot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">MirrorFlow: Exploiting symmetries in joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cascaded scene flow prediction using semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The hci benchmark suite: Stereo and flow ground truth with uncertainties for urban autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krispin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gussefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahimimoghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02371</idno>
		<title level="m">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Troubling trends in machine learning scholarship</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03341</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Phenomenal coherence of moving visual patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Movshon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">300</biblScope>
			<biblScope unit="issue">5892</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Motion illusions as optimal percepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07036</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Endto-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (DAGM)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
