<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
							<email>christopher.morris@tu-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
							<email>matthias.fey@tu-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">McGill University and MILA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
							<email>janeric.lenssen@tu-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Grohe</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically-showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.</p><p>completing the equivalence. Since the power of the 1-WL has been completely characterized, see, e.g., <ref type="bibr" target="#b0">(Arvind et al. 2015;</ref><ref type="bibr" target="#b12">Kiefer, Schweitzer, and Selman 2015)</ref>, we can transfer these results to the case of GNNs, showing that both approaches have the same shortcomings.</p><p>Going further, we leverage these theoretical relationships to propose a generalization of GNNs, called k-GNNs, which are neural architectures based on the k-dimensional WL algorithm (k-WL), which are strictly more powerful than GNNs. The key insight in these higher-dimensional variants is that they perform message passing directly between subgraph structures, rather than individual nodes. This higher-order form of message passing can capture structural information that is not visible at the node-level.</p><p>Graph kernels based on the k-WL have been proposed in the past (Morris, Kersting, and Mutzel 2017). However, a key advantage of implementing higher-order message passing in GNNs-which we demonstrate here-is that we can design hierarchical variants of k-GNNs, which combine graph representations learned at different granularities in an end-to-end trainable framework. Concretely, in the presented hierarchical approach the initial messages in a k-GNN are based on the output of lower-dimensional k -GNN (with k &lt; k), which allows the model to effectively capture graph structures of varying granularity. Many real-world graphs inherit a hierarchical structure-e.g., in a social network we must model both the ego-networks around individual nodes, as well as the coarse-grained relationships between entire communities, see, e.g., (Newman 2003)-and our experimental results demonstrate that these hierarchical k-GNNs are able to consistently outperform traditional GNNs on a variety of graph classification and regression tasks. Across twelve graph regression tasks from the QM9 benchmark, we find that our hierarchical model reduces the mean absolute error by 54.45% on average. For graph classification, we find that our hierarchical models leads to slight performance gains. Key Contributions. Our key contributions are summarized as follows: 1. We show that GNNs are not more powerful than the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Moreover, we show that, assuming a suitable parameter initialization, GNNs have the same power as the 1-WL. 2. We propose k-GNNs, which are strictly more powerful than GNNs. Moreover, we propose a hierarchical version of k-GNNs, so-called 1-k-GNNs, which are able to work with the fine-and coarse-grained structures of a given graph, and relationships between those. 3. Our theoretical findings are backed-up by an experimental study, showing that higher-order graph properties are important for successful graph classification and regression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Graph-structured data is ubiquitous across application domains ranging from chemo-and bioinformatics to image and social network analysis. To develop successful machine learning models in these domains, we need techniques that can exploit the rich information inherent in graph structure, as well as the feature information contained within a graph's nodes and edges. In recent years, numerous approaches have been proposed for machine learning graphs-most notably, approaches based on graph kernels <ref type="bibr" target="#b24">(Vishwanathan et al. 2010)</ref> or, alternatively, using graph neural network algorithms <ref type="bibr" target="#b10">(Hamilton, Ying, and Leskovec 2017b)</ref>.</p><p>Kernel approaches typically fix a set of features in advancee.g., indicator features over subgraph structures or features of local node neighborhoods. For example, one of the most successful kernel approaches, the Weisfeiler-Lehman subtree kernel <ref type="bibr" target="#b24">(Shervashidze et al. 2011)</ref>, which is based on the 1dimensional Weisfeiler-Leman graph isomorphism heuristic <ref type="bibr">(Grohe 2017, pp. 79 ff.)</ref>, generates node features through an iterative relabeling, or coloring, scheme: First, all nodes are assigned a common initial color; the algorithm then iteratively recolors a node by aggregating over the multiset of colors Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. in its neighborhood, and the final feature representation of a graph is the histogram of the resulting node colors. By iteratively aggregating over local node neighborhoods in this way, the WL subtree kernel is able to effectively summarize the neighborhood substructures present in a graph. However, while powerful, the WL subtree kernel-like other kernel methodsis limited because this feature construction scheme is fixed (i.e., it does not adapt to the given data distribution). Moreover, this approach-like the majority of kernel methods-focuses only on the graph structure and cannot interpret continuous node and edge labels, such as real-valued vectors which play an important role in applications such as bio-and chemoinformatics.</p><p>Graph neural networks (GNNs) have emerged as a machine learning framework addressing the above challenges. Standard GNNs can be viewed as a neural version of the 1-WL algorithm, where colors are replaced by continuous feature vectors and neural networks are used to aggregate over node neighborhoods <ref type="bibr" target="#b9">(Hamilton, Ying, and Leskovec 2017a;</ref><ref type="bibr" target="#b13">Kipf and Welling 2017)</ref>. In effect, the GNN framework can be viewed as implementing a continuous form of graph-based "message passing", where local neighborhood information is aggregated and passed on to the neighbors <ref type="bibr" target="#b7">(Gilmer et al. 2017</ref>). By deploying a trainable neural network to aggregate information in local node neighborhoods, GNNs can be trained in an end-to-end fashion together with the parameters of the classification or regression algorithm, possibly allowing for greater adaptability and better generalization compared to the kernel counterpart of the classical 1-WL algorithm.</p><p>Up to now, the evaluation and analysis of GNNs has been largely empirical, showing promising results compared to kernel approaches, see, e.g., <ref type="bibr" target="#b26">(Ying et al. 2018b</ref>). However, it remains unclear how GNNs are actually encoding graph structure information into their vector representations, and whether there are theoretical advantages of GNNs compared to kernel based approaches. Present Work. We offer a theoretical exploration of the relationship between GNNs and kernels that are based on the 1-WL algorithm. We show that GNNs cannot be more powerful than the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs, e.g., the properties of subgraphs around each node. This result holds for a broad class of GNN architectures and all possible choices of parameters for them. On the positive side, we show that given the right parameter initialization GNNs have the same expressiveness as the 1-WL algorithm,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Our study builds upon a wealth of work at the intersection of supervised learning on graphs, kernel methods, and graph neural networks.</p><p>Historically, kernel methods-which implicitly or explicitly map graphs to elements of a Hilbert space-have been the dominant approach for supervised learning on graphs. Important early work in this area includes random-walk based kernels <ref type="bibr" target="#b7">(Gärtner, Flach, and Wrobel 2003;</ref><ref type="bibr" target="#b10">Kashima, Tsuda, and Inokuchi 2003)</ref>) and kernels based on shortest paths <ref type="bibr" target="#b1">(Borgwardt and Kriegel 2005)</ref>. More recently, developments in graph kernels have emphasized scalability, focusing on techniques that bypass expensive Gram matrix computations by using explicit feature maps. Prominent examples of this trend include kernels based on graphlet counting <ref type="bibr" target="#b23">(Shervashidze et al. 2009</ref>), and, most notably, the Weisfeiler-Lehman subtree kernel <ref type="bibr" target="#b24">(Shervashidze et al. 2011)</ref> as well as its higher-order variants <ref type="bibr" target="#b21">(Morris, Kersting, and Mutzel 2017)</ref>. Graphlet and Weisfeiler-Leman kernels have been successfully employed within frameworks for smoothed and deep graph kernels <ref type="bibr" target="#b25">(Yanardag and Vishwanathan 2015a;</ref><ref type="bibr" target="#b25">Yanardag and Vishwanathan 2015b)</ref>. Recent works focus on assignment-based approaches <ref type="bibr" target="#b15">(Kriege, Giscard, and Wilson 2016;</ref><ref type="bibr" target="#b21">Nikolentzos, Meladianos, and Vazirgiannis 2017;</ref><ref type="bibr" target="#b10">Johansson and Dubhashi 2015)</ref>, spectral approaches <ref type="bibr" target="#b14">(Kondor and Pan 2016)</ref>, and graph decomposition approaches <ref type="bibr" target="#b21">(Nikolentzos et al. 2018)</ref>. Graph kernels were dominant in graph classification for several years, leading to new state-of-the-art results on many classification tasks. However, they are limited by the fact that they cannot effectively adapt their feature representations to a given data distribution, since they generally rely on a fixed set of features. More recently, a number of approaches to graph classification based upon neural networks have been proposed. Most of the neural approaches fit into the graph neural network framework proposed by <ref type="bibr" target="#b7">(Gilmer et al. 2017)</ref>. Notable instances of this model include Neural Fingerprints <ref type="bibr" target="#b4">(Duvenaud et al. 2015)</ref>, Gated Graph Neural Networks <ref type="bibr" target="#b17">(Li et al. 2016)</ref>, GraphSAGE <ref type="bibr" target="#b9">(Hamilton, Ying, and Leskovec 2017a)</ref>, SplineCNN <ref type="bibr" target="#b5">(Fey et al. 2018)</ref>, and the spectral approaches proposed in <ref type="bibr" target="#b2">(Bruna et al. 2014;</ref><ref type="bibr" target="#b3">Defferrard, X., and Vandergheynst 2016;</ref><ref type="bibr" target="#b13">Kipf and Welling 2017</ref>)-all of which descend from early work in <ref type="bibr" target="#b19">(Merkwirth and Lengauer 2005)</ref> and <ref type="bibr" target="#b23">(Scarselli et al. 2009b)</ref>. Recent extensions and improvements to the GNN framework include approaches to incorporate different local structures around subgraphs <ref type="bibr" target="#b24">(Xu et al. 2018</ref>) and novel techniques for pooling node representations in order perform graph classification <ref type="bibr" target="#b26">(Zhang et al. 2018;</ref><ref type="bibr" target="#b26">Ying et al. 2018b)</ref>. GNNs have achieved state-of-the-art performance on several graph classification benchmarks in recent years, see, e.g., <ref type="bibr" target="#b26">(Ying et al. 2018b</ref>)-as well as applications such as protein-protein interaction prediction <ref type="bibr" target="#b6">(Fout et al. 2017)</ref>, recommender systems <ref type="bibr" target="#b26">(Ying et al. 2018a)</ref>, and the analysis of quantum interactions in molecules <ref type="bibr" target="#b23">(Schütt et al. 2017)</ref>. A survey of recent advancements in GNN techniques can be found in <ref type="bibr" target="#b10">(Hamilton, Ying, and Leskovec 2017b)</ref>.</p><p>Up to this point (and despite their empirical success) there has been very little theoretical work on GNNs-with the notable exceptions of <ref type="bibr">Li et al.'s (Li, Han, and Wu 2018)</ref> work connecting GNNs to a special form Laplacian smoothing and <ref type="bibr" target="#b16">Lei et al.'s (Lei et al. 2017)</ref> work showing that the feature maps generated by GNNs lie in the same Hilbert space as some popular graph kernels. Moreover, <ref type="bibr" target="#b22">Scarselli et al. (Scarselli et al. 2009a)</ref> investigates the approximation capabilities of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>We start by fixing notation, and then outline the Weisfeiler-Leman algorithm and the standard graph neural network framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation and Background</head><p>A graph G is a pair (V, E) with a finite set of nodes V and a set of edges E ⊆ {{u, v} ⊆ V | u = v}. We denote the set of nodes and the set of edges of G by V (G) and E(G), respectively. For ease of notation we denote the edge {u, v} in E(G) by <ref type="bibr">(u, v)</ref> </p><formula xml:id="formula_0">or (v, u). Moreover, N (v) denotes the neighborhood of v in V (G), i.e., N (v) = {u ∈ V (G) | (v, u) ∈ E(G)}.</formula><p>We say that two graphs G and H are isomorphic if there exists an edge preserving bijection ϕ :</p><formula xml:id="formula_1">V (G) → V (H), i.e., (u, v) is in E(G) if and only if (ϕ(u), ϕ(v)) is in E(H).</formula><p>We write G H and call the equivalence classes induced by isomorphism types.</p><formula xml:id="formula_2">Let S ⊆ V (G) then G[S] = (S, E S ) is the sub- graph induced by S with E S = {(u, v) ∈ E(G) | u, v ∈ S}.</formula><p>A node coloring is a function V (G) → Σ with arbitrary codomain Σ. Then a node colored or labeled graph (G, l) is a graph G endowed with a node coloring l : V (G) → Σ. We say that l(v) is a label or color of v ∈ V (G). We say that a node coloring c refines a node coloring d, written c </p><formula xml:id="formula_3">d, if c(v) = c(w) implies d(v) = d(w) for every v, w in V (G).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weisfeiler-Leman Algorithm</head><p>We now describe the 1-W L algorithm for labeled graphs. Let (G, l) be a labeled graph. In each iteration, t ≥ 0, the 1-WL computes a node coloring c (t) l : V (G) → Σ, which depends on the coloring from the previous iteration. In iteration 0, we set c</p><formula xml:id="formula_4">(0) l = l. Now in iteration t &gt; 0, we set c (t) l (v) = H A S H c (t−1) l (v), { {c (t−1) l (u) | u ∈ N (v)} } (1)</formula><p>where H A S H bijectively maps the above pair to a unique value in Σ, which has not been used in previous iterations. To test two graph G and H for isomorphism, we run the above algorithm in "parallel" on both graphs. Now if the two graphs have a different number of nodes colored σ in Σ, the 1-W L concludes that the graphs are not isomorphic. Moreover, if the number of colors between two iterations does not change, i.e., the cardinalities of the images of c (t−1) l and c (t) l are equal, the algorithm terminates. Termination is guaranteed after at most max{|V (G)|, |V (H)|} iterations. It is easy to see that the algorithm is not able to distinguish all non-isomorphic graphs, e.g., see <ref type="bibr" target="#b3">(Cai, Fürer, and Immerman 1992)</ref>. Nonetheless, it is a powerful heuristic, which can successfully test isomorphism for a broad class of graphs <ref type="bibr" target="#b0">(Babai and Kucera 1979)</ref>.</p><p>The k-dimensional Weisfeiler-Leman algorithm (k-WL), for k ≥ 2, is a generalization of the 1-WL which colors tuples from V (G) k instead of nodes. That is, the algorithm computes a coloring c (t) l,k : V (G) k → Σ. In order to describe the algorithm, we define the j-th neighborhood N j (s) = {(s 1 , . . . , s j−1 , r, s j+1 , . . . , s k ) | r ∈ V (G)} (2) of a k-tuple s = (s 1 , . . . , s k ) in V (G) k . That is, the j-th neighborhood N j (t) of s is obtained by replacing the j-th component of s by every node from V (G). In iteration 0, the algorithm labels each k-tuple with its atomic type, i.e., two k-tuples s and s in V (G) k get the same color if the map s i → s i induces a (labeled) isomorphism between the subgraphs induced from the nodes from s and s , respectively. For iteration t &gt; 0, we define</p><formula xml:id="formula_5">C (t) j (s) = H A S H { {c (t−1) l,k (s ) | s ∈ N j (s)} } ,<label>(3)</label></formula><p>and set</p><formula xml:id="formula_6">c (t) k,l (s) = H A S H c (t−1) k,l (s), C (t) 1 (s), . . . , C (t) k (s) . (4)</formula><p>Hence, two tuples s and s with c</p><formula xml:id="formula_7">(t−1) k,l (s) = c (t−1) k,l (s ) get different colors in iteration t if there exists j in [1 : k]</formula><p>such that the number of j-neighbors of s and s , respectively, colored with a certain color is different. The algorithm then proceeds analogously to the 1-W L. By increasing k, the algorithm gets more powerful in terms of distinguishing non-isomorphic graphs, i.e., for each k ≥ 2, there are non-isomorphic graphs which can be distinguished by the (k + 1)-WL but not by the k-WL <ref type="bibr" target="#b3">(Cai, Fürer, and Immerman 1992)</ref>. We note here that the above variant is not equal to the folklore variant of k-WL described in <ref type="bibr" target="#b3">(Cai, Fürer, and Immerman 1992)</ref>, which differs slightly in its update rule. However, it holds that the k-WL using Equation <ref type="formula">(4)</ref> is as powerful as the folklore (k−1)-WL <ref type="bibr" target="#b7">(Grohe and Otto 2015)</ref>. WL Kernels. After running the WL algorithm, the concatenation of the histogram of colors in each iteration can be used as a feature vector in a kernel computation. Specifically, in the histogram for every color σ in Σ there is an entry containing the number of nodes or k-tuples that are colored with σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Neural Networks</head><p>Let (G, l) be a labeled graph with an initial node coloring</p><formula xml:id="formula_8">f (0) : V (G) → R 1×d that is consistent with l. This means that each node v is annotated with a feature f (0) (v) in R 1×d such that f (0) (u) = f (0) (v) if and only if l(u) = l(v). Alter- natively, f (0) (v)</formula><p>can be an arbitrary real-valued feature vector associated with v. Examples include continuous atomic properties in chemoinformatic applications where nodes correspond to atoms, or vector representations of text in social network applications. A GNN model consists of a stack of neural network layers, where each layer aggregates local neighborhood information, i.e., features of neighbors, around each node and then passes this aggregated information on to the next layer.</p><p>A basic GNN model can be implemented as follows (Hamilton, Ying, and Leskovec 2017b). In each layer t &gt; 0, we compute a new feature</p><formula xml:id="formula_9">f (t) (v) = σ f (t−1) (v)·W (t) 1 + w∈N (v) f (t−1) (w)·W (t) 2 (5) in R 1×e for v, where W (t) 1 and W (t) 2</formula><p>are parameter matrices from R d×e , and σ denotes a component-wise non-linear function, e.g., a sigmoid or a ReLU. <ref type="bibr">1</ref> Following <ref type="bibr" target="#b7">(Gilmer et al. 2017)</ref>, one may also replace the sum defined over the neighborhood in the above equation by a permutation-invariant, differentiable function, and one may substitute the outer sum, e.g., by a column-wise vector concatenation or LSTM-style update step. Thus, in full generality a new feature</p><formula xml:id="formula_10">f (t) (v) is computed as f W1 merge f (t−1) (v), f W2 aggr { {f (t−1) (w) | w ∈ N (v)} } ,<label>(6)</label></formula><p>where f W1 aggr aggregates over the set of neighborhood features and f W2 merge merges the node's representations from step (t − 1) with the computed neighborhood features. Both f W1 aggr and f W2 merge may be arbitrary differentiable, permutation-invariant functions (e.g., neural networks), and, by analogy to Equation 5, we denote their parameters as W 1 and W 2 , respectively. In the rest of this paper, we refer to neural architectures implementing Equation <ref type="formula" target="#formula_10">(6)</ref> as 1-dimensional GNN architectures (1-GNNs).</p><p>A vector representation f GNN over the whole graph can be computed by summing over the vector representations computed for all nodes, i.e.,</p><formula xml:id="formula_11">f GNN (G) = v∈V (G) f (T ) (v),</formula><p>where T &gt; 0 denotes the last layer. More refined approaches use differential pooling operators based on sorting (Zhang et al. 2018) and soft assignments <ref type="bibr" target="#b26">(Ying et al. 2018b)</ref>.</p><p>In order to adapt the parameters W 1 and W 2 of Equations <ref type="formula">(5)</ref> and <ref type="formula" target="#formula_10">(6)</ref>, to a given data distribution, they are optimized in an end-to-end fashion (usually via stochastic gradient descent) together with the parameters of a neural network used for classification or regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship Between 1-WL and 1-GNNs</head><p>In the following we explore the relationship between the 1-WL and 1-GNNs. Let (G, l) be a labeled graph, and let</p><formula xml:id="formula_12">W (t) = W (t ) 1 , W (t ) 2</formula><p>t ≤t denote the GNN parameters given by Equation <ref type="formula">(5)</ref> or Equation <ref type="formula" target="#formula_10">(6)</ref> up to iteration t. We encode the initial labels l(v) by vectors f (0) (v) ∈ R 1×d , e.g., using a 1-hot encoding.</p><p>Our first theoretical result shows that the 1-GNN architectures do not have more power in terms of distinguishing between non-isomorphic (sub-)graphs than the 1-WL algorithm. More formally, let f W1 merge and f W2 aggr be any two functions chosen in (6). For every encoding of the labels l(v) as vectors f (0) (v), and for every choice of W (t) , we have that the coloring c (t) l of 1-WL always refines the coloring f (t) induced by a 1-GNN parameterized by W (t) . Theorem 1. Let (G, l) be a labeled graph. Then for all t ≥ 0 and for all choices of initial colorings f (0) consistent with l, and weights</p><formula xml:id="formula_13">W (t) , c (t) l f (t) .</formula><p>1 For clarity of presentation we omit biases.</p><p>Our second result states that there exist a sequence of parameter matrices W (t) such that 1-GNNs have exactly the same power in terms of distinguishing non-isomorphic (sub-)graphs as the 1-WL algorithm. This even holds for the simple architecture (5), provided we choose the encoding of the initial labeling l in such a way that different labels are encoded by linearly independent vectors.</p><p>Theorem 2. Let (G, l) be a labeled graph. Then for all t ≥ 0 there exists a sequence of weights W (t) , and a 1-GNN architecture such that c</p><formula xml:id="formula_14">(t) l ≡ f (t)</formula><p>. Hence, in the light of the above results, 1-GNNs may viewed as an extension of the 1-WL which in principle have the same power but are more flexible in their ability to adapt to the learning task at hand and are able to handle continuous node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shortcomings of Both Approaches</head><p>The power of 1-WL has been completely characterized, see, e.g., <ref type="bibr" target="#b0">(Arvind et al. 2015)</ref>. Hence, by using Theorems 1 and 2, this characterization is also applicable to 1-GNNs. On the other hand, 1-GNNs have the same shortcomings as the 1-WL. For example, both methods will give the same color to every node in a graph consisting of a triangle and a 4-cycle, although vertices from the triangle and the vertices from the 4-cycle are clearly different. Moreover, they are not capable of capturing simple graph theoretic properties, e.g., triangle counts, which are an important measure in social network analysis <ref type="bibr" target="#b20">(Milo et al. 2002;</ref><ref type="bibr" target="#b21">Newman 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>k-dimensional Graph Neural Networks</head><p>In the following, we propose a generalization of 1-GNNs, so-called k-GNNs, which are based on the k-WL. Due to scalability and limited GPU memory, we consider a set-based version of the k-WL. For a given k, we consider all k-element</p><formula xml:id="formula_15">subsets [V (G)] k over V (G). Let s = {s 1 , . . . , s k } be a k-set in [V (G)] k , then we define the neighborhood of s as N (s) = {t ∈ [V (G)] k | |s ∩ t| = k − 1} . The local neighborhood N L (s) consists of all t ∈ N (s) such that (v, w) ∈ E(G) for the unique v ∈ s \ t and the unique w ∈ t \ s. The global neighborhood N G (s) then is defined as N (s) \ N L (s). 2</formula><p>The set based k-WL works analogously to the k-WL, i.e., it computes a coloring c</p><formula xml:id="formula_16">(t) s,k,l : [V (G)] k → Σ as in ?? based on the above neighborhood. Initially, c (0) s,k,l colors each element s in [V (G)] k with the isomorphism type of G[s].</formula><p>Let (G, l) be a labeled graph. In each k-GNN layer t ≥ 0, we compute a feature vector f</p><formula xml:id="formula_17">(t) k (s) for each k-set s in [V (G)] k . For t = 0, we set f (0) k (s) to f iso (s), a one-hot 2</formula><p>Note that the definition of the local neighborhood is different from the the one defined in <ref type="bibr" target="#b21">(Morris, Kersting, and Mutzel 2017)</ref> which is a superset of our definition. Our computations therefore involve sparser graphs.  </p><formula xml:id="formula_18">f (t) k (s) = σ f (t−1) k (s) · W (t) 1 + u∈N L (s)∪N G (s) f (t−1) k (u) · W (t) 2 .</formula><p>Moreover, one could split the sum into two sums ranging over N L (s) and N G (s) respectively, using distinct parameter matrices to enable the model to learn the importance of local and global neighborhoods. To scale k-GNNs to larger datasets and to prevent overfitting, we propose local k-GNNs, where we omit the global neighborhood of s, i.e.,</p><formula xml:id="formula_19">f (t) k,L (s) = σ f (t−1) k,L (s) · W (t) 1 + u∈N L (s) f (t−1) k,L (u) · W (t) 2 .</formula><p>The running time for evaluation of the above depends on |V |, k and the sparsity of the graph (each iteration can be bounded by the number of subsets of size k times the maximum degree). Note that we can scale our method to larger datasets by using sampling strategies introduced in, e.g., <ref type="bibr" target="#b21">(Morris, Kersting, and Mutzel 2017;</ref><ref type="bibr" target="#b9">Hamilton, Ying, and Leskovec 2017a)</ref>. We can now lift the results of the previous section to the k-dimensional case. Proposition 3. Let (G, l) be a labeled graph and let k ≥ 2. Then for all t ≥ 0, for all choices of initial colorings f (0) k consistent with l and for all weights W (t) ,</p><formula xml:id="formula_20">c (t) s,k,l f (t) k .</formula><p>Again the second result states that there exists a suitable initialization of the parameter matrices W (t) such that k-GNNs have exactly the same power in terms of distinguishing nonisomorphic (sub-)graphs as the set-based k-WL. Proposition 4. Let (G, l) be a labeled graph and let k ≥ 2. Then for all t ≥ 0 there exists a sequence of weights W (t) , and a k-GNN architecture such that</p><formula xml:id="formula_21">c (t) s,k,l ≡ f (t) k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Variant</head><p>One key benefit of the end-to-end trainable k-GNN framework-compared to the discrete k-WL algorithm-is that we can hierarchically combine representations learned at different granularities. Concretely, rather than simply using one-hot indicator vectors as initial feature inputs in a k-GNN, we propose a hierarchical variant of k-GNN that uses the features learned by a (k − 1)-dimensional GNN, in addition to the (labeled) isomorphism type, as the initial features, i.e.,</p><formula xml:id="formula_22">f (0) k (s) = σ f iso (s), u⊂s f (T k−1 ) k−1 (u) · W k−1 ,</formula><p>for some T k−1 &gt; 0, where W k−1 is a matrix of appropriate size, and square brackets denote matrix concatenation.</p><p>Hence, the features are recursively learned from dimensions 1 to k in an end-to-end fashion. This hierarchical model also satisfies Propositions 3 and 4, so its representational capacity is theoretically equivalent to a standard k-GNN (in terms of its relationship to k-WL). Nonetheless, hierarchy is a natural inductive bias for graph modeling, since many real-world graphs incorporate hierarchical structure, so we expect this hierarchical formulation to offer empirical utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Study</head><p>In the following, we want to investigate potential benefits of GNNs over graph kernels as well as the benefits of our proposed k-GNN architectures over 1-GNN architectures. More precisely, we address the following questions: Q1 How do the (hierarchical) k-GNNs perform in comparison to state-of-the-art graph kernels? Q2 How do the (hierarchical) k-GNNs perform in comparison to the 1-GNN in graph classification and regression tasks? Q3 How much (if any) improvement is provided by optimizing the parameters of the GNN aggregation function, compared to just using random GNN parameters while optimizing the parameters of the downstream classification/regression algorithm?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>To compare our k-GNN architectures to kernel approaches we use well-established benchmark datasets from the graph kernel literature ). The nodes of each graph in these dataset is annotated with (discrete) labels or no labels.</p><p>To demonstrate that our architectures scale to larger datasets and offer benefits on real-world applications, we conduct experiments on the Q M 9 dataset <ref type="bibr" target="#b21">(Ramakrishnan et al. 2014;</ref><ref type="bibr" target="#b21">Ruddigkeit et al. 2012;</ref><ref type="bibr" target="#b18">Wu et al. 2018</ref>), which consists of      133 385 small molecules. The aim here is to perform regression on twelve targets representing energetic, electronic, geometric, and thermodynamic properties, which were computed using density functional theory. We used the dataset provided at http;//moleculenet.ai, cf. <ref type="table">Table 3</ref> in the Appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We use the following kernel and GNN methods as baselines for our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Configuration</head><p>We always used three layers for 1-GNN, and two layers for (local) 2-GNN and 3-GNN, all with a hidden-dimension size of 64. For the hierarchical variant we used architectures that use features computed by 1-GNN as initial features for the 2-GNN (1-2-GNN) and 3-GNN (1-3-GNN), respectively. Moreover, using the combination of the former we componentwise concatenated the computed features of the 1-2-GNN and the 1-3-GNN</p><p>(1-2-3-GNN). For the final classification and regression steps, we used a three layer MLP, with binary cross entropy and mean squared error for the optimization, respectively. For classification we used a dropout layer with p = 0.5 after the first layer of the MLP. We applied global average pooling to generate a vector representation of the graph from the computed node features for each k. The resulting vectors are concatenated column-wise before feeding them into the MLP. Moreover, we used the Adam optimizer with an initial learning rate of 10 −2 and applied an adaptive learning rate decay based on validation results to a minimum of 10 −5 . We trained the classification networks for 100 epochs and the regression networks for 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Protocol</head><p>For the smaller datasets, which we use for comparison against the kernel methods, we performed a 10-fold cross validation where we randomly sampled 10% of each training fold to act as a validation set. For the Q M 9 dataset, we follow the dataset splits described in <ref type="bibr" target="#b18">(Wu et al. 2018)</ref>. We randomly sampled 10% of the examples for validation, another 10% for testing, and used the remaining for training. We used the same initial node features as described in <ref type="bibr" target="#b7">(Gilmer et al. 2017)</ref>. Moreover, in order to illustrate the benefits of our hierarchical k-GNN architecture, we did not use a complete graph, where edges are annotated with pairwise distances, as input. Instead, we only used pairwise Euclidean distances for connected nodes, computed from the provided node coordinates. The code was built upon the work of <ref type="bibr" target="#b5">(Fey et al. 2018)</ref> and is provided at https://github. com/chrsmrrs/k-gnn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>In the following we answer questions Q1 to Q3. <ref type="table" target="#tab_0">Table 1</ref> shows the results for comparison with the kernel methods on the graph classification benchmark datasets. Here, the hierarchical k-GNN is on par with the kernels despite the small dataset sizes (answering question Q1). We also find that the 1-2-3-GNN significantly outperforms the 1-GNN on all seven datasets (answering Q2), with the 1-GNN being the overall weakest method across all tasks. <ref type="bibr">3</ref> We can further see that optimizing the parameters of the aggregation function only leads to slight performance gains on two out of three datasets, and that no optimization even achieves better results on the P R O T E I N S benchmark dataset (answering Q3). We contribute this effect to the one-hot encoded node labels, which allow the GNN to gather enough information out of the neighborhood of a node, even when this aggregation is not learned. <ref type="table">Table 2</ref> shows the results for the Q M 9 dataset. On eleven out of twelve targets all of our hierarchical variants beat the 1-GNN baseline, providing further evidence for Q2. For example, on the target H we achieve a large improvement of 98.1% in MAE compared to the baseline. Moreover, on ten out of twelve datasets, the hierarchical k-GNNs beat the baselines from <ref type="bibr" target="#b18">(Wu et al. 2018</ref>). However, the additional structural information extracted by the k-GNN layers does not serve all tasks equally, leading to huge differences in gains across the targets.</p><p>It should be noted that our k-GNN models have more parameters than the 1-GNN model, since we stack two additional GNN layers for each k. However, extending the 1-GNN model by additional layers to match the number of parameters of the k-GNN did not lead to better results in any experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We presented a theoretical investigation of GNNs, showing that a wide class of GNN architectures cannot be stronger than the 1-WL. On the positive side, we showed that, in principle, GNNs possess the same power in terms of distinguishing between non-isomorphic (sub-)graphs, while having the added benefit of adapting to the given data distribution. Based on this insight, we proposed k-GNNs which are a generalization of GNNs based on the k-WL. This new model is strictly stronger then GNNs in terms of distinguishing non-isomorphic (sub-)graphs and is capable of distinguishing more graph properties. Moreover, we devised a hierarchical variant of k-GNNs, which can exploit the hierarchical organization of most real-world graphs. Our experimental study shows that k-GNNs consistently outperform 1-GNNs and beat state-of-the-art neural architectures on largescale molecule learning tasks. Future work includes designing task-specific k-GNNs, e.g., devising k-GNNs layers that exploit expert-knowledge in bio-and chemoinformatic settings.</p><p>Then t ≥ 1, becausec (0) = J as there are no initial colors. Let P 1 , . . . , P p be the color classes ofc (t−1) . That is, for all x, y ∈ V (G) we havec (t−1) (x) =c (t−1) (y) if any only if there is an i ∈ [1 : p] such that x, y ∈ P i . Similarly, let Q 1 , . . . , Q q be the color classes ofc <ref type="bibr">(t)</ref> . Observe that the partition {Q 1 , . . . , Q q } of V (G) refines the partition</p><formula xml:id="formula_23">{P 1 , . . . , P p }. Indeed, if there were i = i ∈ [1:p], k ∈ [1:q] such that P i ∩ Q k = ∅ and P i ∩ Q k = ∅, then all x ∈ P i ∩ Q k , y ∈ P i ∩ Q k would satisfyc (t−1) (x) =c (t−1) (y) andc (t) (x) =c (t) (y), contradicting the minimality of q.</formula><p>Choose v, w ∈ V (G) satisfying <ref type="formula">(7)</ref> and <ref type="formula">(8)</ref>. By <ref type="formula">(7)</ref>, there is an i ∈</p><formula xml:id="formula_24">[1:p] such that |N G (v)∩P i | = |N G (w)∩P i |. Let j 1 , . . . , j ∈ [1:q] such that P i = Q j1 ∪ . . . ∪ Q j . By (8), for all k ∈ [1 : ] we have |N G (v) ∩ Q jk | = |N G (w) ∩ Q jk |. As the Q j are disjoint, this implies |N G (v) ∩ P i | = |N G (w) ∩ P i | , which is a contradiction.</formula><p>Hence, the two update rules are equivalent.</p><p>Corollary 8. For all G and all t ∈ N we havec (t) ≡ c (t) .</p><p>Thus we can use the update ruleΓ for the proof on unlabeled graphs. For the proof, it will be convenient to assume that V (G) = [1:n] (although we still work with the notation V (G)). It follows that n = |V |. A node coloring f (t) defines a matrix F (t) ∈ R n×d where the i th row of F (t) is defined by f (t) (i) ∈ R 1×d . Here we interpret i as a node from V (G). As colorings and matrices can be interpreted as one another, given a matrix F ∈ R V (G)×d we write Γ G (F ) (orΓ G (F )) for a Weisfeiler-Leman iteration on the coloring c F induced by the matrix F . For the GNN computation we provide a matrix based notation. Using the adjacency matrix A ∈ R n×n of G and a coloring F ∈ R n×d , we can write the update rule of the GNN layer as</p><formula xml:id="formula_25">F t+1 = Λ A,W,b (F (t) ) = σ(AF (t) W (t) + bJ),</formula><p>where Λ A,W,b is the refinement operator of GNNs corresponding to a single iteration of the 1-WL. For simplicity of the proof, we choose σ = sgn = 1 if x &gt; 0, −1 otherwise, and the bias as b = −1 .</p><p>Note that we later provide a way to simulate the sign-function using ReLu operations to indicate that choosing the sign function is not really a hard restriction.</p><p>Lemma 9. Let B ∈ Z s×t be a matrix such that 0 ≤ B ij ≤ n − 1 for all i, j and the rows of B are pairwise distinct. Then there is a matrix X ∈ R t×s such that the matrix sgn BX − J) ∈ {−1, 1} s×s is non-singular.</p><p>Proof. Let z = (1, n, n 2 , . . . , n t−1 ) T ∈ R t where n is the upper bound on the matrix entries of B and b = Bz ∈ R s . Then the entries of b are nonnegative and pairwise distinct. Without loss of generality, we assume that</p><formula xml:id="formula_26">b = (b 1 , . . . , b s ) T such that b 1 &gt; b 2 &gt; · · · &gt; b s ≥ 0. Now we choose numbers x 1 , . . . , x s ∈ R such that b i · x j &lt; 1 if i ≥ j, b i · x j &gt; 1 if i &lt; j<label>(9)</label></formula><p>for all i, j ∈ [s] as the b i are ordered. Let x = (x 1 , . . . , x s ) ∈ R 1×s and C = b · x ∈ R s×s andĈ = sgn(C − J).</p><p>Then C has entries C ij = b i · x j , and thus by <ref type="formula" target="#formula_26">(9)</ref>,</p><formula xml:id="formula_27">C =            −1 1 1 1 · · · 1 1 −1 −1 1 · · · 1 . . . . . . . . . . . . −1 · · · −1 1 −1 · · · −1 −1           <label>(10)</label></formula><p>ThusĈ is non-singular. Now we simply let X = z · x. Then BX = C.</p><p>Let us call a matrix row-independent modulo equality if the set of all rows appearing in the matrix is linearly independent.</p><p>Example 10. The matrix </p><formula xml:id="formula_28">     1 1 0 1 1 0 0 1 0 0 1 0 0 1 0      </formula><p>is row-independent modulo equality.</p><p>Note that the all-1 matrix J is row-independent modulo equality in all dimensions.</p><p>Lemma 11. Let d ∈ N, and let F ∈ R n×d be row independent modulo equality. Then there is a W ∈ R d×n such that the matrix Λ A,W,−1 (F ) is row independent modulo equality and Λ A,W,−1 (F ) ≡Γ G (F ) .</p><p>Proof. Let Q 1 , . . . , Q r be the color classes of F (that is, for all</p><formula xml:id="formula_29">v, v ∈ V (G) it holds that F v = F v ⇐⇒ ∃j ∈ [r] : v, v ∈ Q j ). LetF ∈ R r×d be the matrix with rowsF j = F v for all j ∈ [r]</formula><p>, v ∈ Q j . Then the rows ofF are linearly independent, and thus there is a matrix M ∈ R d×r such thatF M is the (r × r) identity matrix. It follows that F M ∈ R n×r is the matrix with entries</p><formula xml:id="formula_30">(F M ) vj = 1 if v ∈ Q j , 0 otherwise.<label>(11)</label></formula><p>Let D ∈ Z n×r be the matrix with entries D vj := |N G (v) ∩ Q j |. Note that</p><formula xml:id="formula_31">AF M = D,<label>(12)</label></formula><p>because for all v ∈ V and j ∈ [t] we have</p><formula xml:id="formula_32">(AF M ) vj = v ∈V (G) A vv (F M ) v j = v ∈Qj A vv = D vj ,</formula><p>where the second equality follows from Equation <ref type="formula" target="#formula_30">(11)</ref>. By the definition of Γ G as the 1-WL operator on uncolored graphs, we have</p><formula xml:id="formula_33">Γ G (F ) ≡ D<label>(13)</label></formula><p>if we view D as a coloring of V . Let P 1 , . . . , P s be the color classes of D, and letD ∈ Z s×r be the matrix with rowsD i = D v for all i ∈ [s] and v ∈ P i . Then 0 ≤D ij ≤ n − 1 for all i, j, and the rows ofD are pairwise distinct. By Lemma 9, there is a matrix X ∈ R r×s such that the matrix sgn(DX − J) ∈ R s×s is non singular. This implies that the matrix sgn(AF M X − J) = sgn(DX − J) is row-independent modulo equality. Moreover, sgn(AF M X − J) ≡ D ≡ Γ G (F ) by (13). We let W ∈ R p×n be the matrix of obtained from M X ∈ R p×s by adding n − s all-0 columns. Then Λ A,W,−1 (F ) = sgn(AF W − J)</p><p>is row-independent modulo equality and Λ A,W,−1 (F ) ≡ sgn(AF M X − J) ≡Γ G (F ).</p><p>Corollary 12. There is a sequence W = (W (t) ) t∈N with W (t) ∈ R n×n such that for all r ∈ N,</p><formula xml:id="formula_34">c (t) ≡ Λ (t) A,W,−1 .</formula><p>wherec (t) is given by the t-fold application ofΓ G on the initial uniform coloring J.</p><p>Remark 13. The construction in Lemma 11 always outputs a matrix with as many columns as there are color classes in the resulting coloring. Thus we can choose d to be n and pad the matrix using additional 0-columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Colored Graphs</head><p>We now extend the computation to colored graphs. In order to do that, we again use an equivalent but slightly different variant of the Weisfeiler-Leman update rule leading to colorings c (t) l,0 instead of the usual c (t) l . We then start by showing that both update rules are equivalent.</p><p>We define Γ G,l to be the refinement operator for the 1-WL, mapping a coloring c</p><formula xml:id="formula_35">(t−1) l,0 to the updated one c (t) l,0 as follows: c (t) l,0 (v) = Γ G,l c (t−1) l,0 (v) = H A S H c (0) l,0 (v), { {c (t−1) l,0 (u) | u ∈ N (v)} } .</formula><p>Note that for Γ G,l we use the initial color c</p><p>l,0 (u) of a node u whereas Γ G used the color c (t−1) l (u) from the previous round. The idea of using those old colors is to make sure that any two nodes which got a different color in iteration t, get different colors in iteration t &gt; t. This is formalized by the following lemma. Proof. Let t ∈ N be minimal such that there are v, w with</p><formula xml:id="formula_37">c (t) l,0 (v) = c (t) l,0 (w)<label>(14)</label></formula><p>and</p><formula xml:id="formula_38">c (t+1) l,0 (v) = c (t+1) l,0 (w).<label>(15)</label></formula><p>Then t ≥ 1, because by (), c</p><formula xml:id="formula_39">l,0 (v) = c (1) l,0 (w) implies c (0) l,0 (v) = c<label>(1)</label></formula><p>l,0 (w). Let P 1 , . . . , P p be the color classes of c (t−1) l,0 , and let Q 1 , . . . , Q q be the color classes of c (t) l,0 . Observe that the partition {Q 1 , . . . , Q q } of V (G) refines the partition {P 1 , . . . , P p }. The argument is the same as in the proof of Lemma 7.</p><p>Choose v, w ∈ V (G) satisfying <ref type="formula" target="#formula_37">(14)</ref> and (15). By <ref type="formula" target="#formula_37">(14)</ref>, either c l,0 (v) = c l,0 (w) or there is an i ∈</p><formula xml:id="formula_41">[1 : p] such that |N G (v) ∩ P i | = |N G (w) ∩ P i |. By (), c l,0 (v) = c l,0 (w) contradicts (15). Thus |N G (v) ∩ P i | = |N G (w) ∩ P i | for some i ∈ [1 : p]. Let j 1 , . . . , j ∈ [1 : q] such that P i = Q j1 ∪ . . . ∪ Q j . By (14), for all k ∈ [1 : ] we have |N G (v) ∩ Q jk | = |N G (w) ∩ Q jk |. As the Q j are disjoint, this implies |N G (v) ∩ P i | = |N G (w) ∩ P i |, which is a contradiction.</formula><p>Corollary 15. For all graphs G and initial vertex colorings l of G we have c</p><formula xml:id="formula_42">(t) l,0 ≡ c (t) l for all t ∈ N.</formula><p>We now consider the slightly modified update rule for 1-GNNs which takes the initial colors l into account. Let the matrix F (0) l,0 ∈ R n×d be an encoding (such as the one-hot encoding) of l such that F (0) l,0 is linearly independent modulo equality. Then</p><formula xml:id="formula_43">F (t+1) l,0 = Λ A,0,W,b (F (t) l,0 ) = F (0) l,0 , Λ A,W,b Λ A,0,W,b (F (t) l,0 )</formula><p>. We now show that this version of GNNs, which can be implemented by the simple variant (1) of GNNs provided in the main paper, is equivalent to 1-WL on colored graphs, proving the main theorem.</p><p>Proof of Theorem 2 (sketch). We prove the theorem by induction over the iteration t. The initial colorings are chosen consistent with l providing c</p><formula xml:id="formula_44">(0) l,0 ≡ F (0) l,0 . For the induction step we assume c (t) l,0 ≡ F (t)</formula><p>l,0 for iteration t. We know by Lemma 11 that the inner part Λ A,W,b Λ A,0,W,b (F (t) l,0 ) of the update rule results in color classes which are identical to the ones thatΓ G would produce. This implies that restricted to each color class Q from Q</p><formula xml:id="formula_45">(t) 1 , . . . , Q (t) q of iteration t, the new color classes Q (t+1) 1 ∩Q, . . . , Q (t+1) q ∩Q restricted to Q match the coloring c (t+1) l,0 | Q that is c (t+1) l,0</formula><p>restricted to Q. This holds as within one color class, the common color of the nodes contains no further information as shown in Lemma 7. Observe that colors are represented by linearly independent row vectors. This especially holds for</p><formula xml:id="formula_46">F (t+1) = Λ A,W,b Λ A,0,W,b (F (t) l,0 ) . In order to show that F (t+1) l,0</formula><p>represents the coloring c (t+1) l,0 we have to prove two properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Given Q</head><formula xml:id="formula_47">(0) i = Q (0) j and u ∈ Q (0) i , v ∈ Q (0) j , we have F (t+1) l,0 (u) = F (t+1) l,0</formula><p>(v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">F</head><p>(t+1) l,0 is linearly independent modulo equality.</p><p>In the first item we interpret F i , all vectors are linearly independent modulo equality as F (t+1) (without subscript) is linearly independent modulo equality. This then extends to all combinations of old color classes and colors from F (t+1) as F (0) l,0 is also linearly independent modulo equality. By induction this chows that c (t) l,0 ≡ F (t) l,0 for all t. Note that the width of the matrices F (t+1) l,0 can be bounded by 2n. The width of F (0) l,0 can trivially be bounded by n (in the worst case every node has a different initial color). The width of F (t+1) can also be bounded by n using Remark 13 as it is the output of a computation using Lemma 11. Using Corollary 15 for the step from c (t+1) l,0 to c (t+1) l finishes the proof of 2.</p><p>Adaptation to the ReLu activation function.</p><p>The above framework can be easily adapted to use the ReLu function as the activation function of our choice, as follows. Recall that the sgn activation function was applied to the matrix C − J, where the matrix C is defined via Equation 9. We claim that a two-fold application of ReLu, interspersed by some elementary matrix operations, also yields the matrix sgn(C − J). This ensures that we can use our GNN architecture to work with the ReLu activation function.</p><p>The proof is as follows. The first application of ReLu to the matrix −(C − J) yields the matrix C (1) := σ ReLu (J − C), which is a lower triangular matrix satisfying C (1) ij = 0 for i &lt; j and C (1) ij &gt; 0 for i ≥ j Let δ be the smallest positive value occurring in C (1) . Note that δ is well defined as there are only a finite number of possible binary input matrices (with values {−1, 1}) for which C (1) can deterministically be computed. The matrix C (2) := − 2 δ C (1) + 2J then satisfies C</p><p>(2) ij = 2 for i &lt; j and C</p><p>(2) ij &lt; 0 for i ≥ j. Another application of ReLu yields an upper triangular matrix C (3) := σ ReLu (C (2) ), where every non-zero entry is equal to 2. Subtracting J from C (3) then yields sgn (C). We now show that those operations can be represented by the GNN architecture. Recall that our GNN architecture was given by stacking a number of iterations of the form, in matrix notation</p><formula xml:id="formula_48">F (t) = σ F (t−1) W (t) 1 + AF (t−1) l W (t) 2 + bW (t) 3<label>(16)</label></formula><p>where the W (t−1) 3 term is the bias that is added in each iteration and σ is a non-linear function such as tanh or ReLu written as σ ReLu .</p><p>To compute σ ReLu (J − C) we can choose W 2 to simulate a computation on sgn (C) instead of sgn (C + J) which works by the linearity of the inner computation. Every layer described by Equation <ref type="formula" target="#formula_10">(16)</ref> is followed by an adjustment layer plus a few modifications in both the previous and upcoming step. As a result, we have a constant (two-fold) increase in the number of layers in our GNN set-up. The following corollary summarizes the above discussion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Two colorings are equivalent if c d and d c, and we write c ≡ d. A color class Q ⊆ V (G) of a node coloring c is a maximal set of nodes with c(v) = c(w) for every v, w in Q. Moreover, let [1 : n] = {1, . . . , n} ⊂ N for n &gt; 1, let S be a set then the set of k-sets [S] k = {U ⊆ S | |U | = k} for k ≥ 2, which is the set of all subsets with cardinality k, and let { {. . .} } denote a multiset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Pooling from 2to 3-GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the proposed hierarchical variant of the k-GNN layer. For each subgraph S on k nodes a feature f is learned, which is initialized with the learned features of all (k − 1)-element subgraphs of S. Hence, a hierarchical representation of the input graph is learned. encoding of the isomorphism type of G[s] labeled by l. In each layer t &gt; 0, we compute new features by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>I M D B -B I N I M D B -M U L P T C -F M N C I 1 M U TA G P T C -M R Kernel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2: Mean absolute errors on the Q M 9 dataset. The far-right column shows the improvement of the best k-GNN model in comparison to the 1-GNN baseline. Target Method D T N N (Wu et al. 2018) M P N N (Wu et al. 2018) 1-G N N 1-2-G N N 1-3-G N N 1-2-3-G N N Gain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Kernel Baselines. We use the Graphlet kernel<ref type="bibr" target="#b23">(Shervashidze et al. 2009</ref>), the shortest-path kernel<ref type="bibr" target="#b1">(Borgwardt and Kriegel 2005)</ref>, the Weisfeiler-Lehman subtree kernel (W L)<ref type="bibr" target="#b24">(Shervashidze et al. 2011)</ref>, the Weisfeiler-Lehman Optimal Assignment kernel (W L -O A)<ref type="bibr" target="#b15">(Kriege, Giscard, and Wilson 2016)</ref>, and the global-local k-WL<ref type="bibr" target="#b21">(Morris, Kersting, and Mutzel 2017)</ref> with k in {2, 3} as kernel baselines. For each kernel, we computed the normalized Gram matrix. We used the C-SVM implementation of LIBSVM (Chang and Lin 2011) to compute the classification accuracies using 10-fold cross validation. The parameter C was selected from {10 −3 , 10 −2 , . . . , 10 2 , 10 3 } by 10-fold cross validation on the training folds. Neural Baselines. To compare GNNs to kernels we used the basic 1-GNN layer of Equation(5), DCNN (Wang et al. 2018), PatchySan (Niepert, Ahmed, and Kutzkov 2016), DGCNN (Zhang et al. 2018). For the Q M 9 dataset we used a 1-GNN layer similar to (Gilmer et al. 2017), where we replaced the inner sum of Equation (5) with a 2-layer MLP in order incorporate edge features (bond type and distance information). Moreover, we compare against the numbers provided in (Wu et al. 2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Lemma 14 .</head><label>14</label><figDesc>Let (G, l) be a colored graph, v, w ∈ V (G), and q ∈ N such that c (w) for all t ≥ t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>and F (t+1) . The first property holds as the row vectors of F (0) l,0 are clearly different, as otherwise Q i = Q j . The second property follows from the fact that linear independence cannot be lost by extending a matrix. Thus within each old color class Q (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>as the all-1 matrix and b = 1. Correspondingly, for σ ReLu (− 2 δ C (1) +2J) we can choose W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracies in percent on various graph benchmark datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that in very recent work, GNNs have shown superior results over kernels when using advanced pooling techniques<ref type="bibr" target="#b26">(Ying et al. 2018b</ref>). Note that our layers can be combined with these pooling layers. However, we opted to use standard global pooling in order to compare a typical GNN implementation with standard off-the-shelf kernels.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the German research council (DFG) within the Research Training Group 2236 UnRAVeL and the Collaborative Research Center SFB 876, Providing Information by Resource-Constrained Analysis, projects A6 and B2.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In the following we provide proofs for Theorem 1, Theorem 2, Proposition 3, and Proposition 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 1</head><p>Theorem 5 (Theorem 1 in the main paper). Let (G, l) be a labeled graph. Then for all t ≥ 0 and for all choices of initial colorings f (0) consistent with l, and weights W (t) , c (t) l f <ref type="bibr">(t)</ref> .</p><p>For the theorem we consider a single iteration of the 1-WL algorithm and the GNN on a single graph.</p><p>Proof of Theorem 1. We show for an arbitrary iteration t and nodes u, v ∈ V (G), that c</p><p>(v) we know from the refinement step of the 1-WL that the old colors c</p><p>} be the multisets of feature vectors of the neighbors of u and v respectively. By the induction hypothesis, we know that M u = M v and f (t) (u) = f (t) (v) such that independent of the choice of f merge and f aggr we get f (t+1) (u) = f (t+1) (v). This holds as the input to both functions f merge and f aggr is identical. This proves c</p><p>and thereby the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 2</head><p>Theorem 6 (Theorem 2 in the main paper). Let (G, l) be a labeled graph. Then for all t ≥ 0 there exists a sequence of weights W (t) , and a 1-GNN architecture such that</p><p>For the proof we start by giving the proof for graphs where all nodes have the same initial color and then extend it to colored graphs. In order to do that we use a slightly adapted but equivalent version of the 1-WL. Note that the extension to colored graphs is mostly technical, while the important idea is already contained in the first case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncolored Graphs</head><p>Let Γ G be the refinement operator for the 1-WL, mapping the old coloring c</p><p>We first show that for uncolored graphs this is equivalent to the update ruleΓ G :</p><p>We denote J as the all-1 matrix where the size will always be clear from the context.</p><p>Proof. Let t ∈ N be minimal such that there are v, w with</p><p>Corollary 16 (ReLu activation). Let (G, l) be a labeled graph. Then there exists a sequence (W t ) t&gt;0 and a 1-GNN architecture based on the simple architecture described in (5) in the main paper using ReLu as activation function, such that c t l = f (2t) for all t ≥ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proofs of Proposition 3 and Proposition 4</head><p>Proposition 17 (Proposition 3 in the main paper). Let (G, l) be a labeled graph and let k ≥ 2. Then for all t ≥ 0, for all choices of initial colorings f</p><p>k consistent with l and for all weights W (t) ,</p><p>k . Proof. The proof follows the arguments in the proof of Theorem 1. We therefore only provide a brief proof by induction on the iteration t. For the base case, i.e. iteration t = 0, the statement holds because the initial coloring f (0) k is chosen to be consistent with the isomorphism types d 0 k,l . For the inductive step, assume that the statement holds until iteration t − 1. Consider two tuples u and v which (i) are not distinguished in the first (t − 1) iterations and (ii) are not distinguished in the t th iteration. Therefore, u and v must have equal number of neighbors from every color class. This implies that the k-GNN update rule yields the same output for u and v. Hence, if two such tuples are distinguished by the k-GNN in the t th iteration, they must be distinguished by the k-WL as well. This finishes the induction and proves the proposition.</p><p>Proposition 18 (Proposition 4 in the main paper). Let (G, l) be a labeled graph and let k ≥ 2. Then for all t ≥ 0 there exists a sequence of weights W (t) and a k-GNN architecture such that</p><p>Proof. Let us simulate the set-based k-WL on a n-vertex graph G via a 1-WL on a graph G ⊗k on O(n k ) vertices, defined as follows. The vertex set of G ⊗k is the set [V (G)] k of all k-element subsets of V (G). The edge set of G ⊗k is defined as follows: two sets s and t are connected by an edge in G ⊗k if and only if |s ∩ t| = k − 1. Observe that the neighborhood of a vertex s in this graph is exactly the set N (s) defined earlier. The initial labeling of the vertices of the graph G ⊗k is determined as follows: For s ∈ V (G ⊗k ) the initial label of s is its isomorphism type. For the above construction, it immediately follows that performing the 1-WL on the graph G ⊗k yields the same coloring, as the one obtained by performing k-WL for the graph G. It remains to define the sequence (W t ) t&gt;0 such that k-GNN simulates the set based k-WL on G. Applying Theorem 2 to the graph G ⊗k results in a sequence W t such that the 1-GNN can simulate 1-WL on G ⊗k using W t . Hence, this sequence can be directly used in the k-GNN to simulate k-WL on G.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the power of color refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arvind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Fundamentals of Computation Theory</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="page" from="339" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Canonical labelling of graphs in linear average time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
	<note>ICDM</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fürer</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fürer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Immerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">; X</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno>27:1-27:27</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>Defferrard, X., and Vandergheynst</editor>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
	<note>An optimal lower bound on the number of variables for graph identifications</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SplineCNN: Fast geometric deep learning with continuous B-spline kernels</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6533" to="6542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flach</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Otto</surname></persName>
		</author>
		<idno>Gilmer et al. 2017</idno>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="797" to="844" />
		</imprint>
	</monogr>
	<note>Pebble games and linear equations</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Descriptive Complexity, Canonisation, and Definable Graph Structure Theory. Lecture Notes in Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning with similarity functions on graphs using matchings of geometric embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<editor>Kashima, Tsuda, and Inokuchi</editor>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kersting</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graphs identified by logics with counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schweitzer</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MFCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="319" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The multiscale laplacian graph kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2982" to="2990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giscard</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Wilson ; Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1615" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deriving neural architectures from sequence and graph kernels</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2024" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting similar programs via the Weisfeiler-Leman graph kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Reuse</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="315" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic generation of complementary descriptors with molecular graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Merkwirth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lengauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1159" to="1168" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Merkwirth and Lengauer</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Milo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5594</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kersting</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Limnios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Reymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename></persName>
		</author>
		<idno>Nikolentzos et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<meeting><address><addrLine>Newman</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2864" to="75" />
		</imprint>
	</monogr>
	<note>SIAM review</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computational capabilities of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SchNet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shervashidze</surname></persName>
		</author>
		<idno>abs/1801.07829</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
	<note>JMLR. Xu et al. 2018. Representation learning on graphs with jumping knowledge networks</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A structural smoothing framework for robust graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2134" to="2142" />
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<idno>Zhang et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4428" to="4435" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
