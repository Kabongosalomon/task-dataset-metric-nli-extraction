<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Label-Wise Document Pre-Training for Multi-Label Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-15">15 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
							<email>liuhan@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center of Intelligence Science and Technology</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caixia</forename><surname>Yuan</surname></persName>
							<email>yuancx@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center of Intelligence Science and Technology</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
							<email>xjwang@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center of Intelligence Science and Technology</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Label-Wise Document Pre-Training for Multi-Label Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-15">15 Aug 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Pre-Training · Document Representation · Multi-Label Classifica- tion</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major challenge of multi-label text classification (MLTC) is to stimulatingly exploit possible label differences and label correlations. In this paper, we tackle this challenge by developing Label-Wise Pre-Training (LW-PT) method to get a document representation with label-aware information. The basic idea is that, a multi-label document can be represented as a combination of multiple label-wise representations, and that, correlated labels always cooccur in the same or similar documents. LW-PT implements this idea by constructing label-wise document classification tasks and trains label-wise document encoders. Finally, the pre-trained label-wise encoder is fine-tuned with the downstream MLTC task. Extensive experimental results validate that the proposed method has significant advantages over the previous state-of-the-art models and is able to discover reasonable label relationship. The code is released to facilitate other researchers. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-label text classification (MLTC) is a task of assigning a document into one or more class labels. In recent years, MLTC have been widely used in many scenarios, such as tag recommendation <ref type="bibr" target="#b6">[7]</ref>, information retrieval <ref type="bibr" target="#b7">[8]</ref>, sentiment analysis <ref type="bibr" target="#b3">[4]</ref>, and so on. Different from traditional single label classification, where one instance is associated with one target label, in MLTC, a text is naturally associated with multiple labels. This makes it both an essential and challenging task in Natural Language Processing (NLP).</p><p>A straightforward solution to MLTC is to decompose the problem into a series of binary text classification problems <ref type="bibr" target="#b2">[3]</ref>, each for one label. Such a solution, however, neglects the fact that information of one label may be helpful for learning another related label, whereas in real-world multi-label text classification applications, one label might be associated with other labels. For example, in music tagging task, music style "metal" is naturally associated with "rock". It is well-accepted that, in order to achieve a good performance, we should not only consider the difference between labels, but also correlation. Especially when some labels have insufficient training examples, the label correlations may provide helpful extra information.</p><p>In recent years, deep learning models such as CNN <ref type="bibr" target="#b9">[10]</ref> and LSTM <ref type="bibr" target="#b8">[9]</ref> have been firmly established as state-of-the-art approaches in text representation. However, these methods cannot learn a label difference since they simply use logistic regression for each label independently to achieve multi-label classification. There are also sequence learning models that attempt for MLTC, such as CNN-RNN <ref type="bibr" target="#b4">[5]</ref>, and Sequence Generation Model (SGM) <ref type="bibr" target="#b17">[18]</ref>, which generate a sequence of possible labels using a RNN decoder. Although the label difference can be captured by attention on the state of decoder, correlation of any two labels cannot be dynamically modeled through the fixed recurrent decoder.</p><p>To alleviate the above existing problems, we proposed a novel pre-training task and model Label-Wise Pre-Training (LW-PT) to get label-wise document representation. Given a target document and one label it possesses, we sample a document as a positive example which also has this label, and several documents as negative ones that don't take this label. The pre-training goal is to learn a classifier distinguishing between positive and negative examples. In this way, we can train a document encoder that captures label-sensitive information. Finally, each document is represented as a concatenation of all label-wise representations.</p><p>Specificly, to explicitly model the label difference, we propose two label-wise encoders by self-attention mechanism into the pre-training task, including Label-Wise LSTM (LW-LSTM) encoder for short documents and Hierarchical Label-Wise LSTM (HLW-LSTM) for long documents. For document representation on each label, they share the same LSTM layer and use different attention weights for different labels.</p><p>Obviously, labels appeared together in a document may have similar concepts. For example, music style label "metal" and "rock" tend to appear together, thus pre-training tasks for these two labels may share the same or similar training samples. Therefore, such label-wise document representation can also capture the label correlations.</p><p>The label-wise document representation is fine-tuned with a MLP layer for multilabel classification. Experiments demonstrate that our method achieves a state-of-art performance and has a substantial improvement compared with several strong baselinses.</p><p>Our contributions are as follows:</p><p>1. We propose a novel label-wise pre-training task and model LW-PT for MLTC task, which encodes document with a combination of label-wise representations by effectively exploiting both label difference and correlation. 2. Two label-wise encoder LW-LSTM and HLW-LSTM are proposed for pre-training and fine-tuning, capturing label-aware information for both short and long document. 3. To the best of our knowledge, this is the first pre-training task to obtain a label-wise document representation. Experiments show that the proposed method achieves outstanding performance than previous approaches on several datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>This section introduces the proposed method, including pre-trained task and multi-label classification model, as shown in the <ref type="figure" target="#fig_0">Figure 1</ref>. The former constructs a label-wise rep-resentation for document, and the latter fuses the multi-label representation for MLTC. is the downstream MLTC model.</p><formula xml:id="formula_0">C-Encoder T-Encoder T C- C+ C- … C-Encoder C-Encoder</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task &amp; Model</head><p>We design a multi-label pre-training task and model Label-Wise Pre-Training (LW-PT). When given a target document T and it's label set {1, ..., k, ..., l}, we sample a positive example C + which has the same label k as target and several negative examples C − which doesn't have the label k. This two parts are combined as candidates {C i } n i=1 with size n. The training goal is to discriminate the positive example from all {C i } n i=1 . We do this for each label k in the label set of targer document to make training samples. <ref type="figure" target="#fig_0">Figure 1</ref>-a shows the architecture of pre-training task and model LW-PT. We denote the target encoder as T-Encoder and candidates encoder as C-Encoder. They have different parameters.</p><p>Therefore, as for the label k, we calculate the representation for the target document T and candidates {C i } n i=1 , and obtain Q t k ∈ R 2d and Q ci k ∈ R 2d .</p><formula xml:id="formula_1">Q t k = T-Encoder(T, k) (1) Q ci k = C-Encoder(C i , k)<label>(2)</label></formula><p>Further, the similarity of any two documents is directly calculated by dot product and trained using the negative log-likelihood loss function.</p><formula xml:id="formula_2">L pt = E t,k,c + ,c − − log exp ((Q t k ) T Q c + k ) exp ((Q t k ) T Q c + k ) + c − exp ((Q t k ) T Q c − k )<label>(3)</label></formula><p>Obviously, we can choose different encoder for it. In MLTC task, different labels may attend on differnt part of the document. Therefore, we design a label-wise encoder. Normally, Label-Wise LSTM (LW-LSTM) can meet the requirements. However, LSTM is not effective for long documents. Inspired by Hierarchical Attention Network (HAN) <ref type="bibr" target="#b18">[19]</ref>, we also introduce Hierarchical Label-Wise LSTM(HLW-LSTM) to model long documents with several sentences. The details are described in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LW-LSTM</head><p>We use BiLSTM and self-attention mechanism to get document representation Q k ∈ R 2d on label k. Denote the document word-level embedding as D ∈ R t×d , where t is the maximum number of words in a document, d is the size of embedding dimision.</p><formula xml:id="formula_3">H = BiLSTM(D) (4) α k = softmax(HU k ) (5) Q k = (α k ) T H (6) where H ∈ R t×2d , α k ∈ R t×1 , and U k ∈ R 2d×1 is a trainable parameter for label k.</formula><p>HLW-LSTM Similar as Hierarchical Attention Networks (HAN) <ref type="bibr" target="#b18">[19]</ref>, we use BiL-STM and hierarchical attention mechanism. As for the label k and document word-level embedding D ∈ R m×t×d , where m is the maximum number of sentences in a document, t is the maximum number of words in a sentence, d is the size of embedding dimision. Thus, D j ∈ R t×d represents the j-th sentence for a given document D.</p><formula xml:id="formula_4">H w j = BiLSTM(D j )<label>(7)</label></formula><p>where H w j ∈ R t×2d . After that, we obtain the j-th sentence vector [S k ] j ∈ R 2d on a specific label k.</p><formula xml:id="formula_5">[α w k ] j = softmax(H w j U w k ) (8) [S k ] j = ([α w k ] j ) T H w j (9) where [α w k ] j ∈ R t×1 . And U w k ∈ R 2d×1</formula><p>is the word-level context vector on label k, which is used to measure the importance of each word on current label. This is a trainable parameter and randomly initialized.</p><p>Thus, each sentence vector S k ∈ R m×2d for document D is obtained. Same as above, we use BiLSTM and self-attention to get document representation Q k ∈ R 2d .</p><formula xml:id="formula_6">H s k = BiLSTM(S k ) (10) α s k = softmax(H s k U s k )<label>(11)</label></formula><formula xml:id="formula_7">Q k = (α s k ) T H s k<label>(12)</label></formula><p>where</p><formula xml:id="formula_8">H s k ∈ R m×2d , α s k ∈ R m×1 . And U s k ∈ R 2d×1</formula><p>is the sentence-level context vector on label k, which is used to measure the importance of each sentence on current label. This is a trainable parameter and randomly initialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-Label Classification</head><p>After pre-training, we use T-Encoder and C-Encoder in the downstream model for MLTC task. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>-b, we concatenate the output of T-Encoder and C-Encoder to get document representation for each label k. Then, each label-wise representation is also concatenated. And we obtain M ∈ R l×4d as the final representation for a given document D.</p><formula xml:id="formula_9">Q t k = T-Encoder(D, k) (13) Q c k = C-Encoder(D, k) (14) M k = [Q t k ; Q c k ]<label>(15)</label></formula><p>After that, we simplely use a MLP layer to predict the probability of each labelŷ.</p><formula xml:id="formula_10">y = sigmoid(W T M )<label>(16)</label></formula><p>where W ∈ R (l×4d)×l is a trainable parameter. We use the cross-entropy loss function for MLTC task.</p><formula xml:id="formula_11">L cls = E D − l k=1 (y k logŷ k + (1 − y k ) log(1 −ŷ k ))<label>(17)</label></formula><p>whereŷ k is the predict probability of label k for current document. And y k is the ground truth of whether label k appeared in current document.</p><p>It should be noted that T-Encoder and C-Encoder are fine-tuned in the MLTC task at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we evaluate the proposed method on two datasets with short or long documents, and also compare with the previous state-of-art models on several widely used metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We use two MLTC datasets RMSC <ref type="bibr" target="#b19">[20]</ref> and AAPD <ref type="bibr" target="#b17">[18]</ref> with different length and language of documents. The former has longer documents with several sentences in Chinese. And the latter is shorter in English. <ref type="table" target="#tab_0">Table 1</ref> shows some statistics.</p><p>-RMSC-V2 <ref type="bibr" target="#b19">[20]</ref>: The dataset is collected from a popular Chinese music review website 2 . For each music, it includes a set of human annotated styles, and associated reviews. 22 styles are defined in it. The dataset contains over 7.1k samples, 288K reviews, and 3.6M words. However, the initial version didn't contains a certain split. Thus, we split the dataset into trian/valid/test by the same ratio as the original paper. -AAPD <ref type="bibr" target="#b17">[18]</ref>: The dataset is collected from a English academic website <ref type="bibr" target="#b2">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evalution Metrics</head><p>We use four widely used evaluation metrics, the same as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>-One-Error: One-error calculates the fraction of samples whose top-1 predicted label is not in the ground truth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Details</head><p>For RMSC-V2 dataset, we use jieba 4 toolkit tokenizer and train word embedding by Skip-gram model <ref type="bibr" target="#b11">[12]</ref>. The embedding dimision and hidden size is 100. For AAPD dataset, we also train word embedding by Skip-gram model <ref type="bibr" target="#b11">[12]</ref>. The embedding dimision and hidden size is 256.</p><p>We use 2 layer LSTM for LW-LSTM encoder. And a hierarchical LSTM layer for HLW-LSTM encoder. In addition, Adam <ref type="bibr" target="#b10">[11]</ref> optimizer with learning rate 0.001 is used. We add layer normalization <ref type="bibr" target="#b1">[2]</ref> and dropout with probability 0.2. In pre-training procedure, 3k iterations are runned and the number of candidates documents is 3. Batch size is 128. In fine-tuning, 20 epochs are runned.</p><p>It should be noted that the pre-training procedure is on dataset of train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Baselines</head><p>We use several MLTC baselines to compare with our method.</p><p>-Binary Relevance (BR) <ref type="bibr" target="#b2">[3]</ref> transforms the multi-label problem into several singlelabel classification problems and ignore the correlation between labels. -Classifier Chains (CC) <ref type="bibr" target="#b15">[16]</ref> transforms the multi-label problem into a chain of several single-label classification problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results</head><p>The complete results on RMSC-V2 and AAPD datasets are shown in <ref type="table">Table 2</ref>. We calculate the One-Error, Hamming-Loss, Macro F1 score and Micro F1 score for several models. Compared to the previous approaches, our LW-LSTM encoder or HLW-LSTM encoder with pre-training mechanism presents a outstanding performance. Adding the fine-tuning procedure further improve the performance on a substantial margin. It should be noted that LW-LSTM model denotes directly use the LW-LSTM encoder in the downstream task model without pre-training mechanism. And the same as HLW-LSTM model. We can see that pre-training and fine-tuning have a significant contribution to the performance. But for which label-wise encoder to use, it's determined by the dataset. For long documents with several sentences (e.g. RMSC), we suppose to choose HLW-LSTM encoder with hierarchical attention. And for short documents with less sentences (e.g. AAPD), we suppose to use LW-LSTM encoder.</p><p>We can see that the Macro F1 score has an outstanding improvement when pretraining mechanism is used. This is because the pre-training approach improves the learning of labels with less frequency by capturing the correlation between labels. And Macro F1 score is the average of each label's F1 score. These experimental results is consistent with our ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Ablation Study</head><p>In this section, some ablation studies to demonstrate the method proposed above are made on RMSC-V2 dataset.</p><p>Encoder To demonstrate that the label-wise encoder are better for multi-label classification, we introduce the traditional LSTM encoder and HAN encoder into pre-trainig mechanism. As shown in <ref type="table">Table 3</ref>, LW-LSTM encoder has a huge improvemence than LSTM encoder for pre-training mechanism, and the same as HLW-LSTM. It indicates the label-wise approach is very effective and plays a vital role for MLTC task.  <ref type="table">Table 2</ref>. Test results on RMSC-V2 and AAPD dataset. PT denotes the pre-training method. FT denotes the fine-tuned method on downstream task. OE and HL denote one-error and hamming loss respectively. "(+)" represents that higher scores are better and "(-)" represents that lower scores are better. "-" means results are not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size of Candidates</head><p>To further explore the pre-training mechanism, we have tried several different size of candidates, such as 3, 4 and 5. As shown in <ref type="table">Table 4</ref>, three candidates in pre-training model with HLW-LSTM encoder and PT pre-training has better performance. Too many candidates may hurt the learning of documents. Too few candidates makes the task too simple to learn effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Analysis</head><p>To examine the ability of discovering the label correlations, we cite three correlated labels "alternative", "metal" and "rock" in music tagging task of RMSC-V2 dataset for example. In <ref type="table">Table 5</ref>, for target song (i.e. document) "Janes Addiction -Nothings Shocking" possesses labels of "alernative" and "rock". We compute its top 50 similar songs via cosine similarity of document representation learnt respectively from three labels "alternative", "metal" and "rock". For each set of 50 similar songs, we calculate the label frequency (i.e., the proportion of songs that contain a specific label) and demonstrate the statistics in <ref type="table">Table 5</ref>. The proportion smaller than 10% are not presented. From <ref type="table">Table 5</ref> we can find that, for the target song, among its top 50 similar songs derived through "alternative"-wise document representation, 76% have label "rock", 42% "alternative" and 26% "metal". The similar observations can also be found for "metal"-wise and "rock"-wise document representation. This reveals that the proposed label-wise document representation is capable of capturing label correlations. Meanwhile, we can see all three columns has similar results, which means the label-wise representation on the original three labels are also similar. It is also interesting to notice that, even the target song doesn't hold the label "metal", it can also be accurately represented using the "metal"-wise representation due to label "metal" is correlated with "alternative" and "rock".  <ref type="table">Table 5</ref>. Songs proportion over top 50 similar songs on "Janes Addiction -Nothings Shocking". P denotes the proportion of songs containing the list labels. Rep denotes the document representation on a specific label.</p><p>As mentioned above, our pre-training mechanism captures the label correlation. Thus, the labels have lower frequency is also predicted accurately. To demonstrate it, we count the frequency of each label and the corresponding F1 score. As shown in the <ref type="figure" target="#fig_3">Figure 2</ref>, frequency are discretized and average F1 score of labels on RMSC-V2 are calculated with HLW-LSTM encoder. And also LW-LSTM encoder for AAPD dataset.</p><p>Obviously, the model have pre-training and fine-tuning has the best performance at most times and are better for lower frequency labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Case Study</head><p>We present some examples for HLW-LSTM models with or without pre-training and fine-tuning mechanism. As shown in <ref type="table">Table 6</ref>, five typical examples are selected from RMSC-V2 dataset in music tagging task. In the first three examples, models with pretraining accurately predict the whole label set. In the fourth, further using the fine-tuning mechanism obtains a accurate prediction. In the last, our pre-training and fine-tuning models both predict a extra label "pop" which is not appeared in the ground truth. However, we visit the website 5 and find this song also have some labels similar to "pop" but not appear in the dataset. Note that the labels are made by human and maybe not accurate and complete. But for the label "indie" which is not predicted, this is where the model needs futher improvement.  <ref type="table">Table 6</ref>. Examples predicted by HLW-LSTM model with or without pre-training and fine-tuning mechanism on RMSC-V2 dataset. PT denotes the pre-training mechanism. FT denotes the finetuning procedure. Labels with red color means the intersection of three predicts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>There are many researches focus on Multi-Label Text Classification. In earlier years, traditional machine learning methods have been widely used in text classification, such as Naive Bayes, SVM, and so on. Therefore, researchers directly transform the multilabel task into several single-label tasks. Binary relevance (BR) <ref type="bibr" target="#b2">[3]</ref> is the earliest method to learn several single-label classifier independently. Label Powerset (LP) <ref type="bibr" target="#b16">[17]</ref> transforms it to a multi-class problem with one multi-class classifier trained on all unique label. Classifier chains (CC) <ref type="bibr" target="#b15">[16]</ref> further convert it to a chain of single-label tasks. However, these methods cannot learn label-wise document representation and have very limited performance beacuse of insufficient learning.</p><p>In neural network models, sequence-to-sequence (Seq2Seq) framework can represent the labels correlation naturally. Therefore, CNN-RNN <ref type="bibr" target="#b4">[5]</ref> and Sequence Generation Model (SGM) <ref type="bibr" target="#b17">[18]</ref> view the MLTC task as a sequence generation problem. Further, set-RNN <ref type="bibr" target="#b14">[15]</ref> presents an adaptation of RNN sequence models to the problem, where the target is a set of labels, not a sequence. However, these methods lack the dynamically modeling of label correlation through the fixed recurrent decoder. Significantly, the computation efficiency of Seq2Seq framework is a huge challenge for practical application.</p><p>Our label-wise document representation is very similar with word embedding. A word may have several meanings and a single static word embedding is difficult to completely represent the entire word. Thus, dynamic word embedding (i.e. pre-trained language models) is proposed to get a specific document representation with context, such as ELMo <ref type="bibr" target="#b13">[14]</ref>, BERT <ref type="bibr" target="#b5">[6]</ref> and so on. But it's also static for different labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We propose a pre-training task and model LW-PT for multi-label text classification. Specifically, two label-wise encoders LW-LSTM and HLW-LSTM are introduced, which handle both short and long documents. In our method, label difference is modeled by the label-wise encoder. Label correlation is also captured in pre-training by the idea, that labels appeared together in a documents may have similar concepts. Experiments show that our method outperforms the previous approaches by a substantial margin. However, how to introduce unsupervised learning and transfer learning into label-wise pre-training mechanism is a further research in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An overview of the proposed approach. (a) is the pre-training task and model LW-PT. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>-Hamming Loss: Hamming loss calculates the fraction of the wrong labels to the total number of ground truth labels. -Macro-F1: Macro F1 takes the average of each label's F1 score. -Micro-F1: Micro F1 calculates the F1 score over all sample-label pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>-Label Powerset (LP)<ref type="bibr" target="#b16">[17]</ref> transforms the multi-label problem to a multi-class problem with one multi-class classifier trained on all unique label.-CNN<ref type="bibr" target="#b9">[10]</ref> attempts to use a convolutional neural network with several different size of kernels, and predicts each label with logistic regression.-LSTM [9] uses a 2-layer LSTM neural network with self-attention on the last layer to get document representation, and predicts each label with logistic regression.-Hierarchical Attention Network (HAN) [19] uses a hierarchical attention network on word and sentence level to get document representation, and predicts each label with logistic regression. -HAN+LG [20] introduces a label graph matrix into HAN. -CNN-RNN [5] utilizes CNN and RNN on Seq2Seq framework to get labels one by one. -SGM+GE [18] computes a weighted global embedding based on all labels as opposed to just the top one at each timestep. -set-RNN [15] presents an adaptation of RNN sequence models to the problem, where the target is a set of labels, not a sequence. -reg-LSTM [1] proposes a simple BiLSTM architecture with regularization. -MAGNET [13] uses a graph attention network to capture the attentive dependency structure among the labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Average F1 score with frequency of labels on RMSC-V2 and AAPD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>in the computer science field. Each sample contains an abstract and the corresponding subjects. 54 labels are defined and 55,840 samples are included. Statistics of datasets. N, V, M are the number of training, validing or testing samples. L is the size of label set. L is the average number of labels per sample. W is the average number of words per document.</figDesc><table><row><cell>Datasets</cell><cell>N</cell><cell>V</cell><cell>M L L</cell><cell>W</cell></row><row><cell cols="5">RMSC-V2 5020 646 1506 22 2.22 497.09</cell></row><row><cell cols="5">AAPD 53,840 1,000 1,000 54 2.41 167.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Ablation tests on RMSC-V2 dataset with label-wise encoder. Ablation tests on RMSC-V2 dataset with different size of candidates, denoted as n. HLW-LSTM+PT model is used.</figDesc><table><row><cell>Model</cell><cell cols="3">OE(-) HL(-) Macro F1(+) Micro F1(+)</cell></row><row><cell>LSTM+PT</cell><cell>19.99 0.0623</cell><cell>48.36</cell><cell>63.71</cell></row><row><cell cols="2">LW-LSTM+PT 17.53 0.0588</cell><cell>65.37</cell><cell>69.62</cell></row><row><cell>HAN+PT</cell><cell>17.93 0.0623</cell><cell>50.02</cell><cell>64.94</cell></row><row><cell cols="2">HLW-LSTM+PT 15.27 0.0583</cell><cell>67.04</cell><cell>70.68</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/laddie132/LW-PT</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://music.douban.com 3 https://arxiv.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/fxsjy/jieba</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://music.douban.com/subject/1774742/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The research is supported by the Fundamental Research Funds for the Central Universities.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rethinking complex neural network architectures for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4046" to="4051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning multi-label scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Boutell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1757" to="1771" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Senticnet 3: a common and common-sense knowledge base for cognition-driven sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Olsher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajagopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-eighth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ensemble application of convolutional and recurrent neural networks for multi-label text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2377" to="2383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilabel classification via calibrated label ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Mencía</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="153" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilabel classification with meta-level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 33rd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-label text classification using attentionbased graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Selvakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sankarasubbu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11644</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adapting rnn sequence prediction model to multi-label set prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019: Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3181" to="3190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">333</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-label classification: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining (IJDWM)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sgm: Sequence generation model for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2018: 27th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3915" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</title>
		<meeting>the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Review-driven multi-label music style classification by exploiting style correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2884" to="2891" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
