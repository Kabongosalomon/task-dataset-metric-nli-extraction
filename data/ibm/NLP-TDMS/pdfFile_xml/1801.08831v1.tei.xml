<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
							<email>shamil@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">NUS Graduate School for Integrative Sciences and Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NUS Graduate School for Integrative Sciences and Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We improve automatic correction of grammatical, orthographic, and collocation errors in text using a multilayer convolutional encoder-decoder neural network. The network is initialized with embeddings that make use of character Ngram information to better suit this task. When evaluated on common benchmark test data sets (CoNLL-2014 and JF-LEG), our model substantially outperforms all prior neural approaches on this task as well as strong statistical machine translation-based systems with neural and task-specific features trained on the same data. Our analysis shows the superiority of convolutional neural networks over recurrent neural networks such as long short-term memory (LSTM) networks in capturing the local context via attention, and thereby improving the coverage in correcting grammatical errors. By ensembling multiple models, and incorporating an N-gram language model and edit features via rescoring, our novel method becomes the first neural approach to outperform the current state-of-the-art statistical machine translation-based approach, both in terms of grammaticality and fluency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>With the increasing number of non-native learners and writers of the English language around the globe, the necessity to improve authoring tools such as error correction systems is increasing. Grammatical error correction (GEC) is a well-established natural language processing (NLP) task that deals with building systems for automatically correcting errors in written text, particularly in non-native written text. The errors that a GEC system attempts to correct are not limited to grammatical errors, but also include spelling and collocation errors.</p><p>GEC in English has gained much attention within the NLP community recently. The phrase-based statistical machine translation (SMT) approach has emerged as the stateof-the-art approach for this task <ref type="bibr" target="#b4">(Chollampatt and Ng 2017;</ref><ref type="bibr" target="#b12">Junczys-Dowmunt and Grundkiewicz 2016)</ref>, in which GEC is treated as a translation task from the language of "bad" English to the language of "good" English. The translation model is learned using parallel error-corrected corpora (source text that contains errors and their corresponding corrected target text). Although neural network (NN) models Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. have been used as features to improve the generalization of the SMT approach <ref type="bibr" target="#b5">(Chollampatt, Taghipour, and Ng 2016)</ref>, SMT still suffers from limitations in accessing the global source and target context effectively. The treatment of words and phrases as discrete entities during decoding also limits its generalization capabilities. To this end, several neural encoder-decoder approaches were proposed for this task <ref type="bibr" target="#b22">(Xie et al. 2016;</ref><ref type="bibr" target="#b23">Yuan and Briscoe 2016;</ref><ref type="bibr" target="#b11">Ji et al. 2017;</ref><ref type="bibr" target="#b20">Schmaltz et al. 2017</ref>). However, their performance still falls substantially behind state-of-the-art SMT approaches.</p><p>All prior neural approaches for GEC relied on using recurrent neural networks <ref type="bibr">(RNNs)</ref>. In contrast to previous neural approaches, our neural approach to GEC is based on a fully convolutional encoder-decoder architecture with multiple layers of convolutions and attention <ref type="bibr" target="#b10">(Gehring et al. 2017)</ref>. Our analysis shows that convolutional neural networks (CNNs) can capture local context more effectively than RNNs as the convolution operations are performed over smaller windows of word sequences. Most grammatical errors are often localized and dependent only on the nearby words. Wider contexts and interaction between distant words can also be captured by a multilayer hierarchical structure of convolutions and an attention mechanism that weights the source words based on their relevance in predicting the target word. Moreover, only a fixed number of non-linearities are performed on the input irrespective of the input length whereas in RNNs, the number of non-linearities is proportional to the length of the input, diminishing the effects of distant words.</p><p>We further improve the performance by ensembling multiple models. Contrary to prior neural approaches, we use a simpler pre-processing method to alleviate the unknown word problem <ref type="bibr">(Sennrich, Haddow, and Birch 2016)</ref>. Rare words are split into multiple frequent sub-words using a byte pair encoding (BPE) algorithm. One of the major weaknesses of prior neural approaches is that they do not incorporate task-specific features nor utilize large native English corpora to good effect. We use such English corpora in our encoder-decoder model to pre-train the word vectors to be used for initializing the embeddings in the encoder and decoder. We also train an N-gram language model to be used as a feature along with edit operation count features in rescoring to produce an overall better output.</p><p>To summarize, this paper makes the following contribu-tions: (1) We successfully employ a convolutional encoderdecoder model trained on BPE tokens as our primary model to achieve state-of-the-art performance for GEC. Ours is the first work to use fully convolutional neural networks for end-to-end GEC.</p><p>(2) We exploit larger English corpora to pre-train word embeddings and to train an N-gram language model to be used as a feature in a rescorer that is trained to optimize the target metric using minimum error rate training <ref type="bibr" target="#b19">(Och 2003)</ref>. <ref type="formula">(3)</ref> We conduct a comparison of attention mechanisms in typical recurrent architectures and our models, and perform error type performance analysis to identify the strengths of our approach over the current state-of-theart SMT approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>GEC gained much attention within the NLP community after the CoNLL-2014 shared task <ref type="bibr" target="#b18">(Ng et al. 2014</ref>) was organized. The shared task dealt with the correction of all grammatical errors in English essays. Since then, the test set for the shared task has been used to benchmark GEC systems. Statistical machine translation has emerged as the state-of-the-art approach <ref type="bibr" target="#b4">(Chollampatt and Ng 2017)</ref> due to its ability to correct various types of errors and complex error patterns, whereas previous approaches relied on building error type-specific classifiers <ref type="bibr" target="#b20">Rozovskaya et al. 2014)</ref>. The SMT framework largely benefits from its ability to incorporate large error-corrected parallel corpora like the publicly available Lang-8 corpus <ref type="bibr" target="#b15">(Mizumoto et al. 2011)</ref>, additional English corpora for training robust language models (LMs), task-specific features (Junczys-Dowmunt and Grundkiewicz 2016), and neural models <ref type="bibr" target="#b5">(Chollampatt, Taghipour, and Ng 2016)</ref>. However, SMT-based systems suffer from limited generalization capabilities compared to neural approaches and are unable to access longer source and target contexts effectively. To address these issues, several neural encoder-decoder approaches relying on RNNs were proposed for GEC.</p><p>Neural Encoder-Decoder Approaches to GEC <ref type="bibr" target="#b23">Yuan and Briscoe (2016)</ref> first applied a popular neural machine translation model, RNNSearch <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015)</ref>, consisting of a bidirectional RNN encoder and an attention-based RNN decoder. They additionally made use of an unsupervised word alignment model and a word-level statistical translation model to replace unknown words in the output. However, they trained their systems on l.9M sentence pairs from the professionally annotated, non-public Cambridge Learner Corpus (CLC), making their models hard to replicate and compare with. <ref type="bibr" target="#b22">Xie et al. (2016)</ref> proposed the use of a character-level recurrent encoder-decoder network for GEC. They trained their models on the publicly available NUCLE <ref type="bibr" target="#b8">(Dahlmeier, Ng, and Wu 2013)</ref> and Lang-8 corpora, along with synthesized examples for frequent error types. They also incorporated an N-gram LM trained on a small subset of the Common Crawl corpus (2.2B N-grams) during decoding to achieve an F 0.5 score of 39.97 on the CoNLL-2014 test set. They further used a supervised edit classifier trained on char-acter and word-level edit operation and pre-trained word embedding features to remove spurious edits and improve the F 0.5 score to 40.56. <ref type="bibr" target="#b11">Ji et al. (2017)</ref> proposed a hybrid word-character model based on the hybrid machine translation model of <ref type="bibr" target="#b13">(Luong and Manning 2016)</ref>, by adding nested levels of attention at the word and character level. Similar to <ref type="bibr" target="#b23">(Yuan and Briscoe 2016)</ref>, they also made use of the non-public CLC corpus in training in addition to Lang-8 and NUCLE, resulting in 2.6M sentence pairs. By further adding a web-scale Common Crawl LM that was used in (Junczys-Dowmunt and Grundkiewicz 2016) in a rescoring step, they achieved an F 0.5 score of 45.15 on the CoNLL-2014 test set. Their rescorer was trained using a simple grid search with fixed step size to get the feature weights and did not make use of task-specific features, whereas we use minimum error rate training <ref type="bibr" target="#b19">(Och 2003)</ref> to find optimal feature weights and use edit operation features and LM features.</p><p>More recently, Schmaltz et al. (2017) used a word-level bidirectional LSTM network trained on Lang-8 and NU-CLE (1.4M sentence pairs) with edit operations (insertions, deletions, and substitutions) marked with special tags in the target sentences. Their untuned model and the baseline that did not have edit operation tags marked yielded a high precision and a low recall. However, when they tuned the weights for the edit operations using a grid search maximizing F 0.5 , their recall went up. Without using any additional models or corpora, their approach achieved F 0.5 score of 41.37 on the CoNLL-2014 test set. Their edit operation tagging method and tuning also implicitly modeled edit operation weights. We model edit operations explicitly in our approach by counting and using them as weighted features in our rescorer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Multilayer Convolutional Encoder-Decoder Neural Network</head><p>Encoder-decoder models are most widely used for machine translation from a source language to a target language. Similarly, an encoder-decoder model can be employed for GEC, where the encoder network is used to encode the potentially erroneous source sentence in vector space and a decoder network generates the corrected output sentence by using the source encoding. The attention mechanism (Bahdanau, Cho, and Bengio 2015) selectively weights different parts of the source sentence during decoding, allowing for a different encoding of the source sentence at every decoding time step. We build our models based on an encoder-decoder architecture with multiple layers of convolutions and attention mechanisms, similar to its use in MT by <ref type="bibr" target="#b10">(Gehring et al. 2017)</ref>. The models are trained on words with rare words segmented into sub-words (Sennrich, Haddow, and Birch 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Consider an input source sentence S given as a sequence of m source tokens s 1 , . . . , s m and s i ∈ V s , where V s is the source vocabulary. The last source token, s m , is a special end-of-sentence marker token. The source tokens are embedded in continuous space as s 1 , . . . , s m . The embedding </p><formula xml:id="formula_0">s i ∈ R d is given by s i = w(s i ) + p(i), where w(s i )</formula><p>is the word embedding and p(i) is the position embedding corresponding to the position i of token s i in the source sentence. Both embeddings are obtained from embedding matrices that are trained along with other parameters of the network.</p><p>The encoder and decoder are made up of L layers each. The architecture of the network is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The source token embeddings, s 1 , . . . , s m , are linearly mapped to get input vectors of the first encoder layer, h 0 1 , . . . , h 0 m , where h 0 i ∈ R h and h is the input and output dimension of all encoder and decoder layers. Linear mapping is done by multiplying a vector with weights W ∈ R h×d and adding the biases b ∈ R h :</p><formula xml:id="formula_1">h 0 i = Ws i + b</formula><p>In the first encoder layer, 2h convolutional filters of dimension 3 × h map every sequence of three consecutive input vectors to a feature vector f 1 i ∈ R 2h . Paddings (denoted by &lt;pad&gt; in <ref type="figure" target="#fig_0">Figure 1</ref>) are added at the beginning and end of the source sentence to retain the same number of output vectors as the source tokens after the convolution operations.</p><formula xml:id="formula_2">f 1 i = Conv(h 0 i−1 , h 0 i , h 0 i+1 )</formula><p>where Conv(·) represents the convolution operation. This is followed by a non-linearity using gated linear units (GLU) <ref type="bibr" target="#b9">(Dauphin et al. 2016)</ref>:</p><formula xml:id="formula_3">GLU(f 1 i ) = f 1 i,1:h • σ(f 1 i,h+1:2h ) where GLU(f 1 i ) ∈ R h ,</formula><p>• and σ represent element-wise multiplication and sigmoid activation functions, respectively, and f 1 i,u:v denotes the elements of f 1 i from indices u to v (both inclusive). The input vectors to an encoder layer are finally added as residual connections. The output vectors of the l th encoder layer are given by,</p><formula xml:id="formula_4">h l i = GLU(f l i ) + h l−1 i i = 1, . . . , m</formula><p>Each output vector of the final encoder layer, h L i ∈ R h , is linearly mapped to get the encoder output vector, e i ∈ R d , using weights W e ∈ R d×h and biases b e ∈ R d :</p><formula xml:id="formula_5">e i = W e h L i + b e i = 1, . . . , m</formula><p>Now, consider the generation of the target word t n at the n th time step in decoding, with n − 1 target words previously generated. For the decoder, paddings are added at the beginning. The two paddings, beginning-of-sentence marker and the previously generated tokens, are embedded as t −2 , t −1 , t 0 , t 1 , . . . , t n−1 in the same way as source token embeddings are computed. Each embedding t j ∈ R d is linearly mapped to g 0 j ∈ R h and passed as input to the first decoder layer. In each decoder layer, convolution operations followed by non-linearities are performed on the previous decoder layer's output vectors g l−1 j , where j = 1, . . . , n:</p><formula xml:id="formula_6">y l j = GLU(Conv(g l−1 j−3 , g l−1 j−2 , g l−1 j−1 )</formula><p>where Conv(·) and GLU(·) represent convolutions and nonlinearities respectively, and y l j becomes the decoder state at the j th time step in the l th decoder layer. The number and size of convolution filters are the same as those in the encoder.</p><p>Each decoder layer has its own attention module. To compute attention at layer l before predicting the target token at the n th time step, the decoder state y l n ∈ R h is linearly mapped to a d-dimensional vector with weights W z ∈ R d×h and biases b z ∈ R d , adding the previous target token's embedding:</p><formula xml:id="formula_7">z l n = W z y l n + b z + t n−1</formula><p>The attention weights α l n,i are computed by a dot product of the encoder output vectors e 1 , . . . , e m with z l n and normalized by a softmax:</p><formula xml:id="formula_8">α l n,i = exp(e i z l n ) m k=1 exp(e k z l n ) i = 1, . . . , m</formula><p>The source context vector x l n is computed by applying the attention weights to the summation of the encoder output vectors and the source embeddings. The addition of the source embeddings helps to better retain information about the source tokens.</p><formula xml:id="formula_9">x l n = m i=1 α l n,i (e i + s i )</formula><p>The context vector x l n is then linearly mapped to c l n ∈ R h . The output vector of the l th decoder layer, g l n , is the summation of c l n , y l n , and the previous layer's output vector g l−1 n . g l n = y l n + c l n + g l−1 n The final decoder layer output vector g L n is linearly mapped to d n ∈ R d . Dropout <ref type="bibr" target="#b21">(Srivastava et al. 2014</ref>) is applied at the decoder outputs, embeddings, and before every encoder and decoder layer. The decoder output vector is then mapped to the target vocabulary size (|V t |) and softmax is computed to obtain target word probabilities.</p><formula xml:id="formula_10">o n = W o d n + b o W o ∈ R |Vt|×d , b o ∈ R |Vt| p(t n = w i |t 1 , . . . , t n−1 , S) = exp(o n,i ) |Vt| k=1 exp(o n,k ) where w i is the i th word in the target vocabulary V t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-Training of Word Embeddings</head><p>We initialize the word embeddings for the source and target words with pre-trained word embeddings learned from a large English corpus. Rare words in this English corpus are split into BPE-based sub-word units as we use similar preprocessing for the parallel corpus that is used to train the network. The word embeddings are computed by representing a word as a bag of character N-grams and summing the skipgram embeddings of these character n-gram sequences, using the fastText tool <ref type="bibr" target="#b2">(Bojanowski et al. 2017</ref>). These embeddings have information about the underlying morphology of words and was empirically found to perform better than initializing the network randomly or using word2vec <ref type="bibr" target="#b14">(Mikolov et al. 2013</ref>) embeddings, which treat words as separate entities and have no information about the character sequences that make up the words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>The model is trained using the negative log-likelihood loss function:</p><formula xml:id="formula_11">L = − 1 N N i=1 1 T i Ti j=1 log p(t i,j |t i,1 , . . . , t i,j−1 , S)</formula><p>where N is the number of training instances in a batch, T i is the number of tokens in the i th reference sentence, t i,j is the j th target word in the reference correction for the i th training instance. The parameters are optimized using Nesterov's Accelerated Gradient Descent (NAG) with a simplified formulation for Nesterov's momentum <ref type="bibr" target="#b1">(Bengio, Boulanger-Lewandowski, and Pascanu 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoding</head><p>The encoder-decoder model estimates the probability of target words given the erroneous source sentence S. The best sequence of target words is obtained by a left-to-right beam search. In a beam search, the top b probable candidates at every decoding time step is retained. The top-scoring candidate in the beam at the end of the search will be the correction hypothesis. The model score of a hypothesis is the sum of the log probabilities of the hypothesis words computed by the network. We also perform ensembling during decoding by averaging the predictions from multiple models in order to compute the log probability scores of the hypothesis words. The models used for ensembling have the same architecture but are trained with different random initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rescoring</head><p>In order to incorporate task-specific features and large language models, we re-score the final beam candidates using a log-linear framework. The score of a correction hypothesis sentence T given the source sentence S is given by,</p><formula xml:id="formula_12">score(T, S) = F i=1 λ i f i (T, S)</formula><p>where, λ i and f i are the i th feature weight and feature function respectively, and F is the number of features. The feature weights are computed by minimum error rate training (MERT) <ref type="bibr" target="#b19">(Och 2003</ref>) on the development set. We use the following sets of features in rescoring in addition to the model score of the hypothesis:</p><p>1. Edit operation (EO) features: Three features denoting the number of token-level substitutions, deletions, and insertions between the source sentence and the hypothesis sentence.</p><p>2. Language model (LM) features: Two features, a 5-gram language model score (i.e., the sum of log probabilities of 5-grams in the hypothesis sentence) and the number of words in the hypothesis. Similar to state-of-theart methods, the language model is trained on the webscale Common Crawl corpus <ref type="bibr" target="#b4">(Chollampatt and Ng 2017;</ref><ref type="bibr"></ref> Junczys-Dowmunt and Grundkiewicz 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup Datasets</head><p>We use two public datasets from prior work, Lang-8 (Mizumoto et al. 2011) and NUCLE <ref type="bibr" target="#b8">(Dahlmeier, Ng, and Wu 2013)</ref>, to create our parallel data. Along with the sentence pairs from NUCLE, we extract and use the English sentence pairs in Lang-8 by selecting essays written by English learners and removing non-English sentences from them using a language identification tool 1 . Sentence pairs that are unchanged on the target side are discarded from the training set. A subset of this data, 5.4K sentence pairs from NUCLE, is taken out to be used as the development data for model selection and training the rescorer. The remaining parallel data that is used for training the encoder-decoder NN consists of 1.3M sentence pairs (18.05M source words and 21.53M target words). We also make use of the larger English corpora from Wikipedia (1.78B words) for pre-training the word embeddings, and a subset of the Common Crawl corpus (94B words) for training the language model for rescoring. Corpora of similar size from the Common Crawl have been used by leading GEC systems <ref type="bibr" target="#b4">(Chollampatt and Ng 2017;</ref><ref type="bibr" target="#b11">Ji et al. 2017</ref>; Junczys-Dowmunt and Grundkiewicz 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>Our evaluation setting is the same as that in the CoNLL-2014 shared task. We evaluate our models and compare them to previous systems on the official CoNLL-2014 test set using the F 0.5 score computed using the MaxMatch scorer . Following prior work, we analyze our neural model choices and perform ablation studies on the CoNLL-2013 shared task test set. We also evaluate the fluency of our model outputs on the recently released JFLEG development and test sets <ref type="bibr" target="#b17">(Napoles, Sakaguchi, and Tetreault 2017)</ref>, which have fluency-based rewrites of learner-written sentences done by native writers in order to make the sentences nativesounding and error-free. The GLEU metric is used to as- sess fluency of corrected text when the error-span and errortype annotations are not provided <ref type="bibr" target="#b16">(Napoles et al. 2015)</ref>. We also calculate the F 0.5 score after automatically extracting the annotation span using the scripts released with the JF-LEG dataset.</p><note type="other">System Parallel Is Data Other CoNLL-2014 Test Set Data Public? Corpora Prec. Recall F0.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model and Training Details</head><p>We extend the publicly available PyTorch-based implementation 2 for training multilayer convolutional models initialized with pre-trained embeddings. Both source and target embeddings are of 500 dimensions. Each of the source and target vocabularies consists of 30K most frequent BPE tokens from the source and target side of the parallel data, respectively. Pre-training is done using fastText with one pass on the Wikipedia corpus using a skip-gram model with a window size of 5. Character N-gram sequences of size between 3 and 6 (both inclusive) are used to compute the word embeddings and other parameters are kept to their default values. The embeddings are updated during training of the encoder-decoder NN. Each of the encoder and decoder is made up of seven convolutional layers, with a convolution window width of 3. The number of layers in the encoder and decoder is set based on development set performance after experimenting with 5, 7, and 9 layers. Output of each encoder and decoder layer is of 1024 dimensions. Dropout 2 https://github.com/facebookresearch/fairseq-py with probability 0.2 is applied on the embeddings, convolution layers, and decoder output. We train every model simultaneously on 3 NVIDIA Titan X GPUs with a batch size of 32 on each GPU and perform validation after every epoch concurrently on another NVIDIA Titan X GPU. A learning rate of 0.25 is used with a learning rate annealing factor of 0.1 and a momentum value of 0.99. We use early stopping and select the best model based on the F 0.5 score on the development set. Training a single model takes around 18 hours. During decoding, a beam width of 12 is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compare our systems to all prior neural approaches for GEC and two state-of-the-art (SOTA) systems. The first SOTA system (Junczys-Dowmunt and Grundkiewicz 2016) employs a word-level SMT approach with task-specific features and a web-scale LM trained on the Common Crawl corpus. The second SOTA system <ref type="bibr" target="#b4">(Chollampatt and Ng 2017)</ref> adds an adapted neural network joint model (NNJM) to a word-level SMT system with task-specific features and a web-scale LM, with further improvement by spelling error correction using a character-level SMT system. In order to compare our neural approach to the SMT approach without using other English corpora, we create two baselines using released models of the SOTA system (Chollampatt and Ng 2017). The first (SMT +NNJM in <ref type="table" target="#tab_0">Table 1)</ref> is this word-level SMT-based system retuned after removing all subsidiary models that make use of additional English corpora such as the word-class LM and the web-scale Common Crawl LM. This system has an adapted NNJM and an operation sequence model (OSM), both trained on the parallel data, and has a single LM trained on the target side of the parallel data. Another non-neural SMT baseline (SMT in <ref type="table" target="#tab_0">Table 1</ref>) is created by further removing the adapted NNJM and retuning on our development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on Benchmark Corpora</head><p>We evaluate our systems based on the grammaticality and fluency of their output sentences.</p><p>Grammaticality We first evaluate different variants of our system on the CoNLL-2014 test data <ref type="table" target="#tab_0">(Table 1)</ref>. Our single model without using any additional corpora or rescoring (MLConv) achieves 45.36 F 0.5 . After ensembling four models (4 ens.), the performance reaches 48.05 F 0.5 and outperforms the previous best neural model without LM <ref type="bibr">(Ji et al. 2017) (41.53 F 0.5</ref> ) by a large margin of 6.52 F 0.5 , despite the latter using much more training data including the nonpublic CLC. Our neural systems also substantially outperform the two comparable SMT baselines, 'SMT' and 'SMT +NNJM'. When re-scoring is performed with edit operation (+EO) features, the performance goes up to 49.78 F 0.5 , outperforming a strong SMT-based system (Junczys-Dowmunt and Grundkiewicz 2016) that uses task-specific features and a web-scale Common Crawl language model. Our system, on the other hand, achieves this level of performance without using any additional English corpora or pre-trained word embeddings. When we train our models by initializing with pre-trained fastText word embeddings (MLConv embed ), decode using an ensemble of four models, and rescore with edit operation features, the performance reaches 50.70 F 0.5 . After adding the web-scale LM in rescoring (+LM), our approach reaches 54.13 F 0.5 , outperforming the best previous published result of (Chollampatt and Ng 2017) (F 0.5 = 53.14) that additionally uses a spelling correction component trained on a spelling corpus. This improvement is statistically significant (p &lt; 0.001). When we make use of the spelling correction component in <ref type="bibr" target="#b4">(Chollampatt and Ng 2017</ref>) (+SpellCheck), our performance reaches 54.79, a statistically significant improvement of 1.65 F 0.5 (p &lt; 0.001) over the best previous published result, and establishes the new state of the art for English GEC. All statistical significance tests were performed using sign test with bootstrap resampling on 100 samples.</p><p>Fluency We also measure the fluency of the outputs on the JFLEG development and test sets ( <ref type="table" target="#tab_2">Table 2)</ref>. Our system with rescoring using edit operation features outperforms the stateof-the-art system with a web-scale LM without spell checking <ref type="bibr" target="#b4">(Chollampatt and Ng 2017)</ref> on both datasets and metrics. This is without adding the web-scale LM to our system. After adding the web-scale LM and using the spell checker, our method achieves the best reported GLEU and F 0.5 scores on these datasets. It is worth noting that our models achieve this   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder and Decoder Architecture</head><p>We analyze the performance of various network architectures without using pre-trained word embeddings on the CoNLL-2013 test set <ref type="table" target="#tab_3">(Table 3</ref>). We experiment with using a bidirectional LSTM in the encoder and an attentional LSTM decoder with a soft attention mechanism (Bahdanau, Cho, and Bengio 2015) (BiLSTM in <ref type="table" target="#tab_3">Table 3</ref>), and compare it to single layer convolutional (SLConv) as well as our proposed multilayer convolutional (MLConv) encoder and decoder models. BiLSTM can capture the entire sentence context from left and right for each input word, whereas SLConv captures only a few surrounding words (equal to the filter width of 3). However, MLConv captures a larger surrounding context (7 layers × filter width 3 = 21 tokens) more effectively, causing it to outperform both SLConv and BiL-STM.</p><p>It is interesting to note that the BiLSTM model has a higher precision than the MLConv model, although its recall is lower. We analyze the attention weights of both models <ref type="figure" target="#fig_1">(Figure 2)</ref> on an example sentence from the CoNLL-2013 test set. The attention weights shown for the MLConv model is the averaged attention weights of all decoder layers. It can be seen that BiLSTM produces a sharper distribution placing higher weights on matching source words as opposed to MLConv which places noticeable probability mass on the surrounding context words also. We observed this trend for all other examples that we tried. This could be the reason that causes BiLSTM to frequently output the source words, leading to a fewer number of proposed corrections and consequently, a higher precision. This analysis demonstrates the ability of MLConv in capturing the context better, thereby favoring more corrections than copying of the source words.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization with Pre-trained Embeddings</head><p>We assess various methods of initializing the source and target word embeddings. <ref type="table" target="#tab_5">Table 4</ref> shows the results of initializing the embeddings randomly as well as with word2vec and fastText on the CoNLL-2013 test set. We train skip-gram models with word2vec and use parameters identical to those we use for fastText. fastText embeddings have access to the character sequences that make up the words and hence are better suited to learn word representations taking morphology into account. We also find that initializing with fastText works well empirically, and hence we choose these embeddings to initialize our network when evaluating on benchmark test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis and Discussion</head><p>We perform error type-specific performance comparison of our system and the state-of-the-art (SOTA) system (Chollampatt and Ng 2017), using the recently released ERRANT toolkit <ref type="bibr" target="#b3">(Bryant, Felice, and Briscoe 2017)</ref> on the CoNLL-2014 test data based on F 0.5 . ERRANT relies on a rulebased framework to identify the error type of corrections proposed by a GEC system. The results on four common error types are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. We find that our ensembled model with the rescorer (+EO+LM) performs competitively on preposition errors, and outperforms the SOTA system on noun-number, determiner, and subject-verb agreement errors. One of the weaknesses of SMT-based systems is in correction of subject-verb agreement errors, because a verb and its subject can be very far apart within a source sentence. On the other hand, even our single model (MLConv embed ) without rescoring is superior to the SOTA SMT-based system in terms of subject-verb agreement errors, since it has access to the entire source context through the global attention mechanism and to longer target context through multiple layers of convolutions in the decoder. From our analysis, we find that a convolutional encoderdecoder NN captures the context more effectively compared to an RNN and achieves superior results. However, RNNs can give higher precision, so a combination of both approaches could be investigated in future. Improved language modeling has been previously shown to improve GEC performance considerably. We leave it to future work to explore the integration of web-scale LM during beam search and the fusion of neural LMs into the network. We also find that a simple preprocessing method that segments rare words into sub-words effectively deals with the rare word problem for GEC, and performs better than character-level models and complex word-character models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We use a multilayer convolutional encoder-decoder neural network for the task of grammatical error correction and achieve significant improvements in performance compared to all previous encoder-decoder neural network approaches. We utilize large English corpora to pre-train and initialize the word embeddings and to train a language model to rescore the candidate corrections. We also make use of edit operation features during rescoring. By ensembling multiple neural models and rescoring, our novel method achieves improved performance on both CoNLL-2014 and JFLEG data sets, significantly outperforming the current leading SMT-based systems. We have thus fully closed the large performance gap that previously existed between neural and statistical approaches for this task. The source code and model files used in this paper are available at https://github.com/nusnlp/mlconvgec2018.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of our multilayer convolutional model with seven encoder and seven decoder layers (only one encoder and one decoder layer are illustrated in detail).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of attention weights of BiLSTM and MLConv models. y-axis shows the target words and x-axis shows the source words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Performance of our models compared to the stateof-the-art system (Chollampatt and Ng 2017) on common error types evaluated on the CoNLL-2014 test set based on F 0.5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on the CoNLL-2014 test set. For single models (MLConv and MLConv embed ), average precision, recall, and F 0.5 of 4 models (trained with different random initializations) are reported. (4 ens.) refers to the ensemble decoding of these 4 models. +EO and +LM refer to re-scoring using edit operation and language model features, respectively. +SpellCheck denotes the addition of the publicly available spell checker proposed in<ref type="bibr" target="#b4">(Chollampatt and Ng 2017)</ref>. L8 refers to the Lang-8 corpus, CC refers to Common Crawl, CLC refers to the non-public Cambridge Learner Corpus, and SP refers to the corpus of misspellings.</figDesc><table><row><cell>5</cell></row></table><note>A smaller subset of CC (2.2B words) was used in (Xie et al. 2016) compared to the rest (94B -97B words).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: Results on the JFLEG development and test</cell></row><row><cell cols="3">sets compared to (Chollampatt and Ng 2017) (C&amp;N). For</cell></row><row><cell cols="3">MLConv embed , average F 0.5 and GLEU of 4 models (trained</cell></row><row><cell cols="3">with different random initializations) are reported.</cell></row><row><cell cols="2">Architecture Prec. Recall</cell><cell>F0.5</cell></row><row><cell>BiLSTM</cell><cell cols="2">52.49 10.95 29.84</cell></row><row><cell>SLConv</cell><cell cols="2">43.65 10.23 26.39</cell></row><row><cell>MLConv</cell><cell cols="2">51.90 12.59 31.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Performance of various architectures on the</cell></row><row><cell>CoNLL-2013 test set.</cell></row><row><cell>level of performance without tuning on the JFLEG develop-</cell></row><row><cell>ment set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results of different embedding initializations on theCoNLL-2013 test set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/saffsd/langid.py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their feedback. This research was supported by Singapore Ministry of Education Academic Research Fund Tier 2 grant MOE2013-T2-1-150.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advances in optimizing recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic annotation and evaluation of error types for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Connecting the dots: Towards human-level grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural network translation models for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NUS at the HOO 2012 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J F</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th Workshop on the Innovative Use of NLP for Building Educational Applications</title>
		<meeting>of the 7th Workshop on the Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner English: The NUS Corpus of Learner English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08083</idno>
		<title level="m">Language modeling with gated convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A nested attention neural hybrid model for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Phrasebased machine translation is state-of-the-art for automatic grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Achieving open vocabulary neural machine translation with hybrid wordcharacter models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mining revision log of language learning SNS for automated Japanese error correction of second language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ground truth for grammatical error correction metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL/IJCNLP</title>
		<meeting>ACL/IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">JFLEG: A fluency corpus and benchmark for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoNLL: Shared Task</title>
		<meeting>CoNLL: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schmaltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">; R</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoNLL: Shared Task</title>
		<meeting>CoNLL: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Proc. ACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Avati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09727</idno>
		<title level="m">Neural language correction with character-based attention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grammatical error correction using neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
