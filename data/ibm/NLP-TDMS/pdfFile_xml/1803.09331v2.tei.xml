<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">StarMap for Category-Agnostic Keypoint and Viewpoint Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
							<email>zhouxy@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Karpur</surname></persName>
							<email>akarpur@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
							<email>linjie.luo@snap.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
							<email>huangqx@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">StarMap for Category-Agnostic Keypoint and Viewpoint Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D vision</term>
					<term>Category-agnostic</term>
					<term>Keypoint estimation</term>
					<term>View- point estimation</term>
					<term>Pose estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic keypoints provide concise abstractions for a variety of visual understanding tasks. Existing methods define semantic keypoints separately for each category with a fixed number of semantic labels in fixed indices. As a result, this keypoint representation is infeasible when objects have a varying number of parts, e.g. chairs with varying number of legs. We propose a category-agnostic keypoint representation, which combines a multi-peak heatmap (StarMap) for all the keypoints and their corresponding features as 3D locations in the canonical viewpoint (CanViewFeature) defined for each instance. Our intuition is that the 3D locations of the keypoints in canonical object views contain rich semantic and compositional information. Using our flexible representation, we demonstrate competitive performance in keypoint detection and localization compared to category-specific state-of-the-art methods. Moreover, we show that when augmented with an additional depth channel (DepthMap) to lift the 2D keypoints to 3D, our representation can achieve state-of-the-art results in viewpoint estimation. Finally, we show that our category-agnostic keypoint representation can be generalized to novel categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic keypoints, such as joints on a human body or corners on a chair, provide concise abstractions of visual objects regarding their compositions, shapes, and poses. Accurate semantic keypoint detection forms the basis for many visual understanding tasks, including human pose estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b53">53]</ref>, hand pose estimation <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b54">54]</ref>, viewpoint estimation <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b37">37]</ref>, feature matching <ref type="bibr" target="#b16">[16]</ref>, fine-grained image classification <ref type="bibr" target="#b49">[49]</ref>, and 3D reconstruction <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b10">10]</ref>.</p><p>Existing methods define a fixed number of semantic keypoints for each object category in isolation <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b24">24]</ref>. A standard approach is to allocate a heatmap channel for each keypoint. Or in other words, keypoints are inferred as separate heat maps according to their encoding order. This approach, however, is not suitable when objects have a varying number of parts, e.g. chairs with varying numbers of legs. The approach is even more limiting when we want to share and use keypoint labels of multiple different categories. In fact, keypoints of different categories do share rich compositional similarities. For instance, chairs and tables may share the same configuration of legs, and motorcycles and bicycles all contain wheels. Category-specific keypoint encodings fail to capture both the intra-category part variations and the inter-category part similarities.</p><p>In this paper, we propose a novel, category-agnostic keypoint representation. Our representation consists of two components: 1) a single channel, multi-peak heatmap, termed StarMap, for all keypoints of all objects; and 2) their respective feature ( <ref type="figure" target="#fig_0">Fig. 1</ref>), termed CanViewFeature, which is defined as the 3D locations in a normalized canonical object view (or a world coordinte system). Specifically, StarMap combines the separate keypoint heat maps in previous approaches <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b26">26]</ref> into a single heat map, and thus unifies the detection of different keypoints. CanViewFeature provides semantic discrimination between keypoints, i.e., through their locations in the normalized canonical object view. One intuition behind this representation is that the distribution of keypoints' 3D locations in the canonical object view encodes rich semantic and compositional information. For example, the locations of all legs are close to the ground, and they are below the seats. Our representation can be obtained via supervised training on any standard datasets with 3D viewpoint annotations, such as Pascal3D+ <ref type="bibr" target="#b44">[44]</ref> and ObjectNet3D <ref type="bibr" target="#b43">[43]</ref>.</p><p>Our representation provides the flexibility to represent varying numbers of keypoints across different categories by eliminting the hard-encoding of keypoints. Additionally, we demonstrate that our representation can still achieve competitive results in keypoint detection and localization compared to the stateof-the-art category-specific approaches <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b37">37]</ref> (Sec 4.2) by using simple nearest neighbor association on the category-level keypoint templates.</p><p>One direct application of our representation is viewpoint estimation <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b21">21]</ref>, which can be achieved by solving a perspective-n-points (PnP) <ref type="bibr" target="#b13">[13]</ref> problem to align the CanViewFeature with the StarMap. Further, we observed considerable performance gains in this task by augmenting the StarMap with an additional depth channel (DepthMap) to lift the 2D image coordinates into 3D. We report state-of-the-art performance compared to previous viewpoint estimation methods <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b37">37]</ref> with ablation studies on each component. Finally, we show our method works well when applied to unseen categories. Full code is publicly available at https://github.com/xingyizhou/StarMap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Keypoint estimation. Keypoint estimation, especially human joint estimation <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b51">51]</ref> and rigid object keypoint estimation <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b52">52]</ref>, is a widely studied problem in computer vision. In the simplest case, a 2D/3D keypoint can be represented by a 2/3-dimension vector and learned by supervised regression. Toshev et al. <ref type="bibr" target="#b35">[35]</ref> first trained a deep neural network for 2D human pose regression and Li et al. <ref type="bibr" target="#b14">[14]</ref> extended this approach to 3D. Starting from Tompson et al. <ref type="bibr" target="#b34">[34]</ref>, the heatmap representation has dominated the 2D keypoint estimation community and has achieved great success in both 2D human pose estimation <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b46">46]</ref> and single category man-made object keypoint detection <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b41">41]</ref>. Recently, the heatmap representation has been generalized in various different directions. Cao et al. <ref type="bibr" target="#b3">[4]</ref> and Newell et al. <ref type="bibr" target="#b23">[23]</ref> extended the single peak heatmap (for single keypoint detection) to a multi-peak heatmap where each peak is one instance of a specific type of keypoint, enabling bottom-up, multiperson pose estimation. Pavlakos et al. <ref type="bibr" target="#b27">[27]</ref> lifted the 2D pixel heatmap to a 3D voxel heatmap, resulting in an end-to-end 3D human pose estimation system. Tulsiani et al. <ref type="bibr" target="#b37">[37]</ref> and Pavlakos et al. <ref type="bibr" target="#b26">[26]</ref> stacked keypoint heatmaps from different object categories together for multi-category object keypoint estimation. Despite good performance gained by these approaches, they share a common limitation: each heatmap is only trained for a specific keypoint type from a specific object. Learning each keypoint individually not only ignores the intra-category variations or inter-category similarities, but also makes the representation inherently impossible to be generalized to unknown keypoint configurations for novel categories.</p><p>Viewpoint estimation. Viewpoint estimation, i.e., estimating an object's orientation in a given frame, is a practical problem in computer vision and robotics <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b26">26]</ref>. It has been well explored by traditional techniques that solve for transformations between corresponding points in the world and image views; this is known as the Perspective-n-Point Problem <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b18">18]</ref>. Lately, viewpoint estimation accuracy and utility have been greatly improved in the deep learning era. Tulsiani et al. <ref type="bibr" target="#b37">[37]</ref> introduced viewpoint estimation as a bin classification prob-lem for each viewing angle (azimuth, elevation and in-plane rotation). Mousavian et al. <ref type="bibr" target="#b21">[21]</ref> augmented the bin classification scheme by adding regression offsets within each bin so that predictions could be more fine-grained. Szeto et al. <ref type="bibr" target="#b31">[31]</ref> used annotated keypoints as additional input to further improve bin classification. To combat scarcity of training data and generic features, Su et al. <ref type="bibr" target="#b30">[30]</ref> proposed to synthesize images with known 3D viewpoint annotations and proposed a geometry-aware loss to further boost the estimation performance. Recently, Pavlakos et al. <ref type="bibr" target="#b26">[26]</ref> proposed to use detected semantic keypoint followed by a PnP algorithm <ref type="bibr" target="#b13">[13]</ref> to solve for the resulting viewpoint matrix and achieved state-of-the-art results. However, this method relies on category-specific keypoint annotation and is not generalizable. On the contrary, our approach is both accurate and category-agnostic, by utilizing category-agnostic keypoints.</p><p>General keypoint detection. There are several related concepts similar to our general semantic keypoint. The most well-known one is the SIFT descriptor <ref type="bibr" target="#b17">[17]</ref>, which aims to detect a large number of interest points based on local and low level image statistics. Also, the heatmap representation has been used in saliency detection <ref type="bibr" target="#b8">[8]</ref> and visual attention <ref type="bibr" target="#b45">[45]</ref>, which detects a region of image which is "important" in the context. Similarly, Altwaijry et al. <ref type="bibr" target="#b0">[1]</ref> used the heatmap representation to detect a set of points that is useful for feature matching. The key difference between our keypoint and the above concepts is that their keypoints do not contain semantic meanings and are not annotated by humans, making them less useful in high level vision tasks such as pose estimation.</p><p>To our best knowledge, we are the first to propose a category-agnostic keypoint representation and show that it is directly applicable to viewpoint estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we describe our approach for learning a category-agnostic keypoint representation from a single RGB image. We begin with describing the representation in Section 3.1. We then introduce how to learn this representation in Section 3.2. Finally, we show a direct application of our representation in viewpoint estimation in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Category-agnostic keypoint representation</head><p>A desired general purpose keypoint representation should be both adaptive (i.e., should be able to represent different content of different visual objects) and semantically meaningful (i.e., should convey certain semantic information for downstream applications).</p><p>So far the most widely used keypoint representation is the category specific stacked keypoint vector <ref type="bibr" target="#b35">[35]</ref>, which represents object keypoints by a N × D vector (N for number of keypoints and D for dimensions), or multi-channel heatmaps <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b24">24]</ref>, which associate each channel with one specific keypoint on a specific object category, e.g., <ref type="bibr" target="#b16">16</ref>-channel heatmaps for human <ref type="bibr">[</ref> heatmaps for chair <ref type="bibr" target="#b42">[42]</ref>. Although these representations are certainly semantically meaningful (e.g., the first channel of human heatmaps is the left ankle), it does not satisfy the adaptive property, e.g., chairs with legged bases and swivel bases cannot be learned together due to varying number of keypoints. As a result, they can not be considered as the same category based on their different keypoint configurations. To generalize heatmaps to multiple categories, a popular approach is to stack all heatmaps from all categories <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b26">26]</ref> (resulting in N c output channels, where N c is the number of keypoints of category c). In such a representation, keypoints from different objects are completely separated, e.g. seat corners from swivel chairs are irrelevant to seat corners from chairs. To merge keypoints from different objects, one has to establish consistent correspondences <ref type="bibr" target="#b50">[50]</ref> between different keypoints across multiple categories, which is difficult or sometimes impossible.</p><p>In this paper, we introduce a hybrid representation that meets all desired properties. As illustrated in <ref type="figure">Figure 2</ref>, our hybrid representation consists of three components, StarMap, CanViewFeature and DepthMap. In particular, StarMap specifies the image coordinates of keypoints where the number of keypoints can vary across different categories; CanViewFeature specifies the 3D locations of keypoints in a canonical coordinate system, which provide an identity for each keypoint; DepthMap lifts 2D keypoints into 3D. As we will see later, it enhances the performance of using this representation for the application of viewpoint estimation. Now we describe each component in more details. StarMap. As shown in <ref type="figure">Figure 2</ref> (top left), StarMap is a single channel heatmap whose local maximums encode the image locations of the underlying points. It is motivated by the success of using one heatmap to encode occurrences of one keypoint on multiple persons <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">23]</ref>. In our setting, we generalize the idea to encode all keypoints of each object. This is in contrast to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">23]</ref>, which use multi-peak heatmaps to detect multiple instances of the same specific keypoint. In our implementation, given a heatmap, we extract the corresponding keypoints by detecting all local maximums, with respect to the 8-ring neighborhood whose values are above 0.05.</p><p>When comparing multi-channel heatmaps and a single channel heatmap, one intuition is that multi-channel heatmaps, which are category-specific and keypoint-specific representations, lead to better accuracy. However, as we will see later, using a single channel allows us to train the representation from bigger training data (multiple categories), leading to an overall better keypoint predictor. We also argue that a single-channel representation (1 channel vs 100+ channels on Pascal3D+ <ref type="bibr" target="#b44">[44]</ref>) is favored when computational and memory resources are limited. On the other hand, StarMap alone does not provide the semantic meaning of each detected point. This drawback motivates the second component of our hybrid keypoint representation. CanViewFeature. CanViewFeature collects the 3D locations of the keypoints in the canonical view. In our implementation, we allocate three channels for CanViewFeature. Specifically, after detecting a keypoint (peak) in StarMap, the values of these three channels at the corresponding pixel specify the 3D location in the canonical coordinate system. The design of CanViewFeature is motivated from recent works on embedding visual objects into latent spaces <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b39">39]</ref>. Such latent spaces provide a shared platform for comparing and linking different visual objects. Our representation shares the same abstract idea, yet we make the embedding explicit in 3D (where we can view the learned representation) and learnable in a supervised manner. This enables additional applications such as viewpoint estimation, as we will discuss later. When considering the space of keypoint configurations in the canonical space, it is easy to find that the feature is invariant to object pose and image appearance (scale, translation, rotation, lighting), little-variance to object shape (e.g., left frontal wheels from different cars are always in the left frontal area), and little variance to object category (e.g., frontal wheels from different categories are always in bottom frontal area).</p><p>Although CanViewFeature only provides 3D locations, we can leverage this to classify the keypoints, by using nearest neighbor association on the categorylevel keypoint templates. DepthMap. CanViewFeature and StarMap are related to each other via a similarity transform (rotation, translation, scaling) and a perspective projection. It is certainly possible to solve a non-linear optimization problem to recover the underlying similarity transform. However, since the network predictions are not perfect, we found that this approach leads to sub-optimal results.</p><p>To stabilize this process and make the relation even simpler, we augment StarMap with one additional channel called DepthMap. The encoding is the same as CanViewFeature. More precisely, we first extract keypoints at peak locations and then access the corresponding pixels to obtain the depth values. When the camera intrinsic parameters are present, we use them to convert image coordinates and depth value into the true 3D location of the corresponding pixel.</p><p>Otherwise, we assume weak-perspective projection, and directly use the image coordinates and depth value as an approximation of the underlying 3D location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Hybrid Keypoint Representation</head><p>Data preparation. Training our hybrid representation requires annotations of 2D keypoints, their corresponding depths, and their corresponding 3D locations in the canonical view. We remark that such training data is feasible to obtain and publicly available <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b43">43]</ref>. 2D keypoint annotations per image are straightforward to retrieve <ref type="bibr" target="#b25">[25]</ref> and thus widely available <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Also, annotating 3D keypoints of a CAD model <ref type="bibr" target="#b47">[47]</ref> is not a hard task, given an interactive 3D UI such as MeshLab <ref type="bibr" target="#b4">[5]</ref>. The canonical view of a CAD model is defined as the front view of an object with the largest 3D bounding box dimension scaled to [−0.5, 0.5] (meaning it is zero centered). Note that just a few 3D CAD models need to be annotated for each category (about 10 per category), because keypoint configuration variation is orders of magnitude smaller than the image appearance variation. Given a collection of images and a small set of CAD models of the corresponding categories, a human annotator is asked to select the closest CAD model to the image's content, as done in Pascal3D+ and ObjectNet3D <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b43">43]</ref>. A coarse viewpoint is also annotated by manually dragging the selected CAD model to align the image appearance. In summary, all the annotations required to train our hybrid representation are relatively easy to acquire. We refer to <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b43">43]</ref> for more details on how to annotate such data.</p><p>We now describe how we calculate the depth annotation. Ideally, the transformation between the canonical view and image pixel coordinate is a fullperspective camera model:</p><formula xml:id="formula_0">s[u v 1] T = A[R|t][x y z 1] T , s.t., R T R = I<label>(1)</label></formula><p>where A describes intrinsic camera parameters, (u, v) is the 2D keypoint location in the image coordinate system, (x, y, z) is the 3D location in canonical coordinate system. R, t, and s are the rotation matrix (i.e. viewpoint), translation vector, and scale factor, respectively. However, the camera intrinsic parameters are most likely unavailable in testing scenarios. In those cases, a weak-perspective camera model is often applied to approximate the 3D-to-2D transformation for keypoint estimation <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b26">26]</ref>, by changing Eq. 1 to </p><formula xml:id="formula_1">s[u − c x v − c y d] T = [R|t][x y z 1] T , s.t., R T R = I<label>(2)</label></formula><formula xml:id="formula_2">points {x i , y i , z i } Nc i=1 . Moreover, the corresponding 2D keypoints {(u i , v i )} Nc i=1</formula><p>are known, so we can simply solve the scale factor s by aligning the (u, v) and (x, y) plane bounding box size: s = max(maxi(xi)−mini(xi),maxi(yi)−mini(yi)) max(maxi(ui)−mini(ui),maxi(vi)−mini(vi)) , which gives rise to the underlying depth value. Network training. As described above, we have full supervision for all of our 3 output components. Training is done as a supervised heatmap regression, i.e., we minimize the L2 distance between the output 5-channel heatmap and their ground truth. Note that for CanViewFeature and DepthMap, we only care about the output at peak locations. Following <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>, we ignore the non-peak output locations rather than forcing them to be zero. This can be simply implemented by multiplying a mask matrix to both the network output and ground truth and then using a standard L2 loss. Implementation details. Our implementation is done in the PyTorch framework. We use a 2-stacks HourglassNetwork <ref type="bibr" target="#b24">[24]</ref>, which is the state-of-the-art architecture for 2D human pose estimation <ref type="bibr" target="#b1">[2]</ref>. We trained our network using curriculum learning, i.e., we first train the network with only StarMap output for 90 epochs and then fine-tune the network with the CanViewFeature followed by DepthMap supervision for additional 90 epochs each. The whole training stages took about 2 days on one GTX 1080 TI GPU. All the hyper-parameters are set to the default values in the original Hourglass implementation <ref type="bibr" target="#b24">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Application in Viewpoint Estimation</head><p>The output of our approach (StarMap, DepthMap and CanViewFeature) can directly be used to estimate the viewpoint of the input image with respect to the canonical view (i.e., camera pose estimation). Specifically, Let</p><formula xml:id="formula_3">p i = (u i − c x , v i − c y , d i ) be the un-normalized 3D coordinate of keypoint p i , where (c x ,<label>c</label></formula><p>y ) is the image center. Let q i be its counterpart in the canonical view. With w i ∈ [0, 1] we denote this keypoint's value on the heatmap, which indicates a confidence score. We solve for a similarity transformation between the image coordinate system and world coordinate system that is parameterized by a scalar s ∈ R + , a rotation R ∈ SO(3), and a translation t. This is done by minimizing the following objective function:</p><formula xml:id="formula_4">s , R , t = argmin s,R,t N I i=1 w i sRp i + t − q i 2 .<label>(3)</label></formula><p>Note that (3) admits an explicit solution as described in <ref type="bibr" target="#b7">[7]</ref>, which we include here for completeness. The optimal rotation is given by</p><formula xml:id="formula_5">R = U diag(1, 1, sign(M ))V T , M := N I i=1 w i (p i − p)(q i − q)<label>(4)</label></formula><p>where U ΣV T = M is the SVD and p, q are the mean of p i , q i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we perform experimental evaluations on the proposed hybrid keypoint representation. We begin with describing the experimental setup in Section 4.1. We then evaluate the accuracy of our keypoint detector and the application in viewpoint estimation in Section 4.2 and Section 4.3, respectively. We then present advanced analysis of our hybrid keypoint representation in Section 4.4. Finally, we show that our category-agnostic keypoint representation can be extended to novel categories in Section 4.5. <ref type="table" target="#tab_6">Table 5</ref> collect some qualitative results, and more results are deferred to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We use Pascal3D+ <ref type="bibr" target="#b44">[44]</ref> as our major evaluation benchmark. This dataset contains 12 man-made object categories with 2K to 4K images per category. We make use of the following annotations in our training: object bounding box, category-specific 2D keypoints (annotations from <ref type="bibr" target="#b2">[3]</ref>), approximate 3D CAD model of the object, viewpoint of the image, and category-specific 3D keypoint annotations (corresponds with the 2D keypoint configuration) in the canonical coordinate system defined on each CAD model. Following <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b30">30]</ref>, evaluation is done on the subset of the validation set that is non-truncated and non-occluded, which contains 2113 samples in total. As the evaluation protocols and baseline approaches vary across different tasks, we will describe them for each specific set of evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Keypoint Localization and Classification</head><p>We first evaluate our method on the keypoint estimation task, which specifies the locations of the predicted keypoints. Since keypoint locations alone do not carry the identities of each keypoint and cannot be used as identity-specific evaluation, we perform the evaluation by using two protocols -namely, with identification inferred from our learned CanViewFeature or with oracle assigned identification. Specifically, for the first protocol, for each category, we calculate the mean of the locations of each keypoint in the world coordinate system among all CAD models and use this as the category-level template. We then associate each keypoint with the ID of its nearest mean annotated keypoint in the template. For the second protocol, we assume a perfect ID assignment (or keypoint classification) by assigning the output keypoint ID as the closest annotation (in image coordinates). The second protocol can also be thought of as randomly perturbing the annotated keypoint order and picking the best one. Following the conventions <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b37">37]</ref>, we use PCK(α = 0.1), or Percentage of Correct Keypoints, as the evaluation metric. PCK considers a keypoint to be correct if its L2 2D pixel distance from the ground truth keypoint location is less than 0.1 × max(h, w), where h and w are the object's bounding box dimensions. The keypoint localization and classification results are shown in <ref type="table" target="#tab_2">Table 1</ref>. We show 3 state-of-the-art methods <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b26">26]</ref> for category-specific keypoint localization for comparison. The evaluation of <ref type="bibr" target="#b26">[26]</ref> is done by ourselves based on their published model. For the first protocol, our result of 78.6% mean PCK(α = 0.1) is marginally better than the state-of-the-arts in 2014 <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b37">37]</ref>, probably because we used a more up-to-date HourglassNetwork <ref type="bibr" target="#b24">[24]</ref>. Our performance is slightly worse than <ref type="bibr" target="#b26">[26]</ref>, who uses the same Hourglass architecture but with stacked category-specific channels output ( c N c output channels in total), which is expected. This is due to the error caused by incorrect keypoint ID association. We emphasize that all counterpart methods are category-specific, thus requiring ground truth object category as input while ours is general.</p><p>The second protocol (Bottom of <ref type="table" target="#tab_2">Table 1</ref>) factors out the error caused by incorrect keypoint ID association. For a fair comparison, we also allow <ref type="bibr" target="#b26">[26]</ref> to change its output order with the oracle nearest location (to eliminate the common left-right flip error <ref type="bibr" target="#b28">[28]</ref>). We can see our score is 92.2%, which is 3.2% higher than that of Pavlakos et al <ref type="bibr" target="#b26">[26]</ref>. This is quite encouraging since our approach is designed to be a general purpose keypoint predictor. This result shows that it is advantageous to train a unified network to predict keypoint locations, as this allows to train a single network with more relevant training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Viewpoint Estimation</head><p>Some qualitative results are shown in <ref type="table">Table.</ref> 5, and more results can be found in the supplementary material.</p><p>As a direct application, we evaluate our hybrid representation on the task of viewpoint estimation. The objective of viewpoint estimation is to predict the azimuth (a), elevation (e), and in-plane rotation (θ) of the image object with respect to the world coordinate system. In our experiment, we follow the conventions <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b30">30]</ref> by measuring the angle between the predicted rotation vector and the ground truth rotation vector:</p><formula xml:id="formula_6">∆(R pred , R gt ) = ||logm(R T pred Rgt)|| F √ 2</formula><p>, where R = R Z (θ)R X (e − π/2)R Z (−a) transforms the viewpoint representation (a, e, θ) into a rotation matrix. Here R X , R Y and R Z are rotations along X, Y and Z axis, respectively.</p><p>We consider two metrics that are commonly applied in the literature <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b30">30]</ref>, namely, Median Error, which is the median of the rotation angle error, and Ac-   <ref type="table">Table 2</ref>. Viewpoint Estimation on Pascal3D+ <ref type="bibr" target="#b44">[44]</ref>. We compare our results with the state-of-the-arts and baselines. The results are shown in Median Error (lower better) and Accuracy (higher better).</p><p>curacy at θ, which is the percentage of keypoints whose error is less than θ. We use θ = π 6 , which is a default setting in the literature. A popular approach for solving viewpoint estimation is to cast the problem as bin classification by discretiziing the space of (a, e, θ) <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b19">19]</ref>. Since network architecture governs the performance of a neural network, we re-train the baseline models <ref type="bibr" target="#b37">[37]</ref> with more modern network architectures <ref type="bibr" target="#b6">[6]</ref>. We implemented a ResNet18 (Res18-Specific) with the same hyper-parameters as <ref type="bibr" target="#b37">[37]</ref> (we also tried VGG <ref type="bibr" target="#b29">[29]</ref> or ResNet50 <ref type="bibr" target="#b6">[6]</ref> but observed very similar or worse performance).</p><p>We also want to remark that although viewpoint estimation itself is not a category-specific task, all the studied preview works have used a category-specific formulation, e.g., use separate last-layer bin classifiers for each category, resulting in 3 × N categories × N bins output units <ref type="bibr" target="#b36">[36]</ref>. We also provide a general 3 × N bins viewpoint estimator as a baseline (Res18-General). <ref type="table">Table 2</ref> compares our approach with previous techniques. Our method outperforms all previous methods and baselines in both testing metrics. Specifically with respect to MedErr, our approach achieved 10.4, which is lower than the prior state-of-the-art result reported in Mousavian et al <ref type="bibr" target="#b21">[21]</ref>. In terms of Acc π 6 , our method outperforms the state-of-the-art result of Su et al <ref type="bibr" target="#b30">[30]</ref>. This is a quite positive result, since <ref type="bibr" target="#b30">[30]</ref> uses additional rendered images for training.</p><p>We further evaluate Acc π 18 , which assesses the percentage of very accurate predictions. In this case, we simply compare against our re-implemented Res18, which achieved similar results with other state-of-the-art techniques. As shown in <ref type="table">Table 2</ref>, our approach is significantly better than Res18-General/Specific with aero bike boat bttl bus car chair  <ref type="table">Table 3</ref>. Results for keypoint classification on Pascal3D+ Dataset <ref type="bibr" target="#b44">[44]</ref>. We show keypoint classification accuracy of each category.  respect to Acc π 18 . This shows the advantage of performing keypoint alignment for pose estimation.</p><p>Note that it is also possible to directly align CanViewFeature with StarMap for viewpoint estimation by a weak-perspective PnP <ref type="bibr" target="#b26">[26]</ref> algorithm (PnP in <ref type="table">Table 2</ref>). In this case, utilizing DepthMap outperforms the direct alignment by 8.1% in terms of Acc π 6 and 1.75% in terms of Acc π 18 , respecctively. On one hand, this shows the usefulness of DepthMap, particularly when the prediction is noisy. On the other hand, the performance of both approaches becomes similar when the predictions are very accurate (Acc π 18 ). This is expected since both approaches should output identical results when the predictions are perfect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Our Hybrid Keypoint Representation</head><p>Analysis of CanViewFeature. We use the ground-truth keypoint location, and compare their learned 3D locations for keypoint classification with popular point features used in the literature, namely, SIFT <ref type="bibr" target="#b17">[17]</ref> and Conv5 of VGG <ref type="bibr" target="#b29">[29]</ref>. For CanViewFeature, we still follow the same procedure of using nearest neighbor for keypoint classification. For SIFT and Conv5, a linear SVM is used to classify the keypoints <ref type="bibr" target="#b16">[16]</ref>. <ref type="table">Table 3</ref> compares CanViewFeature with the two baseline approaches from <ref type="bibr" target="#b16">[16]</ref>. We can see that CanViewFeature is significantly better than baseline approaches. This shows the advantage of using a shared keypoint representation for training a general purpose keypoint detector. Ablation study on representation components. To better understand the importance of each component of our representation and whether they are welltrained, we provide error analysis by replacing each output component with its ground truth. To this end, we use viewpoint estimation as the task for evaluation, and <ref type="table" target="#tab_5">Table 4</ref> summarizes the results. Specifically, replacing StarMap with its ground truth does not provides much performance gains in both metrics, indicating that StarMap is fairly accurate. This is justified by the high keypoint   <ref type="table">Table 6</ref>. Viewpoint estimation for novel categories results on ObjectNet3D+ <ref type="bibr" target="#b43">[43]</ref>. We shown our results in Acc π 6 .</p><p>accuracy reported in Section 4.2. Moreover, replacing either CanViewFeature or DepthMap with the underlying ground truth provides considerable performance gains in terms of Acc π 6 . In particular, using perfect DepthMap leads noticeable decrease in median error. This is expected since the general task of estimating pixel depth remains quite challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Keypoint and Viewpoint Induction for Novel categories</head><p>Our keypoint representation is category-agnostic and is free to be extended to novel object categories <ref type="bibr" target="#b36">[36]</ref>.</p><p>We note that Pascal3D+ <ref type="bibr" target="#b44">[44]</ref> only contains 12 categories and it is hard to learn common inter-category information with such limited category samples. To further verify the generalization ability of our method, we used a newly published large scale 3D dataset, ObjectNet3D <ref type="bibr" target="#b43">[43]</ref>. ObjectNet3D <ref type="bibr" target="#b43">[43]</ref> has the same annotations as Pascal3D+ <ref type="bibr" target="#b44">[44]</ref> but with 100 categories. We evenly hold out 20 categories (every 5 categories sorted in the alphabetical order) from the training data and only used them for testing. Because Shoe and Door do not have keypoint annotation, we remove them from the testing set, resulting in 18 novel categories. Please refer to the supplementary for details on dataset details.</p><p>We compare the performance gap between including and withholding the 18 categories during training. The results are shown in <ref type="table">Table 6</ref>. As expected, the viewpoint estimation accuracy of most categories drops. For some categories (Iron, Knife, Pen, Rifle, Slipper), both experiments fail (with accuracy lower than 20%). One explanation is that these 5 failed categories are small and narrow objects, whose annotations may not be accurate. For example, the keypoint annotations on ObjectNet3D <ref type="bibr" target="#b43">[43]</ref> for small object are not always well-defined (see qualitative results in supplementary), e.g., Key and Spoon have dense keypoints annotation on their silhouette. For half of the 18 novel objects (bookshelf, cellphone, computer, filing cabinet, guitar, microwave, pot, stove, tub), the performance gap between including and withholding training data is less than 10%. This indicates that our representation is fairly general and can extend viewpoint estimation to novel categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motorcycle Bus</head><p>Acc π 6 (Similar Classifier Transfer <ref type="bibr" target="#b37">[37]</ref>) 0.58 0.50 Acc π 6 (General Classifier <ref type="bibr" target="#b37">[37]</ref>) 0.55 0.80 Acc π 6 (General Classifier Res18) 0.58 0.79 Acc π 6 (Ours) 0.55 0.63 <ref type="table">Table 7</ref>. Viewpoint estimation of novel categories on Pascal3D+ <ref type="bibr" target="#b44">[44]</ref>. We compare with the baselines from Tulsiani et al. <ref type="bibr" target="#b36">[36]</ref> and our re-trained ResNet18 <ref type="bibr" target="#b6">[6]</ref> model. The results are shown in Acc π 6 .</p><p>5 Supplementary Material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pose Induction on Pascal3D+</head><p>The pose induction for novel categories problem has been studied by Tulsiani et al. <ref type="bibr" target="#b36">[36]</ref>. They proposed two baselines for viewpoint induction: i) Similar Classifier Transfer (SCT), which uses the viewpoint classifier of a manually defined similar category for the novel category (e.g., use bicycle classifier for motorcycle); 2) General Classifier (GC), which trains a category-agnostic viewpoint classifier (similar to our Res18-General baseline in <ref type="table">Table.</ref> 2 of our main paper).</p><p>For evaluation, they <ref type="bibr" target="#b36">[36]</ref> exclude two categories (Motorcycle and Bus) from the Pascal3D+ training set <ref type="bibr" target="#b44">[44]</ref> and evaluate viewpoint estimation on these two categories with the same protocol of <ref type="bibr" target="#b37">[37]</ref>. We compare our proposed method on viewpoint estimation with their baselines in <ref type="table">Table 7</ref>.</p><p>Our keypoint alignment-based viewpoint estimator achieved lower performance than direct general viewpoint classification. This can be understood from the following factors. First, the viewpoint estimation task has shown itself not to be category-specific. As shown in <ref type="table">Table.</ref> 2 of the main paper, Res18-General has a very close performance with Res18-Specific (Acc π 6 0.79 vs. 0.81), indicating that viewpoint estimation does not benefit a lot from category-specific design. However, keypoint estimation is inherently category-specific, and keypoint definitions vary widely per category. Our system places emphasis on learning the geometry of each training category, and such information is only weakly connected to the viewpoint estimation task. Despite these limitations, our keypoint-based method is able to achieve encouraging results on pose induction (55% accuracy on Motorcycle, 62% accuracy on Bus). Moreover, as indicated in the main paper, the view-point estimation performance of our method is highly correlated with the consistency of keypoint predictions and CanViewFeature. On novel categories, they become less consistent, leading to a drop in viewpoint estimation accuracy. However, one can certainly employ domain adaptation techniques to improve their consistency. We leave this as a direction for future research.</p><p>Our proposed method is currently the only learning-based method to induct keypoint estimation to novel categories. However, we remark that we avoid directly evaluating keypoint localization performance, as keypoint detection task on novel category is ill-posed. Keypoint definitions are subjective on novel objects, e.g., our method consistently predicts frontal lights as keypoint for bus, while the annotations of Pascal3D+ <ref type="bibr" target="#b44">[44]</ref> do not, presumably due to light being defined as a keypoint on a car but not on a bus <ref type="table">.  aeroplane  camera  eraser  jar  pencil  shovel  toothbrush  ashtray  can  eyeglasses  kettle  piano  sign  train  backpack  cap  fan  key  pillow  skate  trash bin  basket  car  faucet  keyboard  plate  skateboard  trophy  bed  cellphone  filing cabinet  knife  pot  slipper  tub  bench  chair  fire extinguisher  laptop  printer  sofa  tvmonitor  bicycle  clock  fish tank  lighter  racket  speaker  vending machine  blackboard coffee maker  flashlight  mailbox  refrigerator  spoon  washing machine  boat  comb  fork  microphone remote control  stapler  watch  bookshelf  computer  guitar  microwave  rifle  stove  wheelchair  bottle  cup  hair dryer  motorbike  road pole  suitcase  bucket  desk lamp  hammer  mouse  satellite dish  teapot  bus  diningtable  headphone  paintbrush  scissors  telephone  cabinet  dishwasher  helmet  pan  screwdriver  toaster  calculator</ref> door iron pen shoe toilet  <ref type="table">Table 9</ref>. Results on MPII. The results are shown in PCKh@0.5, which is the percentage of correct keypoint whose diviation are within 0.5 of head bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ObjectNet3D dataset split</head><p>The detailed training and testing categories split is shown in <ref type="table">Table.</ref> 8. Object-Net3D <ref type="bibr" target="#b43">[43]</ref> contains about 50k training samples in total, but only 20k of them have keypoint annotations. We use the 20k subset of the training set for training and the validation set for testing. In total, we collected 19k images for training, and 4k images for novel categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Human pose estimation.</head><p>In the main paper we have considered evaluating our approach on rigid objects. We show that the results are consistent on a different task, namely, human pose estimation.</p><p>2D human pose estimation We first evaluate StarMap on the task of 2D human pose estimation on the MPII Dataset <ref type="bibr" target="#b1">[2]</ref>, by replacing the 16-channel output of state-of-the-art HourglassNet <ref type="bibr" target="#b24">[24]</ref> with a one-channel StarMap and a two-channel 2D canonical feature. As shown in <ref type="table">Table 9</ref>, our method leads to encouraging results when compared to the default HourglassNet <ref type="bibr" target="#b24">[24]</ref>, especially when assigned oracle identification, which means we can see very similar visual results by using 1 output channel instead of 16.</p><p>3D human pose estimation The DepthMap representation, which associate each 2D joint with a depth value in a map representation, can be a simplified 3D keypoint representation. It is contrast to Zhou et al. <ref type="bibr" target="#b51">[51]</ref> who represent 3D  <ref type="figure">Fig. 3</ref>. Difference between our depth regression module and Zhou et al. <ref type="bibr" target="#b51">[51]</ref>. Left: <ref type="bibr" target="#b51">[51]</ref> architecture, which uses a sub-network for depth regression. Right: Ours architecture, which uses N additional channels for depth regression.</p><p>keypoint as 2D heatmap and depth vector learned with an additional subnetwork. More specifically, Zhou et al. <ref type="bibr" target="#b51">[51]</ref> proposes to decouple the (x, y, z) 3D coordinate into (u, v) image coordinate and depth d (see our Section. 3.2) in a weak-perspective camera model, which enables using rich 2D in-the-wild data <ref type="bibr" target="#b1">[2]</ref> in training. For estimating the depth d of each joint, they use an additional depth regression sub-network on the top of the 2D network, which is cumbersome (i.e., introducing more hyper-parameters for designing the sub-network and increasing the feed forward time). When using our DepthMap encoding, which augments the (u, v) heatmaps with an additional depth channel and associates the depth value on the heatmap peak location, we can replace the sub-network <ref type="bibr" target="#b51">[51]</ref> with channels. We illustrate the difference in <ref type="figure">Fig. 3</ref>. We evaluate it by replacing the regression subnetwork of <ref type="bibr" target="#b51">[51]</ref> with an Nchannel DepthMap for 3D human pose estimation on Human 3.6M dataset <ref type="bibr" target="#b9">[9]</ref>.</p><p>Human3.6M dataset <ref type="bibr" target="#b9">[9]</ref>, which contains about 3.6 millions frames of images, each with accurate 3D human joint location annotations. Following <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b27">27]</ref>, both training and testing are done on a 5× down-sampled subset. We follow the standard protocol to use 5 subjects for training and 2 subjects for testing. The error is measured in mean per joint position error (MPJPE) in millimeters after aligning the root joint location with ground truth and assuming a fixed average scale <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b27">27]</ref>. All the experiment settings are the same with <ref type="bibr" target="#b51">[51]</ref>.</p><p>The results in <ref type="table">Table.</ref> 10 show that our DepthMap representation achieves very close performance with the original design of Zhou et al. <ref type="bibr" target="#b51">[51]</ref>, while saving about 1/5 network parameter (from the depth-regression sub-network). We also compare with Mehta et al. <ref type="bibr" target="#b20">[20]</ref>, who also use a map representation for 3D coordinates. Instead of directly using the (u, v) coordinate from 2D heatmap (with a weak-perspective camera model), they regress the full (x, y, z) coordinates at the peak heatmap location with a full-perspective camera model. Also, they use a modified ResNet50 <ref type="bibr" target="#b6">[6]</ref> architecture instead of HourglassNetwork <ref type="bibr" target="#b24">[24]</ref>. Our results are considerably better than theirs, showing the effectiveness of the decoupled weak-perspective 3D keypoint representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">More Qualitative Results</head><p>This section is removed due to arXiv size limit. Please visit the project page (https://github.com/xingyizhou/StarMap) for more qualitative results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of Canonical View Semantic Feature. It is shared across all object categories. We show 2 categories: chair (in blue) and table (in green). For the left frontal leg of chair on bottom left, it has i) the same CanViewFeature with the same chair keypoint from a different viewpoint (bottom right), ii) similar feature with another chair instance's corresponding keypoint (top right), and iii) similar feature with left frontal leg from a table(top left). We Can View this feature in 3D space (middle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6 ( 6 ( 6 ( 6 ( 18 ( 18 (</head><label>66661818</label><figDesc>0.78 0.44 0.79 0.96 0.90 0.80 N/A N/A 0.74 0.79 0.66 N/A Acc π Mousavian [21]) 0.78 0.83 0.57 0.93 0.94 0.90 0.80 0.68 0.86 0.82 0.82 0.85 0.8103 Acc π Su [30]) 0.74 0.83 0.52 0.91 0.91 0.88 0.86 0.73 0.78 0.90 0.86 0.92 0.82 Acc π Res18-General) 0.79 0.75 0.53 0.90 0.96 0.93 0.62 0.57 0.85 0.82 0.81 0.77 0.7875 Acc π 0.86 0.50 0.92 0.97 0.92 0.79 0.62 0.88 0.92 0.77 0.83 0.8225 Acc π Res18-General) 0.28 0.18 0.17 0.27 0.82 0.61 0.23 0.33 0.18 0.15 0.61 0.27 0.3502 Acc π Res18-Specific) 0.29 0.21 0.21 0.30 0.86 0.62 0.28 0.33 0.21 0.18 0.59 0.30 0.3777 Acc π 18 (PnP) 0.52 0.36 0.13 0.50 0.83 0.65 0.48 0.29 0.31 0.44 0.61 0.27 0.4643 Acc π 18 (Ours) 0.49 0.34 0.14 0.56 0.89 0.68 0.45 0.29 0.28 0.46 0.58 0.37 0.4818</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>6 ( 6 ( 6 (</head><label>666</label><figDesc>aero bike boat bottle bus car chair table mbike sofa train tv mean MedErr (0.86 0.50 0.92 0.97 0.92 0.79 0.62 0.88 0.92 0.77 0.83 0.8225 Acc π GT Star) 0.85 0.84 0.50 0.92 0.96 0.93 0.80 0.38 0.85 0.90 0.77 0.82 0.8211 Acc π GT Star+SCSF) 0.86 0.84 0.63 0.95 0.99 0.95 0.88 0.62 0.84 0.92 0.88 0.85 0.8651 Acc π GT Star+Depth) 0.86 0.93 0.63 0.95 0.97 0.91 0.82 0.38 0.87 0.92 0.84 0.93 0.8637</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where (u, v) specifies the location of the keypoint, d is its associated depth, and (c x , c y ) denotes the center of the image. T be the transformed 3D keypoints in the metric space, we have [u, v, d] = [x/s + c x , y/s + c y , z/s] (with unknown s), which transforms one point from the 3D metric space to the 2D pixel space with an augmented depth value d. In training, let N c be the number of keypoints in category c. Both the viewpoint transformation matrix [R|t] and the canonical points {x i , y i , z i } Nc i=1 are known, and we can calculate the rotated key-</figDesc><table /><note>Letting [x, y, z] = [R|t][x, y, z, 1]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>PCK(α = 0.1) aero bike boat bottle bus car chairtable mbike sofa train tv mean Long. [16] 53.7 60.9 33.8 72.9 70.4 55.7 18.5 22.9 52.9 38.3 53.3 49.2 48.5 Tulsiani. [37] 66.0 77.8 52.1 83.8 88.7 81.3 65.0. 47.3 68.3 58.8 72.0 65.1 68.8 Pavlakos. [26] 84.1 86.9 62.3 87.4 96.0 93.4 76.0 N/A N/A 78.0 58.4 84.8 82.5 Ours 75.2 83.2 54.8 87.0 94.4 90.0 75.4 58.0 68.8 79.8 54.0 85.8 78.6 Pavlakos. [26] Oracle Id 92.3 93.0 79.6 89.3 97.8 96.7 83.9 N/A N/A 85.1 73.3 88.5 89.0 Ours Oracle Id 93.1 92.6 84.1 92.4 98.4 96.0 91.7 90.0 90.1 89.7 83.0 95.2 92.2 2D Keypoint Localization Results. The results are shown in PCK(α = 0.1). Top: our result with nearest canonical feature as keypoint identification. Bottom: results with oracle keypoint identification.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>aero bike boat bottle bus car chair table mbike sofa train tv mean MedErr (Tulsiani [37]) 13.8 17.7 21.3 12.9 5.8 9.1 14.8 15.2 14.7 13.7 8.7 15.4 13.6 MedErr (Mousavian [21]) 13.6 12.5 22.8 8.3 3.1 5.8 11.9 12.5 12.3 12.8 6.3 11.9 11.1 MedErr (Su [30]) 15.4 14.8 25.6 9.3 3.6 6.0 9.7 10.8 16.7 9.5 6.1 12.6 11.7 MedErr (Mahendran [19]) 14.2 18.7 27.2 9.5 3.0 6.9 15.8 14.4 16.4 10.7 6.6 14.3 13.1 MedErr (Res18-General) 14.3 16.7 26.9 13.2 5.8 8.8 17.7 26.7 15.7 14.4 8.8 16.2 13.3 MedErr (Res18-Specific) 14.7 15.8 25.6 13.1 5.7 8.6 16.3 18.1 15.1 13.8 8.2 14.1 12.8 MedErr (PnP) 9.5 14.0 43.6 9.9 3.3 6.6 11.4 64.9 14.3 11.5 7.7 21.8 11.2 MedErr</figDesc><table><row><cell>MedErr (Pavlakos [26])</cell><cell>8.0 13.4 40.7 11.7</cell><cell>2.0 5.5 10.4 N/A N/A 9.6 8.3 32.9 N/A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table mbike</head><label>mbike</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>sofa train tv mean</cell></row><row><cell cols="2">SIFT [16] 35 54 41 76 68 47 39 69</cell><cell>49 52 74 78 57</cell></row><row><cell cols="2">Conv [16] 44 53 42 78 70 45 41 68</cell><cell>53 52 73 76 58</cell></row><row><cell>Ours</cell><cell>77 79 64 96 95 92 84 66</cell><cell>71 90 65 94 81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Error Analysis on Pascal3D+. We show results in Median Error and Accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Qualitative results of our full pipeline on Pascal3D+ [44] Dataset.</figDesc><table><row><cell>1st column:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>List of categories on ObjectNet3D<ref type="bibr" target="#b43">[43]</ref>. The novel categories (only used for testing) is shown in underline.</figDesc><table><row><cell></cell><cell cols="2">Head Shoulder Elbow Wrist Hip Knee Ankle Total</cell></row><row><cell cols="2">HourglassNetwork w. [24] oracle ID 97.44 98.27</cell><cell>94.02 92.22 93.30 90.49 86.02 93.22</cell></row><row><cell>StarMap with oracle Id</cell><cell>92.12 93.65</cell><cell>90.49 86.09 82.40 87.23 82.22 88.17</cell></row><row><cell>HourglassNetwork [24]</cell><cell>96.49 95.38</cell><cell>89.16 84.89 87.73 84.08 80.30 88.39</cell></row><row><cell>StarMap with learned Id</cell><cell>91.00 88.69</cell><cell>83.02 73.58 74.16 76.67 69.01 79.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>Results on H36M [9] Dataset. The results are shown in Mean Per Joint Position Error (in mm).</figDesc><table><row><cell>N Heatmaps</cell><cell>N Heatmaps</cell></row><row><cell></cell><cell>0.4</cell></row><row><cell></cell><cell>0.8</cell></row><row><cell></cell><cell>0.1</cell></row><row><cell>N x 1 vector for depth</cell><cell>N DepthMaps</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We thank Shubham Tulsiani and Angela Lin for the helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to detect and match keypoints with deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Altwaijry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tech</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>BMVC</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MeshLab: an Open-Source Mesh Processing Tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cignoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Callieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Corsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dellepiane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ganovelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ranzuglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Italian Chapter Conference. The Eurographics Association</title>
		<editor>Scarano, V., Chiara, R.D., Erra, U.</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="DOI">10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136</idno>
		<ptr target="https://doi.org/10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-1367" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Closed-form solution of absolute orientation using unit quaternions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICCV</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning category-specific mesh reconstruction from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Category-specific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Regognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for realtime 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o (n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Do convnets learn correspondence? In: Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast and globally convergent pose estimation from video images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mjolsness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07426</idno>
		<title level="m">Joint object category and 3d pose estimation from 2d images</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073596</idno>
		<ptr target="http://gvv.mpi-inf.mpg.de/projects/VNect/21" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pixels to graphs by associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2274" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1920" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Benchmarking and error diagnosis in multi-instance pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09859</idno>
		<title level="m">Click here: Human-localized keypoints as guidance for viewpoint estimation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The vitruvian manifold: Inferring dense correspondences for one-shot human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pose induction for novel object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Regognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dense human body correspondences using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vouga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<title level="m">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Advances In Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Objectnet3d: A large scale database for 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV)</title>
		<meeting><address><addrLine>2, 6, 7, 9</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03917</idno>
		<title level="m">3d hand pose estimation: From current achievements to future goals</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning dense correspondence via 3d-guided cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for 3d keypoint prediction from a single depth scan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05765</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05317</idno>
		<title level="m">Deep kinematic pose regression</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06854</idno>
		<title level="m">Model-based deep hand pose estimation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Sit Take Walk Walk Method Direct Discuss Eat Greet Phone Pose Purch. Sit Down Smoke Photo Wait Walk Dog Pair All</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
