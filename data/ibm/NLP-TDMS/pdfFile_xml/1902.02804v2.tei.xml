<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SiamVGG: Visual Tracking using Deeper Siamese Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
							<email>xiaofan3@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SiamVGG: Visual Tracking using Deeper Siamese Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual tracking</term>
					<term>Siamese network</term>
					<term>similarity-learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, we have seen a rapid development of Deep Neural Network (DNN) based visual tracking solutions. Some trackers combine the DNN-based solutions with Discriminative Correlation Filters (DCF) to extract semantic features and successfully deliver the state-of-the-art tracking accuracy. However, these solutions are highly compute-intensive, which require long processing time, resulting unsecured real-time performance. To deliver both high accuracy and reliable real-time performance, we propose a novel tracker called SiamVGG 1 . It combines a Convolutional Neural Network (CNN) backbone and a cross-correlation operator, and takes advantage of the features from exemplary images for more accurate object tracking. The architecture of SiamVGG is customized from VGG-16, with the parameters shared by both exemplary images and desired input video frames. We demonstrate the proposed SiamVGG on OTB-2013/50/100 and VOT 2015/2016/2017 datasets with the state-ofthe-art accuracy while maintaining a decent real-time performance of 50 FPS running on a GTX 1080Ti. Our design can achieve 2% higher Expected Average Overlap (EAO) compared to the ECO [1] and C-COT [2] in VOT2017 Challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual object tracking is one of the most fundamental topics in computer vision. Building trackers with both high accuracy and reliable real-time performance is always a challenging problem. In the tracking problems, a bounding box is given of an arbitrary object in the first frame, and the goal is to report the location of the same target in following frames. With the extremely high practicability, there are growing number of tracking-related applications, which can be easily found in surveillance systems, Unmanned Aerial vehicles (UAVs), and self-driving cars, etc. However, higher demands of accuracy and real-time performance are proposed by real-life applications since trackers can be easily distracted by the movement of targeted and surrounding objects in real scenarios, such as motion changes, illumination changes, and occlusion issues, and real-time capability can guarantee the satisfaction of input video frame rate. It is hard to satisfy real-time tracking while using DNN-based trackers with high computation complexity and long processing latency. Although we can take advantage of hardware accelerator to speedup DNN inference using GPUs <ref type="bibr">[3,</ref><ref type="bibr" target="#b1">4]</ref>, FPGAs <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b3">6]</ref>, or ASICs <ref type="bibr" target="#b4">[7]</ref>, the tradeoffs between accuracy and real-time performance of DNN-based trackers are not yet fully investigated.</p><p>Recently, a Discriminative Correlation Filters (DCF) based approach has demonstrated its promising real-time performance in tracking applications. This approach transforms convolutional operations from time-domain to frequencydomain so that it largely improves the computation efficiency and speed. One typical example is a tracker called MOSSE <ref type="bibr" target="#b5">[8]</ref>, which can process up to 700 frames per second but it fails to provide acceptable accuracy and requires additional hand-crafted features to recognize the targeted object.</p><p>Another solutions relay on DNNs to capture features of the targeted object. DNNs have demonstrated their potentials of feature extractions especially in object classification <ref type="bibr" target="#b6">[9]</ref>, detection <ref type="bibr" target="#b7">[10]</ref>, and saliency prediction <ref type="bibr" target="#b8">[11]</ref> tasks. By using DNNs, more high-dimensional features can be captured to significantly improve the object tracking algorithms. With the integration of convolutional neural networks (CNNs) as the feature extractors, DCF based trackers begin to show improving performance on various tracking datasets regarding accuracy. However, most of these trackers still cannot meet the real-time requirement (e.g., ≥30 FPS) <ref type="bibr" target="#b0">[1]</ref>, because every incoming frame needs to go through all layers of the CNN to get the up-to-date features, resulting time-consuming procedure. Another problem of DNN-based object trackers is, the feature extractors (the CNN part) are not originally designed and trained for tracking tasks. For example, in ECO tracker <ref type="bibr" target="#b0">[1]</ref>, the feature extractor is the first (Conv-1) and last (Conv-5) convolutional layer of the VGG-m, which are pretrained on image classification datasets but not tracking datasets. Without the dedicated end-to-end training for tracking tasks, the tracking-oriented DNN models may be affected by the training noises and perform poorly. Some models with online learning ability may overcome this problem by fine-tuning the network parameters during tracking, such as using stochastic gradient descent (SGD), but the online learning approach does not satisfy the real-time requirement neither.</p><p>To balance accuracy and speed, offline DNN-based trackers is one promising solution. Among them, the approaches which consider tracking problems as similarity learning problems get the highest attentions recently. These solutions use Siamese Network, such as the fully-convolutional Siamese Network (SiamFC) <ref type="bibr" target="#b9">[12]</ref>, to learn the similar parts of the targeted objects out of the input frames. Since this solution is end-to-end trainable, it is easier to train on object detection datasets. As a result, tracking solutions with Siamese Networks can deliver high accuracy without any fine-tuning or online updating process. However, the discrimination ability of these solutions are highly relay on the feature extraction capabilities of the Siamese Network. For example, our proposed design, SiamVGG (using modified VGG-16), performs much better than the SiamFC (using AlexNet), where more details are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Larger portion of the red color area in the second row represents that the SiamFC can be easily disturbed by the similar objects or even backgrounds, resulting in relatively uncertain output predictions compared to the third row with a better Siamese Network. In this paper, we propose a new approach for object tracking, named SiamVGG, to improve the major drawback (weak discrimination capability) of the current Siamese Network based methods. We adapt more advanced networks for better discrimination capability and eventually improve the proposed Siamese Network based tracker. In our experiments, we notice that not all the networks are suitable for the Siamese structure and we choose the VGG-16 <ref type="bibr" target="#b10">[13]</ref>, which shows the best performance, as the backbone CNN, and it is trained on both ILSVRC dataset <ref type="bibr" target="#b6">[9]</ref>, and Youtube-BB dataset <ref type="bibr" target="#b11">[14]</ref>. We evaluate the proposed SiamVGG in VOT 2015, VOT 2016, VOT 2017, OTB-100, OTB-50, and OTB-2013 datasets <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b14">17]</ref>. The proposed SiamVGG delivers the state-of-the-art performance without tiring fine-tuning efforts for hyperparameters. In addition, the proposed network is very compact so that it can reach 50 FPS for most of the real-time applications.</p><p>The rest of the paper is organized as follows. Related works are introduced in Section 2. The proposed architecture and corresponding configurations are introduced in Section 3. While Section 4 presents the experimental results on six datasets and Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Using CNNs as Feature Extractors</head><p>The features captured by CNNs can enhance the performance of traditional tracking algorithms by bringing sufficient high-level semantic representations to locate the desired targets. One of the successful examples is called Deep-SRDCF [2], which takes advantage of the features extracted by CNN combining with correlation filters. Similar methods, such as C-COT <ref type="bibr" target="#b15">[18]</ref> and ECO <ref type="bibr" target="#b0">[1]</ref>, employ continuous convolution operators to enhance the feature extraction of their trackers. Although these methods can reach the state-of-the-art tracking accuracy, they are unable to deliver high enough frame rate (e.g., Frame pre second) to support real-time object tracking, especially when input images become large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Siamese Network Based Tracker</head><p>To overcome the low-frame-rate challenge, recently published papers try to convert the tracking problem into a saimilarity learning problem, such as SiamFC <ref type="bibr" target="#b9">[12]</ref>. The goal of SiamFC is to find the potential area of the targeted objects in the following frames by comparing with the exemplary images. In this approach, the tracker does not need to perform the online parameter update, which helps to satisfy the frames rate requirement of real-time detection. This work shares its weights for both search column and exemplary column. There are also a growing number of the SiamFC-like structures proposed recently. Among them, EAST <ref type="bibr" target="#b16">[19]</ref> tries to speed up the tracker by learning an agent to decide whether to locate objects with high confidence on an early layer. DSiam <ref type="bibr" target="#b17">[20]</ref> attempts to adjust the exemplars by adding online updating, while SA-Siam <ref type="bibr" target="#b18">[21]</ref> implements a two-branch Siamese Network for with one branch for semantic and the other for appearance. In addition, RASNet <ref type="bibr" target="#b19">[22]</ref> introduces three different attention mechanisms to enhance the performance, and SiamRPN <ref type="bibr" target="#b20">[23]</ref> integrates the region proposal network as the backend to improve the scaling speed. However, most of these methods still use AlexNet as their backbone, which is not good enough for extracting the underlying semantic features for the targeted objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Solution</head><p>The main idea of our design is to deploy a stronger DNN to capture more detailed semantic features and to combine with the advantages of using the Siamese Network. In this section, we first introduce the proposed network architecture, and then we present the corresponding training methods. Finally, we illustrate the tracking configurations in our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SiamVGG Architecture</head><p>Following the idea from the Siamese Network, we choose the modified VGG-16 as the backbone because of its strong transfer learning ability shown in other tasks (e.g., segmentation <ref type="bibr" target="#b21">[24]</ref>, crowd counting <ref type="bibr" target="#b8">[11]</ref>, etc.) and its straightforward structure for easy implementation. We also follow the base design strategy from SiamFC and use it as our baseline design for further comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully-convolutional Siamese Network</head><p>We first introduce the Siamese network using fully-convolutional operations. Assuming L τ is the translation op-</p><formula xml:id="formula_0">erator, we have (L τ x)[u] = x[u − τ ].</formula><p>When the mapping function h for input signals is a fully-convolutional function with integer stride k, it needs to meet the following Equation <ref type="formula" target="#formula_1">(1)</ref>, for translation τ in the valid region of both inputs and outputs.</p><formula xml:id="formula_1">h(L kτ x) = L τ h(x)<label>(1)</label></formula><p>By using the fully convolutional Siamese network, we need to feed in two input images and compute their cross-correlation, f (z, x), which is defined as Equation <ref type="formula">(2)</ref>, where z and x represent the exemplary image and the search image, respectively. In Equation <ref type="formula">(2)</ref>, ϕ represents a convolutional embedding function (e.g., CNNs) and b1 denotes the bias with value b ∈ R. In our experiments, we notice that the b1 contributions nearly nothing to the final performance, so that we remove it in our design for a more compact structure.</p><formula xml:id="formula_2">f (z, x) = ϕ(z) * ϕ(x) + b1</formula><p>(2) <ref type="figure" target="#fig_1">Figure 2</ref> shows the whole network structure of our proposed design with two inputs (x for search image and z for exemplary image) and one output for the score map after running cross-correlation between two inputs. The generated output map indicates the similarity information between the current search image and exemplary image. During tracking, the search image is centered at the previous position of the target which the network predicts. By using the bicubic interpolation, the score map is re-scaled to a specific size which equals to the original score map size multiplied by the down-sample rate of the network. The position of the targeted object is obtained according to generate score map (where the maximum score means the best possible position). Multiple scales are searched in a single inference by stacking a mini-batch of scaled images.</p><p>One interesting fact is that the commonly used padding operation in CNNs fails to show any benefits in the fully-convolutional Siamese Network. One major reason comes from the noise introduced by padding operations, which deteriorates the quality of the output feature maps after pooling and leads to imperfect score maps after running cross-correlation. Eventually, the trackers using Siamese network with padding layers suffer accuracy loss. The most efficient solution is to remove the padding layers for maintaining accuracy, but the computational complexity would be increase exponentially as the layer number grows without using padding. As a result, the network design need to be more careful as to balance the accuracy and corresponding computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Configuration</head><p>We illustrate the proposed network configuration in <ref type="table" target="#tab_0">Table 1</ref>. By removing the fully-connected layers, we keep the first 10 layers from VGG-16 and make the last layer a 1 × 1 kernel as the output. There are two reasons for us to choose VGG-16 as the backbone network. The first reason is that we need to implement a CNN without using padding layers, which is a restriction of using most of the up-to-date CNNs, such as ResNet (with shortcut connections between layers) and GoogLeNet (with Inception modules), since padding operations are required in these networks. While the other reason is the great adaptability of VGG-like networks which allows us to adapt them from image classification to object tracking. To be more specific, these networks can be pre-trained on classification datasets, and then adapted to tracking tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Method</head><p>In this section, we provide details of our training method. By using regular CNN components (including only convolutional layers, Relu, and maxpooling layers), SiamVGG is easy to implement and fast to deploy.</p><p>Generating Ground Truth To train the SiamVGG, we use ILSVRC and Youtube-88 datasets, which cover around 4,000 videos annotated frame-by-frame and more than 100,000 videos annotated every 30 frames, respectively. Because ILSVRC contains more fine-grained information while Youtube-BB covers the coarse-grained information, we set the feeding ratio between these two datasets as 1 : 5.</p><p>We randomly pick two frames from the same video sequence and assemble them into a pair of the search image and the exemplary image. In our design, the size of search image is 255 × 255 while the size of exemplary image is 127 × 127. To keep all images as squares, we add context margin on top of the original images following the Equation <ref type="formula">(3)</ref> and <ref type="bibr" target="#b1">(4)</ref>. Assuming the original image size (w, h), and the context margin p, we can calculate the scale factor s in Equation </p><p>Since the output of the proposed structure in <ref type="figure" target="#fig_1">Figure 2</ref> is 17 × 17, we set the ground truth of the score map to 17 × 17. By using Equation <ref type="formula" target="#formula_4">(5)</ref>, we generate the elements in the score map, which are considered as positive examples if the targeted objects are located in the center within specific radius R using Manhattan distance. y[u] represents the elements of the ground truth while c is the center of the score map. In the example in <ref type="figure" target="#fig_3">Figure 3</ref>, we assume R = 2 and coefficient k = 1. For data augmentation, we randomly stretch the search image on a small scale, from 1.04 −3 to 1.04 3 . Since we have a large amount of training data, other data augmentation methods, like rotation, flipping, or color transformation are not utilized.</p><formula xml:id="formula_4">y[u] = +1 if k u − c ≤ R −1 otherwise .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>We use an end-to-end method to train the proposed SiamVGG. We choose the SoftMargin loss as the loss function (Equation <ref type="formula" target="#formula_5">(6)</ref>), where y is the ground truth score map (y[i] ∈ {+1, −1}) and x is the output score map. n is the total number of elements in the score map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>loss(x, y) =</head><formula xml:id="formula_5">i log(1 + exp(−y[i] × x[i])) n<label>(6)</label></formula><p>During training, we initialize the first ten convolutional layers of our design using VGG-16 pre-trained model on ILSVRC classification dataset, and use the method proposed in <ref type="bibr" target="#b22">[25]</ref> to initialize the output layer. Stochastic gradient descent (SGD) is applied with learning rate from 10 −4 to 10 −7 during training. The whole training process contains over 200 epochs and each epoch is consist of 6,000 sampled pairs. The gradients for each iteration are estimated using minibatches of size 8. We use an IBM S822LC machine with 2×Power8 CPUs, 512GB RAM, and 4×Nvidia P100 GPUs to handle the training of SiamVGG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first start an ablation study of our proposed SiamVGG on OTB100 dataset and then we demonstrate our approach in six different public datasets with the state-of-the-art performance. The implementation of our model is based on the PyTorch framework while the experiments (inferences) are running on a PC with an Intel i7-7700K, 16GB RAM, and Nvidia GTX 1080Ti GPU. We upsample the score map by using bicubic interpolation from 17 × 17 to 273 × 273 (16× upsampling with an element as the center of the score maps). To handle the scale transformation, we search for the object over three scales as 1.040 −1,0,1 with the penalty rate of 0.97 for the scale variation and update the scale with a learning rate of 0.59.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The OTB Benchmarks</head><p>We evaluate the proposed design in OTB benchmarks (OTB-2013, OTB-50, OTB-100), which are one of the most widely used public tracking benchmarks. These benchmarks consider the average per-frame success rate at different thresholds, which means a detection is considered to be successful in a given frame if the intersection-over-union (IoU) between its prediction result and the groundtruth is above a certain threshold. Trackers are then compared regarding area under the curve (AUC) of success rates for one pass evaluation (OPE). The results are shown in <ref type="figure">Figure 4</ref> for OTB benchmarks along with different thresholds. Also, we compare our tracker to other state-of-the-art Siamese network based trackers in <ref type="table">Table 2</ref>. The results show that our proposed SiamVGG have achieved very competitive performance among all these trackers. <ref type="figure">Fig. 4</ref>. Success plots for OPE (one pass evaluation) using OTB-2013, OTB-50, OTB-100 benchmarks with the AUC value presented at the top-right corner in percentage. <ref type="table">Table 2</ref>. AUC value for recently published real-time trackers using Siamese networks. Datas highlighted in red , blue, and green color stand for the first, second, and third place of each benchmarks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>tracker</head><p>OTB-2013 OTB-50 OTB-100 SiamFC-3s <ref type="bibr" target="#b9">[12]</ref> 0.607 0.516 0.582 CFNet <ref type="bibr" target="#b23">[26]</ref> 0.611 0.530 0.568 RASNet <ref type="bibr" target="#b19">[22]</ref> 0.670 -0.642 SA-Siam <ref type="bibr" target="#b18">[21]</ref> 0.677 0.610 0.657 DSiam <ref type="bibr" target="#b17">[20]</ref> 0.656 --</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SiamRPN [23]</head><p>--0.637 SiamVGG 0.665 0.610 0.654</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study on OTB100</head><p>In this section, we explore several factors in the proposed design and eventually determine the final configuration of SiamVGG.</p><p>Batch Normalization The first consideration relates to the batch normalization layer. Since the VGG-16 has two setups as the one with batch normalization layer and the other without, we need to determine which is more suitable in tracking tasks. Although results on ILSVRC classification dataset show that the VGG-16 model with batch normalization can deliver relatively higher accuracy, we notice the other one (without batch normalization) achieves much higher performance in tracking tasks with 0.654 AUC of success plots for OPE than the model with batch normalization (with 0.589 AUC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Youtube-BB Dataset</head><p>We notice that the ILSVRC only contains less than 5,000 video sequences, which are inadequate and easily cause overfitting during training. Thus, we introduce the Youtube-BB dataset including more than 100,000 videos annotated once in every 30 frames for more training material. By combining these two datasets in a particular ratio, the performance of our design has improved from 0.637 to 0.654 (AUC of success plots for OPE) in OTB-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The VOT Benchmarks</head><p>We use the latest version of the Visual Object Tracking toolkit (6.0.3) to start reset-based experiments. The toolkit re-initializes the tracker in five frames after failure is detected (zero overlaps with the ground truth). The performance is measured in terms of expected average overlap (EAO) to reflects both robustness and accuracy quantitatively. For the comparison in VOT2017 participants, we also include the real-time tracking performance as the additional metrics. Comparison results to other state-of-the-art methods on VOT2015, VOT2016, and VOT2017 are reported in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on VOT2015</head><p>The VOT2015 dataset consists of 60 sequences. The performance is evaluated in terms of accuracy (average overlap while tracking successfully) and robustness (failure times). The overall performance is measured using an expected average overlap (EAO) which takes account of both accuracy and robustness quantitatively. We compare our tracker with top 10 trackers recorded in the VOT2015 presentation (remove MDNet from the record because it's trained with data generated from OTB's sequences). Also, we compare our tracker with other previous methods based on the Siamese networks (if their results are reported). The result in <ref type="table" target="#tab_1">Table 3</ref> shows that SiamVGG is able to rank 1st in EAO with 17% enhancement compared to the DeepSRDCF (note that it cannot run at realtime speed). Although our tracker is slower than SiamRPN, our performance is relatively higher than it.  Results on VOT2016 The VOT2016 dataset keeps the same sequences as the VOT2015 with re-annotated bounding boxes. We use the same performance evaluation methods as VOT2015 and compare our tracker to the top 10 trackers in VOT2016 and those using Siamese networks. The result in <ref type="table" target="#tab_2">Table 4</ref> shows that our tracker achieves the first place in both failures and EAO sections, and the second place regarding overlap section. Our tracker can also beat the C-COT with better real-time speed and deliver significant improvement compared to the advanced Siamese network based methods. Results on VOT2017 In VOT2017, the easiest 10 sequences in dataset are replaced by updated sequences. In addition, specific real-time performance is required, where trackers need to run on at least 25 FPS. If the tracker fails to generate each result in 40 ms (25 FPS), the VOT toolkit will keep the bounding box of the last frame as the result of the current frame. As shown in <ref type="table">Table 5</ref>, our tracker achieves the first place in overlap and the third place in EAO section, respectively. Note that most of the top 10 trackers listed in <ref type="table">Table 5</ref> cannot run at real-time speed while maintaining good accuracy. To evaluate the real-time performance, we compare our method to other state-of-the-art real-time trackers reported in VOT2017 and the Siamese network based trackers and show the results in <ref type="table" target="#tab_4">Table 6</ref>. It indicates that our tracker can achieve significant improvement on real-time EAO section with 13% higher performance compared to SiamRPN. Since the VOT toolkit is used to evaluate the FPS performance in VOT2017, the results presented in <ref type="table" target="#tab_4">Table 6</ref> suffer a certain number of FPS drop. In our case, the proposed SiamVGG can reach 50 FPS using a GTX 1080Ti for tracking tasks without evaluating by VOT toolkit. By comparing to the baseline model, SiamFC, our tracker can deliver 51% better EAO without significant speed loss. We also summarize the real-time EAO in <ref type="figure" target="#fig_5">Figure 6</ref> of all selected trackers for intuitive presentation. <ref type="table">Table 5</ref>. We compare our tracker to top 10 trackers recorded in the VOT2017 challenge and several Siamese network based methods regarding overlap, failures, expected average overlap (EAO) using the latest VOT toolkit (6.0.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracker</head><p>Overlap Failures EAO C-COT <ref type="bibr" target="#b15">[18]</ref> 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed a SiamVGG tracker which is an end-to-end DNN-based model featuring offline parameter update from large-scale image pairs (ILSVRC and Youtube-BB datasets). By modifying the network from the baseline SiamFC method, our proposed SiamVGG can deliver significant improvements of tracking performance with the state-of-the-art real-time EAO. SiamVGG is very easy to reduplicate and can be deployed onto IoT devices (with limited computation and memory resources) because of its compact network structure. Experiments showed that our method can outperform most the existing trackers in VOT2015/2016/2017 challenges and OTB-2013/50/100 datasets and we can maintain very high FPS for real-time tracking.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The first row presents the targeted objects (with bounding boxes) in VOT2017, while the second and the third rows represent the score maps of the possible locations of the targeted object using SiamFC and the proposed SiamVGG, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of the proposed SiamVGG with VGG-16 based Siamese network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(3). Regarding the exemplary images, A = 127 × 127 and the context margin is p = (w + h)/4. So in the original frame, the final square is centered at the original center of the bounding box with side length L in Equation (4). One the other hand, the search images are 255 × 255, which means that they are centered at the original bounding box center with 2L side length. s(w + 2p) × s(h + 2p) = A (3) L = (w + 2p) × (h + 2p)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The search image (left) with size 255 × 255, the exemplary image (middle) with sized 127 × 127, and the ground truth score map (right) with size 17 × 17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Snapshots of the results generated by the proposed SiamVGG on VOT2017. From top row to the bottom row are singer3, bolt1, bolt2, matrix, fernando, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The real-time expected average overlap analysis for SiamVGG and other realtime trackers which recorded in the VOT2017 real-time challenge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The backbone architecture of SiamVGG's. All the convolutional layers are integrated with ReLU except the last one working for generating outputs. 'MP' stands for the maxpooling layer. The channel map indicates the number of output and input channels using the format outputchannel × inputchannel.</figDesc><table><row><cell>Activation Size</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>We compare our tracker to top 10 trackers recorded in the VOT2015 challenge (MDNet<ref type="bibr" target="#b24">[27]</ref> is removed) and several Siamese network based methods regarding overlap, failures, and expected average overlap (EAO) using the latest VOT toolkit (6.0.3).</figDesc><table><row><cell>Tracker</cell><cell>Overlap</cell><cell>Failures</cell><cell>EAO</cell></row><row><cell>DeepSRDCF [2]</cell><cell>0.556</cell><cell>16.953</cell><cell>0.318</cell></row><row><cell>EBT [28]</cell><cell>0.459</cell><cell>15.370</cell><cell>0.313</cell></row><row><cell>LDP</cell><cell>0.484</cell><cell>23.897</cell><cell>0.278</cell></row><row><cell>NSAMF</cell><cell>0.525</cell><cell>25.616</cell><cell>0.254</cell></row><row><cell>RAJSSC</cell><cell>0.563</cell><cell>29.761</cell><cell>0.242</cell></row><row><cell>S3Tracker</cell><cell>0.505</cell><cell>27.856</cell><cell>0.240</cell></row><row><cell>SC-EBT</cell><cell>0.542</cell><cell>31.816</cell><cell>0.255</cell></row><row><cell>sPST [29]</cell><cell>0.548</cell><cell>26.253</cell><cell>0.277</cell></row><row><cell>SRDCF [30]</cell><cell>0.553</cell><cell>21.264</cell><cell>0.288</cell></row><row><cell>Struck [31]</cell><cell>0.454</cell><cell>27.153</cell><cell>0.246</cell></row><row><cell>RasNet [22]</cell><cell>-</cell><cell>-</cell><cell>0.327</cell></row><row><cell>SA-Siam [21]</cell><cell>-</cell><cell>-</cell><cell>0.310</cell></row><row><cell>SiamFC-3s [12]</cell><cell>0.534</cell><cell>-</cell><cell>0.289</cell></row><row><cell>SiamFC-5s [12]</cell><cell>0.524</cell><cell>-</cell><cell>0.274</cell></row><row><cell>SiamRPN [23]</cell><cell>0.580</cell><cell>-</cell><cell>0.358</cell></row><row><cell>SiamVGG</cell><cell>0.601</cell><cell>12.506</cell><cell>0.373</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>We compare our tracker to top 10 trackers recorded in the VOT2016 challenge and several Siamese network based methods regarding overlap, failures, expected average overlap (EAO) using the latest VOT toolkit (6.0.3).</figDesc><table><row><cell>Tracker</cell><cell>Overlap</cell><cell>Failures</cell><cell>EAO</cell></row><row><cell>C-COT [18]</cell><cell>0.533</cell><cell>16.582</cell><cell>0.331</cell></row><row><cell>DDC</cell><cell>0.534</cell><cell>20.981</cell><cell>0.293</cell></row><row><cell>DNT</cell><cell>0.509</cell><cell>19.544</cell><cell>0.278</cell></row><row><cell>EBT [28]</cell><cell>0.453</cell><cell>15.194</cell><cell>0.291</cell></row><row><cell>MLDF</cell><cell>0.487</cell><cell>15.044</cell><cell>0.311</cell></row><row><cell>SRBT</cell><cell>0.484</cell><cell>21.325</cell><cell>0.290</cell></row><row><cell>SSAT</cell><cell>0.570</cell><cell>19.272</cell><cell>0.321</cell></row><row><cell>Staple [32]</cell><cell>0.543</cell><cell>23.895</cell><cell>0.295</cell></row><row><cell>STAPLE+</cell><cell>0.552</cell><cell>24.316</cell><cell>0.286</cell></row><row><cell>TCNN [33]</cell><cell>0.547</cell><cell>17.939</cell><cell>0.325</cell></row><row><cell>SA-Siam [21]</cell><cell>-</cell><cell>-</cell><cell>0.291</cell></row><row><cell>SiamFC-A</cell><cell>-</cell><cell>-</cell><cell>0.235</cell></row><row><cell>SiamFC-R</cell><cell>-</cell><cell>-</cell><cell>0.277</cell></row><row><cell>SiamRPN [23]</cell><cell>0.560</cell><cell>-</cell><cell>0.344</cell></row><row><cell>SiamVGG</cell><cell>0.564</cell><cell>14.328</cell><cell>0.351</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>We compare our tracker to top 10 trackers recorded in the VOT2017 real-time challenge and several Siamese network based methods regarding real-time expected average overlap (EAO) and frames per second (FPS) using the latest VOT toolkit (6.0.3). Vision and Pattern Recognition (CVPR), Honolulu, HI, USA. (2017) 21-26 2. Danelljan, M., Hager, G., Shahbaz Khan, F., Felsberg, M.: Convolutional features for correlation filter based visual tracking. In: Proceedings of the IEEE International Conference on Computer Vision Workshops. (2015) 58-66 3. Nvidia: Gpu-based deep learning inference. In: Nvidia White paper. (2015)</figDesc><table><row><cell>Tracker</cell><cell>EAO</cell><cell>FPS</cell></row><row><cell>ASMS</cell><cell>0.168</cell><cell>133.28</cell></row><row><cell>CSRDCF++</cell><cell>0.212</cell><cell>-</cell></row><row><cell>csrf</cell><cell>0.158</cell><cell>16.41</cell></row><row><cell>ECOhc [1]</cell><cell>0.177</cell><cell>21.44</cell></row><row><cell>KFebT [38]</cell><cell>0.169</cell><cell>138.95</cell></row><row><cell>mosse ca</cell><cell>0.139</cell><cell>62.45</cell></row><row><cell>SiamFC [12]</cell><cell>0.182</cell><cell>35.24</cell></row><row><cell>sskcf</cell><cell>0.164</cell><cell>45.40</cell></row><row><cell>Staple [32]</cell><cell>0.170</cell><cell>54.26</cell></row><row><cell>UCT [39]</cell><cell>0.145</cell><cell>14.73</cell></row><row><cell>RasNet [22]</cell><cell>0.223</cell><cell>-</cell></row><row><cell>SA-Siam [21]</cell><cell>0.236</cell><cell>-</cell></row><row><cell>SiamRPN [23]</cell><cell>0.243</cell><cell>-</cell></row><row><cell>SiamVGG</cell><cell>0.275</cell><cell>33.15</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Computer</title>
		<meeting>the 2017 IEEE Conference on Computer</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nvidia Jetson TX2 delivers twice the intelligence to the edge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nvidia White paper</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-performance video content recognition with longterm recurrent convolutional network for fpga</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupnow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field Programmable Logic and Applications (FPL), 2017 27th International Conference on</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dnnbuilder: an automated tool for building high-performance dnn hardware accelerators for fpgas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Computer-Aided Design</title>
		<meeting>the 37th International Conference on Computer-Aided Design</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE 44th Annual International Symposium on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Computer Architecture</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2544" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolo9000: better, faster, stronger. arXiv preprint</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1091" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Youtubeboundingboxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukežič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<title level="m">The visual object tracking vot2017 challenge results</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A novel performance evaluation methodology for single-target trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojíř</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2137" to="2155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02973</idno>
		<title level="m">Learning policies for adaptive tracking with deep feature cascades</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A twofold siamese network for real-time object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4834" to="4843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning attentions: residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4854" to="4863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-toend representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08085</idno>
		<title level="m">Tracking randomly moving objects on edge box proposals</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online object tracking with proposal selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3092" to="3100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4310" to="4318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Struck: Structured output tracking with kernels. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2096" to="2109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1401" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07242</idno>
		<title level="m">Modeling and propagating cnns in a tree structure for visual tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatial-aware regressions for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8962" to="8970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-cue correlation filters for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4844" to="4853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-task correlation particle filter for robust object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time ensemble-based tracker with kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Senna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Bastos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Proceedings of the 30th Conference on Graphics, Patterns and Images (SIBGRAPI&apos;17)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Conference on Graphics, Patterns and Images</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Uct: learning unified convolutional networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision Workshops</title>
		<meeting>of the IEEE Int. Conf. on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1973" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
