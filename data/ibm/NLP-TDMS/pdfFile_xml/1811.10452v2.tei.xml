<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Aware Crowd Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
							<email>weizhe.liu@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
							<email>mathieu.salzmann@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<email>pascal.fua@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Aware Crowd Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art methods for counting people in crowded scenes rely on deep networks to estimate crowd density. They typically use the same filters over the whole image or over large image patches. Only then do they estimate local scale to compensate for perspective distortion. This is typically achieved by training an auxiliary classifier to select, for predefined image patches, the best kernel size among a limited set of choices. As such, these methods are not endto-end trainable and restricted in the scope of context they can leverage.</p><p>In this paper, we introduce an end-to-end trainable deep architecture that combines features obtained using multiple receptive field sizes and learns the importance of each such feature at each image location. In other words, our approach adaptively encodes the scale of the contextual information required to accurately predict crowd density. This yields an algorithm that outperforms state-of-the-art crowd counting methods, especially when perspective effects are strong.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Crowd counting is important for applications such as video surveillance and traffic control. In recent years, the emphasis has been on developing counting-by-density algorithms that rely on regressors trained to estimate the people density per unit area so that the total number can be obtained by integration, without explicit detection being required. The regressors can be based on Random Forests <ref type="bibr" target="#b17">[18]</ref>, Gaussian Processes <ref type="bibr" target="#b6">[7]</ref>, or more recently Deep Nets <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5]</ref>, with most state-of-the-art approaches now relying on the latter.</p><p>Standard convolutions are at the heart of these deeplearning-based approaches. By using the same filters and pooling operations over the whole image, these implicitly rely on the same receptive field everywhere. However, due to perspective distortion, one should instead change the receptive field size across the image. In the past, this has been addressed by combining either density maps extracted from image patches at different resolutions <ref type="bibr" target="#b25">[26]</ref> or feature maps obtained with convolutional filters of different sizes <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5]</ref>. However, by indiscriminately fusing information at all scales, these methods ignore the fact that scale varies continuously across the image. While this was addressed in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30]</ref> by training classifiers to predict the size of the receptive field to use locally, the resulting methods are not end-to-end trainable; cannot account for rapid scale changes because they assign a single scale to relatively large patches; and can only exploit a small range of receptive fields for the networks to remain of a manageable size.</p><p>In this paper, we introduce a deep architecture that explicitly extracts features over multiple receptive field sizes and learns the importance of each such feature at every image location, thus accounting for potentially rapid scale changes. In other words, our approach adaptively encodes the scale of the contextual information necessary to predict crowd density. This is in contrast to crowd-counting approaches that also use contextual information to account for scaling effects as in <ref type="bibr" target="#b31">[32]</ref>, but only in the loss function as opposed to computing true multi-scale features as we do. We will show that it works better on uncalibrated images. When calibration data is available, we will also show that it can be leveraged to infer suitable local scales even better and further increase performance.</p><p>Our contribution is therefore an approach that incorporates multi-scale contextual information directly into an end-to-end trainable crowd counting pipeline, and learns to exploit the right context at each image location. As shown by our experiments, we consistently outperform the state of the art on all standard crowd counting benchmarks, such as ShanghaiTech, WorldExpo'10, UCF CC 50 and UCF QNRF, as well as on our own Venice dataset 1 , which features strong perspective distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early crowd counting methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20]</ref> tended to rely on counting-by-detection, that is, explicitly detecting individual heads or bodies and then counting them. Unfortunately, in very crowded scenes, occlusions make detection difficult, and these approaches have been largely displaced by counting-by-density-estimation ones, which rely on training a regressor to estimate people density in various parts of the image and then integrating. This trend began in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b9">10]</ref>, using either Gaussian Process or Random Forests regressors. Even though approaches relying on low-level features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref> can yield good results, they have now mostly been superseded by CNN-based methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5]</ref>, a survey of which can be found in <ref type="bibr" target="#b35">[36]</ref>. The same can be said about methods that count objects instead of people <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>The people density we want to measure is the number of people per unit area on the ground. However, the deep nets operate in the image plane and, as a result, the density estimate can be severely affected by the local scale of a pixel, that is, the ratio between image area and corresponding ground area. This problem has long been recognized. For example, the algorithms of <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b16">17]</ref> use geometric information to adapt the network to different scene geometries. Because this information is not always readily available, other works have focused on handling the scale implicitly within the model. In <ref type="bibr" target="#b35">[36]</ref>, this was done by learning to predict pre-defined density levels. These levels, however, need to be provided by a human annotator at training time. By contrast, the algorithms of <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref> use image patches extracted at multiple scales as input to a multistream network. They then either fuse the features for final density prediction <ref type="bibr" target="#b25">[26]</ref> without accounting for continuous scale changes or introduce an ad hoc term in the training loss function <ref type="bibr" target="#b31">[32]</ref> to enforce prediction consistency across scales. This, however, does not encode contextual information into the features produced by the network and therefore has limited impact. While <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5]</ref> aim to learn multi-scale features, by using different receptive fields, they combine all of these features to predict the density.</p><p>In other words, while the previous methods account for scale, they ignore the fact that the suitable scale varies smoothly over the image and should be handled adaptively. This was addressed in <ref type="bibr" target="#b15">[16]</ref> by weighting different density maps generated from input images at various scales. However, the density map at each scale only depends on features extracted at this particular scale, and thus may already be corrupted by the lack of adaptive-scale reasoning. Here, we argue that one should rather extract features at multiple scales and learn how to adaptively combine them. While this, in essence, was also the motivation of <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30]</ref>, which train an extra classifier to assign the best receptive field for each image patch, these methods remain limited in several important ways. First, they rely on classifiers, which requires pre-training the network before training the classifier, and thus is not end-to-end trainable. Second, they typically assign a single scale to an entire image patch that can still be large and thus do not account for rapid scale changes. Last, but not least, the range of receptive field sizes they rely on remains limited in part because using much larger ones would require using much deeper architectures, which may not be easy to train given the kind of networks being used.</p><p>By contrast, in this paper, we introduce an end-to-end trainable architecture that adaptively fuses multi-scale features, without explicitly requiring defining patches, but rather by learning how to weigh these features for each individual pixel, thus allowing us to accommodate rapid scale changes. By leveraging multi-scale pooling operations, our framework can cover an arbitrarily large range of receptive fields, thus enabling us to account for much larger context than with the multiple receptive fields used by the abovementioned methods. In Section 4, we will demonstrate that it delivers superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>As discussed above, we aim to exploit context, that is, the large-scale consistencies that often appear in images. However, properly assessing what the scope and extent of this context should be in images that have undergone perspective distortion is a challenge. To meet it, we introduce a new deep net architecture that adaptively encodes multi-level contextual information into the features it produces. We then show how to use these scale-aware features to regress to a final density map, both when the cameras are not calibrated and when they are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scale-Aware Contextual Features</head><p>We formulate crowd counting as regressing a people density map from an image. Given a set of N training images {I i } 1≤i≤N with corresponding ground-truth density maps {D gt i }, our goal is to learn a non-linear mapping F parameterized by θ that maps an input image I i to an estimated density map D est i (I i ) = F(I i , θ) that is as similar as possible to D gt i in L 2 norm terms. Following common practice <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b22">23]</ref>, our starting point is a network comprising the first ten layers of a pretrained VGG-16 network <ref type="bibr" target="#b33">[34]</ref>. Given an image I, it outputs features of the form</p><formula xml:id="formula_0">f v = F vgg (I) ,<label>(1)</label></formula><p>which we take as base features to build our scale-aware ones.</p><p>As discussed in Section 2, the limitation of F vgg is that it encodes the same receptive field over the entire image. To remedy this, we compute scale-aware features by performing Spatial Pyramid Pooling <ref type="bibr" target="#b10">[11]</ref> to extract multi-scale context information from the VGG features of Eq. 1. Specifically, as illustrated at the bottom of <ref type="figure" target="#fig_0">Fig. 1</ref>, we compute these    scale-aware features as</p><formula xml:id="formula_1">z I h K U m J b Y V Y A L 0 = " &gt; A A A B 7 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a Z d u 9 m E 3 Y l Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y X F h 7 e m W F n 3 j C V w q D r f j u l t f W N</formula><formula xml:id="formula_2">z I h K U m J b Y V Y A L 0 = " &gt; A A A B 7 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a Z d u 9 m E 3 Y l Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y X F h 7 e m W F n 3 j C V w q D r f j u l t f W N</formula><formula xml:id="formula_3">z I h K U m J b Y V Y A L 0 = " &gt; A A A B 7 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a Z d u 9 m E 3 Y l Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y X F h 7 e m W F n 3 j C V w q D r f j u l t f W N</formula><formula xml:id="formula_4">s j = U bi (F j (P ave (f v , j), θ j )) ,<label>(2)</label></formula><p>where, for each scale j, P ave (·, j) averages the VGG features into k(j) × k(j) blocks; F j is a convolutional network with kernel size 1 to combine the context features across channels without changing their dimensions. We do this because SPP keeps each feature channel independent, thus limiting the representation power. We verified that without this the performance drops. This is in contrast to earlier arthitectures that convolve to reduce the dimension <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43]</ref>; and U bi represents bilinear interpolation to up-sample the array of contextual features to be of the same size as f v . In practice, we use S = 4 different scales, with corresponding block sizes k(j) ∈ {1, 2, 3, 6} since it shows better performance compared with other settings. The simplest way to use our scale-aware features would be to concatenate all of them to the original VGG features f v . This, however, would not account for the fact that scale varies across the image. To model this, we propose to learn to predict weight maps that set the relative influence of each scale-aware feature at each spatial location. To this end, we first define contrast features as</p><formula xml:id="formula_5">c j = s j − f v .<label>(3)</label></formula><p>They capture the differences between the features at a specific location and those in the neighborhood, which often is an important visual cue that denotes saliency. Note that, for human beings, saliency matters. For example, in the image of <ref type="figure" target="#fig_4">Fig. 2</ref>, the eye is naturally drawn to the woman at the center in part because edges in the rest of the image all point in her direction and that edges at her location do not. In our context, these contrast features provide us with important information to understand the local scale of each image region. We therefore exploit them as input to auxiliary networks with weights θ j sa that compute the weights ω j assigned to each one of the S different scales we use. Each such network outputs a scale-specific weight map of the form ω j = F j sa (c j , θ j sa ) .</p><p>(4) F j sa is a 1×1 convolutional layer followed by a sigmoid function to avoid division by zero. We then employ these weights to compute our final contextual features as</p><formula xml:id="formula_6">f I = f v | S j=1 ω j s j S j=1 ω j ,<label>(5)</label></formula><p>where [·|·] denotes the channel-wise concatenation operation, and is the element-wise product between a weight map and a feature map. Altogether, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the network F(I, θ) extracts the contextual features f I as discussed above, which are then passed to a decoder consisting of several dilated convolutions that produces the density map. The specific architecture of the network is described in <ref type="table" target="#tab_0">Table 1</ref>. As shown by our experiments, this network already outperforms the state of the art on all benchmark datasets, without explicitly using information about camera geometry. As discussed below, however, these results can be further improved when such information is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Geometry-Guided Context Learning</head><p>Because of perspective distortion, the contextual scope suitable for each region varies across the image plane. Hence, scene geometry is highly related to contextual information and could be used to guide the network to better adjust to the scene context it needs.</p><p>We therefore extend the previous approach to exploiting geometry information when it is available. To this end, we represent the scene geometry of image I i with a perspective map M i , which encodes the number of pixels per meter in the image plane. Note that this perspective map has the same spatial resolution as the input image. We therefore use it as input to a truncated VGG-16 network. In other words, the base features of Eq. 1 are then replaced by features of the form</p><formula xml:id="formula_7">f g = F vgg (M i , θ g ) ,<label>(6)</label></formula><p>where F vgg is a modified VGG-16 network with a single input channel. To initialize the weights corresponding to this channel, we average those of the original three RGB channels. Note that we also normalize the perspective map M i to lie within the same range as the RGB images. Even though this initialization does not bring any obvious difference in the final counting accuracy, it makes the network converge much faster.</p><p>To further propagate the geometry information to later stages of our network, we exploit the modified VGG features described above, which inherently contain geometry information, as an additional input to the auxiliary network of Eq. 4. Specifically, the weight map for each scale is then layer front-end(F vgg ) layer back-end decoder 1 -2 3×3×64 conv-1 1 3×3×512 conv-2 2 × 2 max pooling 2 3×3×512 conv-2 3 -4 3×3×128 conv-1 3 3×3×512 conv-2 2 × 2 max pooling 4 3×3×256 conv-2 5 -7 3×3×256 conv-1 5 3×3×128 conv-2 2 × 2 max pooling 6 3×3×64 conv-2 8 -10 3×3×512 conv-1 7 1×1×1 conv-1 computed as</p><formula xml:id="formula_8">ω j = F j gc ([c j |f g ] , θ j gc ) .<label>(7)</label></formula><p>These weight maps are then used as in Eq. 5. <ref type="figure" target="#fig_6">Fig. 3</ref> depicts the corresponding architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Details and Loss Function</head><p>Whether with or without geometry information, our networks are trained using the L 2 loss defined as</p><formula xml:id="formula_9">L(θ) = 1 2B B i=1 D gt i − D est i 2 2 ,<label>(8)</label></formula><p>where B is the batch size. To obtain the ground-truth density maps D gt i , we rely on the same strategy as previous work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b29">30]</ref>. Specifically, to each image I i , we associate a set of c i 2D points P gt i = {P j i } 1≤j≤ci that denote the position of each human head in the scene. The corresponding ground-truth density map D gt i is obtained by convolving an image containing ones at these locations and zeroes elsewhere with a Gaussian kernel N gt (p|µ, σ 2 ) <ref type="bibr" target="#b20">[21]</ref>. We write</p><formula xml:id="formula_10">∀p ∈ I i , D gt i (p|I i ) = ci j=1 N gt (p|µ = P j i , σ 2 ) ,<label>(9)</label></formula><p>where µ and σ represent the mean and standard deviation of the normal distribution. To produce the comparative results we will show in Section 4, we use the same σ as the methods we compare against.</p><p>To minimize the loss of Eq. 8, we use Stochastic Gradient Descent (SGD) with batch size 1 for various size dataset and Adam with batch size 32 for fixed size dataset. Furthermore, during training, we randomly crop image patches of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the proposed approach. We first introduce the evaluation metrics and benchmark datasets we use in our experiments. We then compare our approach to state-of-the-art methods, and finally perform a detailed ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Metrics</head><p>Previous works in crowd density estimation use the mean absolute error (M AE) and the root mean squared error (RM SE) as evaluation metrics <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36]</ref>. They are defined as</p><formula xml:id="formula_11">M AE = 1 N N i=1 |z i −ẑ i | and RM SE = 1 N N i=1 (z i −ẑ i ) 2 ,</formula><p>where N is the number of test images, z i denotes the true number of people inside the ROI of the ith image andẑ i the estimated number of people. In the benchmark datasets discussed below, the ROI is the whole image except when explicitly stated otherwise. Note that number of people can be recovered by integrating over the pixels of the predicted density maps asẑ i = p∈Ii D est i (p|I i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmark Datasets and Ground-truth Data</head><p>We use five different datasets to compare our approach to recent ones. The first four were released along with recent papers and have already been used for comparison purposes since. We created the fifth one ourselves and will make it publicly available as well.</p><p>ShanghaiTech <ref type="bibr" target="#b41">[42]</ref>. It comprises 1,198 annotated images with 330,165 people in them. It is divided in part A with 482 images and part B with 716. In part A, 300 images form the training set and, in part B, 400. The remainder are used for testing purposes. For a fair comparison with earlier work <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref>, we created the ground-truth density maps in the same manner as they did. Specifically, for Part A, we used the geometry-adaptive kernels introduced in <ref type="bibr" target="#b41">[42]</ref>, and for part B, fixed kernels. In <ref type="figure" target="#fig_7">Fig. 4</ref>, we show one image from each part, along with the ground-truth density maps and those estimated by our algorithm. <ref type="bibr" target="#b14">[15]</ref>. It comprises 1,535 jpeg images with 1,251,642 people in them. The training set is made of 1,201 of these images. Unlike in ShanghaiTech, there are dramatic variations both in crowd density and image resolution. The ground-truth density maps were generated by adaptive Gaussian kernels as in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCF-QNRF</head><p>UCF CC 50 <ref type="bibr" target="#b13">[14]</ref>. It contains only 50 images with a people count varying from 94 to 4,543, which makes it challenging for a deep-learning approach. For a fair comparison again, the ground-truth density maps were generated using fixed kernels and we follow the same 5-fold cross-validation protocol as in <ref type="bibr" target="#b13">[14]</ref>: We partition the images into 5 10-image groups. In turn, we then pick four groups for training and the remaining one for testing. This gives us 5 sets of results and we report their average.   <ref type="figure" target="#fig_8">Fig. 5</ref> depicts three of these images and the associated camera calibration data. We generate the ground-truth density maps as in our baselines <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5]</ref>. As in previous work <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref> on this dataset, we report the MAE of each scene, as well as the average over all scenes.</p><p>Venice. The four datasets discussed above have the advantage of being publicly available but do not contain precise calibration information. In practice, however, it can be readily obtained using either standard photogrammetry techniques or onboard sensors, for example when using a drone to acquire the images. To test this kind of scenario, we used a cellphone to film additional sequences of the Piazza San Marco in Venice, as seen from various viewpoints on the second floor of the basilica, as shown in the top two rows of <ref type="figure" target="#fig_8">Fig. 5</ref>. We then used the white lines on the ground to compute camera models. As shown in the bottom two rows of <ref type="figure" target="#fig_8">Fig. 5</ref>, this yields a more accurate calibration than in WorldExpo'10. The resulting dataset contains 4 different sequences and in total 167 annotated frames with fixed 1,280 × 720 resolution. 80 images from a single long sequence are taken as training data, and we use the images from the remaining 3 sequences for testing purposes. The ground-truth density maps were generated using fixed Gaussian kernels as in part B of the ShanghaiTech dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparing against Recent Techniques</head><p>In <ref type="table" target="#tab_2">Tables 2, 3</ref>, 4, and 5, we compare our results to those of the method that returns the best results for each one of the 4 public datasets, as currently reported in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>M AE RM SE Idrees et al. <ref type="bibr" target="#b13">[14]</ref> 315 508 MCNN <ref type="bibr" target="#b41">[42]</ref> 277 426 Encoder-Decoder <ref type="bibr" target="#b2">[3]</ref> 270 478 CMTL <ref type="bibr" target="#b34">[35]</ref> 252 514 Switch-CNN <ref type="bibr" target="#b30">[31]</ref> 228 445 Resnet101 <ref type="bibr" target="#b11">[12]</ref> 190 277 Densenet201 <ref type="bibr" target="#b12">[13]</ref> 163 226 Idrees et al. <ref type="bibr" target="#b14">[15]</ref> 132 191 OURS-CAN 107 183 Model M AE RM SE Idrees et al. <ref type="bibr" target="#b13">[14]</ref> 419.5 541.6 Zhang et al. <ref type="bibr" target="#b40">[41]</ref> 467.0 498.5 MCNN <ref type="bibr" target="#b41">[42]</ref> 377.6 509.1 Switch-CNN <ref type="bibr" target="#b30">[31]</ref> 318.1 439.2 CP-CNN <ref type="bibr" target="#b35">[36]</ref> 295.8 320.9 ACSCP <ref type="bibr" target="#b31">[32]</ref> 291.0 404.6 Liu et al. <ref type="bibr" target="#b23">[24]</ref> 337.6 434.3 D-ConvNet <ref type="bibr" target="#b32">[33]</ref> 288.4 404.7 IG-CNN <ref type="bibr" target="#b29">[30]</ref> 291.4 349.4 ic-CNN <ref type="bibr" target="#b27">[28]</ref> 260.9 365.5 CSRNet <ref type="bibr" target="#b18">[19]</ref> 266.1 397.5 SANet <ref type="bibr" target="#b4">[5]</ref> 258.4 334.9 OURS-CAN 212.2 243.7  They are those of <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and <ref type="bibr" target="#b31">[32]</ref>, respectively. In each case, we reprint the results as given in these papers and add those of OURS-CAN, that is, our method as described in Section 3.1. On the first three datasets, we consistently and clearly outperform all other methods. On the World-Expo'10 dataset, we also outperform them on average, but As can be seen in the bottom right image, the ellipse surface corresponds to an area that could be filled by many more people that could realistically fit in a 1m radius circle. By contrast, the ellipse deformations are more consistent and accurate for Venice, which denotes a better registration. not in every scene. More specifically, in Scenes 2 and 4 that are crowded, we do very well. By contrast, the crowds are far less dense in Scenes 1 and 5. This makes context less informative and our approach still performs honorably but looses its edge compared to the others. Interestingly, as can be seen in <ref type="table" target="#tab_5">Table 5</ref>, in such uncrowded scenes, a detectionbased method such as DecideNet <ref type="bibr" target="#b20">[21]</ref> becomes competitive whereas it isn't in the more crowded ones. In <ref type="figure">Fig. 6</ref>, we use a Venice image to show how well our approach does compared to the others in the crowded parts of the scene.</p><p>The first three datasets do not have any associated camera calibration data, whereas WorldExpo'10 comes with a rough estimation of the image plane to ground plane homography and Venice with an accurate one. We therefore used these homographies to run OURS-ECAN, our method as described in Section 3.2. We report the results in Tables 5 and 6. Unsurprisingly, OURS-ECAN clearly further improves on OURS-CAN when the calibration data is accurate as for Venice and even when it is less so as for World-Expo, but by a smaller margin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Finally, we perform an ablation study to confirm the benefits of encoding multiple level contextual information and of introducing contrast features.    Concatenating and Weighting VGG Features. We compare our complete model without geometry, OURS-CAN, against two simplified versions of it. The first one, VGG-SIMPLE, directly uses VGG-16 base features f v as input to the decoder subnetwork. In other words, it does not adapt for scale. The second one, VGG-CONCAT, concatenates all scale-aware features {s j } 1≤j≤S to the base features instead of computing their weighted linear combination, and then passes the resulting features to the decoder. We compare these three methods on the ShanghaiTech Part A, which has often been used for such ablation studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19]</ref>. As can be seen in <ref type="table" target="#tab_9">Table 7</ref>, concatenating the VGG features as in VGG-CONCAT yields a significant boost, and weighing them as in OURS-CAN a further one.</p><p>Contrast Features. To demonstrate the importance of using contrast features to learn the network weights, we compare OURS-CAN against VGG-NCONT that uses the scale features s j instead of the contrast ones to learn the weight maps. As can be seen in <ref type="table" target="#tab_9">Table 7</ref>, this also results in a substantial performance loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Perspectives</head><p>In this paper, we have shown that encoding multi-scale context adaptively, along with providing an explicit model of perspective distortion effects as input to a deep net, substantially increases crowd counting performance. In particular, it yields much better density estimates in high-density regions. This is of particular interest for crowd counting from mobile cameras, such as those carried by drones. In future work, we will therefore augment the image data with the information provided by the drone's inertial measurement unit to compute perspective distortions on the fly and allow monitoring from the moving drone.</p><p>We will also expand our approach to process consecutive images simultaneously and enforce temporal consistency, which among other things implies correcting ground-truth densities to also account for perspective distortions and be able to properly reason in the terms of ground-plane densities instead of image-plane densities, which none of the approaches discussed in this paper do. We did not do it either so that our results could be properly compared to the state of the art. However, as shown in <ref type="figure" target="#fig_9">Fig. 7</ref>, the price to pay is that the estimated densities, because they are close to this image-based ground truth, need to be corrected for perspective distortion before they can be treated as groundplane densities. An obvious improvement would therefore be to directly regress to ground densities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 &lt;</head><label>1</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " R E K h q o + 1 6 7 D W 3 X o j I a X c e R b G l f E = " &gt; A A A B 8 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g q 2 F J p T N d t M u 3 W z C 7 k Q o o X / D i w d F v P p n v P l v 3 L Q 5 a O s L C w / v z D C z b 5 h K Y d B 1 v 5 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / U D 8 8 6 p o k 0 4 x 3 W C I T 3 Q u p 4 V I o 3 k G B k v d S z W k c S v 4 Y T m 6 L + u M T 1 0 Y k 6 g G n K Q 9 i O l I i E o y i t X y P + C h i b o h H a o N 6 w 2 2 6 c 5 F V 8 E p o Q K n 2 o P 7 l D x O W x V w h k 9 S Y v u e m G O R U o 2 C S z 2 p + Z n h K 2 Y S O e N + i o n Z R k M 9 v n p E z 6 w x J l G j 7 F J K 5 + 3 s i p 7 E x 0 z i 0 n T H F s V m u F e Z / t X 6 G 0 X W Q C 5 V m y B V b L I o y S T A h R Q B k K D R n K K c W K N P C 3 k r Y m G r K 0 M Z U h O A t f 3 k V u h d N z / L 9 Z a N 1 U 8 Z R h R M 4 h X P w 4 A p a c A d t 6 A C D F J 7 h F d 6 c z H l x 3 p 2 P R W v F K W e O 4 Y + c z x / X Y p A 8 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R E K h q o + 1 6 7 D W 3 X o j I a X c e R b G l f E = " &gt; A A A B 8 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g q 2 F J p T N d t M u 3 W z C 7 k Q o o X / D i w d F v P p n v P l v 3 L Q 5 a O s L C w / v z D C z b 5 h K Y d B 1 v 5 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / U D 8 8 6 p o k 0 4 x 3 W C I T 3 Q u p 4 V I o 3 k G B k v d S z W k c S v 4 Y T m 6 L + u M T 1 0 Y k 6 g G n K Q 9 i O l I i E o y i t X y P + C h i b o h H a o N 6 w 2 2 6 c 5 F V 8 E p o Q K n 2 o P 7 l D x O W x V w h k 9 S Y v u e m G O R U o 2 C S z 2 p + Z n h K 2 Y S O e N + i o n Z R k M 9 v n p E z 6 w x J l G j 7 F J K 5 + 3 s i p 7 E x 0 z i 0 n T H F s V m u F e Z / t X 6 G 0 X W Q C 5 V m y B V b L I o y S T A h R Q B k K D R n K K c W K N P C 3 k r Y m G r K 0 M Z U h O A t f 3 k V u h d N z / L 9 Z a N 1 U 8 Z R h R M 4 h X P w 4 A p a c A d t 6 A C D F J 7 h F d 6 c z H l x 3 p 2 P R W v F K W e O 4 Y + c z x / X Y p A 8 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R E K h q o + 1 6 7 D W 3 X o j I a X c e R b G l f E = " &gt; A A A B 8 3 i c b Z B N S 8 N A E I Y n 9 a v W r 6 p HL 4 t F 8 F Q S E f R Y 9 O K x g q 2 F J p T N d t M u 3 W z C 7 k Q o o X / D i w d F v P p n v P l v 3 L Q 5 a O s L C w / v z D C z b 5 h K Y d B 1 v 5 3 K 2 v r G 5 l Z 1 u 7 a z u 7 d / U D 8 8 6 p o k 0 4 x 3 W C I T 3 Q u p 4 V I o 3 k G B k v d S z W k c S v 4 Y T m 6 L + u M T 1 0 Y k 6 g G n K Q 9 i O l I i E o y i t X y P + C h i b o h H a o N 6 w 2 2 6 c 5 F V 8 E p o Q K n 2 o P 7 l D x O W x V w h k 9 S Y v u e m G O R U o 2 C S z 2 p + Z n h K 2 Y S O e N + i o n Z R k M 9 v n p E z 6 w x J l G j 7 F J K 5 + 3 s i p 7 E x 0 z i 0 n T H F s V m u F e Z / t X 6 G 0 X W Q C 5 V m y B V b L I o y S T A h R Q B k K D R n K K c W K N P C 3 k r Y m G r K 0 M Z U h O A t f 3 k V u h d N z / L 9 Z a N 1 U 8 Z R h R M 4 h X P w 4 A p a c A d t 6 A C D F J 7 h F d 6 c z H l x 3 p 2 P R W v F K W e O 4 Y + c z x / X Y p A 8 &lt; / la t e x i t &gt; VGG features fv &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V D j 2 I 0 h + T r y K 3 v 4 9 y 5 k f X 5 m j U v E = " &gt; A A A B 7 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a R d u t m E 3 U 2 h h P 4 G L x 4 U 8 e o P 8 u a / c d v m o K 0 v L D y 8 M 8 P O v G E q u D a u + + 2 U N j a 3 t n f K u 5 W 9 / Y P D o + r x S U s n m W L o s 0 Q k q h N S j Y J L 9 A 0 3 A j u p Q h q H A t v h + H 5 e b 0 9 Q a Z 7 I J z N N M Y j p U P K I M 2 q s 5 U f 9 f D L r V 2 t u 3 V 2 I r I N X Q A 0 K N f v V r 9 4 g Y V m M 0 j B B t e 5 6 b m q C n C r D m c B Z p Z d p T C k b 0 y F 2 L U o a o w 7 y x b I z c m G d A Y k S Z Z 8 0 Z O H + n s h p r P U 0 D m 1 n T M 1 I r 9 b m 5 n + 1 b m a i 2 y D n M s 0 M S r b 8 K M o E M Q m Z X 0 4 G X C E z Y m q B M s X t r o S N q K L M 2 H w q N g R v 9 e R 1 a F 3 V P c u P 1 7 X G X R F H G c 7 g H C 7 B g x t o w A M 0 w Q c G H J 7 h F d 4 c 6 b w 4 7 8 7 H s r X k F D O n 8 E f O 5 w 8 d o I 7 f &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V D j 2 I 0 h + T r y K 3 v 4 9 y 5 k f X 5 m j U v E = " &gt; A A A B 7 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a R d u t m E 3 U 2 h h P 4 G L x 4 U 8 e o P 8 u a / c d v m o K 0 v L D y 8 M 8 P O v G E q u D a u + + 2 U N j a 3 t n f K u 5 W 9 / Y P D o + r x S U s n m W L o s 0 Q k q h N S j Y J L 9 A 0 3 A j u p Q h q H A t v h + H 5 e b 0 9 Q a Z 7 I J z N N M Y j p U P K I M 2 q s 5 U f 9 f D L r V 2 t u 3 V 2 I r I N X Q A 0 K N f v V r 9 4 g Y V m M 0 j B B t e 5 6 b m q C n C r D m c B Z p Z d p T C k b 0 y F 2 L U o a o w 7 y x b I z c m G d A Y k S Z Z 8 0 Z O H + n s h p r P U 0 D m 1 n T M 1 I r 9 b m 5 n + 1 b m a i 2 y D n M s 0 M S r b 8 K M o E M Q m Z X 0 4 G X C E z Y m q B M s X t r o S N q K L M 2 H w q N g R v 9 e R 1 a F 3 V P c u P 1 7 X G X R F H G c 7 g H C 7 B g x t o w A M 0 w Q c G H J 7 h F d 4 c 6 b w 4 7 8 7 H s r X k F D O n 8 E f O 5 w 8 d o I 7 f &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V D j 2 I 0 h + T r y K 3 v 4 9 y 5 k f X 5 m j U v E = " &gt; A A A B 7 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a R d u t m E 3 U 2 h h P 4 G L x 4 U 8 e o P 8 u a / c d v m o K 0 v L D y 8 M 8 P O v G E q u D a u + + 2 U N j a 3 t n f K u 5 W 9 / Y P D o + r x S U s n m W L o s 0 Q k q h N S j Y J L 9 A 0 3 A j u p Q h q H A t v h + H 5 e b 0 9 Q a Z 7 I J z N N M Y j p U P K I M 2 q s 5 U f 9 f D L r V 2 t u 3 V 2 I r I N X Q A 0 K N f v V r 9 4 g Y V m M 0 j B B t e 5 6 b m q C n C r D m c B Z p Z d p T C k b 0 y F 2 L U o a o w 7 y x b I z c m G d A Y k S Z Z 8 0 Z O H + n s h p r P U 0 D m 1 n T M 1 I r 9 b m 5 n + 1 b m a i 2 y D n M s 0 M S r b 8 K M o E M Q m Z X 0 4 G X C E z Y m q B M s X t r o S N q K L M 2 H w q N g R v 9 e R 1 a F 3 V P c u P 1 7 X G X R F H G c 7 g H C 7 B g x t o w A M 0 w Q c G H J 7 h F d 4 c 6 b w 4 7 8 7 H s r X k F D O n 8 E f O 5 w 8 d o I 7 f &lt; / l a t e x i t &gt; scale features s j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 7 O / o 3 O p k c U 0 d I Y a W J p l F T h B R s 8 = " &gt; A A A B 7 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a Z d u 9 m E 3 Y l Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y X F h 7 e m W F n 3 j C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D J J p h n 3 W S I T 3 Q m p 4 V I o 7 q N A y T u p 5 j Q O J W + H 4 9 t Z v f 3 E t R G J e s B J y o O Y D p W I B K N o L d / 0 8 8 d p v 1 p z 6 + 5 c Z B W 8 A m p Q q N m v f v U G C c t i r p B J a k z X c 1 M M c q p R M M m n l V 5 m e E r Z m A 5 5 1 6 K i M T d B P l 9 2 S s 6 s M y B R o u 1 T S O b u 7 4 m c x s Z M 4 t B 2 x h R H Z r k 2 M / + r d T O M r o N c q D R D r t j i o y i T B B M y u 5 w M h O Y M 5 c Q C Z V r Y X Q k b U U 0 Z 2 n w q N g R v + e R V a F 3 U P c v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X k F D P H 8 E f O 5 w 8 f T I 7 g &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 7 O / o 3 O p k c U 0 d I Y a W J p l F T h B R s 8 = " &gt; A A A B 7 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a Z d u 9 m E 3 Y l Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y X F h 7 e m W F n 3 j C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D J J p h n 3 W S I T 3 Q m p 4 V I o 7 q N A y T u p 5 j Q O J W + H 4 9 t Z v f 3 E t R G J e s B J y o O Y D p W I B K N o L d / 0 8 8 d p v 1 p z 6 + 5 c Z B W 8 A m p Q q N m v f v U G C c t i r p B J a k z X c 1 M M c q p R M M m n l V 5 m e E r Z m A 5 5 1 6 K i M T d B P l 9 2 S s 6 s M y B R o u 1 T S O b u 7 4 m c x s Z M 4 t B 2 x h R H Z r k 2 M / + r d T O M r o N c q D R D r t j i o y i T B B M y u 5 w M h O Y M 5 c Q C Z V r Y X Q k b U U 0 Z 2 n w q N g R v + e R V a F 3 U P c v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X k F D P H 8 E f O 5 w 8 f T I 7 g &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p 7 O / o 3 O p k c U 0 d I Y a W J p l F T h B R s 8 = " &gt; A A A B 7 H i c b Z B N S 8 N A E I Y n 9 a v W r 6 p H L 4 t F 8 F Q S E f R Y 9 O K x g m k L b S i b 7 a Z d u 9 m E 3 Y l Q Q n + D F w + K e P U H e f P f u G 1 z 0 N Y X F h 7 e m W F n 3 j C V w q D r f j u l t f W N z a 3 y d m V n d 2 / / o H p 4 1 D J J p h n 3 W S I T 3 Q m p 4 V I o 7 q N A y T u p 5 j Q O J W + H 4 9 t Z v f 3 E t R G J e s B J y o O Y D p W I B K N o L d / 0 8 8 d p v 1 p z 6 + 5 c Z B W 8 A m p Q q N m v f v U G C c t i r p B J a k z X c 1 M M c q p R M M m n l V 5 m e E r Z m A 5 5 1 6 K i M T d B P l 9 2 S s 6 s M y B R o u 1 T S O b u 7 4 m c x s Z M 4 t B 2 x h R H Z r k 2 M / + r d T O M r o N c q D R D r t j i o y i T B B M y u 5 w M h O Y M 5 c Q C Z V r Y X Q k b U U 0 Z 2 n w q N g R v + e R V a F 3 U P c v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X k F D P H 8 E f O 5 w 8 f T I 7 g &lt; / l a t e x i t &gt; bilinear interpolation contrast features c j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x m r 6 J R T D C F p h u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>z a 3 y d m V n d 2 / / o H p 4 1 D J J p h n 3 W S I T 3 Q m p 4 V I o 7 q N A y T u p 5 j Q O J W + H 4 9 t Z v f 3 E t R G J e s B J y o O Y D p W I B K N o L Z / 1 8 8 d p v 1 p z 6 + 5 c Z B W 8 A m p Q q N m v f v U G C c t i r p B J a k z X c 1 M M c q p R M M m n l V 5 m e E r Z m A 5 5 1 6 K i M T d B P l 9 2 S s 6 s M y B R o u 1 T S O b u 7 4 m c x s Z M 4 t B 2 x h R H Z r k 2 M / + r d T O M r o N c q D R D r t j i o y i T B B M y u 5 w M h O Y M 5 c Q C Z V r Y X Q k b U U 0 Z 2 n w q N g R v + e R V a F 3 U P c v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X k F D P H 8 E f O 5 w 8 G z I 7 Q &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x m r 6 J R T D C F p h u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>z a 3 y d m V n d 2 / / o H p 4 1 D J J p h n 3 W S I T 3 Q m p 4 V I o 7 q N A y T u p 5 j Q O J W + H 4 9 t Z v f 3 E t R G J e s B J y o O Y D p W I B K N o L Z / 1 8 8 d p v 1 p z 6 + 5 c Z B W 8 A m p Q q N m v f v U G C c t i r p B J a k z X c 1 M M c q p R M M m n l V 5 m e E r Z m A 5 5 1 6 K i M T d B P l 9 2 S s 6 s M y B R o u 1 T S O b u 7 4 m c x s Z M 4 t B 2 x h R H Z r k 2 M / + r d T O M r o N c q D R D r t j i o y i T B B M y u 5 w M h O Y M 5 c Q C Z V r Y X Q k b U U 0 Z 2 n w q N g R v + e R V a F 3 U P c v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X k F D P H 8 E f O 5 w 8 G z I 7 Q &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x m r 6 J R T D C F p h u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>z a 3 y d m V n d 2 / / o H p 4 1 D J J p h n 3 W S I T 3 Q m p 4 V I o 7 q N A y T u p 5 j Q O J W + H 4 9 t Z v f 3 E t R G J e s B J y o O Y D p W I B K N o L Z / 1 8 8 d p v 1 p z 6 + 5 c Z B W 8 A m p Q q N m v f v U G C c t i r p B J a k z X c 1 M M c q p R M M m n l V 5 m e E r Z m A 5 5 1 6 K i M T d B P l 9 2 S s 6 s M y B R o u 1 T S O b u 7 4 m c x s Z M 4 t B 2 x h R H Z r k 2 M / + r d T O M r o N c q D R D r t j i o y i T B B M y u 5 w M h O Y M 5 c Q C Z V r Y X Q k b U U 0 Z 2 n w q N g R v + e R V a F 3 U P c v 3 l 7 X G T R F H G U 7 g F M 7 B g y t o w B 0 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X k F D P H 8 E f O 5 w 8 G z I 7 Q &lt; / l a t e x i t &gt; ) ⇥ k(j) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q 7 k R r t K T k R 5 k 7 d 3 i A x P V v w 5 T B u A = " &gt; A A A B + X i c b Z B N S 8 N A E I Y n 9 a v W r 6 h H L 4 t F q J e S i K D H o h e P F W w t t K F s t p t 2 7 e a D 3 U m h h P 4 T L x 4 U 8 e o / 8 e a / c d P m o K 0 v L D y 8 M 8 P M v n 4 i h U b H + b Z K a + s b m 1 v l 7 c r O 7 t 7 + g X 1 4 1 N Z x q h h v s V j G q u N T z a W I e A s F S t 5 J F K e h L / m j P 7 7 N 6 4 8 T r r S I o w e c J t w L 6 T A S g W A U j d W 3 7 X H t 6 Z z 0 U I R c k 5 z 7 d t W p O 3 O R V X A L q E K h Z t / + 6 g 1 i l o Y 8 Q i a p 1 l 3 X S d D L q E L B J J 9 V e q n m C W V j O u R d g x E 1 m 7 x s f v m M n B l n Q I J Y m R c h m b u / J z I a a j 0 N f d M Z U h z p 5 V p u / l f r p h h c e 5 m I k h R 5 x B a L g l Q S j E k e A x k I x R n K q Q H K l D C 3 E j a i i j I 0 Y V V M C O 7 y l 1 e h f V F 3 D d 9 f V h s 3 R R x l O I F T q I E L V 9 C A O 2 h C C x h M 4 B l e 4 c 3 K r B f r 3 f p Y t J a s Y u Y Y / s j 6 / A G c R Z J V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q 7 k R r t K T k R 5 k 7 d 3i A x P V v w 5 T B u A = " &gt; A A A B + X i c b Z B N S 8 N A E I Y n 9 a v W r 6 h H L 4 t F q J e S i K D H o h e P F W w t t K F s t p t 2 7 e a D 3 U m h h P 4 T L x 4 U 8 e o / 8 e a / c d P m o K 0 v L D y 8 M 8 P M v n 4 i h U b H + b Z K a + s b m 1 v l 7 c r O 7 t 7 + g X 1 4 1 N Z x q h h v s V j G q u N T z a W I e A s F S t 5 J F K e h L / m j P 7 7 N 6 4 8 T r r S I o w e c J t w L 6 T A S g WA U j d W 3 7 X H t 6 Z z 0 U I R c k 5 z 7 d t W p O 3 O R V X A L q E K h Z t / + 6 g 1 i l o Y 8 Q i a p 1 l 3 X S d D L q E L B J J 9 V e q n m C W V j O u R d g x E 1 m 7 x s f v m M n B l n Q I J Y m R c h m b u / J z I a a j 0 N f d M Z U h z p 5 V p u / l f r p h h c e 5 m I k h R 5 x B a L g l Q S j E k e A x k I x R n K q Q H K l D C 3 E j a i i j I 0 Y V V M C O 7 y l 1 e h f V F 3 D d 9 f V h s 3 R R x l O I F T q I E L V 9 C A O 2 h C C x h M 4 B l e 4 c 3 K r B f r 3 f p Y t J a s Y u Y Y / s j 6 / A G c R Z J V &lt; / l a t e x i t &gt; &lt; l at e x i t s h a 1 _ b a s e 6 4 = " Q 7 k R r t K T k R 5 k 7 d 3 i A x P V v w 5 T B u A = " &gt; A A A B + X i c b Z B N S 8 N A E I Y n 9 a v W r 6 h H L 4 t F q J e S i K D H o h e P F W w t t K F s t p t 2 7 e a D 3 U m h h P 4 T L x 4 U 8 e o / 8 e a / c d P m o K 0 v L D y 8 M 8 P M v n 4 i h U b H + b Z K a + s b m 1 v l 7 c r O 7 t 7 + g X 1 4 1 N Z x q h h v s V j G q u N T z a W I e A s F S t 5 J F K e h L / m j P 7 7 N 6 4 8 T r r S I o w e c J t w L 6 T A S g W A U j d W 3 7 X H t 6 Z z 0 U I R c k 5 z 7 d t W p O 3 O R V X A L q E K h Z t / + 6 g 1 i l o Y 8 Q i a p 1 l 3 X S d D L q E L B J J 9 V e q n m C W V j O u R d g x E 1 m 7 x s f v m M n B l n Q I J Y m R c h m b u / J z I a a j 0 N f d M Z U h z p 5 V p u / l f r p h h c e 5 m I k h R 5 x B a L g l Q S j E k e A x k I x R n K q Q H K l D C 3 E j a i i j I 0 Y V V M C O 7 y l 1 e h f V F 3 D d 9 f V h s 3 R R x l O I F T q I E L V 9 C A O 2 h C C x h M 4 B l e 4 c 3 K r B f r 3 f p Y t J a s Y u Y Y / s j 6 / A G c R Z J V &lt; / l a t e x i t &gt; ) ⇥ k(j)&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q 7 k R r t K T k R 5 k 7 d 3 i A x P V v w 5 T B u A = " &gt; A A A B + X i c b Z B N S 8 N A E I Y n 9 a v W r 6 h H L 4 t F q J e S i K D H o h e P F W w t t K F s t p t 2 7 e a D 3 U m h h P 4 T L x 4 U 8 e o / 8 e a / c d P m o K 0 v L D y 8 M 8 P M v n 4 i h U b H + b Z K a + s b m 1 v l 7 c r O 7 t 7 + g X 1 4 1 N Z x q h h v s V j G q u N T z a W I e A s F S t 5 J F K e h L / m j P 7 7 N 6 4 8 T r r S I o w e c J t w L 6 T A S g W A U j d W 3 7 X H t 6 Z z 0 U I R c k 5 z 7 d t W p O 3 O R V X A L q E K h Z t / + 6 g 1 i l o Y 8 Q i a p 1 l 3 X S d D L q E L B J J 9 V e q n m C W V j O u R d g x E 1 m 7 x s f v m M n B l n Q I J Y m R c h m b u / J z I a a j 0 N f d M Z U h z p 5 V p u / l f r p h h c e 5 m I k h R 5 x B a L g l Q S j E k e A x k I x R n K q Q H K l D C 3 E j a i i j I 0 Y V V M C O 7 y l 1 e h f V F 3 D d 9 f V h s 3 R R x l O I F T q I E L V 9 C A O 2 h C C x h M 4 B l e 4 c 3 K r B f r 3 f p Y t J a s Y u Y Y / s j 6 / A G c R Z J V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q 7 k R r t K T k R 5 k 7 d 3 i A x P V v w 5 T B u A = " &gt; A A A B + X i c b Z B N S 8 N A E I Y n 9 a v W r 6 h H L 4 t F q J e S i K D H o h e P F W w t t K F s t p t 2 7 e a D 3 U m hh P 4 T L x 4 U 8 e o / 8 e a / c d P m o K 0 v L D y 8 M 8 P M v n 4 i h U b H + b Z K a + s b m 1 v l 7 c r O 7 t 7 + g X 1 4 1 N Z x q h h v s V j G q u N T z a W I e A s F S t 5 J F K e h L / m j P 7 7 N 6 4 8 T r r S I o w e c J t w L 6 T A S g W A U j d W 3 7 X H t 6 Z z 0 U I R c k 5 z 7 d t W p O 3 O R V X A L q E K h Z t / + 6 g 1 i l o Y 8 Q i a p 1 l 3 X S d D L q E L B J J 9 V e q n m C W V j O u R d g x E 1 m 7 x s f v m M n B l n Q I J Y m R c h m b u / J z I a a j 0 N f d M Z U h z p 5 V p u / l f r p h h c e 5 m I k h R 5 x B a L g l Q S j E k e A x k I x R n K q Q H K l D C 3 E j a i i j I 0 Y V V M C O 7 y l 1 e h f V F 3 D d 9 f V h s 3 R R x l O I F T q I E L V 9 C A O 2 h C C x h M 4 B l e 4 c 3 K r B f r 3 f p Y t J a s Y u Y Y / s j 6 / A G c R Z J V &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q 7 k R r t K T k R 5 k 7 d 3 i A x P V v w 5 T B u A = " &gt; A A A B + X i c b Z B N S 8 N A E I Y n 9 a v W r 6 h H L 4 t F q J e S i K D H o h e P F W w t t K F s t p t 2 7 e a D 3 U m h h P 4 T L x 4 U 8 e o / 8 e a / c d P m o K 0 v L D y 8 M 8 P M v n 4 i h U b H + b Z K a + s b m 1 v l 7 c r O 7 t 7 + g X 1 4 1 N Z x q h h v s V j G q u N T z a W I e A s F S t 5 J F K e h L / m j P 7 7 N 6 4 8 T r r S I o w e c J t w L 6 T A S g W A U j d W 3 7 X H t 6 Z z 0 U I R c k 5 z 7 d t W p O 3 O R V X A L q E K h Z t / + 6 g 1 i l o Y 8 Q i a p 1 l 3 X S d D L q E L B J J 9 V e q n m C W V j O u R d g x E 1 m 7 x s f v m M n B l n Q I J Y m R c h m b u / J z I a a j 0 N f d M Z U h z p 5 V p u / l f r p h h c e 5 m I k h R 5 x B a L g l Q S j E k e A x k I x R n K q Q H K l D C 3 E j a i i j I 0 Y V V M C O 7 y l 1 e h f V F 3 D d 9 f V h s 3 R R x l O I F T q I E L V 9 C A O 2 h C C x h M 4 B l e 4 c 3 K r B f r 3 f p Y t J a s Y u Y Y / s j 6 / A G c R Z J V &lt; / l a t e x i t &gt; Context-Aware Network. (Top) RGB images are fed to a font-end network that comprises the first 10 layers of the VGG-16 network. The resulting local features are grouped in blocks of different sizes by average pooling followed by a 1×1 convolutional layer. They are then up-sampled back to the original feature size to form the contrast features. Contrast features are further used to learn the weights for the scale-aware features that are then fed to a back-end network to produce the final density map. (Bottom) As shown in this expanded version of the first part of the network, the contrast features are the difference between local features and context features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Context and saliency. People's gaze tends be drawn to the person in the center, probably because most the image edges point in that direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 4</head><label>1</label><figDesc>the size of the original image at different locations. These patches are further mirrored to double the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Expanded Context-Aware Network. To account for camera registration information when available, we add a branch to the architecture of Fig. 1. It takes as input a perspective map that encodes local scale. Its output is concatenated to the original contrast features and the resulting scale-aware features are used to estimate people density.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Crowd density estimation on ShanghaiTech. First row: Image from Part A. Second row: Image from Part B. Our model adjusts to rapid scale changes and delivers density maps that are close to the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Calibration in Venice and WorldExpo'10. (Top row) Images of Piazza San Marco taken from different viewpoints. (Middle row) We used the regular ground patterns to accurately register the cameras in each frame. The red ellipse overlaid in red is the projection of a 1m radius circle from the ground plane to the image plane. (Bottom row) The same 1m radius circle overlaid on three WorldExpo'10 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Image-plane density vs ground-plane density. (a) The two purple boxes highlight patches in which the crowd density per square meter is similar in the top image. (b) Ground-truth image density obtained by averaging the head annotations in the image plane as is done in all the approaches discussed in this paper, including ours. The bottom two patches are expanded versions of the same two purple boxes. The density appears much larger in one than in the other due to perspective distortion that increases the image density further away from the camera. (c) The density estimation returned by OURS-ECAN. (d) The ground-truth density normalized for image-scale variations so that it can be interpreted as a density per square meter. (e) The OURS-ECAN density similarly normalized. Note that the estimated densities in the two small windows now fall in the same range of values, which is correct.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Network architecture of proposed model Convolutional layers are represented as "(kernel size) × (kernel size) ×</figDesc><table><row><cell>(number of filters) conv-(dilation rate)".</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparative results on the ShanghaiTech dataset.</figDesc><table><row><cell>WorldExpo'10 [41]. It comprises 1,132 annotated video</cell></row><row><cell>sequences collected from 103 different scenes. There are</cell></row><row><cell>3,980 annotated frames, with 3,380 of them used for train-</cell></row><row><cell>ing purposes. Each scene contains a Region Of Interest</cell></row><row><cell>(ROI) in which people are counted. The bottom row of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparative results on the UCF QNRF dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparative results on the UCF CC 50 dataset.</figDesc><table><row><cell>Model</cell><cell cols="6">Scene1 Scene2 Scene3 Scene4 Scene5 Average</cell></row><row><cell cols="2">Zhang et al. [41] 9.8</cell><cell cols="3">14.1 14.3 22.2</cell><cell>3.7</cell><cell>12.9</cell></row><row><cell>MCNN [42]</cell><cell>3.4</cell><cell cols="3">20.6 12.9 13.0</cell><cell>8.1</cell><cell>11.6</cell></row><row><cell cols="2">Switch-CNN [31] 4.4</cell><cell cols="3">15.7 10.0 11.0</cell><cell>5.9</cell><cell>9.4</cell></row><row><cell>CP-CNN [36]</cell><cell>2.9</cell><cell cols="3">14.7 10.5 10.4</cell><cell>5.8</cell><cell>8.9</cell></row><row><cell>ACSCP [32]</cell><cell cols="3">2.8 14.05 9.6</cell><cell>8.1</cell><cell>2.9</cell><cell>7.5</cell></row><row><cell>IG-CNN [30]</cell><cell>2.6</cell><cell cols="3">16.1 10.15 20.2</cell><cell>7.6</cell><cell>11.3</cell></row><row><cell>ic-CNN[28]</cell><cell cols="2">17.0 12.3</cell><cell>9.2</cell><cell>8.1</cell><cell>4.7</cell><cell>10.3</cell></row><row><cell>D-ConvNet [33]</cell><cell>1.9</cell><cell cols="2">12.1 20.7</cell><cell>8.3</cell><cell>2.6</cell><cell>9.1</cell></row><row><cell>CSRNet [19]</cell><cell>2.9</cell><cell>11.5</cell><cell>8.6</cell><cell>16.6</cell><cell>3.4</cell><cell>8.6</cell></row><row><cell>SANet [5]</cell><cell>2.6</cell><cell>13.2</cell><cell>9.0</cell><cell>13.3</cell><cell>3.0</cell><cell>8.2</cell></row><row><cell>DecideNet [21]</cell><cell cols="3">2.0 13.14 8.9</cell><cell cols="3">17.4 4.75 9.23</cell></row><row><cell>OURS-CAN</cell><cell>2.9</cell><cell cols="2">12.0 10.0</cell><cell>7.9</cell><cell>4.3</cell><cell>7.4</cell></row><row><cell>OURS-ECAN</cell><cell>2.4</cell><cell>9.4</cell><cell>8.8</cell><cell>11.2</cell><cell>4.0</cell><cell>7.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparative results in MAE terms on the World-Expo'10 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Density estimation in Venice. Original image, ROI, ground truth density map within the ROI, and density maps estimated both by the baselines and our method. Note how much more similar the density map produced by OURS-ECAN is to the ground truth than the others, especially in the upper corner of the ROI, where people density is high.</figDesc><table><row><cell cols="2">Original image Region of interest Ground truth</cell><cell>MCNN [42]</cell></row><row><cell>Switch-CNN [31] CSRNet [19]</cell><cell>OURS-CAN</cell><cell>OURS-ECAN</cell></row><row><cell>Figure 6:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparative results on the Venice dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">M AE RM SE</cell></row><row><cell>VGG-SIMPLE</cell><cell>68.0</cell><cell>113.4</cell></row><row><cell>VGG-CONCAT</cell><cell>63.4</cell><cell>108.7</cell></row><row><cell>VGG-NCONT</cell><cell>63.1</cell><cell>106.4</cell></row><row><cell>OURS-CAN</cell><cell>62.3</cell><cell>100.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on the ShanghaiTech part A dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/view/weizheliu/home/ projects/context-aware-crowd-counting</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was supported in part by the Swiss Federal Office for Defense Procurement.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive Object Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Alison</forename><surname>Victor Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Counting in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<title level="m">Segnet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Bayesian Detection of Independent Motion in Crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="594" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scale Aggregation Network for Accurate and Efficient Crowd Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Privacy Preserving Crowd Monitoring: Counting People Without People Models or Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Sheng John</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian Poisson Regression for Crowd Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Counting Everyday Objects in Everyday Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Selvaju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature Mining for Localised Crowd Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Count with Regression Forest and Structured Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Fiaschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>Koethe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2685" to="2688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-Source Multi-Scale Counting in Extremely Dense Crowd Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhmmad</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishan</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somaya</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crowd Counting by Adaptively Fusing Predictions from an Image Pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Incorporating Side Information by Adaptive Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debarun</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to Count Objects in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shape-Based Human Detection and Segmentation via Hierarchical Part-Template Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="604" to="618" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decidenet: Counting Varying Density Crowds through Attention Guided Detection and Density Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann1</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Crowd Counting Using Deep Recurrent Spatial-Aware Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SSD: Single Shot Multibox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Leveraging Unlabeled Data for Crowd Counting by Learning to Rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards Perspective-Free Object Counting with Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Onoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Counting Crowded Moving Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="705" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Iterative Crowd Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Divide and Grow: Capturing Huge Diversity in Crowd Images with Incrementally Growing CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><forename type="middle">N</forename><surname>Sajjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukundhan</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Switching Convolutional Neural Network for Crowd Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Crowd Counting via Adversarial Cross-Scale Consistency Pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crowd Counting with Deep Negative Correlation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CNN-based Cascaded Multi-task Learning of High-level Prior and Density Estimation for Crowd Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generating High-Quality Crowd Density Maps Using Contextual Pyramid CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1879" to="1888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Airport Detection in Remote Sensing Images Based on Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detection of Multiple, Partially Occluded Humans in a Single Image by Bayesian Combination of Edgelet Part Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatiotemporal Modeling for Crowd Counting in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5161" to="5169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-Scene Crowd Counting via Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Single-Image Crowd Counting via Multi-Column Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
