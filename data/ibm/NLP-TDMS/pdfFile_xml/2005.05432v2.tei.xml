<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Target-Independent Domain Adaptation for WBC Classification using Generative Latent Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Pandey</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathosh</forename><surname>Ap</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Kyatham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Mishra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tathagato</forename><forename type="middle">Rai</forename><surname>Dastidar</surname></persName>
						</author>
						<title level="a" type="main">Target-Independent Domain Adaptation for WBC Classification using Generative Latent Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-WBC</term>
					<term>Microscopic imaging</term>
					<term>Unsupervised do- main adaptation</term>
					<term>Generative models</term>
					<term>VAE</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automating the classification of camera-obtained microscopic images of White Blood Cells (WBCs) and related cell subtypes has assumed importance since it aids the laborious manual process of review and diagnosis. Several State-Of-The-Art (SOTA) methods developed using Deep Convolutional Neural Networks suffer from the problem of domain shift -severe performance degradation when they are tested on data (target) obtained in a setting different from that of the training (source). The change in the target data might be caused by factors such as differences in camera/microscope types, lenses, lightingconditions etc. This problem can potentially be solved using Unsupervised Domain Adaptation (UDA) techniques albeit standard algorithms presuppose the existence of a sufficient amount of unlabelled target data which is not always the case with medical images. In this paper, we propose a method for UDA that is devoid of the need for target data. Given a test image from the target data, we obtain its 'closest-clone' from the source data that is used as a proxy in the classifier. We prove the existence of such a clone given that infinite number of data points can be sampled from the source distribution. We propose a method in which a latent-variable generative model based on variational inference is used to simultaneously sample and find the 'closest-clone' from the source distribution through an optimization procedure in the latent space. We demonstrate the efficacy of the proposed method over several SOTA UDA methods for WBC classification on datasets captured using different imaging modalities under multiple settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Automating the classification of camera-obtained microscopic images of White Blood Cells (WBCs) and related cell subtypes has assumed importance since it aids the laborious manual process of review and diagnosis. Several State-Of-The-Art (SOTA) methods developed using Deep Convolutional Neural Networks suffer from the problem of domain shift -severe performance degradation when they are tested on data (target) obtained in a setting different from that of the training (source). The change in the target data might be caused by factors such as differences in camera/microscope types, lenses, lightingconditions etc. This problem can potentially be solved using Unsupervised Domain Adaptation (UDA) techniques albeit standard algorithms presuppose the existence of a sufficient amount of unlabelled target data which is not always the case with medical images. In this paper, we propose a method for UDA that is devoid of the need for target data. Given a test image from the target data, we obtain its 'closest-clone' from the source data that is used as a proxy in the classifier. We prove the existence of such a clone given that infinite number of data points can be sampled from the source distribution. We propose a method in which a latent-variable generative model based on variational inference is used to simultaneously sample and find the 'closest-clone' from the source distribution through an optimization procedure in the latent space. We demonstrate the efficacy of the proposed method over several SOTA UDA methods for WBC classification on datasets captured using different imaging modalities under multiple settings.</p><p>Index Terms-WBC, Microscopic imaging, Unsupervised domain adaptation, Generative models, VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A. Background M ICROSCOPIC review of Peripheral Blood Smear (PBS) slides by clinical pathologists is considered as the gold standard for detection of various disorders <ref type="bibr" target="#b0">[1]</ref>. This requires manual counting and classification of various types of cells, including White Blood Cells (WBCs or leukocytes) and analysing their morphological characteristics in PBS slides. The presence, absence, or relative counts of these cells help in the diagnosis of several types of diseases, including different Copyright (c) 2019 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org. <ref type="bibr">Prashant</ref>  forms of blood cancer, anaemia, and presence of parasites like in malaria. This process of manual review is both laborious and error prone. In addition, due to variations in stain, smearing process, the differentiation between various subclasses of cells is often blurry. It takes significant expertise and experience to correctly classify all types of cells. Lack of qualified medical professionals, especially in non-urban areas of developing countries, accentuates the problem. Furthermore, the misdiagnosis, often caused by lack of adequate time to examine a slide thoroughly, can even lead to fatalities. Thus, automating and standardising this process is a pressing need.</p><p>Several attempts have been made to automate some of these manual processes using methods ranging from classical computer vision <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> to image cytometry <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. While classical vision techniques suffer from issues like poor-generalization, image cytometry is limited by its operational speed and inability to engineer complex features <ref type="bibr" target="#b6">[7]</ref>. An alternative is to harness the power of Deep Convolutional Neural Networks (CNNs) in addressing some of these issues <ref type="bibr" target="#b7">[8]</ref>. In SC-CNN <ref type="bibr" target="#b8">[9]</ref>, a weighted sum of multiple classifiers is used to predict the class label of cell nuclei detected with a Spatially Constrained CNN. In <ref type="bibr" target="#b9">[10]</ref>, a Conditional Generative Adversarial Network (cGAN) <ref type="bibr" target="#b10">[11]</ref> is used for nuclei segmentation, a fundamental task for cell classification. MGCNN <ref type="bibr" target="#b11">[12]</ref> is a White Blood Cells classification framework that combines modulated Gabor wavelet <ref type="bibr" target="#b12">[13]</ref> and deep CNN kernels. A few commercial products too have been built utilizing some of these techniques. CellaVision <ref type="bibr" target="#b13">[14]</ref>, Shonit <ref type="bibr" target="#b14">[15]</ref>, etc., automate the counting and classification of leukocytes and other blood cells. These systems consist of an automated microscope equipped with a digital camera, which captures the images of a biological sample on a glass slide. A software based analysis system, built using CNN models, is then used to localise and classify different types of cells in the sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motivation and Problem setting</head><p>Even though the aforementioned models and systems are effective in their own ways, they suffer from certain issues that may limit their utility. For instance, Deep CNN models used for microscopic image classification are typically trained using proprietary datasets. These datasets tend to be homogeneous in terms of the capture device -microscopes, lens and cameras used. This homogeneity and limited number of images in the training dataset cause the models trained on them to over-fit on specific characteristics of the image capturing device. As a result, when images captured with a different device or camera are presented to these models, they often wrongly classify new images, even though the trained human observers will have no difficulty in classification (images shown in <ref type="figure">Figure 3</ref>). Hence, as the image capturing device changes, these models fail to adapt to the new input data distribution. This is known as the domain shift problem. Domain shift also occurs when the underlying imaging modality itself changes. For instance, a deep learning model trained on Flow Cytometry images <ref type="bibr" target="#b5">[6]</ref> will not readily generalize for microscopic PBS images even though both capture WBCs. The problem of domain shift exists not only for medical images, but for any deep learning system trained with single image source <ref type="bibr" target="#b15">[16]</ref>.</p><p>A natural solution to this problem is to (re)-train the model with large amount of data obtained from the new device. However, generating sufficient quantity of annotated medical data is a time consuming and costly process. In addition, bottlenecks such as regulatory clearances, cause a large development cycle and delay in building such systems. We consider one such problem in this paper, where performance of CNNs trained on a dataset from a single source camera for automatic classification of images of WBCs taken from PBS, degrade when tested on unseen target dataset collected from different cameras. This falls within the ambit of a well-known computer vision problem known as Unsupervised Domain Adaptation (UDA). However, almost all the SOTA methods on UDA <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> need access to the unlabelled target data during the time of training. While it may be feasible to obtain unlabelled target data, retraining of the UDA model for every newly emerging target domain might be infeasible, post their deployment in the field. Therefore, an unsupervised domain adaptation method that can operate without target data is desirable <ref type="bibr" target="#b18">[19]</ref>. Motivated by these observations, in this paper we propose a UDA technique for WBC classification with following core contributions: 1) We propose a UDA technique that does not require access to the target data during the time of training. 2) We cast the problem of UDA as finding the 'closestclone' in the source domain for a given target image that is used as a proxy for the target image in the classifier trained on the source data. 3) We theoretically prove the existence of the 'closestclone' given that infinite data points can be sampled from the source distribution. 4) We propose an optimization method over the latent space of variational inference based Deep generative model, to find the aforementioned clone through implicit sampling. 5) We demonstrate through extensive experimentation, the efficacy of the proposed method over several stateof-the-art UDA techniques for WBC classification on several datasets obtained using different imaging modalities with multiple domain shifts. We also validate our algorithm on the standard datasets used for UDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Unsupervised Domain Adaptation (UDA) refers to the design of techniques aimed at improving the performance of machine learning tasks such as classification and segmentation when the classifier is trained using labels only from a source domain and tested on data from related but a shifted target domain. In this section, we present a review of the state-ofthe-art UDA techniques based on their principle of operation and their use in the medical imaging community.</p><p>1) Adversarial-learning: These methods <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> learn domain-invariant representations using the principles of adversarial learning. ADDA <ref type="bibr" target="#b15">[16]</ref> employs a source network, pre-trained with labeled source data. Adversarial adaptation is performed by learning a target network such that a domain discriminator fails to predict the domain labels of the source and target features. During inference, the target images are mapped to the shared feature space by using the target network which are predicted by the source classifier. Generate To Adapt (GTA) <ref type="bibr" target="#b17">[18]</ref> learns domain invariant embeddings using a joint generative-discriminative set-up. During training, a feature extraction network outputs embeddings that are used by label prediction network for classification with a Generative Adversarial Network (GAN) framework to generate realistic source images. DIRT-T [20] employs a Virtual Adversarial Domain Adaptation (VADA) model that pushes the decision boundaries away from regions of high data density by penalizing violation of the cluster assumption in the target domain. Transferable Adversarial Training (TAT) <ref type="bibr" target="#b20">[21]</ref> generates transferable examples to fill in the gap between the source and target domains without distorting feature distributions. Domain Agnostic Learning (DAL) <ref type="bibr" target="#b21">[22]</ref> uses Deep Adversarial Disentangled Auto-Encoders (DADA) to disentangle domaininvariant features in the latent space by minimizing the mutual information between domain-invariant and domain-specific features. The principles of adversarial feature learning has been used in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> to transform real images to a syntheticlike representation using unlabeled synthetic endoscopy images and achieve stain independence. In <ref type="bibr" target="#b24">[25]</ref>, a siamese architecture with adversarial training is used to improve the classification performance of target prostate histopathology whole-slide images. Zhang et al. <ref type="bibr" target="#b25">[26]</ref> used adversarial learning for a noise adaptation task that allows a trained model to work effectively for medical images with different noise patterns.</p><p>2) Target Reconstruction: These approaches for UDA reconstructs source or target samples as an auxiliary task that simultaneously focuses on creating a shared representation between the two domains while keeping the individual characteristics of each domain intact. CyCADA <ref type="bibr" target="#b26">[27]</ref> adapts between domains by aligning both generative and latent space representations, with cycle and semantic consistency loss. PixelDA <ref type="bibr" target="#b27">[28]</ref> learns transformation in the pixel space from one domain to the other using task-specific and content-similarity losses. SBADA-GAN <ref type="bibr" target="#b28">[29]</ref> maps source samples into the target domain and vice versa by imposing a class consistency loss to improve the quality of reconstructed images. I2I Adapt <ref type="bibr" target="#b29">[30]</ref> is a framework that learns from the source domain and adapt to the target domain by extraction of domain agnostic features, domain specific reconstruction with cycle consistency losses. Tulder et al. <ref type="bibr" target="#b30">[31]</ref> proposed a representation learning method that transforms data from different sources to a shared feature representation using per-feature normalization, a crossmodality based objective function. Goetz et al. <ref type="bibr" target="#b31">[32]</ref> used domain adaptation to correct the sampling bias introduced with sparsely labeled MR images for tissue classification.</p><p>3) Divergence Minimization: In these methods, source and target distributions are aligned by minimizing a divergence measure between the two distributions. Joint Adaptation Networks (JAN) <ref type="bibr" target="#b32">[33]</ref> learns a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a Joint Maximum Mean Discrepancy (JMMD) criterion. Maximum Classifier Discrepancy (MCD) <ref type="bibr" target="#b33">[34]</ref> aligns distributions of source and target by utilizing the task-specific decision boundaries. Task-specific classifiers are trained to detect the target samples that are far from the support of the source. Contrastive Adaptation Network (CAN) <ref type="bibr" target="#b34">[35]</ref> estimates the underlying label hypothesis of target samples through clustering and adapts the feature representations according to the Contrastive Domain Discrepancy (CDD) metric. Pacheco et al. <ref type="bibr" target="#b35">[36]</ref> addressed the discrepancies related to the stem cell differentiation process by minimizing a Maximum Mean Discrepancy (MMD) based loss function in a Recurrent Neural Network (RNN) classifier. 4) Domain Randomization: Domain Randomization [37] (DR) is another class of methods related to UDA that are used to improve the generalization of classifiers. The idea is to reduce the domain shift by randomizing properties in the training environment (like source domain). Every data point in the source domain is perturbed randomly during training while assigning the same ground truth to the perturbed samples. In methods such as <ref type="bibr" target="#b37">[38]</ref>, cinematically rendered source domain images are varied in color and texture. For RGB images, such transformations can be obtained by varying hue, saturation, contrast and brightness. In <ref type="bibr" target="#b38">[39]</ref>, source images intensity is divided into multiple non-overlapping ranges. A random perturbation is added to the start/end pixel values by sampling from a Gaussian distribution. Finally, one of the following randomisation is applied to each range, a) shift the intensity values by adding a random value from a uniform distribution or b) transform the intensity values using cumulative distribution function of beta distribution or c) simply invert the intensity range. <ref type="bibr" target="#b39">[40]</ref> varies source images background color, add uniform noise, change the illumination and distort source images with different scaling factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motivation</head><p>All the UDA methods mentioned in the previous section assumes that one has access to images from the target distribution. These images are either used to retrain the original classifier in a domain-invariant way <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> or to align the target distribution to the source distribution <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. Also, in most of the methods <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b34">35]</ref>, the original classifier trained on the source data is altered, so that a new decision boundary is learned using the images from the target data in an unsupervised manner. However, in many practical situations, such as the current one, there would neither be access to the target data nor the scope to retrain the classifier. Further, a new unseen target domain may arise in the field which was not used during adaptation.</p><p>We propose to address these issues in this paper by first assuming that the classifier learned on the source data (Oracle classifier) will perform well as long as the data comes from the source distribution. Subsequently, (i) we learn to sample from the source distribution and (ii) given an image from the target distribution, we find an image from the source distribution that is arbitrarily close ('closest-clone') to the given target image, under some distance metric. Finally, the target image is replaced with its 'closest-clone' from the source distribution before its class is inferred by the Oracle classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Existence of closest source 'clone'</head><p>To begin with, we prove that given an image from the target distribution, there exists an arbitrarily close image in the source distribution (named as 'closest-clone'), provided infinite data can be sampled from the source distribution <ref type="bibr" target="#b40">[41]</ref>.</p><p>Let P s (x) and P t (x) denote the source and the target distributions, respectively. We assume that the the underlying random variable on which P s and P t are defined, forms a separable metric space</p><formula xml:id="formula_0">{X, D} where D is some distance metric. Let S n = {x 1 , x 2 , x 3 , ...., x n } be i.i.d. points drawn from P s (x) andx T be any point drawn from P t (x).</formula><p>The following lemma asserts that as n → ∞, there exists a point in S n that it arbitrarily close tox T , with probability one.</p><formula xml:id="formula_1">Lemma 1. Ifx S ∈ S n is the point such that D{x T ,x S } &lt; D{x T , x} ∀ x ∈ S n ,</formula><p>then as n → ∞,x S converges tox T with probability 1 (Refer supplementary material for proof).</p><p>Lemma 1 guarantees that given an image from the target distribution, an image from the source distribution, that is arbitrarily close to the given target image can be found out given the following requirements are met:</p><p>• Given a few images from the source distribution P s , one can sample infinite images from it. • Given infinite samples from P s , it is possible to find the 'closest-clone' (under D) in P s , to the target imagex T . To satisfy the above requirements, in subsequent sections, we employ variational inference based sampling methods on the source distribution with which one can implicitly sample and find the 'closest-clone' simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Variational inference for source sampling</head><p>In variational inference based generative models <ref type="bibr" target="#b41">[42]</ref>, it is assumed that the data or the observed variable (in this case images from P s ) is generated via a two step process: (i) sample from the distribution P θ (z) of an unobserved or latent variable z, (ii) given a data point from the latent variable, sample from the conditional distribution P θ (x|z) to obtain the data. Owing to the fact that the parameters of the true latent prior P θ (z) and data conditional P θ (x|z) are unknown, and the posterior P θ (z|x) is intractable, a variational distribution, Q φ (z|x) is used to approximate the true posterior. With this, it can be shown that the log-likelihood of the observed data will decompose into two terms (Eq. 1), an irreducible nonnegative KL-divergence between P θ (z|x) and Q φ (z|x) and the Evidence Lower Bound (ELBO) given by Eq. 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge Operator</head><formula xml:id="formula_2">E 3 1 2 8 128 x Pθ(x|z) Qφ(z|x) L p E Qφ(z|x) [ln (P θ (x|z))] ≈ L r = x −x 2 2 L r Pψ perceptual features l th layer DKL[Qφ(z|x)||Pθ(z)]</formula><formula xml:id="formula_3">ln P θ (x) = L(θ, φ) + D KL [Q φ (z|x)||P θ (z|x)]<label>(1)</label></formula><p>Here, L(θ, φ) represents ELBO which is given by,</p><formula xml:id="formula_4">L(θ, φ) = E Q φ (z|x) [ln (P θ (x|z))] − D KL [Q φ (z|x)||P θ (z)]</formula><p>(2) In Eq. 1, the KL-term is irreducible and non-negative and thus, L(θ, φ) serves as a lower bound on the data loglikelihood which is optimized. In deep generative model frameworks, Q φ (z|x) and P θ (x|z) are parameterized using probabilistic encoder g φ (that outputs the parameters µ z and σ z of a distribution) and decoder h θ neural networks with parameters φ and θ respectively, that maps the data space into latent space and vice-versa. Additionally, P θ (z) is taken to be an arbitrary prior on z which is usually a 0 mean and unit variance Gaussian distribution. The first term in Eq. 2 is approximated using a norm-based divergence metric between the input and the output of the decoder as below:</p><formula xml:id="formula_5">E Q φ (z|x) [ln (P θ (x|z))] ≈ L r = x −x 2 2<label>(3)</label></formula><p>Note that Eq. 3 can be seen as 'reconstruction' or 'Auto-Encoding' of the data. Further, the second term in ELBO employs a variational approximation to the true posterior P θ (z|x). Thus, the aforementioned method is famously referred to as the Variational Auto-Encoder (VAE) <ref type="bibr" target="#b41">[42]</ref>. For the current problem of interest, a VAE is trained using the images from the source distribution S n and once trained, the decoder network serves as a sampler for the source distribution using a two step process: (i) sample z ∼ N (0, I), (ii) sample x as the output of the decoder h θ .</p><p>VAEs are know to produce blurred images in their conventional formulation with norm-based losses. To address this, we use the edge information (extracted using standard edge detectors) of the input image by passing it to the decoder via a skip connection, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Rationale behind this is that unlike features such as colour and contrast, edges are in general invariant to the changes in camera characteristics. Edge information reduces the blurring due to the decoder as shown in <ref type="figure" target="#fig_6">Figure 9</ref> and ablation studies in <ref type="table" target="#tab_2">Table VI</ref>.</p><p>Further, we also incorporate the perceptual loss, which is known to enhance the generation quality of VAEs, along with the standard norm-based losses. Perceptual loss L p between two images x andx is defined as the Euclidean distance between the representations or the features obtained under a pre-trained classifier model P ψ . Mathematically,</p><formula xml:id="formula_6">L p = P ψ (x) − P ψ (x) 2 2 (4)</formula><p>The idea behind L p is that the distance metrics in a representational space learned by a classifier model trained on large scale data are better than on raw image space. This is shown to enhance image quality in several applications <ref type="bibr" target="#b42">[43]</ref>. <ref type="figure" target="#fig_0">Figure  1</ref> depicts the network diagram of the VAE on the source data with the proposed edge concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Finding 'closest-clone' through Latent Search</head><p>As mentioned in the previous sections, the objective is to simultaneously sample and search for the 'closest-clone' in the source distribution, given a sample from target distribution. Suppose a VAE has been trained on the source distribution P s (x), the decoder h θ of which outputs a 'de-novo' image from P s (x) by taking a normally distributed latent variable as input. That is,</p><formula xml:id="formula_7">∀ z ∼ N (0, I),x = h θ (z) ∼ P s (x)<label>(5)</label></formula><p>Our goal is to find the 'closest-clone' under some distance metric D, for any given image from the target distribution. Mathematically, given ax T ∼ P t (x), findx S as follows:</p><formula xml:id="formula_8">x S = h θ (z S ) : D{x T ,x S } &lt; D{x,x T } ∀ x = h θ (z) ∼ P s (x) (6)</formula><p>Since D is computable and h θ is a neural network that outputs a sample from P s (x) as a function of the latent variable z, findingx S (Eq. 7) can be cast an optimization problem over z with minimization of D as the objective:</p><formula xml:id="formula_9">z S = argmin z D x T , h θ (z) (7) x S = h θ (z S )<label>(8)</label></formula><p>The optimization problem is Eq. 7 can be solved using gradient descent based techniques on the decoder network h θ * θ * are the parameters of the decoder network trained only on the source images S n with respect to z. This implies that given any input image, the optimization problem in Eq. 7 will be solved to find its 'closest-clone' in the source distribution which is used as a proxy in the original classifier trained only on S n . We call the iterative procedure of findingx S through optimization using h θ * as the Latent Search (LS). Finally, inspired by the observations made in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, we propose to use Structural Similarity Index (SSIM) loss for D to conduct the Latent Search. Unlike norm-based losses, SSIM loss helps in preservation of structural information as compared to discrete pixel level information. SSIM is defined in <ref type="bibr" target="#b45">[46]</ref> using the three aspects of similarities, luminance l(x,x) , contrast c(x,x) and structure s(x,x) that are measured for a pair of images {x,x} as follows:</p><formula xml:id="formula_10">l(x,x) = 2µ x µx + C 1 µ 2 x + µ 2 x + C 1 (9) c(x,x) = 2σ x σx + C 2 σ x 2 + σx 2 + C 2 (10) s(x,x) = σ xx + C 3 σ x σx + C 3<label>(11)</label></formula><p>where µ's denote sample means and σ's denote variances. C 1 , C 2 and C 3 are constants as defined in <ref type="bibr" target="#b45">[46]</ref>. With these, SSIM and the corresponding loss function L ssim , for a pair of images {x,x} are defined as:</p><formula xml:id="formula_11">SSIM(x,x) = l(x,x) α · c(x,x) β · s(x,x) γ<label>(12)</label></formula><p>where α &gt; 0, β &gt; 0 and γ &gt; 0 are parameters used to adjust the relative importance of the three components.</p><formula xml:id="formula_12">L ssim (x,x) = 1 − SSIM(x,x)<label>(13)</label></formula><p>Since our method does not utilize target images and employs generative Latent Search, we call our method Target-Independent Generative Domain Adaptation (TIGDA). The target independence of our method refers to the fact that we do not use target data during training, unlike SOTA UDA methods. The inference for TIGDA is shown in <ref type="figure" target="#fig_8">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION DETAILS A. Training of the VAE</head><p>The Encoder g φ and Decoder h θ network architectures for the VAE are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We use Sobel Edge operator for Edge concatenation. Edges of the input image are concatenated with the output of tanh nonlinearity as shown in  <ref type="figure" target="#fig_8">Figure 2</ref>. Latent Search procedure during inference with TIGDA. The latent vector z is initialized with a random sample drawn from N (0, 1). Iterations over the latent space z are performed to minimize the Structural Similarity loss L ssim between the input target imagex T and the predicted target imagex, which is the output of the trained decoder (blue dotted lines). After convergence of L ssim loss, the optimal latent vectorz S , generates the 'closest-clone'x S which is used to predict the class ofx T using the classifier C ψ trained on source samples. <ref type="figure" target="#fig_0">Figure 1</ref>. The VAE is trained using (i) the Mean squared error reconstruction loss L r between the real and VAE reconstructed images and (ii) the perceptual loss L p for which the features are taken from the l th layer of the VGG-16 (10 th layer) or RestNet-50 (38 th layer) classifier trained on source images for WBC classification task. The hidden layers of Encoder and Decoder networks use Leaky ReLU and tanh as activation functions with the dimensionality of the latent space being 64. VAE is trained using a standard gradient descent procedure with RMSprop optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inference through Latent Search</head><p>Once the VAE is trained, given an imagex T from the target distribution, the Latent Search algorithm searches for an optimal latent vectorz S that generates its 'closest-clone' x S from P S . The search is performed by minimizing the SSIM loss L ssim between the input target imagex T and VAE reconstructed target image. The latent vector is optimized using a gradient-based optimization procedure, performed for K (a hyper-parameter) iterations over the latent space of the VAE for every target image. The gradient based optimization is implemented with Nesterov Accelerated Gradient method with a momentum of 0.5. Finally, the class for the input target image is assigned the same as the one given by the source classifier C ψ onx S . C ψ is a VGG-16 or RestNet-50 classifier trained on source images. Note that our algorithm solves an optimization problem before predicting class for every input target image. However, since it involves only a forward-pass through a trained neural network (decoder h θ * ), the time taken is only of the order of few milliseconds on standard CPUs. The complete algorithmic steps and the architectural details for TIGDA are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DATASET DETAILS</head><p>The datasets used in this study will be described in this section. Peripheral blood smear (PBS) consists primarily of three cell types -RBC (Red Blood Cell or erythrocyte), WBC  (White Blood Cell or leukocyte) and platelet (or thrombocyte). Each of these primary classes have subclasses. The subclasses of WBCs are: neutrophil, lymphocyte, monocyte, eosinophil, basophil, immature granulocytes and atypical/blast cells. Apart from these, there are other types of cells and artefacts which can have appearance similar to leukocytes. These are -nucleated red blood cell (NRBC), large platelets, platelet clumps, and stain artefacts <ref type="bibr" target="#b0">[1]</ref>. In the current study, we consider classification of 11 categories of which seven are subtypes of WBCs and rest four are NRBC, large platelets, platelet clumps, and stain artefacts (images shown in <ref type="figure">Figure 3</ref>). Data used in our experiments comprises images from the PBS slides processed after complete de-identification to remove all the patient information, including age and gender. These were collected from two large clinical laboratories in Bangalore, India. The internal ethics committee of the respective laboratories approved the study. The samples were collected retrospectively without prospective patient recruitment.</p><p>The hardware consists of the following components, (a) Optical system: Consists of an optical tube (40X or 100X Plan Achromat objective and 10X eyepiece) and Abbe Condenser with white LED source, (b) Camera: The system is built such that either a mobile phone or a USB camera can be fitted on top of the eyepiece with a 3D printed attachment, aligning the optical axis of the tube/eyepiece with the camera, (c) Hardware control: A small PCB designed to receive USB commands and drive motors and LED, (d) XYZ slide stage: The XYZ platform is built using commercially available low-cost ball screws and stepper motors, along with some machined parts <ref type="bibr" target="#b46">[47]</ref>. The images used in this work are captured through 3 different cameras -One cell phone make (iPhone 6s) and two brands of USB camera (from e-con systems [48] and das-Cam [49]). All cameras had resolution of at least 13MP with varying hardware and optical designs that induce the domain shift. For example, econ camera has an AR1335 CMOS image sensor and lens with 1/3.2" optical format while das-Cam contains an OV13850 CMOS sensor with a lens of 1/3.06" form factor.</p><p>Images are collected only from the 'monolayer' region of the slides -where the red blood cells are just touching each other. This is the area of the slide which is typically used for manual analysis <ref type="bibr" target="#b0">[1]</ref>. Slides prepared using varied staining types were used. The images are of size approximately 13MP, with a spatial resolution of around 5.5 pixels per micron. WBC and other similar looking cells (as described above) are localised in these images using a U-Net <ref type="bibr" target="#b47">[50]</ref> based technique described in <ref type="bibr" target="#b14">[15]</ref>. Each sample slides can potentially yield hundreds of unique WBC candidates. For annotation, we cropped 128 × 128 area around the WBCs identified by the extraction model. These cells are then presented to three different certified medical professionals for annotating into different subtypes, using an in-house web based annotation tool. There is usually a high degree (as high as 20%) of interobserver variability in the data annotation process. Therefore, we use only those images where at least 2 out of 3 clinical pathologists agree on the class while the rest of the images are rejected. <ref type="table" target="#tab_2">Table I</ref> describes the summary of the datasets named as A, B and C corresponding to three cameras used.  <ref type="table" target="#tab_2">UDA TASKS ON WBC AND RELATED MICROSCOPIC IMAGES CAPTURED WITH THREE DIFFERENT CAMERAS  A, B AND C. X→Y INDICATES MODEL TRAINED ON IMAGES FROM SOURCE CAMERA X AND TESTED ON IMAGES FROM TARGET CAMERA Y. RESULTS  ARE REPORTED AS AN AVERAGE OVER FIVE INDEPENDENT RUNS USING VARIOUS STATE-OF-THE-ART UDA AND DOMAIN RANDOMIZATION METHODS.  NOTE THAT WHILE ALL UDA METHODS PERFORM BETTER THAN THE SOURCE ONLY MODEL, TIGDA OFFERS THE BEST PERFORMANCE DESPITE NOT   USING THE TARGET IMAGES</ref>    <ref type="bibr" target="#b15">[16]</ref>, GTA <ref type="bibr" target="#b17">[18]</ref>, DIRT-T <ref type="bibr" target="#b19">[20]</ref>, TAT <ref type="bibr" target="#b20">[21]</ref>, DAL <ref type="bibr" target="#b21">[22]</ref> and TIGDA on domain adaptation task A→C. We used different markers and different colors to denote 11 categories. It is seen that TIGDA offers better clustering as compared to the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Benchmarking Experiments</head><p>In the first set of experiments, we benchmark performance of the baseline classifier with the following experiments: (a) Train and test on the same dataset type (A/B/C), (b) Train and test by combining images from all dataset types (A+B+C), (c) Train on one dataset and test on the other (all six combinations) with and without class balancing. The notation X→Y symbolizes training on a dataset X and testing on Y. <ref type="table" target="#tab_2">Table III</ref> lists the results of experiment (a) which establishes an upper bound on the performance and (b) where it is seen that the performance degrades when all images from all three datasets are combined. This is due to the existence of domain shift between the datasets that makes learning difficult even with supervision. Moreover, combining datasets is not possible in the UDA setting where the labels are not known for the target data. Results of experiment (c) are shown in <ref type="table" target="#tab_2">Table IV</ref> where it is seen that the accuracy severely degrades when train and test sets are from different domains despite inducing an artificial class balance. The goal of UDA techniques is to improve the accuracies reported in <ref type="table" target="#tab_2">Table IV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Experiments</head><p>The first set of task is of classification across 11 classes with classifiers trained on one (source) dataset and tested on another  (target) dataset. We report average classification accuracies with standard-deviation (averaged over five independent runs) with two backbone architectures for the source classifier: ResNet-50 and VGG-16. For all the UDA tasks, the VAE is trained with the entire source data and tested on the entire target data. <ref type="table" target="#tab_2">Table II</ref> compares the performance of TIGDA with 12 SOTA UDA baselines, along with the accuracy without any UDA (called Source Only). It is seen that although all the UDA methods improve upon the Source Only performance, TIGDA offers the best performance despite not using any <ref type="figure">Figure 5</ref>. A-Distance (lower is better) of JAN <ref type="bibr" target="#b32">[33]</ref>, MCD <ref type="bibr" target="#b33">[34]</ref>, CAN <ref type="bibr" target="#b34">[35]</ref> and TIGDA.</p><p>data from the target distribution. The confusion matrix for a few methods is given in the <ref type="figure" target="#fig_8">Figure 2</ref> of the Supplementary material. We also compare with three Domain Randomization (DR) techniques, DR1 <ref type="bibr" target="#b37">[38]</ref>, DR2 <ref type="bibr" target="#b38">[39]</ref> and DR3 <ref type="bibr" target="#b39">[40]</ref>. While DR provides performance boost, they have poorer performance as compared to TIGDA. This is because DR methods typically work well when the unseen target is within the scope of the class of random perturbations that are made on the source which is not the case always. In TIGDA on the other hand, every target image is made to resemble the source image through implicit sampling. Since VAE learns to sample from the entire source domain, the domain shift is implicitly reduced during inference without explicitly assuming any form for the shift. It is also observed that the performance of the classifier when trained and tested on single source domain (around 92-95% for all the datasets) do not degrade with TIGDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) t-SNE:</head><p>To further examine our hypothesis, in <ref type="figure" target="#fig_3">Figure 4</ref> we depict the t-SNE <ref type="bibr" target="#b48">[51]</ref> plots of features generated by adversarial based UDA methods (ADDA <ref type="bibr" target="#b15">[16]</ref>, GTA <ref type="bibr" target="#b17">[18]</ref>, DIRT-T <ref type="bibr" target="#b19">[20]</ref>, TAT <ref type="bibr" target="#b20">[21]</ref> and DAL <ref type="bibr" target="#b21">[22]</ref>) for the domain adaptation task A→C. For TIGDA, we plot the embeddings of the latent variablez S obtained through the LS on the target images. It is seen that the representation generated by the LS of TIGDA is more separated compared to those generated by adversarial training based UDA methods. A similar observation is made on the first two principal component plots of the latent representations (Please refer to <ref type="figure" target="#fig_0">Figure 1</ref> in supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) A-Distance:</head><p>To ascertain the closeness of the 'closestclones' obtained through the LS, to the source distribution, we compute the A-distance <ref type="bibr" target="#b49">[52]</ref>, which is a measure of similarity between two probability distributions. Similar feature distributions will have lower A-distance between them as compared to dissimilar feature distributions. A-distance is given byd A = 2(1 − 2 ) where is the generalization error of a linear SVM classifier trained to discriminate between the source and target domains. <ref type="figure">Figure 5</ref> displaysd A for the four domain adaptation tasks with JAN <ref type="bibr" target="#b32">[33]</ref> features, MCD <ref type="bibr" target="#b33">[34]</ref> features and CAN <ref type="bibr" target="#b34">[35]</ref> features, respectively. In our case,d A is measured between the latent vectors (produced by the Encoder of the VAE) of the source images and the latent vectors of the 'closest-clones' for target images obtained from Latent Search. We observe thatd A is smallest in our case as compared to other methods for all the tasks. This implies that the features obtained using TIGDA are transferable between the source and target domains, aiding better adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera B Camera A PixelDA I2IAdapt</head><p>CyCADA SBADA-GAN TIGDA real image <ref type="figure">Figure 6</ref>. Translation of images from one domain (Camera B) to other (Camera A) using reconstruction based domain adaptation methods: PixelDA <ref type="bibr" target="#b27">[28]</ref>, I2IAdapt <ref type="bibr" target="#b29">[30]</ref>, CyCADA <ref type="bibr" target="#b26">[27]</ref>, SBADA-GAN <ref type="bibr" target="#b28">[29]</ref>. In TIGDA, we depict the 'closest-clones' of Camera B (target) images in the Camera A (source) domain. It is seen that TIGDA preserves the edges, perceptual quality and structural details in the generated clones. 3) Qualitative examination: To qualitatively examine the performance of the reconstruction-based methods, we plot the transformed target samples from (source) Camera B to (target) Camera A for different methods as shown in <ref type="figure">Figure 6</ref>. It is seen that I2IAdapt <ref type="bibr" target="#b29">[30]</ref> and SBADA-GAN <ref type="bibr" target="#b28">[29]</ref> are not able to capture fine subtleties of partially visible White Blood Cells in microscopic images that results in poor performance. PixelDA <ref type="bibr" target="#b27">[28]</ref> and CyCADA <ref type="bibr" target="#b26">[27]</ref> result in blurry images while TIGDA generated images are better where it is seen that the subtleties like edge information are well-preserved.</p><p>In summary, we have demonstrated that TIGDA achieves better performance over the SOTA adversarial, divergence and reconstruction based UDA methods without any requirement for target images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) One-shot learning:</head><p>Even though TIGDA does not utilize the target data during training, target image is used for LS during inference. Therefore, we also compare TIGDA with SOTA one-shot learning techniques in <ref type="table" target="#tab_7">Table V</ref>. In one-shot learning methods, a single target image is used during training for adaptation. It is seen that TIGDA outperforms such techniques. This is because, in one-shot learning methods, the target image that is used for training is fixed which restricts the learnability. However in TIGDA, no target image is used during training but a fresh latent search is conducted on each input target image during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation studies</head><p>To examine the contributions made by each of the proposed components, we conduct several ablation experiments on TIGDA in this section.</p><p>1) Effect of number of iterations on LS: The inference of TIGDA involves a gradient-based optimization through the decoder network h θ * to generate the 'closest-clone' for a given target image. In <ref type="figure" target="#fig_4">Figure 7</ref>, we show the transformation of a few target images after every 200 iterations. It can be seen that as the number of iterations increase, the target images change their characteristics to move towards the source distribution. Quantitatively, we plot the accuracy as a function of number (a) Inference on camera C microscopic images when the model is trained on camera A images.</p><p>(b) Inference on camera C microscopic images when the model is trained on camera B images. of iterations in <ref type="figure" target="#fig_5">Figure 8</ref> where it is seen that it saturates around 500-600 iterations. We thus used 600 iterations in all the previous experiments in <ref type="table" target="#tab_2">Table II</ref>.</p><p>2) Effect of the Edge concatenation: As described earlier, the edge-map of the input image is concatenated with one of the layers of decoder both while training and inference. <ref type="figure" target="#fig_6">Figure  9b</ref> shows the quality of image generated after Latent Search when the model was trained without edge concatenation (wEc). It can be observed that edge information of the nucleus and surrounding cells is lost resulting in a blurry image. Further, the accuracy drops to 57.6% if edge concatenation is removed from VAE for the task A→B as evident from <ref type="table" target="#tab_2">Table  VI</ref>, whereas the accuracy for TIGDA is 76.2% for the same task. Similarly, the accuracy drops to 60.3% for the task B→C without edge concatenation while it is 74.8% for TIGDA. 3) Effect of Perceptual loss L p : We have used a perceptual model P ψ trained on source samples while training the VAE. Perceptual loss minimizes the Euclidean distance between the (perceptual) feature vectors of input and reconstructed source images. It measures image similarities more robustly than perpixel losses (e.g., Mean squared error). It ensures that the VAE reconstructed image is semantically similar to the input. We can observe from <ref type="figure" target="#fig_6">Figure 9c</ref> that VAE reconstructed image without perceptual loss (wPl) during training, has different color and texture patterns from the real target image shown in <ref type="figure" target="#fig_6">Figure 9a</ref>. The finer background details are missing in <ref type="figure" target="#fig_6">Figure  9c</ref>. Such images will result in a poor latent space and the performance on target images will drop during inference. <ref type="table" target="#tab_2">Table  VI</ref> shows that the accuracy drops to 53.4% for the task A→B without perceptual loss while it is 76.2% for TIGDA that uses perceptual loss during training. Similarly the accuracy drops to 57.1% for the task B→C when perceptual loss was not employed during training but the accuracy on the same task is 74.8% with perceptual loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Effect of Latent Search and other Loss functions:</head><p>To validate the importance of the Latent Search procedure, in <ref type="figure" target="#fig_6">Figure 9d</ref> we show the VAE reconstructed images without Latent Search for the target image shown in <ref type="figure" target="#fig_6">Figure 9a</ref>. <ref type="figure" target="#fig_6">Figure  9e</ref> shows the generated image after Latent Search for the task C→A. It is observed (empirically) that the 'closest-clone' obtained through TIGDA shown in <ref type="figure" target="#fig_6">Figure 9e</ref> is visually more closer to the source domain as compared to VAE reconstructed image shown in <ref type="figure" target="#fig_6">Figure 9d</ref>. When no Latent Search is employed, the accuracy for the tasks A→B and B→C drops to 43.7% and 46.9% respectively as shown in <ref type="table" target="#tab_2">Table VI</ref>. To affirm the usefulness of the choice of SSIM as loss for the Latent Search, we implemented Latent Search with three different losses, Mean Squared Error (MSE), Mean Absolute Error (MAE) and Structural Similarity Index (SSIM) loss and found that SSIM loss is the best performing among the three. SSIM loss compares pixels and their corresponding neighborhoods in two images, preserving the luminance, contrast and structure information. On the other hand, MSE or MAE measures only the absolute pixel differences rather than the structural differences. <ref type="figure" target="#fig_5">Figure 8a</ref> and 8b depict the outcome of these ablation studies where the superiority of the SSIM loss is seen over MSE and MAE for the tasks A→C and B→C respectively. Table VI summarizes all the ablation studies conducted on two domain adaptation tasks with different combinations of the components. It can be noted that the best performance is observed by utilizing all the three components: Edge concatenation, perceptual loss and Latent Search procedure. Thus, with all the aforementioned studies, we have demonstrated the utility of all the individual components used in TIGDA for UDA task on WBC classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Effect of other hyperparameters:</head><p>In this section, we study the effect of four hyperparameters: (a) the window size for the SSIM loss used for Latent Search, (b) the position of the Edge-operator in the decoder network, (c) use of Skip connection as in <ref type="bibr" target="#b47">[50]</ref> instead of edge concatenation, (d) number of source samples required to generate high-fidelity images using VAE. <ref type="figure" target="#fig_0">Figure 10</ref>(a) depicts the change in the performance for A→B with varying window sizes of SSIM. While the performance varies with different window sizes, the best accuracy is observed with the default choice of 11 that is used in all our experiments.</p><p>Next, in <ref type="figure" target="#fig_0">Figure 10</ref>(b), we vary the layer of the decoder to concatenate edges. It is seen that the performance is best at the penultimate layers since the edges are used only to reduce the blurriness of the generated image that occurs near the last few layers of the decoder. Providing the edge information at initial layers of the decoder, regularizes more than required, thus degrading the quality of the generated image.</p><p>To further quantify the effect of edge concatenation as a regularizer, we replace it with another type of spatial contiguity in the form of skip connections as in a segmentation network such as UNet <ref type="bibr" target="#b47">[50]</ref>. We have used five different types of skip connections. Type-1 refers to no skip connection. Type-2 connects FC1 layer (Refer to Supplementary material for the names of the layers in the architecture) of the encoder with FC2 layer of the decoder network. Type-3 connects all the layers in the encoder with layers of corresponding dimensions in the decoder (like a U-Net). Type-4 connects Conv1 layer in the encoder with Conv9 layer of the decoder. Type-5 is combination of Type-2 and Type-4 skip connections. We observe in <ref type="figure" target="#fig_0">Figure 10</ref>(c) that having skip connection is better than not having it since it regularizes the network. Further, Type-4, that connects the initial layers of the encoder with final layers of the decoder, has the best performance. This can be explained by the fact that initial layers of the CNNs are known to extract edge-like features which is shown to enhance the performance in the given task. Connecting more layers as in Type-3 and Type-5 leads to over regularization and degrades the performance. However, explicit edge concatenation still provides the best performance.</p><p>In the final plot <ref type="figure" target="#fig_0">Figure 10(d)</ref>, we report the Fréchet Inception Distance (FID) <ref type="bibr" target="#b54">[57]</ref>, that quantifies the quality of the generated data (lower the better) for any generative model, as a function of the number of source samples used to train the VAE. It is seen that with the increase in number of images for training VAE, the quality of generated images improve as shown by the FID values. Therefore, with about 10K samples, one can expect the VAE to sample high-fidelity source images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. TIGDA BEYOND PBS</head><p>In this section, we examine the effectiveness of the proposed method TIGDA on two datasets, Imaging Flow Cytometry <ref type="bibr" target="#b5">[6]</ref> and Office-31 <ref type="bibr" target="#b59">[62]</ref>, apart from PBS. In Cytometry dataset, WBCs from whole blood samples were stained using a ImageStream-X MK II imaging flow cytometer. A three channel image is extracted with two bright-field (at wavelengths of 420 nm -480 nm and 570 nm -595 nm) and a darkfield channel. Four classes of WBCs are employed in this study: Eosinophil (1470 images), Neutrophil (4809 images), Lymphocyte (4570 images) and Monocyte (1239 images). The objective of this experiment is to examine if TIGDA can perform domain adaptation when the source is Cytometry data and the target is PBS and vice versa. Since Cytometry data doesn't have the notion of color, we take the grayscale version of the PBS dataset with a 60 × 60 central crop (in all the images) representing the nucleus. <ref type="figure" target="#fig_0">Figure 11</ref> depicts a sample image from each class of the Cytometry dataset and the PBS dataset which apparently shows a significant domain shift. Office-31 <ref type="bibr" target="#b59">[62]</ref>, a publicly available standard dataset for UDA tasks (some sample images are given in the supplementary material), contains images from 31 common object types taken with three different imaging sources namely Dslr (D), Webcam (W) and Amazon (A). The objective of UDA is to adapt between these three domains. <ref type="table" target="#tab_2">Table VII</ref> lists the results of TIGDA along with some SOTA UDA methods for domain adaptation tasks on both the Office-31 and Cytometry datasets. It is seen that on Cytometry and gray-PBS datasets, TIGDA performs the best by significantly improving upon the Source Only model for gray-PBS→Cyto. and Cyto.→gray-PBS tasks. Whereas, on the Office-31 dataset, TIGDA's average performance is comparable (less than a percent) to the best SOTA method. All these experiments firmly demonstrate the effectiveness of TIGDA in UDA despite not using the target data during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this work, we have considered the problem of domain shift occurring with the CNN-based classifiers for WBC classification. The performance of the existing deep learning based techniques is known to degrade with the change in camera characteristics. We cast the problem of performance degradation of WBC classifiers with the change in camera as  <ref type="figure" target="#fig_0">Figure 10</ref>. (a) Accuracy of TIGDA on task A→B by selecting different window sizes in SSIM during Latent Search (b) Performance of TIGDA when the edges of input images are concatenated with different convolutional layers in decoder h θ (c) Performance of TIGDA when edge concatenation is replaced with different types of skip connections between encoder g φ and decoder h θ layers. Window size of 11 gives the best performance. For the same task, edge concatenation is better than skip connections. (d) FID of VAE generated images when TIGDA is trained on dataset A with different number of images ranging from 2,000 (2K) to 10,000 (10K).  <ref type="figure" target="#fig_0">Figure 11</ref>. Imaging Flow Cytometry <ref type="bibr" target="#b5">[6]</ref> and grayscale Peripheral Blood Smear (gray-PBS) White Blood Cell datasets. that of Unsupervised Domain Adaptation (UDA) and propose a method that is devoid of need for access to the target data during training. We have demonstrated the efficacy of the proposed method for UDA with experiments on multiple datasets acquired under different settings. A few possible future directions can be: (i) extension of TIGDA for medical data beyond WBC, (ii) combining multiple sources for UDA. Proof. Let B r (x T ) be a closed ball of radius r aroundx T under the metric D. That is, B r (x T ) = {x : D{x T , x} ≤ r}.</p><p>Since X is a separable metric space, ∀r &gt; 0, B r (x T ) has non-zero probability measure <ref type="bibr" target="#b39">[40]</ref>. That is,</p><formula xml:id="formula_13">Pr B r (x T ) Br(x T ) P s (x) dx &gt; 0<label>(14)</label></formula><p>For any δ &gt; 0 , the probability that none of the points in S n are within the ball B δ (x T ) of radius δ is given by:</p><p>Pr min i=1,2..,n D{x i ,x T } ≥ δ = 1 − Pr B δ (x T ) n (15) Therefore, the probability ofx S ∈ S n , lying within B δ (x T ) is given by:</p><formula xml:id="formula_14">Pr x S ∈ B δ (x T ) = 1 − 1 − Pr B δ (x T ) n<label>(16)</label></formula><p>= 1 as n → ∞</p><p>Thus, given any δ &gt; 0, with probability 1, ∃x S ∈ S n that is within δ distance fromx T as n → ∞      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XI. ALGORITHM FOR TIGDA</head><formula xml:id="formula_16">µ (i) z , σ (i) z ← g φ (x i ) 5: sample z i ∼ N (µ (i) z , σ (i) z 2 ) 6:x i ← h θ (z i ) 7: L r ← B i=1 x i −x i</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The architecture for the Variational Auto-Encoder in the proposed method (TIGDA). Edges of the input microscopic image is concatenated with the features from the decoder h θ . The encoder and decoder parameters φ, θ are optimized with reconstruction loss Lr, KL-divergence loss D KL and the perceptual loss Lp. The perceptual model P ψ outputs l th layer features of VGG-16 (or ResNet-50) classifier trained on source data. A zero mean and unit variance isotropic Gaussian prior is imposed over the latent space z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>t-SNE plots of features generated by ADDA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Illustration of Latent Search in TIDGA. VAE reconstructs images prior to LS. The closest-clones obtained after every 200 iterations are shown. A transformation is observed from the target to the source domain as the LS progresses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Performance of gradient-based Latent Search during inference on target microscopic images for two domain adaptation tasks using different objective functions; MSE=Mean Squared Error, MAE=Mean Absolute Error, SSIM=Structural Similarity Index. It is seen that the loss saturates around 500-600 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Ablation of TIGDA for task C→A. (wEc=without Edge concatenation, wPl=without Perceptual loss, wLS=without Latent Search). The best source-like features are observed in the image with all the components of TIGDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Accu. vs. SSIM window sizes. (b) Accu. vs. Edge concat. position. (c) Accuracy vs Skip connections. (d) FID vs. no. of training images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 )</head><label>2</label><figDesc>ψ (x i ) − P ψ (x i ) || N (0, 1) 10:L h ← L r + L p 11: φ ← φ + η∇ φ L g 12: θ ← θ + η∇ θ L h 13: until convergence of φ,θ Inference -Latent Search for target images Input: Target imagex T , Trained decoder h θ * , Learning rate η. Output: 'closest-clone'x S for the target imagẽ x T . 14: sample z from N (0, 1) 15: repeat 16:L ssim ← 1 − SSIM(x T , h θ * (z)) 17: z ← z + η∇ z L ssim 18: until convergence of L ssim 19:z S ← z 20:x S ← h θ * (z S )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>PCA plots of the first two principal component using features generated by ADDA, GTA, DIRT-T, TAT, DAL and TIGDA on task A→C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .Figure 14 .</head><label>1314</label><figDesc>Confusion Matrices for ADDA, GTA, DIRT-T, TAT, DAL and TIGDA on task A→C. Classes are Neutrophil (NE), Lymphocyte (LY), Monocyte (MO), Eosinophil (EO), Basophil (BA), Immature granulocytes (IG), Atypical (AT), Nucleated red blood cells (NR), Giant platelets (GP), Platelet clumps (PC), Artefact (AF). Samples from the Office-31 dataset from the three sources, Amazon, Dslr and Webcam.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Pandey and Prathosh AP are with Department of Electrical Engineering, Indian Institute of Technology Delhi, New Delhi 110016, India. Deepak Mishra is with Department of Computer Science and Engineering, Indian Institute of Technology Jodhpur, Rajasthan 342037, India. Vinay Kyatham and Tathagato Rai Dastidar are with SigTuple Technologies Pvt. Ltd., Bangalore 560102, India. Email: getprashant57@gmail.com, prathoshap@iitd.ac.in, dmishra@iitj.ac.in, trd@sigtuple.com, vinay.k@sigtuple.com. The code for our implementation is available at https://github.com/prinshul/WBC-Classification-UDA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Samples of White Blood Cells and related microscopic images (categorized into 11 classes) taken from three different cameras A, B and C.</figDesc><table><row><cell>Neutrophil Lymphocyte Monocyte</cell><cell>Eosinophil</cell><cell>Basophil</cell><cell>IG</cell><cell>Atypical</cell><cell>NRBC</cell><cell>GP</cell><cell>PC</cell><cell>Artefact</cell></row><row><cell>Camera A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Camera B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Camera C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(IG=Immature granulocytes, NRBC=Nucleated red blood cells, GP=Giant platelets, PC=Platelet clumps). It is to be noted that there are no visually distinctive features across cameras but it is easy for a human-pathologist to correctly classify despite camera changes. On the other hand, deep learning models fail to generalize across cameras.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table I NUMBER</head><label>I</label><figDesc></figDesc><table><row><cell>Camera</cell><cell>NE</cell><cell>LY</cell><cell>MO</cell><cell>EO</cell><cell>BA</cell><cell>IG</cell><cell cols="4">Atypical NRBC GP PC Artefact</cell><cell>Train /Test</cell></row><row><cell>A</cell><cell cols="5">3,885 1,507 2,224 2,076 65</cell><cell>863</cell><cell>984</cell><cell>651</cell><cell>486 138</cell><cell>2,550</cell><cell>10,849 /4,580</cell></row><row><cell>B</cell><cell cols="2">2,045 1,840</cell><cell>612</cell><cell>373</cell><cell cols="2">67 1,073</cell><cell>2,257</cell><cell>97</cell><cell>918 796</cell><cell>1,437</cell><cell>7,997 /3,518</cell></row><row><cell>C</cell><cell>85</cell><cell>43</cell><cell>144</cell><cell>85</cell><cell>12</cell><cell>323</cell><cell>861</cell><cell>321</cell><cell>303 11</cell><cell>16</cell><cell>1,548 /656</cell></row></table><note>OF WHITE BLOOD CELLS AND RELATED MICROSCOPIC IMAGES FOR EACH SUBTYPE (CLASS) CAPTURED WITH THREE DIFFERENT CAMERAS A, B AND C. (NE=NEUTROPHIL, LY=LYMPHOCYTE, MO=MONOCYTE, EO=EOSINOPHIL, BA=BASOPHIL, IG=IMMATURE GRANULOCYTES, NRBC=NUCLEATED RED BLOOD CELLS, GP=GIANT PLATELETS, PC=PLATELET CLUMPS).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II ACCURACY</head><label>II</label><figDesc>(MEAN ± STD%) VALUES FOR</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>7±0.5 51.3±0.4 35.8±0.6 46.2±0.2 22.8±0.6 26.9±0.4 37.4±0.5 47.6±0.4 31.2±0.3 40.1±0.5 17.6±0.6 22.7±0.2 DR1 [38] 52.5±0.3 57.7±0.1 43.6±0.2 51.7±0.4 34.5±0.3 36.2±0.2 44.6±0.1 50.9±0.2 38.2±0.4 46.5±0.3 27.3±0.3 30.8±0.2 DR2 [39] 60.3±0.2 65.4±0.3 55.9±0.2 64.2±0.4 44.6±0.3 49.8±0.4 54.1±0.1 59.6±0.2 48.7±0.1 60.5±0.4 41.3±0.3 45.2±0.1 DR3 [40] 50.4±0.2 53.4±0.4 40.5±0.2 49.8±0.3 29.5±0.3 32.7±0.4 41.8±0.3 47.5±0.2 35.9±0.1 42.1±0.2 23.6±0.2 28.3±0.3 ADDA [16] 43.5±0.1 52.7±0.2 37.3±0.1 48.1±0.5 24.9±0.4 29.1±0.5 39.3±0.2 50.1±0.3 33.6±0.4 43.3±0.2 19.8±0.4 25.2±0.5 GTA [18] 56.2±0.4 66.3±0.5 48.1±0.2 56.7±0.6 35.5±0.4 37.8±0.1 52.6±0.7 62.1±0.3 41.9±0.6 50.7±0.3 30.1±0.1 33.8±0.7 59.1±0.8 66.2±0.5 47.8±0.4 50.6±0.5 61.5±0.3 68.4±0.4 54.6±0.7 58.8±0.6 41.3±0.6 42.5±0.4 SBADA-GAN [29] 66.3±0.2 70.5±0.2 60.3±0.3 65.6±0.4 46.4±0.7 51.1±0.1 62.7±0.6 67.9±0.8 53.8±0.7 58.7±0.2 42.7±0.4 44.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet-50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">VGG-16</cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell>A→B</cell><cell>A→C</cell><cell>B→A</cell><cell>B→C</cell><cell>C→A</cell><cell>C→B</cell><cell>A→B</cell><cell>A→C</cell><cell>B→A</cell><cell>B→C</cell><cell>C→A</cell><cell>C→B</cell></row><row><cell>Source Only</cell><cell cols="12">42.7±0.6</cell></row><row><cell>TAT [21]</cell><cell cols="12">65.8±0.5 70.5±0.4 54.8±0.3 63.1±0.7 44.7±0.2 48.2±0.3 61.7±0.5 67.3±0.4 50.6±0.4 58.3±0.6 40.3±0.1 42.5±0.1</cell></row><row><cell>DIRT-T [20]</cell><cell cols="12">55.7±0.5 65.1±0.6 49.2±0.2 55.4±0.3 34.2±0.3 37.5±0.4 53.1±0.8 61.9±0.7 40.7±0.5 50.3±0.5 31.3±0.4 32.9±0.7</cell></row><row><cell>DAL [22]</cell><cell cols="12">64.7±0.2 69.4±0.3 56.3±0.2 62.7±0.4 43.5±0.1 47.5±0.5 60.8±0.2 66.5±0.5 51.8±0.4 59.1±0.3 39.7±0.1 41.1±0.2</cell></row><row><cell>CyCADA [27]</cell><cell cols="12">67.2±0.5 73.7±0.1 58.2±0.2 64.5±0.6 48.4±0.4 50.2±0.3 62.3±0.3 70.2±0.2 53.4±0.4 59.7±0.2 42.6±0.6 43.9±0.7</cell></row><row><cell>PixelDA [28]</cell><cell cols="12">65.9±0.2 71.6±0.7</cell></row><row><cell>I2IAdapt [30]</cell><cell cols="12">64.4±0.6 68.7±0.5 61.2±0.3 65.4±0.4 45.2±0.1 49.7±0.6 63.9±0.8 65.1±0.1 52.5±0.7 55.6±0.4 43.8±0.8 45.3±0.3</cell></row><row><cell>JAN [33]</cell><cell cols="12">49.6±0.2 58.2±0.5 43.3±0.2 54.7±0.4 30.2±0.7 35.4±0.8 43.5±0.6 54.2±0.4 39.1±0.3 47.5±0.3 26.3±0.4 31.4±0.6</cell></row><row><cell>MCD [34]</cell><cell cols="12">55.4±0.4 67.1±0.8 49.2±0.7 55.8±0.6 36.1±0.2 39.2±0.5 50.9±0.7 63.2±0.4 42.3±0.3 50.4±0.5 31.9±0.8 34.8±0.5</cell></row><row><cell>CAN [35]</cell><cell cols="2">67.8±0.4 71.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>3±0.5 63.4±0.5 65.4±0.3 47.3±0.2 51.2±0.4 61.9±0.8 68.1±0.3 54.6±0.6 59.3±0.4 40.9±0.2 45.7±0.8 TIGDA (Ours) 76.2±0.3 80.1±0.4 72.3±0.5 74.8±0.6 53.5±0.4 56.2±0.3 71.8±0.5 76.7±0.2 63.2±0.5 68.6±0.7 50.8±0.2 55.1±0.4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table III BENCHMARKING</head><label>III</label><figDesc>A,B AND C DATASETS USING RESNET-50 CLASSIFIER WITH DIFFERENT TRAIN AND TEST SETS. IT IS SEEN THAT COMBINING ALL DATASETS MAKES LEARNING DIFFICULT BECAUSE OF DOMAIN SHIFT.</figDesc><table><row><cell>Measure</cell><cell>A→A</cell><cell>B→B</cell><cell>C→C</cell><cell>(A+B+C)→(A+B+C)</cell></row><row><cell cols="4">Train Acc. 98.6±0.1 99.3±0.2 100.0±0.0</cell><cell>98.7±0.2</cell></row><row><cell cols="4">Test Acc. 95.2±0.2 94.0±0.3 92.5±0.1</cell><cell>84.4±0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table IV ACCURACY</head><label>IV</label><figDesc>ON RESNET-50 CLASSIFIERS FOR DIFFERENT ADAPTATION TASKS. IN THE SECOND ROW, ALL THE THREE DATASETS ARE MADE TO HAVE SAME SIZE BY RANDOMLY SUBSAMPLING THE DATASETS. Balance 42.7±0.5 51.3±0.4 35.8±0.6 46.2±0.2 22.8±0.6 26.9±0.4 With Balance 40.4±0.1 36.2±0.4 38.9±0.2 30.5±0.2 24.5±0.4 28.2±0.3</figDesc><table><row><cell>Measure</cell><cell>A→B</cell><cell>A→C</cell><cell>B→A</cell><cell>B→C</cell><cell>C→A</cell><cell>C→B</cell></row><row><cell>W/o</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table V COMPARISON</head><label>V</label><figDesc>OF TIGDA WITH ONE-SHOT LEARNING METHODS.</figDesc><table><row><cell>Method</cell><cell>A→B</cell><cell>C→B</cell></row><row><cell>ProtoNet [53]</cell><cell cols="2">61.9±0.1 49.6±0.3</cell></row><row><cell cols="3">MatchingNet [53] 57.6±0.2 43.7±0.1</cell></row><row><cell>DAPN [54]</cell><cell cols="2">68.9±0.2 51.9±0.2</cell></row><row><cell>DN4 [55]</cell><cell cols="2">55.4±0.1 44.6±0.2</cell></row><row><cell>FADA [56]</cell><cell cols="2">60.6±0.3 45.9±0.3</cell></row><row><cell>TIGDA (Ours)</cell><cell cols="2">76.2±0.3 56.2±0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table VI ABLATION</head><label>VI</label><figDesc>OF DIFFERENT COMPONENTS OF TIGDA DURING TRAINING AND INFERENCE; EDGE, PERCEPTUAL LOSS Lp AND LATENT SEARCH (LS). ACCURACY (MEAN ± STD%) VALUES ARE REPORTED AS AN AVERAGE OVER FIVE INDEPENDENT RUNS FOR TWO TASKS.</figDesc><table><row><cell>Edge L p LS</cell><cell>A→B</cell><cell>B→C</cell></row><row><cell></cell><cell cols="2">35.8±0.2 39.5±0.1</cell></row><row><cell></cell><cell cols="2">39.7±0.4 42.2±0.3</cell></row><row><cell></cell><cell cols="2">38.9±0.5 43.4±0.3</cell></row><row><cell></cell><cell cols="2">50.2±0.3 52.8±0.2</cell></row><row><cell></cell><cell cols="2">43.7±0.2 46.9±0.5</cell></row><row><cell></cell><cell cols="2">57.6±0.4 60.3±0.2</cell></row><row><cell></cell><cell cols="2">53.4±0.3 57.1±0.4</cell></row><row><cell></cell><cell cols="2">76.2±0.3 74.8±0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table VII ACCURACY</head><label>VII</label><figDesc>(MEAN ± STD%) VALUES FOR UDA TASKS ON OFFICE-31 AND IMAGING FLOW CYTOMETRY (CYTO.) AND GRAYSCALE PERIPHERAL BLOOD SMEAR (GRAY-PBS) WHITE BLOOD CELL DATASETS. RESULTS ARE REPORTED AS AN AVERAGE OVER FIVE INDEPENDENT RUNS USING VARIOUS SOTA UDA METHODS USING RESNET-50 CLASSIFIER. NOTE THAT WHILE ALL UDA METHODS PERFORM BETTER THAN THE SOURCE ONLY MODEL, TIGDA OFFERS SIGNIFICANT PERFORMANCE ENHANCEMENT DESPITE NOT USING THE TARGET IMAGES DURING TRAINING. Ours) 93.2±0.2 99.4±0.4 99.8±0.1 93.6±0.3 76.7±0.2 75.7±0.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Office-31</cell><cell></cell><cell></cell><cell></cell><cell>WBC</cell></row><row><cell>Models</cell><cell>A→W</cell><cell>D→W</cell><cell>W→D</cell><cell>A→D</cell><cell>D→A</cell><cell>W→A</cell><cell cols="2">Avg gray-PBS→Cyto. Cyto.→gray-PBS Avg</cell></row><row><cell>Source Only</cell><cell cols="7">68.4±0.2 96.7±0.1 99.3±0.1 68.9±0.2 62.5±0.3 60.7±0.3 76.1</cell><cell>42.6±0.1</cell><cell>22.2±0.2</cell><cell>32.4</cell></row><row><cell>JAN [33]</cell><cell cols="7">85.4±0.3 97.4±0.2 99.8±0.2 84.7±0.3 68.6±0.3 70.0±0.4 84.3</cell><cell>67.5±0.2</cell><cell>57.2±0.3</cell><cell>62.3</cell></row><row><cell>MADA [58]</cell><cell cols="7">90.0±0.2 97.4±0.1 99.6±0.1 87.8±0.2 70.3±0.4 66.3±0.1 85.2</cell><cell>73.3±0.2</cell><cell>61.8±0.3</cell><cell>67.5</cell></row><row><cell>SimNet [59]</cell><cell cols="7">88.6±0.5 98.2±0.2 99.7±0.2 85.3±0.3 73.4±0.8 71.8±0.6 86.2</cell><cell>76.4±0.2</cell><cell>66.8±0.2</cell><cell>71.6</cell></row><row><cell>GTA [18]</cell><cell cols="7">89.5±0.5 97.9±0.3 99.8±0.4 87.7±0.5 72.8±0.3 71.4±0.4 86.5</cell><cell>75.2±0.4</cell><cell>66.5±0.3</cell><cell>70.8</cell></row><row><cell>DAAA [60]</cell><cell cols="7">86.8±0.2 99.3±0.1 100.0±0.0 88.8±0.4 74.3±0.2 73.9±0.2 87.2</cell><cell>75.8±0.3</cell><cell>68.2±0.1</cell><cell>72.0</cell></row><row><cell>CDAN [61]</cell><cell cols="7">94.1±0.1 98.6±0.1 100.0±0.0 92.9±0.2 71.0±0.3 69.3±0.3 87.7</cell><cell>78.6±0.2</cell><cell>67.1±0.1</cell><cell>72.8</cell></row><row><cell>CAN [35]</cell><cell cols="7">94.5±0.3 99.1±0.2 99.8±0.2 95.0±0.3 78.0±0.3 77.0±0.3 90.6</cell><cell>79.4±0.3</cell><cell>68.9±0.2</cell><cell>74.1</cell></row><row><cell cols="8">TIGDA (3 89.7</cell><cell>80.3±0.4</cell><cell>71.4±0.3</cell><cell>75.8</cell></row><row><cell>Eosinophil</cell><cell>Neutrophil</cell><cell cols="2">Lymphocyte</cell><cell>Monocyte</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cytometry</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>gray-PBS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Target-Independent Domain Adaptation for WBC Classification using Generative Latent Search -Supplementary-Prashant Pandey, Prathosh AP, Vinay Kyatham, Deepak Mishra and Tathagato Rai Dastidar X. PROOF FOR LEMMA 1 : Lemma 2. Ifx S ∈ S n is the point such that D{x T ,x S } &lt; D{x T , x} ∀ x ∈ S n , then as n → ∞,x S converges tox T with probability 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Source dataset S n = {x 1 , ..., x n }, Number of source images n, Encoder g φ , Decoder h θ , Trained Perceptual Model P ψ , Learning rate η, Batchsize B. Output: Optimal parameters φ * , θ * . 1: Initialize parameters φ, θ 2: repeat 3:sample batch {x i } from dataset S n , for i = 1, ..., B</figDesc><table><row><cell>Algorithm 1 Target-Independent Generative Domain</cell></row><row><cell>Adaptation (TIGDA)</cell></row><row><cell>Training VAE on source data</cell></row><row><cell>Input:</cell></row></table><note>4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table VIII ENCODER</head><label>VIII</label><figDesc>ARCHITECTURE FOR THE VARIATIONAL AUTO-ENCODER (VAE) IN THE PROPOSED METHOD (TIGDA). CONVOLUTION KERNEL IS 3 × 3 AND FOR LEAKY RELU α = 0.2.</figDesc><table><row><cell>Layer (type)</cell><cell>Output shape</cell></row><row><cell>encoder input (InputLayer)</cell><cell>(128, 128, 3)</cell></row><row><cell>Conv1 (Convolution)</cell><cell>(128, 128, 128)</cell></row><row><cell>leakyReLU1 (Activation)</cell><cell>(128, 128, 128)</cell></row><row><cell>Conv2 (Convolution)</cell><cell>(64, 64, 128)</cell></row><row><cell>leakyReLU2 (Activation)</cell><cell>(64, 64, 128)</cell></row><row><cell>Conv3 (Convolution)</cell><cell>(32, 32, 128)</cell></row><row><cell>leakyReLU3 (Activation)</cell><cell>(32, 32, 128)</cell></row><row><cell>Conv4 (Convolution)</cell><cell>(16, 16, 128)</cell></row><row><cell>leakyReLU4 (Activation)</cell><cell>(16, 16, 128)</cell></row><row><cell>Conv5 (Convolution)</cell><cell>(8, 8, 128)</cell></row><row><cell>leakyReLU5 (Activation)</cell><cell>(8, 8, 128)</cell></row><row><cell>Conv6 (Convolution)</cell><cell>(4, 4, 128)</cell></row><row><cell>leakyReLU6 (Activation)</cell><cell>(4, 4, 128)</cell></row><row><cell>FC1 (Dense)</cell><cell>(1024)</cell></row><row><cell>Z (Dense)</cell><cell>(64)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table IX DECODER</head><label>IX</label><figDesc>ARCHITECTURE FOR THE VAE IN TIGDA. CONVOLUTION KERNEL IS 3 × 3 AND FOR LEAKY RELU α = 0.2.</figDesc><table><row><cell>Layer (type)</cell><cell>Output shape</cell></row><row><cell>decoder input (InputLayer)</cell><cell>(64)</cell></row><row><cell>FC2 (Dense)</cell><cell>(1024)</cell></row><row><cell>leakyReLU7 (Activation)</cell><cell>(1024)</cell></row><row><cell>FC3 (Dense)</cell><cell>(2048)</cell></row><row><cell>leakyReLU8 (Activation)</cell><cell>(2048)</cell></row><row><cell>Deconv1 (Deconvolution)</cell><cell>(4, 4, 128)</cell></row><row><cell>leakyReLU9 (Activation)</cell><cell>(4, 4, 128)</cell></row><row><cell>Deconv2 (Deconvolution)</cell><cell>(8, 8, 128)</cell></row><row><cell>leakyReLU10 (Activation)</cell><cell>(8, 8, 128)</cell></row><row><cell>Deconv3 (Deconvolution)</cell><cell>(16, 16, 128)</cell></row><row><cell>leakyReLU11 (Activation)</cell><cell>(16, 16, 128)</cell></row><row><cell>Deconv4 (Deconvolution)</cell><cell>(32, 32, 128)</cell></row><row><cell>leakyReLU12 (Activation)</cell><cell>(32, 32, 128)</cell></row><row><cell>Deconv5 (Deconvolution)</cell><cell>(64, 64, 128)</cell></row><row><cell>leakyReLU13 (Activation)</cell><cell>(64, 64, 128)</cell></row><row><cell>Deconv6 (Deconvolution)</cell><cell>(128, 128, 3)</cell></row><row><cell>tanh1 (Activation)</cell><cell>(128, 128, 3)</cell></row><row><cell>edge input (InputLayer)</cell><cell>(128, 128, 6)</cell></row><row><cell>edge concat (Concatenate)</cell><cell>(128, 128, 9)</cell></row><row><cell>Conv7 (Convolution)</cell><cell>(128, 128, 128)</cell></row><row><cell cols="2">leakyReLU14 (Activation) (128, 128, 128)</cell></row><row><cell>Conv8 (Convolution)</cell><cell>(128, 128, 128)</cell></row><row><cell cols="2">leakyReLU15 (Activation) (128, 128, 128)</cell></row><row><cell>Conv9 (Convolution)</cell><cell>(128, 128, 3)</cell></row><row><cell>tanh2 (Activation)</cell><cell>(128, 128, 3)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. ACKNOWLEDGEMENTS</head><p>We sincerely thank the Associate Editor and the Anonymous Reviewers for their thoughtful comments that helped to significantly improve our paper. We also thank Maxim Lippeveld, Ghent University for his generous help in providing and navigating through the Cytometry dataset. We thank Sameer Ambekar and Aayush Tyagi for their help in experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A beginner&apos;s guide to blood cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Bain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Performance of cellavision dm96 in leukocyte classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Higa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Naugler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of pathology informatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The classification of white blood cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="298" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segmentation and classification of white blood cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Bikhet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Tolba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Shaheen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 00CH37100)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2259" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluation of segmentation algorithms on cell populations using cdf curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hagwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="380" to="390" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classification of human white blood cells using machine learning for stain-free imaging flow cytometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lippeveld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ladlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Filby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peralta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cytometry Part A</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning in label-free cell classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahjoubfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K</forename><surname>Blaby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Niazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jalali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21471</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fine-grained leukocyte classification with deep residual learning for microscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grudtsin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="243" to="252" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Cree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1196" to="1206" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep adversarial training for multi-organ nuclei segmentation in histopathology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Salimian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Durr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Blood cell classification based on hyperspectral imaging with modulated gabor and cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image representation using 2d gabor wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="959" to="971" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Performance evaluation of the cellavision dm96 system: Wbc differentials by automated digital image analysis supported by an ann</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-I</forename><surname>Bengtsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Casey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Keefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Beatrice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Grzybek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Lewandrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Van Cott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of clinical pathology</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="770" to="781" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analyzing microscopic images of peripheral blood smear using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mundhra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheluvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rampure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Dastidar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks,&quot; in Domain Adaptation in Computer Vision Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="189" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8503" to="8512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Skin segmentation from nir images using unsupervised domain adaptation through generative latent search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ambekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08696</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>ArXiv:1802.08735</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transferable adversarial training: A general approach to adapting deep classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain agnostic learning with disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised reverse domain adaptation for synthetic medical images via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Durr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2572" to="2581" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for facilitating stainindependent supervised &amp; unsupervised segmentation: A study on kidney histology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadermayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Klinkhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Noise adaptation generative adversarial network for medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3722" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From source to target and back: symmetric bi-directional adaptive gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8099" to="8108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning cross-modality representations from multi-modal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Tulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="638" to="648" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dalsa: domain adaptation for supervised learning from sparsely annotated mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Binczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Polanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tarnawski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bobek-Billewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Koethe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleesiek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stieltjes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="196" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
		<respStmt>
			<orgName>JMLR. org</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4893" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An unsupervised domain adaptation approach to classification of stem cell-derived cardiomyocytes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning with cinematic rendering: fine-tuning deep neural networks using photorealistic medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Durr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics in Medicine &amp; Biology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">185012</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Training deep networks on domain randomized synthetic x-ray data for cardiac interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cimen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ceccaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurzendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rhode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mountney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deceptionnet: Network-driven domain randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="532" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation via disentangled representations: Application to cross-modality liver segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="255" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ultrasound image enhancement using structure oriented adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Soin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1349" to="1353" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Whole slide imaging system using deep learning-based automated focusing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Dastidar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ethirajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Optics Express</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="480" to="491" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Domain-adaptive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08626</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7260" to="7268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Few-shot adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iranmanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6670" to="6680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8004" to="8013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
