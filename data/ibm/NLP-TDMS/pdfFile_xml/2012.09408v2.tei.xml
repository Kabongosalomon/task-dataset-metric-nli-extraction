<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactive Speech and Noise Modeling for Speech Enhancement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyu</forename><surname>Zheng</surname></persName>
							<email>zhengchengyu@cuc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Communication University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
							<email>yzhang@cuc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Communication University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Srinivasan</surname></persName>
							<email>sriram.srinivasan@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
							<email>yanlu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interactive Speech and Noise Modeling for Speech Enhancement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech enhancement is challenging because of the diversity of background noise types. Most of the existing methods are focused on modelling the speech rather than the noise. In this paper, we propose a novel idea to model speech and noise simultaneously in a two-branch convolutional neural network, namely SN-Net. In SN-Net, the two branches predict speech and noise, respectively. Instead of information fusion only at the final output layer, interaction modules are introduced at several intermediate feature domains between the two branches to benefit each other. Such an interaction can leverage features learned from one branch to counteract the undesired part and restore the missing component of the other and thus enhance their discrimination capabilities. We also design a feature extraction module, namely residualconvolution-and-attention (RA), to capture the correlations along temporal and frequency dimensions for both the speech and the noises. Evaluations on public datasets show that the interaction module plays a key role in simultaneous modeling and the SN-Net outperforms the state-of-the-art by a large margin on various evaluation metrics. The proposed SN-Net also shows superior performance for speaker separation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Speech enhancement aims at separating speech from background interference signals. Mainstream deep learningbased methods learn to predict the speech signal in a supervised manner, as shown in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>. Most prior works operate in the time-frequency (T-F) domain by predicting a mask between noisy and clean spectra <ref type="bibr" target="#b41">(Wang, Narayanan, and Wang 2014;</ref><ref type="bibr" target="#b43">Williamson, Wang, and Wang 2015)</ref> or directly predicting the clean spectrum <ref type="bibr" target="#b48">(Xu et al. 2013;</ref><ref type="bibr" target="#b31">Tan and Wang 2018)</ref>. Some methods operate in the time domain by estimating speech signals from raw-waveform noisy signals in an end-to-end way <ref type="bibr" target="#b6">(Fu et al. 2017;</ref><ref type="bibr" target="#b22">Pascual, Bonafonte, and Serra 2017;</ref><ref type="bibr" target="#b21">Pandey and Wang 2019)</ref>. These methods have considerably improved the quality of enhanced speech compared with traditional signal processing based schemes. However, speech distortion or residual noise can often be observed in the enhanced speech, showing that there are still correlations between predicted speech and the residual signal by subtracting enhanced speech from noisy signal. <ref type="bibr">*</ref> The work was done at Microsoft Research Asia. Instead of only predicting speech and ignoring the characteristics of background noises, traditional signal processing and modeling based methods mostly take the other way (see <ref type="figure" target="#fig_0">Figure 1</ref> (b)), i.e. estimating noise or building noise models for speech enhancement <ref type="bibr" target="#b0">(Boll 1979;</ref><ref type="bibr" target="#b8">Hendriks, Heusdens, and Jensen 2010;</ref><ref type="bibr" target="#b40">Wang and Brookes 2017;</ref><ref type="bibr" target="#b44">Wilson et al. 2008;</ref><ref type="bibr" target="#b17">Mohammadiha, Smaragdis, and Leijon 2013)</ref>. Some model-based methods instead model both speech and noise <ref type="bibr">(Srinivasan, Samuelsson, and Kleijn 2005b,a)</ref>, possibly with alternate model update. However, they typically cannot generalize well when prior noise assumption cannot be met or the interference signal is not structured. In deep-learningbased methods, two recent attempts <ref type="bibr">Anderson 2017, 2018)</ref> focus on directly predicting noise considering that noise is dominant in low-SNR conditions. However, the benefit is limited.</p><p>The remaining correlation between predicted speech and noise motivates us to explore the information flow between speech and noise estimations, as shown in <ref type="figure" target="#fig_0">Figure 1 (c)</ref>. Since speech-related information exists in predicted noise, and vice versa, adding information communication between them may help to recover some missing components and remove undesired information from each other. In this paper, we propose a two-branch convolutional neural network, namely SN-Net, to simultaneously predict speech and noise signals. Between them are information interaction modules, by which noise or speech related information are extracted from the noise branch and added back to speech features to counteract the undesired noise part or recover the missing speech, and vice versa. In this way, the discrimination capability is largely enhanced. The two branches share the same network structure, which is an encoderdecoder-based model with several residual-convolution-andattention (RA) blocks in between for separation. Motivated by the success of self-attention technique in machine translation and computer vision tasks <ref type="bibr" target="#b34">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b39">Wang et al. 2018)</ref>, we propose to combine temporal self-attention and frequency-wise self-attention parallelly inside each RA block for capturing global dependency along temporal and frequency dimensions in a separable way.</p><p>Our main contributions are summarized as follows.</p><p>• We propose to simultaneously model speech and noise in a two-branch deep neural network and introduce information flow between them. In this way, speech part is enhanced while residual noise is suppressed for speech estimation, and vice versa. • We propose a RA block for feature extraction. Separable self-attention is utilized in this block to globally capture the temporal and frequency dependencies. • We validate the superiority of proposed scheme in an ablation study and comparison with state-of-the-art algorithms on two public datasets. Moreover, we extend our method to speaker separation, which also shows great performance. These results demonstrate the superiority and potential of the proposed method.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Learning-based Speech Enhancement</head><p>Deep learning-based methods mainly study how to build a speech model. According to the adopted signal domain, these methods can be classified into two categories. Time-Frequency (T-F) domain methods take T-F representation, either complex or log power spectrum of the magnitude, as input. They typically estimate a real or complex ratio mask for each T-F bin to map noisy spectra to speech spectra <ref type="bibr" target="#b43">(Williamson, Wang, and Wang 2015;</ref><ref type="bibr" target="#b41">Wang, Narayanan, and Wang 2014;</ref><ref type="bibr" target="#b2">Choi et al. 2019)</ref> or directly predict the speech representation <ref type="bibr" target="#b48">(Xu et al. 2013;</ref><ref type="bibr" target="#b31">Tan and Wang 2018)</ref>. Timedomain methods take waveform as input and typically extract a hidden representation of the raw waveform through an encoder and reconstruct an enhanced version from that <ref type="bibr" target="#b6">(Fu et al. 2017;</ref><ref type="bibr" target="#b22">Pascual, Bonafonte, and Serra 2017;</ref><ref type="bibr" target="#b21">Pandey and Wang 2019)</ref>. Although these methods have shown great improvements over traditional methods, they only focus on modeling speech and neglect the importance of understanding noise characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Noise-Aware Speech Enhancement</head><p>Noise information is often considered in traditional signal processing based methods <ref type="bibr" target="#b0">(Boll 1979;</ref><ref type="bibr" target="#b8">Hendriks, Heusdens, and Jensen 2010;</ref><ref type="bibr" target="#b40">Wang and Brookes 2017)</ref> with prior distribution assumptions for speech and noise. However, it is a challenging task to estimate the noise power spectral density for non-stationary noises and thus mostly stationary noise is assumed. They are unsuitable in generalization to low SNR and non-stationary noise conditions. Instead, some model-based methods build models for speech and noise and show more promising results, e.g., codebook <ref type="bibr">(Srinivasan, Samuelsson, and Kleijn 2005b,a)</ref> and nonnegative matrix factorization (NMF) <ref type="bibr" target="#b44">(Wilson et al. 2008</ref>; Mohammadiha, Smaragdis, and Leijon 2013) based methods. However, they either need prior knowledge of the noise type <ref type="bibr">(Srinivasan, Samuelsson, and Kleijn 2005b,a)</ref> or are only effective for structured noise <ref type="bibr" target="#b44">(Wilson et al. 2008</ref>; Mohammadiha, Smaragdis, and Leijon 2013); therefore their generalization capability is limited. Deep learning-based methods can better generalize to various noise conditions. There are also some attempts on incorporating noise information, for example, by adding constraints to loss functions <ref type="bibr" target="#b4">(Fan et al. 2019;</ref><ref type="bibr" target="#b49">Xu, Elshamy, and Fingscheidt 2020;</ref><ref type="bibr" target="#b47">Xia et al. 2020)</ref> or by directly predicting noise instead of speech <ref type="bibr">Anderson 2017, 2018)</ref>. The former does not model noise at all and the characteristics of noise are not exploited. The latter loses the speech information and show even worse quality than corresponding speech prediction method in low SNR and unseen noise conditions. A more relevant work utilizes two deep auto encoders (DAEs) to estimate speech and noise <ref type="bibr" target="#b30">(Sun et al. 2015)</ref> . It first trains a DAE for speech spectrum reconstruction and then introduces another DAE to model noise with the constraint that the sum of outputs of the two DAEs is equal to the noisy spectrum.</p><p>Different from aforementioned approaches, we proposed a two-branch CNN to predict speech and noise simultaneously and introduce interaction modules at several intermediate layers to make them benefit from each other. Such a paradigm makes it suitable for speaker separation as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Two-Branch Neural Networks</head><p>Two-branch neural networks have been explored in various tasks for capturing cross-modality information <ref type="bibr" target="#b18">(Nam, Ha, and Kim 2017;</ref><ref type="bibr" target="#b38">Wang et al. 2019)</ref> or different levels of information <ref type="bibr" target="#b26">(Simonyan and Zisserman 2014;</ref><ref type="bibr" target="#b37">Wang et al. 2020</ref>). For speech enhancement, a two-branch modeling is proposed to predict the amplitude and phase of the enhanced signal, respectively <ref type="bibr" target="#b50">(Yin et al. 2020)</ref>. In this paper, we aim to exploit the two correlated tasks, i.e. speech and noise estimations and explicitly modeling them in an interactive twobranch framework for better discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Self-Attention Model</head><p>Self-attention mechanism has been widely used in many tasks, e.g., machine translation <ref type="bibr" target="#b34">(Vaswani et al. 2017</ref>), image generation <ref type="bibr" target="#b52">(Zhang et al. 2019</ref>) and video question answering . For video, spatio-temporal attention is also considered to exploit long-term dependency along both spatial and temporal dimensions <ref type="bibr" target="#b46">(Wu et al. 2019</ref>). Recently, speech-related tasks have also benefited from selfattention, e.g., speech recognition <ref type="bibr" target="#b25">(Salazar, Kirchhoff, and Huang 2019)</ref> and speech enhancement <ref type="bibr" target="#b11">(Kim, El-Khamy, and Lee 2020;</ref><ref type="bibr" target="#b13">Koizumi et al. 2020</ref>). In these works, selfattention is applied along the temporal dimension only, neglecting the global dependency inside each frame. Motivated by the spatio-temporal attention in video-related tasks, we propose to employ both frequency-wise and temporal selfattention to better capture dependencies along different dimensions. Such an attention is employed in both speech and noise branches for simultaneous modeling the two signals.</p><p>3 Proposed Method 3.1 Overview <ref type="figure" target="#fig_1">Figure 2</ref> shows the overall network structure of SN-Net. The input is the complex T-F spectrum computed by shorttime Fourier transform (STFT), denoted as X I ∈ R T ×F ×2 , where T is the number of frames and F is the number of frequency bins. There are two branches in SN-Net, one of which predicts speech and the other predicts noise. They share the same network structure but have separate network parameters. Each branch is an encoder-decoder based structure, with several RA blocks inserted inbetween them. In this way, it is capable of simultaneously mining the potential of different components of the noisy signal. Between the two branches are interaction modules designed to transform and share information. After each branch gets its output, a merge branch is employed to adaptively combine the two outputs to generate the final enhanced speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder and Decoder</head><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref> (a), the encoder has three 2-D convolutional layers, each with a kernel size of (3, 5). The stride is (1, 1) for the first layer and (1, 2) for the following two. The channel numbers are 16, 32, 64, respectively. As a result, the output feature of the encoder is F E k ∈ R T ×F ×C , where F = F 4 , C = 64 and k ∈ {S, N }. S and N denote speech and noise branches, respectively. For simplicity, the subscript k will be ignored in the following.</p><p>The decoder consists of three gated blocks followed by one 2-D convolutional layer, which reconstructs the output F D ∈ R T ×F ×2 . As shown in <ref type="figure" target="#fig_2">Figure 3</ref> (b), the gated block learns a multiplicative mask on corresponding feature from the encoder, aiming to suppress its undesired part. The masked encoder feature is then concatenated with the deconvolutional feature and fed into another 2-D convolutional layer to generate the residual representation. After three gated blocks, the final convolutional layer learns the amplitude gain and the phase for reconstruction, similar to that in <ref type="bibr" target="#b2">(Choi et al. 2019)</ref>. The kernel size for all 2-D deconvolutional layers is (3,5). The stride is (1,2) for the first two gated blocks and (1,1) for the last one. The channel numbers are 32, 16, 2, respectively. All the 2-D convolutional layers in the decoder have a kernel size of (1,1), a stride of (1,1) and a channel number the same as that of their deconv layers. All the convolutional layers in the encoder and the decoder are followed by a batch normalization (BN) and a parametric ReLU (PReLU). No down-sampling is performed along the temporal dimension to preserve the temporal resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RA Block</head><p>The RA block is designed to extract features and perform separation for both speech and noise branches. It is challenging because of the diversities of noise types and the difference between speech and noises. We employ the separable self-attention (SSA) technique to capture the global dependencies along temporal and frequency dimensions, respectively. It is intuitive to use attention for these two dimensions as humans tend to put more attention to some parts of an audio signal (e.g., speech) while less to the surrounding part (e.g., noise) and they perceive differently on different frequencies. When it comes to the speech-noise network in SN-Net, the SSA modules in speech and noise branches perceive signals differently, which will be demonstrated in the ablation study section afterwards.</p><p>In SN-Net, there are four RA blocks between the encoder and the decoder. Each block consists of two residual blocks and a SSA module, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, capturing both local and global dependencies inside the signal. Each residual block has two 2-D convolutional layers with a kernel size of (5,7), a stride of (1,1) and the same number of channels as their inputs. The output feature of two residual blocks F Res i ∈ R T ×F ×C (i ∈ {1, 2, 3, 4} represents the i th RA block and will be ignored in the following) is fed parallelly into temporal self-attention and frequency-wise selfattention blocks. These two attention blocks produce the outputs F T emp ∈ R T ×F ×C and F F req ∈ R T ×F ×C . The three features F Res , F T emp and F F req are then concatenated and fed into a 2-D convolutional layer to generate the block output F RA ∈ R T ×F ×C , used in the interaction module.</p><p>For self-attention, we employ the scaled dot-product self- </p><formula xml:id="formula_0">F k t = Reshape t (Conv(F Res )), k ∈ {K, Q, V } , SA t = Sof tmax(F Q t · (F K t ) T / C 2 × F ) · F V t , F T emp = F Res + Conv(Reshape t * (SA t )),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">F k t ∈ R T ×( C 2 ×F ) , SA t ∈ R T ×( C<label>2</label></formula><p>×F ) and F T emp ∈ R T ×F ×C , respectively. (·) denotes matrix multiplication. Reshape t (·) denotes a tensor reshape from</p><formula xml:id="formula_2">R T ×F × C 2 to R T ×( C 2 ×F )</formula><p>and Reshape t * (·) is the opposite. The frequency-wise self-attention is given by</p><formula xml:id="formula_3">F k f = Reshape f (Conv(F Res )), k ∈ {K, Q, V } , SA f = Sof tmax(F Q f · (F K f ) T / C 2 × T ) · F V f , F F req = F Res + Conv(Reshape f * (SA f )),<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">F k f ∈ R F ×( C 2 ×T ) , SA f ∈ R F ×( C 2 ×T ) and F F req ∈ R T ×F ×C , respectively. Reshape f (·) reshapes a tensor from R T ×F × C 2 to R F ×( C<label>2</label></formula><p>×T ) . In the above equations, Conv denotes a convolutional layer followed by BN and PReLU. All the convolutional layers have a kernel size of (1,1) and a stride of (1,1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Interaction Module</head><p>In SN-Net, the speech and noise branches share the same input signal, which suggests that the internal features of two branches are correlated. In light of this, we propose an interaction module to exchange information between the branches. With this block, information transformed from the noise branch is expected to enhance the speech part and counteract the noise features inside the speech branch, and vice versa. We will show in ablation study afterwards that this module plays a key role in simultaneously modeling the speech and noises.</p><p>The structure of the interaction module is shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Taking speech branch as an example, feature from the noise branch F RA N is first concatenated with that from the speech branch F RA S . They are then fed into a 2-D convolutional layer to generate a multiplicative mask M N , predicting the suppressed and preserved areas of F RA N . A residual representation H N 2S is then obtained by multiplying M N with F RA N elementally. Finally, the block adds F RA S and H N 2S to get a "filtered" version of the speech feature, which will be fed into the next RA block. The process is given by</p><formula xml:id="formula_5">F RA Sout = F RA S + F RA N * M ask(F RA N , F RA S ), F RA Nout = F RA N + F RA S * M ask(F RA S , F RA N ),<label>(3)</label></formula><p>where M ask(·) is short for concatenation, convolution and sigmoid operations. ( * ) denotes element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Merge Branch</head><p>After reconstructing the speech and noise signals in two branches, a merge module is further employed to combine the two outputs. This is done in the time domain to achieve the cross-domain benefit <ref type="bibr" target="#b12">(Kim et al. 2018</ref>). The two decoder outputs are transformed to time-domain and overlapped framed representation using the same window length as the STFT we use, resulting ins ∈ R T ×K and n ∈ R T ×K , where K is the frame size. These two representations are stacked with the noisy waveform x and fed into the merge branch. The merge network uses a 2-D convolutional layer, followed by an temporal self-attention block to capture global temporal dependency and two other convolutional layers to learn an element-wise mask m ∈ R T ×K .</p><p>The kernel size of all three convolutional layers is (3,7) and the channel number is 3, 3, 1, respectively. BN and PReLU are used after each convolutional layer except the last one. Sigmoid activation is used in the last layer. Finally, the 2D enhanced signal is obtained bŷ</p><formula xml:id="formula_6">s = m ×s + (1 − m) × (x −ñ).<label>(4)</label></formula><p>The 1D signal is reconstructed fromŝ after overlap and add.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Three public datasets are used in our experiments. DNS Challenge The DNS challenge  at Interspeech 2020 provides a large dataset for training. It includes 500 hours clean speech across 2150 speakers collected from Librivox and 60000 noise clips from Audioset <ref type="bibr" target="#b7">(Gemmeke et al. 2017)</ref> and Freesound with 150 classes. For training, we synthesized 500 hours noisy samples with SNR levels of -5dB, 0dB, 5dB, 10dB and 15dB. For evaluation, we use 150 synthetic noisy samples without reverberation inside the test set, whose SNR levels are randomly distributed between 0 dB and 20 dB.</p><p>Voice Bank + DEMAND This is a small dataset created by <ref type="bibr" target="#b33">Valentini-Botinhao et al. (Valentini-Botinhao et al. 2016)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>To evaluate the quality of the enhanced speech, the following objective measures are used. Higher scores indicate better quality.</p><p>• SSNR: Segmental SNR.</p><p>• SDR <ref type="bibr" target="#b36">(Vincent, Gribonval, and Févotte 2006)</ref>: Signal-todistortion ratio.</p><p>• PESQ <ref type="bibr" target="#b23">(Rec 2005)</ref>: Perceptual evaluation of speech quality, using the wide-band version recommended in ITU-T P.862.2 (from -0.5 to 4.5).</p><p>• CSIG <ref type="bibr" target="#b9">(Hu and Loizou 2007)</ref>: Mean opinion score (MOS) prediction of the signal distortion (from 1 to 5).</p><p>• CBAK <ref type="bibr" target="#b9">(Hu and Loizou 2007)</ref>: MOS prediction of the intrusiveness of background noises (from 1 to 5).</p><p>• COVL <ref type="bibr" target="#b9">(Hu and Loizou 2007)</ref>: MOS prediction of the overall effect (from 1 to 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Input All signals are resampled to 16kHz and clipped to 2 seconds long. We take the STFT complex spectrum as input, with a Hann window of length 20ms, a hop length of 10ms and a DFT length of 320.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>The loss function includes three terms, i.e. L = L Speech + αL N oise + βL M erge , where L Speech , L N oise and L M erge represent the loss of three branches, respectively. α and β are weighting factors balancing among the three. All terms use a mean-squre-error (MSE) loss on the power-law compressed STFT spectrum (Ephrat et al.  <ref type="bibr">2018</ref>). An inverse STFT and forward STFT are conducted on speech and noise branches before calculating the loss to ensure STFT consistency as that in <ref type="bibr" target="#b45">(Wisdom et al. 2019)</ref>.</p><p>Training The proposed algorithm is implemented in Ten-sorFlow. We use adam optimizer with a learning rate of 0.0002. All the layers are initialized with Xavier initialization. The training is conducted in two stages. The speech and noise branches are jointly trained first with the loss weight α = 1 and β = 0. Then the merge branch is trained with the parameters of previous two fixed, using only the loss L M erge . We train both stages for 60 epochs for DNS Challenge and 400 epochs for Voice Bank + DEMAND dataset.</p><p>The batch size for all experiments is set to 32, unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Objective Quality We first evaluate the effectiveness of different parts of the proposed SN-Net based on the DNS Challenge dataset. As shown in <ref type="table">Table 1</ref>, we take the speech branch without SSA as the baseline. After adding SSA to the single-branch model, we observe a 0.69 dB gain on SDR and 0.23 on PESQ. By comparing "Speech branch" with "SN-Net w/o interaction", we can see that when no interaction is employed, adding another branch with merge module at the output only marginally improves the SDR by 0.29 dB and no improvement on PESQ. After introducing the information flow, it evidently improves the SDR by 0.77 dB and PESQ by 0.11 compared to single branch. These results verify the effectiveness of the proposed RA and interaction modules for simultaneously modeling speech and noises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Information Flow</head><p>In order to further understand how the interaction module works, we visualize the input feature, the output and the feature transformed from the other branch of this module in <ref type="figure" target="#fig_5">Figure 6</ref>. An audio signal corrupted by white noises is used for illustration, whose spectrum is shown in the first column.  The transformed feature shown in <ref type="figure" target="#fig_5">Figure 6</ref> (b) is learned from the feature in (d) and added to the feature in (a), resulting in the output feature of speech branch in (c) and vice versa. Comparing (a) and (c), we can see that the speech area is better separated with noise after interaction. For noise branch, the speech part is mostly removed in (f) compared with (d). These results show that the interaction module indeed helps the simultaneous speech and noise modeling with better separation capabilities. In terms of interchanged information, the undesired speech part in (d) is counteracted by features learned from the speech branch (e.g., the second channel of the noise branch) and the undesired noise part in (a) is suppressed by features learned from the noise branch (e.g., the third channel of the speech branch). These observations comply with our previous analysis.</p><p>Visualization of Separable Self-Attention We further visualize the attention matrix to explore what it has learned. <ref type="figure" target="#fig_6">Figure 7</ref> shows the temporal self-attention matrix inside different RA blocks for the same audio signal as that in <ref type="figure" target="#fig_5">Figure  6</ref>. From (a) and (b), we can see that besides the diagonal    <ref type="figure" target="#fig_7">Figure 8</ref> shows the frequency-wise self-attention matrix for the same audio signal. For speech branch, the focus goes from low-frequency area to full frequencies and from local to global, showing that as the network goes deeper, the frequency-wise self-attention tends to capture global dependency along the frequency dimension. For noise branch, all four RA blocks show a local attention as white noises have a constant power spectral density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with the State-of-the-Art</head><p>Speech Enhancement <ref type="table" target="#tab_2">Table 2</ref> shows the comparisons with state-of-the-art methods on Voice Bank + DEMAND. SEGAN <ref type="bibr" target="#b22">(Pascual, Bonafonte, and Serra 2017)</ref> and MMSE-GAN <ref type="bibr" target="#b27">(Soni, Shah, and Patil 2018)</ref> are two GAN-based methods. PHASEN <ref type="bibr" target="#b50">(Yin et al. 2020</ref>) is a two-branch T-F domain approach where one branch predicts the amplitude and the other predicts the phase. <ref type="bibr" target="#b13">Koizumi et al. (Koizumi et al. 2020</ref>) is a multi-head self-attention based method. Our method outperforms all of them in almost all metrics. The large improvements on PESQ, CSIG and COVL indicate that our method preserves better speech quality. <ref type="table" target="#tab_3">Table 3</ref> shows the comparison with state-of-the-art methods on DNS Challenge dataset. TCNN (Pandey and Wang 2019) is a time-domain low-latency approach. We implemented two versions of it. "TCNN" is exactly the same as described in the paper and "TCNN-L" is the long-latency version using the same T-F domain loss function as ours. Conv-TasNet-SNR <ref type="bibr" target="#b14">(Koyama et al. 2020</ref>) and DTLN (West-   <ref type="bibr" target="#b1">(Choi et al. 2020)</ref> and PoCoNet <ref type="bibr" target="#b10">(Isik et al. 2020</ref>) are non-real-time methods, among which the PoCoNet took 1st place in the 2020 DNS challenge's Non-Real-Time track.</p><p>Since narrow-band PESQ number was reported in the DTLN paper, we used the released model 1 to generate the enhanced speech and compute the metrics. For other methods, we use the numbers reported in their papers. Our method outperforms all of them by a large margin.</p><p>Extension to Speaker Separation As SN-Net can simultaneously model two signals, it is natural to extend it for speaker separation task. The merge branch is removed as two outputs are needed. Permutation invariant training <ref type="bibr" target="#b51">(Yu et al. 2017</ref>) is employed during training to avoid the permutation problem. We conduct the two-speaker separation experiment based on the TIMIT corpus. The batch size is set to 16. For comparison, we train a non-causal version of Conv-TasNet <ref type="bibr" target="#b16">(Luo and Mesgarani 2019)</ref>, the state-of-the-art method, using the released code 3 . The results are shown in <ref type="table" target="#tab_5">Table 4</ref>. We use SDR improvement (SDRi) and PESQ for evaluation. Our method achieves a considerable gain on PESQ by 0.36 and SDRi by 0.82 dB, compared with Conv-TasNet. This suggests that our method is not limited to specific tasks and has the potential to extract different additive parts from a mixture signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel two-branch convolutional neural network to interactively modeling speech and noises for speech enhancement. Particularly, an interaction between two branches is proposed to leverage information learned from the other branch to enhance the target signal modeling. This interaction makes the simultaneous modeling of two signals feasible and effective. Moreover, we design a sophisticated RA block for feature extraction of both branches, which can accommodate the diversities across speech and various noise signals. Evaluations verify the effectiveness of these modules and our method significantly outperforms the state-of-the-art. The two-signal simultaneous modeling paradigm makes it applicable to speaker separation as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of different methods. (a) Most existing deep-learning-based methods directly model speech. (b) Most traditional methods predict speech with noise estimate. (c) Our method simultaneously models speech and noise with information interaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall network structure of SN-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Encoder-decoder structure. The dashed arrow denotes the separation module using RA blocks. (b) Detailed structure of the gated block inside the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Structure of the RA block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Structure of the interaction module. attention here. Considering the computational complexity, channels are reduced by half inside SSA. The temporal selfattention can be represented as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Log-scale feature visualization for the fourth interaction module. (a) Input feature of speech branch. (b) Transformed feature from noise to speech branch. (c) Output feature of speech branch. (d) Input feature of noise branch. (e) Transformed feature from speech to noise branch. (f) Output feature of noise branch. Three channels with the highest activities are visualized here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of temporal self-attention matrices from different RA blocks. (a) Speech branch. (b) Noise branch. Each matrix is linearly scaled to [0, 1]..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of frequency-wise self-attention matrices from different RA blocks. (a) Speech branch. (b) Noise branch. Each matrix is linearly scaled to [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>TIMIT Corpus This dataset is used for our speaker separation experiment. It contains recordings of 630 speakers, each reading 10 sentences and there are 462 speakers in the training set and 168 speakers in the test set. Two sentences from different speakers are mixed with random SNRs to generate mixture utterances. Shorter sentences are zero padded to match the size of longer ones. In total, the training set includes 4620 sentences and the test set 1680 sentences.</figDesc><table><row><cell>Models</cell><cell cols="2">SDR(dB) PESQ</cell></row><row><cell>Noisy</cell><cell>9.09</cell><cell>1.58</cell></row><row><cell>Speech branch w/o SSA (baseline)</cell><cell>18.06</cell><cell>3.05</cell></row><row><cell>Speech branch</cell><cell>18.75</cell><cell>3.28</cell></row><row><cell>SN-Net w/o interaction</cell><cell>19.04</cell><cell>3.29</cell></row><row><cell>SN-Net</cell><cell>19.52</cell><cell>3.39</cell></row><row><cell cols="3">Table 1: Ablation study on DNS Challenge dataset</cell></row><row><cell cols="3">Clean speech clips are collected from the Voice Bank corpus</cell></row><row><cell cols="3">(Veaux, Yamagishi, and King 2013) with 28 speakers for</cell></row><row><cell cols="3">training and another 2 unseen speakers for test. Ten noise</cell></row><row><cell cols="3">types with two artificially generated and eight real record-</cell></row><row><cell cols="3">ings from DEMAND (Thiemann, Ito, and Vincent 2013) are</cell></row><row><cell cols="3">used for training. Five other noise types from DEMAND are</cell></row><row><cell cols="3">chosen for the test, without overlapping with the training set.</cell></row><row><cell cols="3">The SNR values are 0dB, 5dB, 15dB and 20dB for training</cell></row><row><cell cols="2">and 2.5dB, 7.5dB, 12.5dB and 17.5dB for test.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quality comparisons on Voice Bank + DEMAND</figDesc><table><row><cell>Methods</cell><cell cols="2">SDR(dB) PESQ</cell></row><row><cell>Noisy</cell><cell>9.09</cell><cell>1.58</cell></row><row><cell>TCNN</cell><cell>16.86</cell><cell>2.34</cell></row><row><cell>TCNN-L</cell><cell>16.58</cell><cell>2.78</cell></row><row><cell>Conv-TasNet-SNR</cell><cell>-</cell><cell>2.73</cell></row><row><cell>DTLN</cell><cell>16.54</cell><cell>2.34</cell></row><row><cell>MultiScale+</cell><cell>-</cell><cell>2.71</cell></row><row><cell>PoCoNet</cell><cell>-</cell><cell>2.75</cell></row><row><cell>Ours</cell><cell>19.52</cell><cell>3.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Quality comparisons on DNS Challenge line, each frame shows strong attentiveness to other frames and speech and noise branches behave differently for each RA module. This is reasonable as the two branches model different signals and their focus differs. For noise branch, the attention goes from local to global as the network goes deeper. The noise branch shows wider attentiveness than the speech branch as white noises spread in all frames while speech signal occurs only at some time.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Two-speaker speech separation on TIMIT hausen and Meyer 2020) are real-time approaches. Multi-Scale+</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/breizhn/DTLN 2 Note that Conv-TasNet outputs 8khz audios. We use narrowband PESQ here instead of wide-band. Accordingly, we downsample audios to 8khz for our method to match this evaluation.3 https://github.com/kaituoxu/Conv-TasNet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Suppression of acoustic noise in speech using spectral subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00687</idno>
		<title level="m">Phaseaware Single-stage Speech Denoising and Dereverberation with U-Net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Phaseaware speech enhancement with deep complex U-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03619</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Noise prior knowledge learning for speech enhancement via gated convolutional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asia-Pacific</forename></persName>
		</author>
		<title level="m">Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="662" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Raw waveform-based speech enhancement by fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
		<imprint>
			<biblScope unit="page" from="6" to="012" />
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MMSE based noise PSD tracking with low complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4266" to="4269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluation of objective quality measures for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Helwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04470</idno>
		<title level="m">PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings, Semi-Supervised Conversational Data, and Biased Loss</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">T-GSA: Transformer with Gaussian-Weighted Self-Attention for Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6649" to="6653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-domain processing via hybrid denoising networks for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08914</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speech enhancement using selfadaptation and multi-head self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yaiabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Maxuxama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="181" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring the Best Loss Function for DNN-Based Low-latency Speech Enhancement with Temporal Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11611</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond rnns: Positional self-attention with co-attention for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8658" to="8665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised speech enhancement using nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mohammadiha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2140" to="2151" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A noise prediction and time-domain subtraction approach to deep neural network based speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">O</forename><surname>Odelowo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="372" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A study of training targets for deep neural network-based speech enhancement using noise prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">O</forename><surname>Odelowo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5409" to="5413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TCNN: Temporal convolutional neural network for real-time speech enhancement in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6875" to="6879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09452</idno>
		<title level="m">SEGAN: Speech enhancement generative adversarial network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wideband extension to recommendation P. 862 for the assessment of wideband telephone networks and speech codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Telecommunication Union</title>
		<imprint>
			<biblScope unit="volume">862</biblScope>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>CH-Geneva</orgName>
		</respStmt>
	</monogr>
	<note>P.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13981</idno>
		<title level="m">The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets, Subjective Testing Framework, and Challenge Results</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Selfattention networks for connectionist temporal classification in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7115" to="7119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="568" to="576" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Timefrequency masking-based speech enhancement using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5039" to="5043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Samuelsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Codebook driven short-term predictor parameter estimation for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Samuelsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="176" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unseen noise estimation using separable deep auto encoder for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="104" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">terspeech</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3229" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The diverse environments multi-channel acoustic noise database: A database of multichannel environmental noise recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3591" to="3591" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The voice bank corpus: Design, collection and data analysis of a large regional accent speech database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 International Conference Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dual Path Interaction Network for Video Moment Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4116" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning two-branch neural networks for image-text matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="394" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Model-based speech enhancement in the modulation domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="580" to="594" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Westhausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Meyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07551</idno>
		<title level="m">Dual-Signal Transformation LSTM Network for Real-Time Noise Suppression</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Speech denoising using nonnegative matrix factorization with priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4029" to="4032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Differentiable consistency constraints for improved deep speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="900" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video prediction with temporal-spatial attention mechanism and deep perceptual similarity branch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1594" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Weighted Speech Distortion Losses for Neural-Network-Based Real-Time Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="871" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An experimental study on speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Using Separate Losses for Speech and Noise in Mask-Based Speech Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elshamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7519" to="7523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">PHASEN: A Phase-and-Harmonics-Aware Speech Enhancement Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9458" to="9465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speakerindependent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
