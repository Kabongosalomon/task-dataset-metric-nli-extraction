<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ERNIE-ViL: Knowledge Enhanced Vision-Language Representations through Scene Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><surname>Tang</surname></persName>
							<email>tangjiji@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
							<email>yinweichong@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
							<email>tianhao@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wuhua@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
							<email>wanghaifeng@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ERNIE-ViL: Knowledge Enhanced Vision-Language Representations through Scene Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a knowledge-enhanced approach, ERNIE-ViL, which incorporates structured knowledge obtained from scene graphs to learn joint representations of vision-language. ERNIE-ViL tries to build the detailed semantic connections (objects, attributes of objects and relationships between objects) across vision and language, which are essential to vision-language cross-modal tasks. Utilizing scene graphs of visual scenes, ERNIE-ViL constructs Scene Graph Prediction tasks, i.e., Object Prediction, Attribute Prediction and Relationship Prediction tasks in the pre-training phase. Specifically, these prediction tasks are implemented by predicting nodes of different types in the scene graph parsed from the sentence. Thus, ERNIE-ViL can learn the joint representations characterizing the alignments of the detailed semantics across vision and language. After pre-training on large scale image-text aligned datasets, we validate the effectiveness of ERNIE-ViL on 5 cross-modal downstream tasks. ERNIE-ViL achieves state-of-the-art performances on all these tasks and ranks the first place on the VCR leaderboard with an absolute improvement of 3.7%. * indicates equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Motivated by pre-trained models like BERT <ref type="bibr" target="#b4">(Devlin et al. 2018</ref>) and GPT <ref type="bibr" target="#b17">(Radford et al. 2018)</ref> which have significantly improved the performance of many NLP tasks, researchers <ref type="bibr" target="#b11">Li et al. 2019a;</ref><ref type="bibr" target="#b19">Su et al. 2019;</ref><ref type="bibr" target="#b12">Li et al. 2019b;</ref><ref type="bibr" target="#b3">Chen et al. 2019</ref>) have noticed the importance of pre-training for vision-language tasks, e.g., Visual Question Answering(VQA) <ref type="bibr" target="#b2">(Antol et al. 2015)</ref> and Visual Commonsense Reasoning (VCR) <ref type="bibr" target="#b28">(Zellers et al. 2019)</ref>.</p><p>Existing vision-language pre-training methods attempt to learn joint representations through visual grounding tasks on large image-text datasets, including Masked Language Modelling based on randomly-masked sub-words, Masked Region Prediction and Image-Text Matching at the image/textlevel. However, based on randomly-masking and predicting the sub-words, current models did not distinguish common words and words describing the detailed semantics <ref type="bibr" target="#b8">(Johnson et al. 2015)</ref>, e.g., objects("man", "boat"), attributes of objects("boat is white"), relationships between objects("man standing on boat"). These methods neglect the importance of constructing detailed semantic alignments across vision and language, therefore the trained models can not well represent fine-grained semantics required by some real scenes. As shown in <ref type="figure">Figure 1</ref>, the detailed semantics are essential to distinguish the listed scenes which mainly differ in objects, attributes and relationships. Hence, better joint visionlanguage representations should characterize detailed semantic alignments across the modalities.</p><p>Inspired by the knowledge masking strategy of ERNIE , which aims at learning more structured knowledge by masking phrases and named entities rather than individual sub-words, we propose ERNIE-ViL, that incorporates knowledge obtained from scene graphs <ref type="bibr" target="#b8">(Johnson et al. 2015)</ref> to construct better representations for vision-language joint modelling. Through constructing Scene Graph Prediction tasks, ERNIE-ViL puts more emphasis on detailed semantic alignments across vision and language. Concretely, we implement these pre-training tasks by masking and predicting different types of nodes in the scene graph parsed from the sentence. By concentrating on understanding detailed semantic words rather than common words, these Scene Graph Prediction tasks force the model to extract object/attribute/relationship information from the visual modality, thus establish semantic connections between vision and language. Pre-training with the Scene Graph Prediction tasks, ERNIE-ViL learns the visionlanguage detailed semantic alignments.</p><p>We pre-train ERNIE-ViL on two large commonly-used image-text out-of-domain datasets, namely Conceptual Captions <ref type="bibr" target="#b18">(Sharma et al. 2018</ref>) and SBU Captions <ref type="bibr" target="#b16">(Ordonez, Kulkarni, and Berg 2011)</ref>. To evaluate the performance of ERNIE-ViL, we conduct experiments on various visionlanguage tasks, (1) Visual Question Answering (VQA 2.0) <ref type="bibr" target="#b2">(Antol et al. 2015)</ref>, (2) Visual Commonsense Reasoning (VCR) <ref type="bibr" target="#b28">(Zellers et al. 2019)</ref>, (3) Region-to-Phrase Grounding (RefCOCO+) <ref type="bibr" target="#b9">(Kazemzadeh et al. 2014</ref>), (4, 5) Image-text Retrieval / Text-image Retrieval (Flickr30K) <ref type="bibr" target="#b26">(Young et al. 2014</ref>). On all these tasks, ERNIE-ViL obtains significant improvements compared to those models pretrained on the same datasets. Especially, on the Region-to-Phrase grounding task that relies more heavily on detailed semantic alignments, we achieve an improvement of 2.4% on both testsets. To compare with the models pretrained on both out-(a) Objects</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Attributes (c) Relationships</head><p>A tan dog and a little girl kiss. A black dog playing with a purple toy. A man in red plaid rides his bike in a park.</p><p>The little girl is kissing the brown cat.</p><p>A black dog playing with a green toy. An older man repairing a bike tire in a park. <ref type="figure">Figure 1</ref>: Similar scene pairs from the Flick30K datasets <ref type="bibr" target="#b26">(Young et al. 2014)</ref>. It is the detailed semantics that determine the interpretation of the scenes, objects (dog, cat) in scene pair (a), attributes(purple, green) in scene pair (b) and relationships(rides, repairing) in scene pair (c).</p><p>of-domain and in-domain datasets, we continually pre-train ERNIE-ViL on MS-COCO <ref type="bibr" target="#b14">(Lin et al. 2014)</ref> and Visual-Genome <ref type="bibr" target="#b10">(Krishna et al. 2017</ref>) (in-domain datasets for downstream tasks). ERNIE-ViL achieves the state-of-the-art performances on all downstream tasks. Also ERNIE-ViL obtains the best single model performance and ranks the first place on the leaderboard with an absolute improvement of 3.7% on the Q→AR task compared to the state-of-theart performance. And our code and pre-trained models are scheduled to be public. Overall, the contributions of our method are three-folds:</p><p>• To the best of our knowledge, ERNIE-ViL is the first work that has introduced structured knowledge to enhance vision-language pre-training.</p><p>• ERNIE-ViL constructs Scene Graph Prediction tasks during the pre-training of vision-language joint representations, putting more emphasis on the cross-modal detailed semantics alignments.</p><p>• ERNIE-ViL achieves state-of-the-art performances on 5 downstream cross-modal tasks and ranks the first place on the VCR leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-modal Pre-training</head><p>Inspired by text pre-training models <ref type="bibr" target="#b4">(Devlin et al. 2018)</ref>, many cross-modal pre-training models for vision-language have been proposed. These researchers put their efforts mainly on three aspects, which are model architecture, pretraining tasks and pre-training data.</p><p>• Model Architecture Current works are based on different variables of Transformers <ref type="bibr" target="#b23">(Vaswani et al. 2017</ref>). Most of them <ref type="bibr" target="#b11">(Li et al. 2019a;</ref><ref type="bibr" target="#b19">Su et al. 2019;</ref><ref type="bibr" target="#b30">Zhou et al. 2019;</ref><ref type="bibr" target="#b12">Li et al. 2019b;</ref><ref type="bibr" target="#b6">Huang et al. 2020</ref>) used a uniform crossmodal Transformer modelling both image and text representations, while the others like ViLBERT ) and LXMERT (Tan and Bansal 2019) were based on two-stream cross-modal Transformers, which brings more specific representations for images and texts.</p><p>• Pre-training Tasks Inspired by the pre-training tasks in text models, Masked Language Model and similar Masked Region Prediction tasks ) are utilized in cross-modal pre-training. And similar to Next-Sentence Prediction, Image-Text Matching <ref type="bibr" target="#b19">Su et al. 2019;</ref><ref type="bibr" target="#b3">Chen et al. 2019</ref>) task is also widely used. However, based on randomly masking and predicting sub-words, these methods did not distinguish the common words and words describing the detailed semantics. Hence, the cross-modal fine-grained semantic alignments cannot be well characterized in those learned joint representations.</p><p>• Pre-training Data Unlike text pre-training models that can leverage tremendous natural language data, visionlanguage tasks require high-quality aligned image-text data that are hard to obtain. Conceptual Captions <ref type="bibr" target="#b18">(Sharma et al. 2018</ref>) and SBU Captions <ref type="bibr" target="#b16">(Ordonez, Kulkarni, and Berg 2011)</ref> are two widely-used datasets for image-text pre-training, with 3.0M and 1.0M image-description pairs respectively. These two datasets are out-of-domain for vision-language downstream tasks, while some existing works <ref type="bibr" target="#b3">(Chen et al. 2019;</ref><ref type="bibr" target="#b6">Huang et al. 2020</ref>) incorpate indomain datasets, such as MS-COCO and Visual-Genome, that are highly correlated with downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Graph</head><p>Scene graphs contain structured knowledge of visual scenes, including the present objects, attributes of objects, and relationships between objects. As a beneficial prior knowledge </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-stream Cross-modal Transformers</head><p>Figure 2: Illustration of Scene Graph Prediction tasks for ERNIE-ViL. Given detected regions of the image and token sequence of the text, ERNIE-ViL uses a two-stream cross-modal Transformers network to model the joint vision-language representations. Based on the scene graph parsed from the text using Scene Graph Parser, we construct Object Prediction, Attribute Prediction and Relationship Prediction tasks to learn cross-modal detailed semantics alignments.</p><p>describing the detailed semantics of images and captions, scene graphs have led to many state-of-the-art models in image captioning <ref type="bibr" target="#b25">(Yang et al. 2019</ref>), image retrieval , VQA (Zhang, Chao, and Xuan 2019) and image generation <ref type="bibr" target="#b7">(Johnson, Gupta, and Fei-Fei 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>In this section, we first introduce the architecture of ERNIE-ViL. Then we illustrate our newly-proposed Scene Graph Prediction tasks. Finally, pre-training with Scene Graph Prediction tasks in ERINE-ViL is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architecture</head><p>The vision-language model aims at learning the joint representations that integrates information of both modalities and the alignments across the modalities. The inputs of ERNIE-ViL are a sentence and an image. Given a sequence of words and an image, we introduce the methods to embed the inputs to the feature space and the vision-language encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Embedding</head><p>We adopt the similar word preprossessing method as BERT. The input sentence is tokenized into sub-word tokens using WordPiece approach. Special tokens such as [CLS] and [SEP] are also added to the tokenized text sequence to form the text sequence as</p><formula xml:id="formula_0">{[CLS], w1, . . . w T , [SEP]}.</formula><p>The final embedding for each sub-word token is generated by combining its original word embedding, segment embedding and sequence position embedding.</p><p>Image Embedding For the image, we first use a pretrained object detector to detect the salient image regions from the image. The pooling features before multi-class classification layer are utilized as the region features. We also encode the location features for each region via a 5-</p><formula xml:id="formula_1">dimensional vector ( x1 W , y1 H , x2 W , y2 H , (y2−y1)(x2−x1) W H</formula><p>) for the region position and the fraction of image area covered, where (x 1 , y 1 ) and (x 2 , y 2 ) denote the coordinates of topleft and bottom-right corner while W and H are the width and height of the input image. The location vectors are projected to form the location features, which are then summed with the region visual features. We also add a special feature [IMG] that denotes the representation of the entire image (i.e. mean-pooled visual features with a spatial encoding corresponding to the entire image) to form the final region sequence {[IMG], v 1 , . . . , v I }.</p><p>Vision-Language Encoder Given the embedding of image regions and the words for the sentence </p><formula xml:id="formula_2">{[IMG], v 1 , . . . , v I , [CLS], w1, . . . w T ; [SEP]},</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Graph Prediction</head><p>Detailed semantics, includes objects, attributes of objects, and relationships between objects, are essential to the understanding of visual scenes <ref type="bibr" target="#b8">(Johnson et al. 2015)</ref>. As the scene shown in <ref type="figure">Figure 2</ref>, detailed semantics describes the vi-sual scene from different aspects. The objects, such as "cat", "car", "woman" are the fundamental elements in the scene. And associated attributes, such as "little", "brown", "blue" characterize shape and color of objects. Relationships such as "on top of", "putting" represent the spatial connections and actions between objects. Therefore detailed semantics are crucial in accurately understanding visual scenes. Since the goal of vision-language joint representations is to engrave the semantic connections across modalities, detailed semantic alignments are significantly important in crossmodal learning.</p><p>Scene graphs encode various fine-grained semantic information. Utilizing structured knowledge obtained from scene graphs, ERNIE-ViL learns the cross-modal detailed semantic alignments. As shown in <ref type="figure">Figure 2</ref>, according to the scene graph parsed from the text, we construct the corresponding Scene Graph Prediction tasks, including Object Prediction task, Attribute Prediction task, and Relationship Prediction task. These tasks force ERNIE-ViL to model the correlations of detailed semantics across modalities. For example, as the relationship words "on top of" is masked, based on the language context, the model may predict that the missing word is "under" or "into". These words are grammatically fluent in the sentence, but are inconsistent with the scene "the cat is on top of the car". Through training the Relationship Prediction task, the model obtains the spatial relation of the corresponding objects("car", "cat") from the image, thus can accurately predict that the missing word is "on top of". Through constructing Scene Graph Prediction tasks, ERNIE-ViL learns cross-modal detailed semantic alignments.</p><p>Scene graph parsing Given the text sentence w, we parse it into a scene graph <ref type="bibr" target="#b8">(Johnson et al. 2015)</ref>, which denotes as</p><formula xml:id="formula_3">G(w) =&lt; O(w), E(w), K(w) &gt;, where O(w) is the set of objects mentioned in w, E(w) ⊆ O(w) × R(w) × O(w)</formula><p>is the set of hyper-edges representing relationship triplets, and R(w) is the set of relationship nodes between object nodes. K(w) ⊆ O(w) × A(w) is the set of attribute pairs, where A(w) is the set of attribute nodes associated with object nodes. Scene graphs describe the objects in more details with various associated attributes and relationships between objects. Thus integrating the knowledge of scene graphs can benefit learning more fine-grained joint representations for the vision-language. In this paper, the Scene Graph Parser provided by Anderson <ref type="bibr" target="#b0">(Anderson et al. 2016</ref>) is adopted to parse texts to scene graphs. For a more intuitive understanding, we illustrate a specific case for the parsed scene graph from the text in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Object Prediction Objects are the dominant elements of visual scenes, thus playing an important role in constructing the representations of semantic information. Predicting the objects forces the model to build the vision-language connections at object level.</p><p>Firstly, for all the object nodes in the scene graph, we randomly select 30% of them to mask. And for each selected object node O(w), we replace it with the special token [MASK] in probability of 80%, another random token in sentence: w A woman in blue dress is putting her little white cat on top of a brown car in front of her house. objects:O(w) dress, woman, cat, car, house relationships:R(w) in, putting, on-top-of, in-front-of attributes: A(w) blue, white, little, brown probability of 10%, and keep it in probability of 10%. Note that the objects actually correspond to the sub-sequences of text in the sentence, therefore the object masking are implemented by masking the corresponding sub-sequences in the text. For Object Prediction, ERNIE-ViL recover these masked object tokens, denoted as w oi , based on their surrounding words w and all image regions v, by minimizing the negative log-likelihood:</p><formula xml:id="formula_4">L obj (θ) = −E (w,v)∼D log(P (w oi |w \wo i , v)) (1)</formula><p>Attribute Prediction Attributes characterize the specific information of the visual objects, such as color or shape of the objects, therefore representing the detailed information in the visual scenes in more fine-grained level.</p><p>Similarly, we randomly select 30% of the attribute pairs in the scene graph, and the mask strategy here is the same as that in Object Prediction. Since the attribute nodes in the scene graph are attached to objects, we keep the associated object while masking out the attribute node A(w) in each selected K(w) ⊆ O(w) × A(w).</p><p>Given object words w oi in attribute pair w oi , w ai , Attribute Prediction is to recover the masked tokens w ai of attribute pairs. Based on the object tokens w oi , other surrounding words w and all image regions v, Attribute Prediction minimizes the negative log-likelihood:</p><formula xml:id="formula_5">L attr (θ) = −E (w,v)∼D log(P (w ai |w oi , w \wa i , v)) (2)</formula><p>Relationship Prediction Relationships describe the actions (semantic) or relative position (geometry) between the objects of the visual scenes, which contributes to distinguish scenes with same objects but different relationships.</p><p>Thus, ERNIE-ViL constructs the Relationship Prediction task to learn cross-modal relationships connections. When performing the mask strategy of selected relationship triplets E(w) ⊆ O(w) × R(w) × O(w), we keep the objects and mask out the relationship node R(w). Specifically, given object tokens w oi1 , w oi2 in relationship triplet w oi1 , w ri , w oi2 , this task recovers the masked relationship tokens, predicting the probability for each masked relation tokens w ri . Thus the context for the prediction is the given object tokens w oi1 , w oi2 , other surrounding words from the text and all image regions v. The loss for this task is: Implementation Details For each image-text pair in the training, the pre-processing is performed as follows. For the image, we adopt Faster R-CNN <ref type="bibr" target="#b1">(Anderson et al. 2018)</ref> to select salient image regions and extract region features. Specifically, regions with class detection probability exceeds a confidence threshold of 0.2 are selected and 10 to 36 boxes are kept. And for each kept region, the mean-pooled convolutional representation is used as the region feature. For the text, we parse the scene graph from the sentence using the Scene Graph Parser and adopt WordPieces to tokenize the sentence similar to BERT. For the masking strategies, we randomly mask 15% of tokens, 30% of scene graph nodes, and 15% of image regions. For the Image-text Matching task, we randomly select a image for each text to form the negative image-text pair. Note that only items in the positive pairs will be considered for token and region prediction tasks.</p><formula xml:id="formula_6">L rel (θ) = −E (w,v)∼D log(P (w ri |w oi1 , w oi2 , w \wr i , v))<label>(3)</label></formula><p>We train ERNIE-ViL on two scale settings: ERNIE-ViLbase and ERNIE-ViL-large, which mainly differ in model depth of the text stream. The detailed settings of text and visual streams are shown in <ref type="table" target="#tab_5">Table 2</ref>. And similar to Vil-BERT , cross-transformers are used to co-at tent the two streams. We initialize the text stream parameters from ERNIE 2.0 , and implement ERNIE-ViL via PaddlePaddle. After then, ERINE-ViL is pre-trained on a total batch size of 512 for 700k steps on 8 V100 GPUs, using adam optimizer with initial learning rates of 1e-4 and Noam <ref type="bibr" target="#b23">(Vaswani et al. 2017)</ref> as learning rate decay schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downstream Tasks</head><p>Visual Commonsense Reasoning (VCR) The Visual Commonsense Reasoning (VCR) <ref type="bibr" target="#b28">(Zellers et al. 2019</ref>) task contains two sub-tasks: visual question answering (Q→A)  and answer justification (QA→R), which are both multiple choice problems. The holistic setting (Q→AR) requires both the chosen answer and chosen rationale to be correct. In visual question answering (Q→A) task, we concatenate the question and each candidate answer for the language modality. We take dot product of final hidden state h <ref type="bibr">[CLS]</ref> and h <ref type="bibr">[IMG]</ref> to predict matching score with an additional FC layer. For the answer justification (QA→R) task, we concatenate the question, the answer and each candidate rationale as the input of the text stream. Similar with UNITER <ref type="bibr" target="#b3">(Chen et al. 2019</ref>), a second-stage pre-training is adopted on VCR dataset. And then we fine-tune the model over 6 epochs with a batch size of 64 and adopt Adam optimizer with initial learning rate of 1e-4.</p><p>Visual Question Answering (VQA) The VQA task requires answering natural language questions according to images. VQA 2.0 dataset <ref type="bibr" target="#b2">(Antol et al. 2015)</ref> contains 204k images and 1.1M questions about these images. Also additional question-answer pairs from Visual Genome are used for data augmentation as in UNITER <ref type="bibr" target="#b3">(Chen et al. 2019</ref>). We treat VQA as a multi-label classification task -assigning a soft target score to each answer based on its relevancy to the 10 human answer responses. We take dot product of final hidden state h <ref type="bibr">[CLS]</ref> and h <ref type="bibr">[IMG]</ref> to map this representation into 3,129 possible answers with an additional twolayer MLP. Fine-tuning of VQA model is performed over 12 epochs on batch size of 256 and using Adam optimizer with initial learning rate of 1e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grounding Referring Expressions</head><p>The referring expression task is to localize an image region given a natural language reference. We evaluate the task on RefCOCO+ dataset <ref type="bibr" target="#b9">(Kazemzadeh et al. 2014)</ref>. Bounding box proposals provided by Mattnet <ref type="bibr" target="#b27">(Yu et al. 2018</ref>) are utilized. The representation for each region is denoted by its final hidden state h vi with an additional FC layer. Each region i is labelled as positive only when the IoU between it and the ground truth box is over 0.5. We fine-tune the model over 20 epochs with a batch size of 256 and adopt Adam optimizer with initial learning rate of 1e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Retrieval &amp; Text Retrieval</head><p>Caption-based image retrieval is a task of identifying an image from a pool based on a caption describing its content. Flickr30K <ref type="bibr" target="#b26">(Young et al. 2014</ref>) contains 31,000 images and 5 captions for each image. Adopting the same split in ViLBERT , we use each of 1,000 images for validation and for testing and the rest for training. We take dot product of final hidden state of h <ref type="bibr">[CLS]</ref> and h <ref type="bibr">[IMG]</ref> to predict matching score  s(w, v) for each image-text pair with an additional FC layer. We utilize circle loss <ref type="bibr" target="#b20">(Sun et al. 2020)</ref> with 20 random negative samples for each image-text pair. We trained 40 epochs using Adam optimizer with a initial learning rate 1e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare ERNIE-ViL against other cross-modal pretraining models and the results are illustrated in <ref type="table" target="#tab_7">Table 3</ref>. Among the methods pre-trained on the same out-ofdomain datasets (CC and SBU), ERNIE-ViL obtains the best performances on all 5 downstream tasks. For the visual reasoning tasks, ERNIE-ViL-large achieves a significant improvement of 6.60% on VCR (Q→AR) task and 1.74% on VQA (test-std) task compared with VLBERT-large. On visual grounding task, ERNIE-ViL-large obtains an improvement of 2.40% for both testA split and testB split on Re-fCOCO+ task compared to VLBERT-large. On the crossmodal retrieval tasks, where no large models pre-trained on out-of-domain datasets has released results, ERNIE-ViLbase achieves an imporvement of 2.94% on R@1 for image retrieval and 0.50% on R@1 for text retrieval compared with Unicoder-VL-base.</p><p>For further comparison with those models pretrained with both out-of-domain and in-domain datasets, we pre-train ERINE-ViL with all these datasets. As illustrated in <ref type="table" target="#tab_7">Table  3</ref>, ERINE-ViL-large acheives state-of-the-art performances on these tasks compared to existing works, e.g., UNITER, OSCAR ) and VILLA <ref type="bibr" target="#b5">(Gan et al. 2020</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>Effectiveness of Scene Graph Prediction tasks To verify the effectiveness of Scene Graph Prediction (SGP) tasks, we first conduct experiments with ERNIE-ViL-base settings based on the text parameters initialized from BERT. As illustrated in <ref type="table" target="#tab_9">Table 4</ref>, pre-training with SGP tasks in ERNIE-ViL brings significant improvements across all downstream tasks. Especially on Grounding Referring Expressions and Retrieval tasks, those require understanding detailed semantics alignments, SGP tasks make an improvement of 0.69% accuracy on RefCOCO+ and 2.22% of R@1 for image retrieval on Flickr30K.</p><p>Note that text parameter initialized from ERNIE 2.0 can lead to further improvements on all tasks and a relatively large improvement on VCR task. We considere that through continually learning on various pre-training tasks, ERNIE 2.0 learned more common sense knowledge which benefits the VCR task.</p><p>Overall, the SGP tasks significantly contribute to the    Cloze Test To get a more intuitively understanding of the improvements brought by SGP tasks, we conduct the language cloze test conditioned on the visual modality. In the cloze test, language tokens represent detailed semantics (objects, attributes and relationships) are masked from the text and the model is required to infer them with the context from both the text and the image. To construct the dataset, we sampled 15,000 image-text pairs from Flickr30K dataset and 5,000 objects, attributes and relationships tokens each are selected. For the prediction, the top one accuracy (ACC@1) and top five accuracy (ACC@5) are adopted as the evaluation metric. The comparison of prediction results between two models, which are pre-trained models with SGP task and without SGP task, are illustrated in <ref type="table" target="#tab_11">Table 6</ref>. The textstream parameters of both models are initialized from BERT. An absolute improvement of 1.20% for objects, 3.08% for relationships and 1.84% for attributes on ACC@1 demonstrates that ERNIE-ViL pre-trained with SGP tasks learns better cross-modal detailed semantics alignments. Moreover, we illustrate some cases in <ref type="table" target="#tab_10">Table 5</ref>, and the top 5 possible predictions are shown in the right columns. As in case 1-2, model pre-trained without SGP tasks cannot make the right predictions as it didn't learn accurate alignments of detailed semantics, without distinguishing common words and detailed semantics words while pre-training. While in case 3, the model can predict the reasonable tokens but with lower confidence compared with model pre-trained with SGP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We proposed ERNIE-ViL to learn the joint representations of vision and language. In addition to conventional MLM for cross-modal pre-training, we introduce Scene graph Prediction tasks to characterize the cross-modal detailed semantic alignments. Experiment results on various downstream tasks demonstrate the improvements of incorporating structured knowledge obtained from scene graphs during cross-modal pre-training. For future work, scene graphs extracted from images could also be incorporated into cross-modal pretraining. Moreover, Graph Neural Networks that integrate more structured knowledge could be considered as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>we use two-stream cross-modal Transformers to joint model the intra-modal and inter-modal representations. Similar to ViLBERT (Lu et al. 2019), ERNIE-ViL consists of two parallel Transformer encoders for image and text segments, which are cross-attended with cross-modal Transformer blocks. The model outputs embeddings for each input of both the image and text. We take h [IMG] and h [CLS] as the holistic image and text representations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The scene graph parsed from the caption of the visual scene. For simplicity, we only list all the nodes leaving out the connections between them.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Pre-training with Scene Graph PredictionSimliar to ViLBERT, ERNIE-ViL also adopts Masked Language Modelling(MLM) to capture the syntactic and lexical information in the text. Moreover, Masked Region Prediction and Image-text Matching are utilized for visual modality and cross-modality respectively. The losses for all these pre-training tasks are summed.</figDesc><table><row><cell>Experiments</cell></row><row><cell>Training ERNIE-ViL</cell></row><row><cell>Pre-training Data We use the Conceptual Captions (CC)</cell></row><row><cell>dataset (Sharma et al. 2018) and SBU Captions (SBU)</cell></row><row><cell>dataset (Ordonez, Kulkarni, and Berg 2011) as pre-training</cell></row><row><cell>data. CC is a collection of 3.3 million image-caption pairs</cell></row><row><cell>automatically scraped from alt-text enabled web images and</cell></row><row><cell>SBU is a similar vision-language dataset which has 1.0 mil-</cell></row><row><cell>lion image-caption pairs. Since some links have become bro-</cell></row><row><cell>ken, only about 3.0 million pairs for CC dataset and 0.8 mil-</cell></row><row><cell>lion pairs for SBU dataset are available and utilized in our</cell></row><row><cell>experiments. Note that CC and SBU are image-caption pairs</cell></row><row><cell>automatically collected from the web and have no intersec-</cell></row><row><cell>tions with the down-stream task datasets, thus act as out-of-</cell></row><row><cell>domain datasets for training vision-language models.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Settings for ERNIE-ViL model. L: number of layers, H : hidden size, A : number of self-attention heads, F : feed-forward/filter size.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Results of downstream vision-language tasks for ERNIE-ViL model, compared with previous state-of-the-art pretrained models. IR: Image Retrieval. TR: Text Retrieval. For VCR task which has private test set, we only report the test results (in parentheses) for ERNIE-ViL models pre-trained on out-of-domain datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Results of downstream vision-language tasks for ERNIE-ViL pre-trainging with/without Scene Graph Prediction (SGP) tasks, and using different text stream parameters initialization. IR &amp; TR: image retrieval &amp; text retrieval on Flickr30K.</figDesc><table><row><cell>Image</cell><cell>Text</cell><cell>with SGP task</cell><cell>without SGP task</cell></row><row><cell>1</cell><cell>a black dog about to catch a flying disc .</cell><cell></cell><cell></cell></row><row><cell></cell><cell>two men wearing red jack-</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ets are looking out over</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>some water and one man</cell><cell></cell><cell></cell></row><row><cell></cell><cell>has yellow earphones on his</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ears .</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>a little boy in a green shirt kicks a ball</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Examples of cloze test predictions for ERNIE-ViL pre-training with and without SGP tasks. Masked token are colored in bold and red. The probabilities of the top 5 predictions, denoted as the light purple bars, are listed in the right columns.</figDesc><table><row><cell>Nodes</cell><cell cols="2">without SGP tasks ACC@1 ACC@5</cell><cell cols="2">with SGP tasks ACC@1 ACC@5</cell></row><row><cell>objects</cell><cell>57.14</cell><cell>79.22</cell><cell>58.34</cell><cell>80.80</cell></row><row><cell>attributes</cell><cell>44.32</cell><cell>67.58</cell><cell>46.16</cell><cell>70.30</cell></row><row><cell>relationships</cell><cell>47.57</cell><cell>68.10</cell><cell>50.65</cell><cell>71.54</cell></row><row><cell>overall</cell><cell>49.75</cell><cell>71.75</cell><cell>51.75</cell><cell>74.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Cloze test results for ERNIE-ViL. An improvement of 2.0% on overall ACC@1 between models with/without SGP tasks.state-of-the-art results of ERNIE-ViL.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<title level="m">Large-Scale Adversarial Training for Visionand-Language Representation Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<title level="m">Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Oscar: Objectsemantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openaiassets/research-covers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10857</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ernie 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12412</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6609" to="6618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Autoencoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1307" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6720" to="6731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An empirical study on leveraging scene graphs for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12133</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11059</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
