<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Multi-Label Zero-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshita</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mohamed Bin Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Fahad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mohamed Bin Zayed University of AI</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Universitat Aut√≤noma de Barcelona</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Multi-Label Zero-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-label zero-shot learning strives to classify images into multiple unseen categories for which no data is available during training. The test samples can additionally contain seen categories in the generalized variant. Existing approaches rely on learning either shared or labelspecific attention from the seen classes. Nevertheless, computing reliable attention maps for unseen classes during inference in a multi-label setting is still a challenge. In contrast, state-of-the-art single-label generative adversarial network (GAN) based approaches learn to directly synthesize the class-specific visual features from the corresponding class attribute embeddings. However, synthesizing multi-label features from GANs is still unexplored in the context of zero-shot setting. In this work, we introduce different fusion approaches at the attribute-level, featurelevel and cross-level (across attribute and feature-levels) for synthesizing multi-label features from their corresponding multi-label class embeddings. To the best of our knowledge, our work is the first to tackle the problem of multilabel feature synthesis in the (generalized) zero-shot setting. Comprehensive experiments are performed on three zeroshot image classification benchmarks: NUS-WIDE, Open Images and MS COCO. Our cross-level fusion-based generative approach outperforms the state-of-the-art on all three datasets. Furthermore, we show the generalization capabilities of our fusion approach in the zero-shot detection task on MS COCO, achieving favorable performance against existing methods. The source code is available at https: //github.com/akshitac8/Generative_MLZSL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-label classification is a challenging problem where the task is to recognize all labels in an image. Typical examples of multi-label classification include, MS COCO <ref type="bibr" target="#b21">[22]</ref> and NUS-WIDE <ref type="bibr" target="#b4">[5]</ref> datasets, where an image may contain several different categories (labels). Most recent multilabel classification approaches address the problem by utilizing attention mechanisms <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>, recurrent neural networks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25]</ref>, graph CNNs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4]</ref> and label correlations <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b5">6]</ref>. However, these approaches do not tackle the problem of multi-label zero-shot classification, where the task is to classify images into multiple new "unseen" categories at test time, without being given any corresponding visual example during the training. Different from zeroshot learning (ZSL), the test samples can belong to the seen or unseen classes in generalized zero-shot learning (GZSL). Here, we tackle the challenging problem of largescale multi-label ZSL and GZSL.</p><p>Existing multi-label (G)ZSL approaches address the problem by utilizing global image representations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48]</ref>, structured knowledge graphs <ref type="bibr" target="#b19">[20]</ref> and attention-based mechanisms <ref type="bibr" target="#b13">[14]</ref>. In contrast to the multi-label setting, single-label (generalized) zero-shot learning, where an image contains at most one category label, has received significant attention <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. Stateof-the-art single-label (G)ZSL approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b25">26]</ref> are generative in nature. These approaches exploit the power of generative models, such as generative adversarial networks (GANs) <ref type="bibr" target="#b10">[11]</ref> and variational autoencoder (VAE) <ref type="bibr" target="#b16">[17]</ref> to synthesize unseen class features. Typically, a feature synthesizing generator is utilized to construct single-label features. The generative approaches currently dominate single-label ZSL due to their ability to synthesize unseen class (fake) features by learning the underlying data distribution of seen classes (real) features. Nevertheless, the generator only synthesizes single-label features in the existing ZSL frameworks. To the best of our knowledge, the problem of designing a feature synthesizing generator for multi-label ZSL paradigm is yet to be explored.</p><p>In this work, we address the problem of multi-label (generalized) zero-shot learning by introducing an approach based on the generative paradigm. When designing a generative multi-label zero-shot approach, the main objective is to synthesize semantically consistent multi-label visual features from their corresponding class attributes (embeddings). Multi-label visual features can be synthesized in two ways. (i) One approach is to integrate class-specific attribute embeddings at the input of the generator to produce a global image-level embedding vector. We call this approach <ref type="figure">Figure 1</ref>. A conceptual illustration of our three fusion approaches: attribute-level (ALF), feature-level (FLF) and cross-level feature (CLF), on an example image with three classes. Given an image, a feature extractor F extracts real visual features (green diamond) in X . ALF integrates the individual class-specific embeddings to generate a global image-level embedding (red diamond) in E. The image-level embedding is then used to synthesize the multi-label features (blue diamond). Different from ALF, the FLF synthesizes class-specific features and integrates them in the feature space X . These integrated features are shown as the yellow diamond. Our CLF takes as input synthesized features from ALF and FLF and enriches each respective feature by taking guidance from the other. The resulting enriched features are integrated to obtain the final image-level feature representation (purple diamond). The final image representation combines the advantage of label dependency of ALF and class-specific discriminability of FLF.</p><p>attribute-level fusion (ALF). Here, the image-level embedding represents the holistic distribution of the positive labels in the image (see <ref type="figure">Fig. 1</ref>). Since the generator in ALF performs global image-level feature generation, it is able to better capture label dependencies (correlations among the labels) in an image. However, such a feature generation has lower class-specific discriminability since the discriminative information with respect to the individual classes is not explicitly encoded. (ii) A second approach is to synthesize the features from the class-specific embeddings individually and then integrate them in the visual feature space. We call this approach feature-level fusion (FLF), as shown in <ref type="figure">Fig. 1</ref>. Although FLF better preserves the class-specific discriminative information in the synthesized features, it does not explicitly encode the label dependencies in an image due to synthesizing features independently of each other. This motivates us to investigate an alternative fusion approach for multi-label feature synthesis.</p><p>In this work, we introduce an alternative fusion approach that combines the advantage of label dependency of ALF and the class-specific discriminability of FLF, during multilabel feature synthesis. We call this approach as cross-level feature fusion (CLF), as in <ref type="figure">Fig. 1</ref>. The CLF approach utilizes each individual-level feature and attends to the bi-level context (from ALF and FLF). As a result, individual-level features adapt themselves to produce enriched synthesized features, which are then pooled to obtain the CLF output. In addition to multi-label zero-shot classification, we investigate the proposed multi-label feature generation CLF approach for (generalized) zero-shot object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>We propose a generative approach for multi-label (generalized) zero-shot learning. To the best of our knowledge, we are the first to explore the problem of multi-label feature synthesis in the zero-shot setting. We investigate three different fusion approaches (ALF, FLF and CLF) to synthesize multi-label features. Our CLF approach combines the advantage of label dependency of ALF and the class-specific discriminability of FLF. Further, we integrate our fusion approaches into two representative generative architectures: f-CLSWGAN <ref type="bibr" target="#b41">[42]</ref> and f-VAEGAN <ref type="bibr" target="#b42">[43]</ref>.</p><p>We evaluate our (generalized) zero-shot classification approach on three datasets: NUS-WIDE <ref type="bibr" target="#b4">[5]</ref>, Open Images <ref type="bibr" target="#b18">[19]</ref> and MS COCO <ref type="bibr" target="#b21">[22]</ref>. Our CLF approach achieves consistent improvement in performance over both ALF and FLF. Furthermore, CLF outperforms state-of-the-art methods on all datasets. On the large-scale Open Images dataset, our CLF achieves absolute gains of 18.7% and 17.6% over the state-of-the-art in terms of GZSL F1 score at top-K predictions, K ‚àà {10, 20}, respectively. In addition to classification, we evaluate CLF for (generalized) zero-shot object detection, achieving favorable results against existing methods on MS COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Generative Single-Label Zero-Shot Learning</head><p>As discussed earlier, state-of-the-art single-label zeroshot approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref> are generative in nature, utilizing the power of generative models (e.g., GAN <ref type="bibr" target="#b10">[11]</ref>, VAE <ref type="bibr" target="#b16">[17]</ref>) to synthesize unseen class features. Here, each image is assumed to have a single object category label (e.g., CUB <ref type="bibr" target="#b38">[39]</ref>, FLO <ref type="bibr" target="#b26">[27]</ref> and AWA <ref type="bibr" target="#b40">[41]</ref>). Problem Formulation: Let x ‚àà X denote the encoded feature instances of images and y ‚àà Y s the corresponding class labels from the set of S seen class labels Y s = {y 1 , . . . , y S }. Let Y u = {y S+1 , . . . , y C } denote the set of U unseen classes, which is disjoint from the seen classes Y s . Here, the total number of seen and unseen classes is denoted by C=S+U . The relationships among all the seen and unseen classes are described by the category-specific semantic embeddings e(k) ‚àà E, ‚àÄk ‚àà Y s ‚à™Y u . To learn the ZSL and GZSL classifiers, existing single-label GAN-based approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b42">43]</ref> first learn a generator using the seen class features x s and corresponding class embeddings e(y s ). Then, the learned generator and the unseen class embeddings e(y u ) are used to synthesize the unseen class featuresx u . The resulting synthesized featuresx u , along with the real seen class features x s , are further deployed to train the final classifiers f z : X ‚ÜíY U and f gz : X ‚ÜíY C . Next, we briefly describe the feature synthesis stage employed in existing single-label GAN-based zero-shot frameworks, to generate unseen class features.</p><p>Typically, GAN-based zero-shot classification frameworks utilize a feature synthesizing generator G : Z√óE‚ÜíX and a discriminator D. Both G and D compete against each other in a two player minimax game. While D attempts to accurately distinguish real image features x from generated featuresx, G attempts to fool D by generating features that are semantically close to real features. Since class-specific features are to be synthesized, a conditional Wasserstein GAN <ref type="bibr" target="#b1">[2]</ref> is employed due to its more stable training, by conditioning both G and D on the embeddings e(y). Here, G learns to synthesize class-specific features x from the corresponding single-label embeddings e(y), given byx = G(z, e(y)). Nevertheless, the generator only synthesizes single-label features in existing zero-shot learning frameworks. To the best of our knowledge, the problem of designing a feature synthesizing generator for the multilabel zero-shot learning paradigm is yet to be investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generative Multi-Label Zero-Shot Learning</head><p>As discussed earlier, most real-world tasks involve multi-label recognition, where an image can contain multiple and wide range of category labels (e.g., MS COCO <ref type="bibr" target="#b21">[22]</ref>, NUS-WIDE <ref type="bibr" target="#b4">[5]</ref> and Open Images <ref type="bibr" target="#b18">[19]</ref>). The multi-label classification problem becomes more challenging in the zero-shot learning setting, where the test set either contains only unseen classes (ZSL) or both seen and unseen classes (GZSL). In this work, we propose a generative multi-label zero-shot learning approach that exploits the capabilities of generative models to learn the underlying data distribution of seen classes. This helps to mimic the fully-supervised setting by synthesizing (fake) features for unseen classes. While generative approaches have been extensively studied for single-label zero-shot learning, we are the first to address the problem of multi-label feature synthesis in the multi-label (generalized) zero-shot setting. Next, we describe the problem formulation of generative multi-label (generalized) zero-shot learning. Problem Formulation: In contrast to single-label zeroshot learning, here, x ‚àà X denotes the encoded feature instances of multi-label images and y ‚àà {0, 1} S the corresponding multi-hot labels from the set of S seen class labels Y s . Let x denote the multi-label feature instance of an image and y, its multi-hot label with n positive classes in the image. Then, the set of attribute embeddings for the image can be denoted as e(y) = {e(y j ), ‚àÄj : y[j] = 1}, where |e(y)| = n. Here, we use GloVe <ref type="bibr" target="#b28">[29]</ref> vectors of the class names as the attribute embeddings e(y j ), as in <ref type="bibr" target="#b13">[14]</ref>. Now, the generator's task is to learn to synthesize the multilabel features x from the set of associated embedding vectors e(y). Post-training of G, multi-label features corresponding to the unseen classes are synthesized. The resulting synthesized features along with the real seen class features are deployed to train the final ZSL/GZSL classifiers f z : X ‚Üí{0, 1} U and f gz : X ‚Üí{0, 1} C . Next, we investigate three different approaches to synthesize unseen multi-label features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generative Multi-Label Feature Synthesis</head><p>To synthesize multi-label features, we introduce three fusion approaches: attribute-level fusion (ALF), feature-level fusion (FLF) and cross-level feature fusion (CLF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Attribute-Level Fusion</head><p>In attribute-level fusion (ALF) approach, a global imagelevel embedding vector is obtained from a set of classspecific embedding vectors that correspond to multiple labels in the image. The image-level embedding represents the global description of the positive labels in an image. The global embedding e ¬µ is obtained by averaging the individual class embeddings e(y). This embedding e ¬µ is then input to the generator G a along with the noise z for synthesizing the featurex a . The attribute-level fusion is then given by</p><formula xml:id="formula_0">e ¬µ = 1 n j:y[j]=1 e(y j ) andx a = G a (z, e ¬µ ).</formula><p>(1) <ref type="figure" target="#fig_0">Fig. 2</ref> shows the feature generation ofx a from the global embedding e ¬µ . The generator G a in ALF performs global image-level feature generation, thereby capturing label dependencies (correlations among labels) in an image. However, such a feature generation from the global embedding x a has lower class-specific discriminability since it does not explicitly encode discriminative information with respect to individual classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Feature-Level Fusion</head><p>Here, we introduce a feature-level fusion (FLF) approach, which synthesizes the features from the class-specific embeddings individually. This allows the FLF approach to better preserve the class-specific discriminative information in the synthesized features. Different from the ALF that integrates the embedding vectors, the FLF approach first inputs the n class-specific embeddings e(y j ) to G f in order to generate n class-specific latent featuresx j . These features are then integrated through an average operation to obtain the final synthesized featurex f . The feature-level fusion (FLF) is denoted b·ªπ <ref type="figure" target="#fig_0">Fig. 2</ref> shows the feature generation ofx f from the individual embeddings e(y j ). We observe from Eq. 2 that for a fixed noise z, the generator G f synthesizes a fixed latent featurex j for class j, regardless of the presence/absence of other classes in an image. Thus, while the generated latent featuresx j better preserve class-specific discriminative information of the positive classes j present in the image, G f synthesizes them independently of each other. As a result, the synthesized featurex f does not explicitly encode the label dependencies in an image. As discussed above, both the aforementioned fusion approaches (ALF and FLF) have shortcomings when synthesizing multi-label features. Next, we introduce a fusion approach that combines the advantage of the label dependency of ALF and class-specific discriminability of FLF.</p><formula xml:id="formula_1">x j = G f (z, e(y j )) andx f = 1 n j:y[j]=1x j . (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Cross-Level Feature Fusion</head><p>The proposed cross-level feature fusion (CLF) aims to combine the advantages of both ALF and FLF. The CLF approach (see <ref type="figure" target="#fig_0">Fig. 2</ref>) incorporates label dependency and classspecific discriminability in the feature generation stage as in the ALF and FLF, respectively. To this end,x f andx a are forwarded to a feature fusion block. Inspired by the multi-headed self-attention <ref type="bibr" target="#b34">[35]</ref>, the feature fusion block enriches each respective feature by taking guidance from the other branch. Specifically, we create a matrixx ‚àà R 2√ód by stacking the individual featuresx f andx a . Then, these features are linearly projected to a low-dimensional space (d = d /H) to create query-key-value triplets using a total of H projection heads,</p><formula xml:id="formula_2">q h =xW Q h , k h =xW K h , v h =xW V h , where W Q h , W K h , W V h ‚àà R d√ód and h ‚àà {1, 2, .</formula><p>., H}. For each feature, a status of its current form is kept in the 'value' embedding, while the query vector derived from each input feature is used to find its correlation with the keys obtained from both the features, as we elaborate below.</p><p>Given these triplets from each head, the features undergo two levels of processing (i) intra-head processing on the triplet and (ii) cross-head processing. For the first case, the following equation is used to relate each query vector with 'keys' derived from both the features. The resulting normalized relation scores (r h ‚àà R 2√ó2 ) are used to reweight the corresponding value vectors, thereby obtaining the attended features Œ± h ‚àà R 2√ód ,</p><formula xml:id="formula_3">Œ± h = r h v h , where r h = œÉ( q h k T h ‚àö d ).</formula><p>To aggregate information across all heads, these attended low-dimensional features from each head are concatenated and processed by an output layer to generate the original</p><formula xml:id="formula_4">d-dimensional output vectors o i ‚àà R 2√ód , o i = (Œ± 1 ‚äï Œ± 2 ‚äï . . . Œ± H )W O ,<label>(3)</label></formula><p>where W O ‚àà R d√ód is a learnable weight matrix and ‚äï denotes concatenation. After obtaining the self-attended features o, a residual branch is added from the input to the attended features and further processed with a small residual sub-network f (¬∑) to help the network first focus on the local neighbourhood and then progressively pay attention to the other-level features,</p><formula xml:id="formula_5">o = f (x + o) + (x + o), s.t.,√µ ‚àà R 2√ód .<label>(4)</label></formula><p>This encourages the network to selectively focus on adding complimentary information to the source vectorsx. Finally, we mean-pool the matrix√µ along the row dimension to obtain a single cross-level fused featurex c ‚àà R d ,</p><formula xml:id="formula_6">x c = 1 2 j‚àà{1,2}√µ j,k .<label>(5)</label></formula><p>The cross-level fused featurex c is obtained by effectively fusing the features generated from ALF and FLF. As a result,x c explicitly encodes the correlation among labels in the image in addition to the class-specific discriminative information of the positive classes present in the image. Next, we describe the integration of our CLF approach in two representative generative architectures for multi-label (generalized) zero-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Label Zero-Shot Classification</head><p>We integrate our fusion approaches in two representative generative architectures (f-CLSWGAN <ref type="bibr" target="#b41">[42]</ref> and f-VAEGAN <ref type="bibr" target="#b42">[43]</ref>) for multi-label zero-shot classification. Both f-CLSWGAN and f-VAEGAN have been shown to achieve promising performance for single-label zero-shot classification. Since CLF integrates both ALF and FLF, we only describe here the integration of CLF in the two classification frameworks. Next, we describe the integration of CLF in f-CLSWGAN, followed by f-VAEGAN.</p><p>Briefly, f-CLSWGAN comprises a conditional WGAN (conditioned on the embeddings e(y)) and a seen class classifier f cls . Here, we replace the standard generator of the f-CLSWGAN with our multi-label feature generation (CLF) to synthesize multi-label featuresx c . The resulting multilabel WGAN loss is given by</p><formula xml:id="formula_7">L W = E[D(x s , e(y s ))] ‚àí E[D(x c s , e(y s ))]‚àí (6) ŒªE[(||‚àáxD(x, e(y s ))|| 2 ‚àí 1) 2 ],</formula><p>wherex c s is synthesized using Eq. 5 for the seen classes, Œª is the penalty coefficient andx is a convex combination of x s andx c s . Furthermore, a classifier f cls , trained on the seen classes, is employed to encourage the generator to synthesize features that are well suited for final ZSL/GZSL classification. The final objective for training our CLF-based f-CLSWGAN in a multi-label setting is given by</p><formula xml:id="formula_8">min G max D L W ‚àí Œ±E[BCE(f cls (x c s , y s )],<label>(7)</label></formula><p>where BCE(f cls (x c s ), y s ) denotes the standard binary cross entropy loss between the predicted multi-label f cls (x s ) and the ground-truth multi-label y s of feature x s .</p><p>In addition to f-CLSWGAN, we also integrate our CLF into f-VAEGAN <ref type="bibr" target="#b42">[43]</ref> framework to perform multi-label feature synthesis. The f-VAEGAN extends f-CLSWGAN by combining a conditional VAE <ref type="bibr" target="#b16">[17]</ref> along with a conditional <ref type="figure">Figure 3</ref>. Our CLF-based f-VAEGAN architecture, which integrates the proposed multi-label feature synthesis CLF approach into the f-VAEGAN. The standard f-VAEGAN extends f-CLSWGAN by integrating a conditional VAE along with a conditional WGAN, utilizing a shared generator between them. f-VAEGAN comprises an encoder E, shared generator and a discriminator D, conditioned on the class-specific embeddings e. In the proposed CLF-based f-VAEGAN shown here, we replace the single-label shared generator in f-VAEGAN with our multi-label generator G for synthesizing multi-labelx c . The resulting synthesized featuresx c are then passed to the discriminator for training the generator using the loss terms: LKL, LBCE and LW . WGAN, utilizing a shared generator between them. For more details on f-VAEGAN, we refer to <ref type="bibr" target="#b42">[43]</ref>. Similar to our CLF-based f-CLSWGAN described earlier, we replace the single-label shared generator in f-VAEGAN with our multi-label generator (CLF) for synthesizing multi-labelx c . The resulting CLF-based f-VAEGAN is trained similar to the standard f-VAEGAN, using the original loss formulation <ref type="bibr" target="#b42">[43]</ref>. <ref type="figure">Fig. 3</ref> shows our CLF-based f-VAEGAN architecture for multi-label (generalized) zero-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets: We evaluate our generative multi-label zero-shot approach on three benchmarks: NUS-WIDE <ref type="bibr" target="#b4">[5]</ref>, Open Images <ref type="bibr" target="#b18">[19]</ref> and MS COCO <ref type="bibr" target="#b21">[22]</ref>. The NUS-WIDE dataset comprises nearly 270K images with 81 human-annotated categories, in addition to the 925 labels obtained from Flickr user tags. As in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b47">48]</ref>, the 925 and 81 labels are used as seen and unseen classes, respectively. The Open Images (v4) is a large-scale dataset consisting of 9 million training images along with 41,620 and 125,456 validation and testing images, respectively. The scale of Open Images is larger than other multi-label datasets, such as NUS-WIDE and MS COCO. This dataset is partially annotated with human labels and machine-generated labels. Here, 7,186 labels, having at least 100 images in the training, are selected as seen classes. The most frequent 400 test labels, which are not present in the training data are selected as unseen classes, as in <ref type="bibr" target="#b13">[14]</ref>. The MS COCO dataset is divided into training and validation sets with 82,783 and 40,504 images, respectively. It has been previously used for multi-label zero-shot object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> but not for multi-label zero-shot classifica-tion. Here, we perform multi-label zero-shot classification experiments by using the same split (65 seen and 15 unseen classes), as in detection works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>. Evaluation Metrics: For evaluating our approach on multilabel (generalized) zero-shot classification, we use the mean Average Precision (mAP) and F1 score at top-K predictions, similar to <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48]</ref>. While the mAP measure captures the image ranking accuracy of the model for each label, the F1 measure captures the model's ability to correctly rank the labels in each image. Implementation Details: Following existing zero-shot classification works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b13">14]</ref>, we use the pretrained VGG-19 backbone to extract features from multi-label images.</p><p>Image-level features of size d=4,096, corresponding to FC7 layer output are used as input to our GAN. We use the l 2 normalized 300 dimensional GloVe <ref type="bibr" target="#b28">[29]</ref> vectors corresponding to the category names as the attribute embeddings e(k) ‚àà E, as in <ref type="bibr" target="#b13">[14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We first present an ablation study w.r.t. our fusion approaches: attribute-level (ALF), feature-level (FLF) and cross-level feature (CLF) on the NUS-WIDE dataset. We evaluate our fusion strategies with both architectures: f-CLSWGAN and f-VAEGAN. Tab. 1 shows the comparison, in terms of F1 score (K‚àà{3, 5}) and mAP for both ZSL and GZSL tasks. In the case of ZSL, the ALF-based f-CLSWGAN achieves mAP score of 22.1. The FLF-based f-CLSWGAN obtains similar performance with mAP score of 22.6. The CLF-based f-CLSWGAN achieves improved performance with mAP score of 23.7. Similarly, the CLFbased f-CLSWGAN obtains consistent improvement over both ALF and FLF-based f-CLSWGAN in terms of F1 score (K‚àà{3, 5}). In the case of GZSL, ALF and FLF obtain F1 scores (K=3) of 16.8 and 17.0, respectively. Our CLF approach achieves improved performance with F1 score of 17.9. Similarly, CLF performs favorably against both ALF <ref type="table">Table 1</ref>. Classification performance comparison of the three fusion approaches (ALF, FLF and CLF) for both ZSL and GZSL tasks on the NUS-WIDE dataset. The comparison is shown in terms of F1 score (K‚àà{3, 5}) and mAP. We present the evaluation of our fusion approaches with both architectures: f-CLSWGAN and f-VAEGAN. Regardless of the underlying generative architecture, our CLF approach achieves consistent improvement in performance, on all metrics, over both ALF and FLF for ZSL and GZSL tasks. Best results are in bold. and FLF in terms of F1 at K=5 and mAP metrics. As in f-CLSWGAN, we also observe the CLF approach to achieve consistent improvement in performance over both ALF and FLF, when integrated in the more sophisticated f-VAEGAN. In the case of ZSL, our CLF-based f-VAEGAN achieves absolute gains of 2.8% and 2.4% in terms of mAP over ALF and FLF, respectively. Similarly, CLF-based f-VAEGAN achieves consistent improvement in performance in terms of F1 score, over both ALF and FLF. Furthermore, CLF-based f-VAEGAN performs favorably against the other two fusion approaches in case of GZSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion</head><p>The aforementioned results show that our CLF approach achieves consistent improvement in performance over both ALF and FLF for both ZSL and GZSL, regardless of the underlying architecture. Furthermore, the best results are obtained when integrating our CLF in f-VAEGAN. Next, we compare our CLF-based f-VAEGAN, denoted as our approach hereafter, with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">State-of-the-art Comparison</head><p>NUS-WIDE: Tab. 2 shows the state-of-the-art comparison for zero-shot (ZSL) and generalized zero-shot (GZSL) classification. The results are reported in terms of mAP and F1 score at top-K predictions (K‚àà{3, 5}). In addition, we also report the precision (P) and recall (R) for each F1 score. For the ZSL task, Fast0Tag <ref type="bibr" target="#b47">[48]</ref> approach that finds principal directions in the word vector space for ranking the relevant tags ahead of the irrelevant tags, achieves mAP score of 15.1. The recently introduced LESA approach <ref type="bibr" target="#b13">[14]</ref>, which utilizes a shared multi-attention mechanism to predict all labels in an image, achieves improved performance over Fast0Tag <ref type="bibr" target="#b47">[48]</ref>, with mAP score of 19.4. LESA also ob- <ref type="table">Table 2</ref>. State-of-the-art comparison for ZSL and GZSL tasks on the NUS-WIDE and Open Images datasets. We report the results in terms of mAP and F1 score at K‚àà{3, 5} for NUS-WIDE and K‚àà{10, 20} for Open Images. Our approach outperforms the state-of-the-art for both ZSL and GZSL tasks, in terms of mAP and F1 score, on both datasets. Best results are in bold. tains better performance over both one attention per label and per cluster. Our approach achieves state-of-the-art results with an absolute gain of 6.3% in terms mAP, over the best existing approach LESA. Similarly, our approach obtains consistent improvement in classification performance over the state-of-the-art in terms of F1 score (K‚àà{3, 5}).</p><p>For the GZSL task, Fast0Tag and one attention per label achieve similar performance in terms of mAP. LESA obtains improved classification results among existing methods with mAP score of 5.6. Our approach achieves mAP score of 8.9, outperforming LESA with an absolute gain of 3.3%. Similarly, our approach achieves consistent improvement in classification performance with absolute gains of 4.5% and 5.2% over LESA in terms of F1 score at K=3 and K=5, respectively. <ref type="figure">Fig. 4</ref> shows the classification mAP improvement of our approach w.r.t. LESA for 15 unseen labels with the largest performance improvement as well as 15 unseen labels with the largest drop. Our proposed approach significantly improves (more than 26%) on several unseen labels, such as waterfall, bear, dog, protest and food, while having relatively smaller (less than 12%) negative impact on labels, such as sunset, person, sky, water and rocks. We observe our approach to be particularly better on animal categories (11 out of 13). We also observe our approach to struggle in case of abstract concepts (e.g. sunset, sky, reflection). In total, our approach outperforms LESA on 57 out of 81 labels. Open Images: Tab. 2 shows the state-of-the-art comparison for ZSL and GZSL classification. The results are reported in terms of mAP and F1 score at top-K predictions (K‚àà{10, 20}). Compared to the NUS-WIDE dataset, Open Images has significantly larger number of labels. This makes the ranking problem within an image more challeng-  <ref type="figure">Figure 4</ref>. Classification mAP improvement comparison between our approach and LESA on NUS-WIDE. We show the comparison for 15 unseen labels with the largest gain as well 15 unseen labels with the largest drop. Our approach significantly improves (more than 26%) on several unseen labels (e.g., waterfall, bear, dog, protest and food), while having relatively smaller (less than 12%) negative impact on other labels. Best viewed zoomed in.</p><p>ing, reflected by the lower F1 scores in the table. For the ZSL task, LESA obtains F1 scores of 1.4 and 1.0 at K=10 and K=20, respectively. Our approach performs favorably against LESA with F1 scores of 2.5 and 2.2 at K=10 and K=20, respectively. A similar performance gain is also observed for the mAP metric. It is worth noting that this dataset has 400 unseen labels, thereby making the problem of ZSL challenging. As in ZSL, our approach also achieves consistent improvement in performance, in terms of both F1 and mAP, over the state-of-the-art for the GZSL task. MS COCO: Although the problem of zero-shot object detection is investigated, we are the first to evaluate zero-shot classification on this dataset. Tab. 3 shows the state-of-theart comparison for ZSL and GZSL classification. Since the maximum number of unseen classes in an image is 3 in the validation set, we only report the F1 score at K=3. We re-implement LabelEM, CONSE and Fast0tag since their codes are not publicly available. For LESA, we obtain the results using the code-base from authors. Our approach ob-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Standard Multi-label Classification</head><p>In addition to zero-shot classification, we evaluate our approach for standard multi-label classification where all labels have training images. Tab. 4 shows the state-ofthe-art comparison for standard multi-label classification on NUS-WIDE with 81 human annotated labels. As in <ref type="bibr" target="#b13">[14]</ref>, we remove all test samples without any label in the 81 label set. Among existing methods, CNN-RNN and LESA achieve mAP scores of 28.3 and 31.5, respectively. Our approach outperforms existing methods, achieving mAP score of 46.7. The proposed approach also performs favorably against existing methods in terms of F1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Extension to Zero-shot Object Detection</head><p>Lastly, we also evaluate our multi-label feature generation approach (CLF) for zero-shot object detection (ZSD). ZSD strives for simultaneous classification and localization of previously unseen objects. In the generalized settings, the test set contains both seen and unseen object categories <ref type="table">Table 5</ref>. State-of-the-art comparison for ZSD and GZSD tasks on MS COCO. The results are reported in terms of Recall. For GZSD, we report the harmonic mean (HM) between seen and unseen classes. Our approach performs favorably against existing methods. Best results are in bold. (GZSD). Recently, the work of <ref type="bibr" target="#b11">[12]</ref> introduce a zero-shot detection approach (SUZOD) where features are synthesized, conditioned upon class-embeddings, and integrated in the popular Faster R-CNN framework <ref type="bibr" target="#b30">[31]</ref>. The feature generation stage is jointly driven by the classification loss in the semantic embedding space for both seen and unseen categories. Their approach addresses multi-label zero-shot detection by constructing single-label features for each region of interest (RoI). Different from SUZOD <ref type="bibr" target="#b11">[12]</ref>, we first generate a pool of multi-label RoI features by integrating random sets of individual single-label RoI features. These integrated multi-label RoI features are then used as real features to train our CLF-based classification architecture.</p><p>Tab. 5 shows the state-of-the-art comparison, in terms of recall, for ZSD and GZSD detection on MS COCO. For GZSD, Harmonic Mean (HM) of performances for seen and unseen classes are reported. Similar to SUZOD <ref type="bibr" target="#b11">[12]</ref>, we also report the results by using f-CLSWGAN as the underlying generative architecture. Our approach performs favorably against existing methods for both ZSD and GZSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We investigate the problem of multi-label feature synthesis in the zero-shot setting. We introduce three different fusion approaches (ALF, FLF and CLF) to synthesize multi-label features. Our ALF synthesizes features by integrating class-specific attribute embeddings at the generator input. On the other hand, FLF synthesizes features from class-specific embeddings individually and integrates them in feature space. Our CLF combines the advantages of both ALF and FLF, using each individual-level feature and attends to the bi-level context. Consequently, individual-level features adapt themselves producing enriched synthesized features that are pooled to obtain final output. We integrate our fusion approaches in two generative architectures. Our approach outperforms existing zero-shot methods on three multi-label classification benchmarks. Lastly, we also show the effectiveness of our approach for zero-shot detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of three different approaches to synthesize unseen multi-label features. The attribute-level fusion (ALF) generates a global image-level embedding vector from a set of class-specific embedding vectors corresponding to multiple labels in an image (Sec. 3.1.1). In ALF, the generator synthesizes global featuresx a that capture the correlations among the labels in the image. On the other hand, the feature-level fusion (FLF) synthesizes features from individual class-specific embeddings (Sec. 3.1.2). As a result, the generator produces class-specific latent features which are then integrated to obtain synthesized featuresx f . The cross-level fusion (CLF) combines the advantages of ALF and FLF, during feature generation (Sec. 3.1.3). Specifically, it uses each individual-level feature (x a ,x f ) to attend to the bi-level context and adapt itself to generate√µ a ,√µ f . These enriched features are then pooled to obtain the CLF output, which represents our final synthesized featurex c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The encoder E, discriminator D and generators G a and G f are all two layer fully connected (FC) networks with 4,096 hidden units and have Leaky ReLU as the non-linearity. The number of heads H in our CLF is set to 8. The sub-network f (¬∑) in CLF is a two layer FC network with 8,192 hidden units. The feature synthesizing network is trained with a learning rate of 10 ‚àí4 . The WGAN is trained with (batch size, epoch) of (64, 50), (128, 1) and (64, 70) on NUS-WIDE, Open Images and MS COCO, respectively. For the f-CLSWGAN variant, Œ± is set to 0.1, while the coefficient Œª is set to 10 in the f-VAEGAN variant. The ZSL and GZSL classifiers: f z and f gz are trained for 50 epochs with (batch size, learning rate) of (300, 10 ‚àí3 ), (100, 10 ‚àí4 ) and (100, 10 ‚àí3 ) on NUS-WIDE, Open Images and MS COCO, respectively. The WGAN and the classifiers are trained using ADAM with (Œ≤ 1 , Œ≤ 2 ) as (0.5, 0.999). All parameters are chosen via cross-validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>w a t e r f a ll b e a r d o g p r o t e s t f o o d f la g s b ir d s p la n e e lk t r a in c a t f lo w e r s b o a t s m a p b r id g e z e b r a v a ll e y w h a le s p e r s o n t o w n c lo u d s r e f le c t io n p la n t s la k e s u n r o c k s w a t e r v e h ic le s k y s u n s e t 10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>State-of-the-art comparison for ZSL and GZSL tasks on the MS COCO dataset having 65 seen and 15 unseen classes. We report the results in terms of mAP and F1 score at K=3. Our approach performs favorably against existing approaches for both ZSL and GZSL tasks. Best results are in bold.</figDesc><table><row><cell>Method</cell><cell>Task</cell><cell>F1 (K=3)</cell><cell>F1 (K=5)</cell><cell>mAP</cell></row><row><cell>CONSE [28]</cell><cell>ZSL GZSL</cell><cell>18.4 19.6</cell><cell>-18.9</cell><cell>13.2 7.7</cell></row><row><cell>LabelEM [1]</cell><cell>ZSL GZSL</cell><cell>10.3 6.7</cell><cell>-7.9</cell><cell>9.6 4.0</cell></row><row><cell>Fast0tag [48]</cell><cell>ZSL GZSL</cell><cell>37.5 33.8</cell><cell>-34.6</cell><cell>43.3 27.9</cell></row><row><cell>LESA [14]</cell><cell>ZSL GZSL</cell><cell>33.6 26.7</cell><cell>-28.0</cell><cell>31.8 17.5</cell></row><row><cell>Our Approach</cell><cell>ZSL GZSL</cell><cell>43.5 44.1</cell><cell>-43.4</cell><cell>52.2 33.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Standard multi-label classification performance comparison on NUS-WIDE. The results are reported in terms of mAP and F1 score at K‚àà{3, 5}. Our approach achieves superior performance compared to existing methods. Best results are in bold.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>K=3 R</cell><cell>F1</cell><cell>P</cell><cell>K=5 R</cell><cell>F1</cell><cell>mAP</cell></row><row><cell>Logistic [34]</cell><cell cols="7">46.1 57.3 51.1 34.2 70.8 46.1 21.6</cell></row><row><cell>WARP [10]</cell><cell cols="6">49.1 61.0 54.4 36.6 75.9 49.4</cell><cell>3.1</cell></row><row><cell>WSABIE [40]</cell><cell cols="6">48.5 60.4 53.8 36.5 75.6 49.2</cell><cell>3.1</cell></row><row><cell>Fast0Tag [48]</cell><cell cols="7">48.6 60.4 53.8 36.0 74.6 48.6 22.4</cell></row><row><cell>CNN-RNN [37]</cell><cell cols="7">49.9 61.7 55.2 37.7 78.1 50.8 28.3</cell></row><row><cell>One Attention per Label [16]</cell><cell cols="7">51.3 63.7 56.8 38.0 78.8 51.3 32.6</cell></row><row><cell cols="8">One Attention per Cluster [14] 51.1 63.5 56.6 37.6 77.9 50.7 31.7</cell></row><row><cell>LESA [14]</cell><cell cols="7">52.3 65.1 58.0 38.6 80.0 52.0 31.5</cell></row><row><cell>Our Approach</cell><cell cols="7">53.5 66.5 59.3 39.4 81.6 53.1 46.7</cell></row><row><cell cols="8">tains superior (G)ZSL classification performance, in terms</cell></row><row><cell cols="7">of F1 score and mAP, compared to existing methods.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and L√©on Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tat-Seng Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep convnet for multi-label classification with partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-modal cycle-consistent generalized zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Vijay Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transductive multi-view zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Alexander Toshev, and Sergey Ioffe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4894</idno>
	</analytic>
	<monogr>
		<title level="m">Deep convolutional ranking for multilabel image annotation</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pougetabadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Warde-Farley</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Syed Waqas Zamir, and Fahad Shahbaz Khan. Synthesizing the unseen for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV, 2020. 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative dual adversarial network for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A shared multi-attention framework for multi-label zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Elhamifar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-label zero-shot learning with structured knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Kuan</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Leveraging the invariant side of generative zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection for generalized zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devraj</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Kumar Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaib</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Costa: Co-occurrence statistics for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximizing subset accuracy with recurrent neural networks in multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneldo</forename><surname>Loza Menc√≠a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyunwoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>F√ºrnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Latent embedding feedback and discriminative features for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshita</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07833</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICVGIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Polarity loss for zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multi-label classification: An overview. IJDWM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning from noisy largescale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Caltech-ucsd birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010- 001</idno>
	</analytic>
	<monogr>
		<title level="j">Caltech</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature generating networks for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Zeynep Akata. f-vaegan-d2: A feature generating framework for any-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Orderless recurrent models for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Vacit Oguz Yazici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnau</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartlomiej</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Twardowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention-driven dynamic graph convolutional network for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Zero-shot classification with discriminative semantic representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-modality attention with semantic graph embedding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renchun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast zeroshot image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
