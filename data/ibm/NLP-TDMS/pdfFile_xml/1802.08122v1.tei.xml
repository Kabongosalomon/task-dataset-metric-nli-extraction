<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harmonious Attention Network for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<email>wei.li@qmul.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Vision Semantics Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
							<email>s.gong@qmul.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Vision Semantics Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Harmonious Attention Network for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing person re-identification (re-id) methods either assume the availability of well-aligned person bounding box images as model input or rely on constrained attention selection mechanisms to calibrate misaligned images. They are therefore sub-optimal for re-id matching in arbitrarily aligned person images potentially with large human pose variations and unconstrained auto-detection errors. In this work, we show the advantages of jointly learning attention selection and feature representation in a Convolutional Neural Network (CNN) by maximising the complementary information of different levels of visual attention subject to re-id discriminative learning constraints. Specifically, we formulate a novel Harmonious Attention CNN (HA-CNN) model for joint learning of soft pixel attention and hard regional attention along with simultaneous optimisation of feature representations, dedicated to optimise person re-id in uncontrolled (misaligned) images. Extensive comparative evaluations validate the superiority of this new HA-CNN model for person re-id over a wide variety of state-ofthe-art methods on three large-scale benchmarks including CUHK03, Market-1501, and DukeMTMC-ReID.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (re-id) aims to search people across non-overlapping surveillance camera views deployed at different locations by matching person images. In practical re-id scenarios, person images are typically automatically detected for scaling up to large visual data <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b17">18]</ref>. Auto-detected person bounding boxes are typically not optimised for re-id due to misalignment with background clutter, occlusion, missing body parts ( <ref type="figure" target="#fig_0">Fig. 1</ref>). Additionally, people (uncooperative) are often captured in various poses across open space and time. These give rise to the notorious image matching misalignment challenge in cross-view re-id <ref type="bibr" target="#b6">[7]</ref>. There is consequently an inevitable need for attention selection within arbitrarily-aligned bounding boxes as an integral part of model learning for re-id.</p><p>There are a few attempts in the literature for solving the problem of re-id attention selection within person bounding boxes. One common strategy is local patch calibration and saliency weighting in pairwise image matching <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b35">36]</ref>. However, these methods rely on handcrafted features without deep learning jointly more expressive feature representations and matching metric holistically (end-to-end). A small number of attention deep learning models for re-id have been recently developed for reducing the negative effect from poor detection and human pose change <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b26">27]</ref>. Nevertheless, these deep methods implicitly assume the availability of large labelled training data by simply adopting existing deep architectures with high complexity in model design. Additionally, they often consider only coarse region-level attention whilst ignoring the fine-grained pixel-level saliency. Hence, these techniques are ineffective when only a small set of labelled data is available for model training whilst also facing noisy person images of arbitrary misalignment and background clutter.</p><p>In this work, we consider the problem of jointly deep learning attention selection and feature representation for optimising person re-id in a more lightweight (with less parameters) network architecture. The contributions of this work are: (I) We formulate a novel idea of jointly learning multi-granularity attention selection and feature representation for optimising person re-id in deep learning. To our knowledge, this is the first attempt of jointly deep learning multiple complementary attention for solving the person reid problem. (II) We propose a Harmonious Attention Con-volutional Neural Network (HA-CNN) to simultaneously learn hard region-level and soft pixel-level attention within arbitrary person bounding boxes along with re-id feature representations for maximising the correlated complementary information between attention selection and feature discrimination. This is achieved by devising a lightweight Harmonious Attention module capable of efficiently and effectively learning different types of attention from the shared re-id feature representation in a multi-task and endto-end learning fashion. (III) We introduce a cross-attention interaction learning scheme for further enhancing the compatibility between attention selection and feature representation given re-id discriminative constraints. Extensive comparative evaluations demonstrate the superiority of the proposed HA-CNN model over a wide range of state-of-theart re-id models on three large benchmarks CUHK03 <ref type="bibr" target="#b17">[18]</ref>, Market-1501 <ref type="bibr" target="#b43">[44]</ref>, and DukeMTMC-ReID <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most existing person re-id methods focus on supervised learning of identity-discriminative information, including ranking by pairwise constraints <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref>, discriminative distance metric learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b3">4]</ref>, and deep learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b18">19]</ref>. These methods assume that person images are well aligned, which is largely invalid given imperfect detection bounding boxes of changing human poses. To overcome this limitation, attention selection techniques have been developed for improving re-id by localised patch matching <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b45">46]</ref> and saliency weighting <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43]</ref>. These are inherently unsuitable by design to cope with poorly aligned person images, due to their stringent requirement of tight bounding boxes around the whole person and high sensitivity of the hand-crafted features.</p><p>Recently, a few attention deep learning methods have been proposed to handle the matching misalignment challenge in re-id <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16]</ref>. The common strategy of these methods is to incorporate a regional attention selection sub-network into a deep re-id model. For example, Su et al. <ref type="bibr" target="#b26">[27]</ref> integrate a separately trained pose detection model (from additional labelled pose ground-truth) into a part-based re-id model. Li et al. <ref type="bibr" target="#b16">[17]</ref> design an end-to-end trainable part-aligning CNN network for locating latent discriminative regions (i.e. hard attention) and subsequently extract and exploit these regional features for performing re-id. Zhao et al. <ref type="bibr" target="#b41">[42]</ref> exploit the Spatial Transformer Network <ref type="bibr" target="#b10">[11]</ref> as the hard attention model for searching re-id discriminative parts given a pre-defined spatial constraint. However, these models fail to consider the noisy information within selected regions at the pixel level, i.e. no soft attention modelling, which can be important. While soft attention modelling for re-id is considered in <ref type="bibr" target="#b21">[22]</ref>, this model assumes tight person boxes thus less suitable for poor detections.</p><p>The proposed HA-CNN model is designed particularly to address the weaknesses of existing deep methods as above by formulating a joint learning scheme for modelling both soft and hard attention in a single re-id deep model. This is the first attempt of modelling multi-level correlated attention in deep learning for person re-id to our knowledge. In addition, we introduce cross-attention interaction learning for enhancing the complementary effect between different levels of attention subject to re-id discriminative constraints. This is impossible to do for existing methods due to their inherent single level attention modelling. We show the benefits of joint modelling multi-level attention in person re-id in our experiments. Moreover, we also design an efficient attention CNN architecture for improving the model deployment scalability, an under-studied but practically important issue for re-id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Harmonious Attention Network</head><p>Given n training bounding box images I = {I i } n i=1 from n id distinct people captured by non-overlapping camera views together with the corresponding identity labels as</p><formula xml:id="formula_0">Y = {y i } n i=1 (where y i ∈ [1, · · · , n id ])</formula><p>, we aim to learn a deep feature representation model optimal for person reid matching under significant viewing condition variations. To this end, we formulate a Harmonious Attention Convolutional Neural Network (HA-CNN) that aims to concurrently learn a set of harmonious attention, global and local feature representations for maximising their complementary benefit and compatibility in terms of both discrimination power and architecture simplicity. Typically, person parts location information is not provided in person re-id image annotation (i.e. only weakly labelled without fine-grained). Therefore, the attention model learning is weakly supervised in the context of optimising re-id performance. Unlike most existing works that simply adopting a standard deep CNN network typically with a large number of model parameters (likely overfit given small size labelled data) and high computational cost in model deployment <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b7">8]</ref>, we design a lightweight (less parameters) yet deep (maintaining strong discriminative power) CNN architecture by devising a holistic attention mechanism for locating the most discriminative pixels and regions in order to identify optimal visual patterns for re-id. We avoid simply stacking many CNN layers to gain model depth. This is particularly critical for re-id where the label data is often sparse (large models are more likely to overfit in training) and the deployment efficiency is very important (slow feature extraction is not scalable to large surveillance video data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HA-CNN Overview</head><p>We consider a multi-branch network architecture for our purpose. The overall objective of this multi-branch scheme and the overall architecture composi-   tion is to minimise the model complexity therefore reduce the network parameter size whilst maintaining the optimal network depth. The overall design of our HA-CNN architecture is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. This HA-CNN model contains two branches: (1) One local branch (consisting of T streams of an identical structure): Each stream aims to learn the most discriminative visual features for one of T local image regions of a person bounding box image.</p><p>(2) One global branch: This aims to learn the optimal global level features from the entire person image. For both branches, we select the Inception-A/B units <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b28">29]</ref> as the basic building blocks <ref type="bibr" target="#b0">1</ref> .</p><p>In particular, we used 3 Inception-A and 3 Inception-B blocks for building the global branch, and 3 Inception-B blocks for each local stream. The width (channel number) of each Inception is denoted by d 1 , d 2 and d 3 . The global network ends with a global average pooling layer and a fully-connected (FC) feature layer with 512 outputs. For the local branch, we also use a 512-D FC feature layer which fuses the global average pooling outputs of all streams. To reduce the model parameter size, we share the first conv layer between global and local branches and the samelayer Inceptions among all local streams. For our HA-CNN model training, we utilise the cross-entropy classification loss function for both global and local branches, which optimise person identity classification.</p><p>For attention selection within each bounding box of some unknown misalignment, we consider a harmonious attention learning scheme that aims to jointly learn a set of complementary attention maps including hard (regional) attention for the local branch and soft (spatial/pixel-level and channel/scale-level) attention for the global branch.</p><p>We further introduce a cross-attention interaction learning scheme between the local and global branches for further enhancing the harmony and compatibility degree whilst simultaneously optimising per-branch discriminative fea-ture representations.</p><p>We shall now describe more details of each component of the network design as follows.  Regional Attention (part-wise). Layer type is indicated by background colour: grey for convolutional (conv), brown for global average pooling, and blue for fully-connected layers. The three items in the bracket of a conv layer are: filter number, filter shape, and stride. The ReLU <ref type="bibr" target="#b14">[15]</ref> and Batch Normalisation (BN) <ref type="bibr" target="#b9">[10]</ref> (applied to each conv layer) are not shown for brevity.</p><formula xml:id="formula_1">Reduce Sigmoid ℎ× , 1 Resize 1, 1×1, 1 2 Tanh 1, 3×3, 2 , 1×1, 1 , 1×1, 1 ℎ × × , 1×1, 1 ℎ × × 1 1 × 1 × ℎ × × (b) (c) (a) (d) (a)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Harmonious Attention Learning</head><p>Conceptually, our Harmonious Attention (HA) is a principled union of hard regional attention <ref type="bibr" target="#b10">[11]</ref>, soft spatial <ref type="bibr" target="#b33">[34]</ref> and channel attention <ref type="bibr" target="#b8">[9]</ref>. This simulates functionally the dorsal and ventral attention mechanism of human brain <ref type="bibr" target="#b32">[33]</ref> in the sense of modelling soft and hard attention simultaneously. The soft attention learning aims at selecting the fine-grained important pixels, while the hard attention learning is dedicated to searching the coarse latent (weakly supervised) discriminative regions. They are thus largely complementary with high compatibility to each other in functionality. Intuitively, their combination can relieve the modelling burden of soft attention and resulting in more discriminative and robust model learning from the same (particularly small) training data. We propose a novel Harmonious Attention joint learning strategy to unite the three distinct types of attention with only a small number of additional parameters. We take a block-wise (module-wise) attention design, that is, each HA module is specifically optimised to attend the input feature representations at its own level alone. In the CNN hierarchical framework, this naturally allows for hierarchical multi-level attention learning to progressively refine the attention maps, in the spirit of the divide and conquer design <ref type="bibr" target="#b4">[5]</ref>. As a result, we can significantly reduce the attention search space (i.e. the model optimisation complexity) whilst allow multi-scale selectiveness of hierarchical features to enrich the final feature representations. Such progressive and holistic attention modelling is both intuitive and essential for person re-id due to that (1) the surveillance person images often have cluttered background and uncontrolled appearance variations therefore the optimal attention patterns of different images can be highly variable, and (2) a re-id model typically needs robust (generalisable) model learning given very limited training data (significantly less than common image classification tasks). Next, we describe the design of our Harmonious Attention module in details. (I) Soft Spatial-Channel Attention The input to a Harmonious Attention module is a 3-D tensor X l ∈ R h×w×c where h, w, and c denote the number of pixel in the height, width, and channel dimensions respectively; and l indicates the level of this module in the entire network (multiple such modules). Soft spatial-channel attention learning aims to produce a saliency weight map A l ∈ R h×w×c of the same size as X. Given the largely independent nature between spatial (inter-pixel) and channel (inter-scale) attention, we propose to learn them in a joint but factorised way as:</p><formula xml:id="formula_2">A l = S l × C l<label>(1)</label></formula><p>where S l ∈ R h×w×1 and C l ∈ R 1×1×c represent the spatial and channel attention maps, respectively. We perform the attention tensor factorisation by designing a two-branches unit ( <ref type="figure" target="#fig_4">Fig. 3(a)</ref>): One branch to model the spatial attention S l (shared across the channel dimension), and another branch to model the channel attention C l (shared across both height and width dimensions). By this design, we can compute efficiently the full soft attention A l from C l and S l with a tensor multiplication. Our design is more efficient than common tensor factorisation algorithms <ref type="bibr" target="#b13">[14]</ref> since heavy matrix operations are eliminated.</p><p>(1) Spatial Attention We model the spatial attention by a tiny (10 parameters) 4-layers sub-network ( <ref type="figure" target="#fig_4">Fig. 3(b)</ref>). It consists of a global cross-channel averaging pooling layer (0 parameter), a conv layer of 3 × 3 filter with stride 2 (9 parameters), a resizing bilinear layer (0 parameter), and a scaling conv layer (1 parameter). In particular, the global averaging pooling, defined as,</p><formula xml:id="formula_3">S l input = 1 c c i=1 X l 1:h,1:w,i<label>(2)</label></formula><p>is designed especially to compress the input size of the subsequent conv layer with merely 1 c times of parameters needed. This cross-channel pooling is reasonable because in our design all channels share the identical spatial attention map. We finally add the scaling layer for automatically learning an adaptive fusion scale in order to optimally combining the channel attention described next.</p><p>(2) Channel Attention We model the channel attention by a small (2 c 2 r parameters, see more details below) 4-layers squeeze-and-excitation sub-network ( <ref type="figure" target="#fig_4">Fig. 3(c)</ref>). Specifically, we first perform a squeeze operation via an averaging pooling layer (0 parameters) for aggregating feature information distributed across the spatial space into a channel signature as</p><formula xml:id="formula_4">C l input = 1 h × w h i=1 w j=1 X l i,j,1:c<label>(3)</label></formula><p>This signature conveys the per-channel filter response from the whole image, therefore providing the complete information for the inter-channel dependency modelling in the subsequent excitation operation, formulated as</p><formula xml:id="formula_5">C l excitation = ReLU( W ca 2 × ReLU(W ca 1 C l input ))<label>(4)</label></formula><p>where W ca 1 ∈ R c r ×c ( c 2 r parameters) and W ca 2 ∈ R c× c r ( c 2 r parameters) denote the parameter matrix of 2 conv layers in order respectively, and r (16 in our implementation) represents the bottleneck reduction rate. Again, this bottleneck design is for reducing the model parameter number from c 2 (using one conv layer) to ( c 2 r + c 2 r ), e.g. only need 1 8 times of parameters when r = 16.</p><p>For facilitating the combination of the spatial attention and channel attention, we further deploy a 1×1× c convolution (c 2 parameters) layer to compute blended full soft attention after tensor multiplication. This is because the spatial and channel attention are not mutually exclusive but with a co-occurring complementary relationship. Finally, we use the sigmoid operation (0 parameter) to normalise the full soft attention into the range between 0.5 and 1. Remarks Our model is similar to the Residual Attention (RA) <ref type="bibr" target="#b33">[34]</ref> and Squeeze-and-Excitation (SE) <ref type="bibr" target="#b8">[9]</ref> concepts but with a number of essential differences: (1) The RA requires to learn a much more complex soft attention subnetwork which is not only computationally expensive but  also less discriminative when the training data size is small typical in person re-id. (2) The SE considers only the channel attention and implicitly assumes non-cluttered background, therefore significantly restricting its suitability to re-id tasks under cluttered surveillance viewing conditions.</p><p>(3) Both RA and SE consider no hard regional attention modelling, hence lacking the ability to discover the correlated complementary benefit between soft and hard attention learning.</p><p>(II) Hard Regional Attention The hard attention learning aims to locate latent (weakly supervised) discriminative T regions/parts (e.g. human body parts) in each input image at the l-th level. We model this regional attention by learning a transformation matrix as:</p><formula xml:id="formula_6">A l = s h 0 t x 0 s w t y<label>(5)</label></formula><p>which allows for image cropping, translation, and isotropic scaling operations by varying two scale factors (s h , s w ) and the 2-D spatial position (t x , t y ). We use pre-defined region size by fixing s h and s w for limiting the model complex. Therefore, the effective modelling part of A l is only t x and t y , with the output dimension as 2×T (T the region number). To perform this learning, we introduce a simple 2layers (2×T ×c parameters) sub-network ( <ref type="figure" target="#fig_4">Fig. 3(d)</ref>). We exploit the first layer output (a c-D vector) of the channel attention (Eq. (3)) as the first FC layer (2 × T × c parameters) input for further reducing the parameter size while sharing the available knowledge in spirit of the multi-task learning principle <ref type="bibr" target="#b5">[6]</ref>. The second layer (0 parameter) performs a tanh scaling (the range of [−1, 1]) to convert the region position parameters into the percentage so as to allow for positioning individual regions outside of the input image boundary. This specially takes into account the cases that only partial person is detected sometimes. Note that, unlike the soft attention maps that are applied to the input feature representation X l , the hard regional attention is enforced on that of the corresponding network block to generate T different parts which are subsequently fed into the corresponding streams of the local branch (see the dashed arrow on the top of <ref type="figure" target="#fig_2">Fig 2)</ref>. Remarks The proposed hard attention modelling is conceptually similar to the Spatial Transformer Network (STN) <ref type="bibr" target="#b10">[11]</ref> because both are designed to learn a transformation matrix for discriminative region identification. However, they differ significantly in design: (1) The STN attention is network-wise (one level of attention learning) whilst our HA is module-wise (multiple levels of attention learning). The latter not only eases the attention modelling complexity (divide-and-conquer design) and but also provides additional attention refinement in a sequential manner.</p><p>(2)</p><p>The STN utilises a separate large sub-network for attention modelling whilst the HA-CNN exploits a much smaller sub-network by sharing the majority model learning with the target-task network using a multi-task learning design ( <ref type="figure" target="#fig_6">Fig. 4)</ref>, therefore superior in both higher efficiency and lower overfitting risk.</p><p>(3) The STN considers only hard attention whilst HA-CNN models both soft and hard attention in an end-to-end fashion so that additional complementary benefits are exploited.</p><p>(III) Cross-Attention Interaction Learning Given the joint learning of soft and hard attention above, we further consider a cross-attention interaction mechanism for enriching their joint learning harmony by interacting the attended local and global features across branches. Specifically, at the l-th level, we utilise the global-branch feature X (l,k) G of the k-th region to enrich the corresponding local-branch feature X (l,k) L by tensor addition as</p><formula xml:id="formula_7">X (l,k) L = X (l,k) L + X (l,k) G<label>(6)</label></formula><p>where X (l,k) G is computed by applying the hard regional attention of the (l + 1)-th level's HA attention module (see the dashed arrow in <ref type="figure" target="#fig_2">Fig. 2)</ref>. By doing so, we can simultaneously reduce the complexity of the local branch (fewer layers) since the learning capability of the global branch can be partially shared. During model training by backpropagation, the global branch takes gradients from both the global and local branches as</p><formula xml:id="formula_8">∆W (l) G = ∂LG ∂X (l) G ∂X (l) G ∂W (l) G + T k=1 ∂LL ∂X (l,k) L ∂X (l,k) L ∂W (l) G (7)</formula><p>Therefore, the global L G and local L L loss quantities are concurrently utilised in optimising the parameters W (l) G of the global branch. As such, the learning of the global branch is interacted with that of the local branch at multiple levels, whilst both are subject to the same re-id optimisation constraint. Remarks By design, cross-attention interaction learning is subsequent to and complementary with the harmonious attention joint reasoning above. Specifically, the latter learns soft and hard attention from the same input feature representations to maximise their compatibility (joint attention generation), whilst the former optimises the correlated complementary information between attention refined global and local features under the person re-id matching constraint (joint attention application). Hence, the composition of both forms a complete process of joint optimisation of attention selection for person re-id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Person Re-ID by HA-CNN</head><p>Given a trained HA-CNN model, we obtain a 1,024-D joint feature vector (deep feature representation) by concatenating the local (512-D) and the global (512-D) feature vectors. For person re-id, we deploy this 1,024-D deep feature representation using only a generic distance metric without any camera-pair specific distance metric learning, e.g. the L2 distance. Specifically, given a test probe image I p from one camera view and a set of test gallery images {I g i } from other non-overlapping camera views: (1) We first compute their corresponding 1,024-D feature vectors by forward-feeding the images to a trained HA-CNN model, denoted as x p = [x p g ; x p l ] and {x g i = [x g g ; x g l ]}.</p><p>(2) We then compute L2 normalisation on the global and local features, respectively. (3) Lastly, we compute the crosscamera matching distances between x p and x g i by the L2 distance. We then rank all gallery images in ascendant order by their L2 distances to the probe image. The probabilities of true matches of probe person images in Rank-1 and among the higher ranks indicate the goodness of the learned HA-CNN deep features for person re-id tasks. Datasets and Evaluation Protocol For evaluation, we selected three large-scale person re-id benchmarks, Market-1501 <ref type="bibr" target="#b36">[37]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b46">[47]</ref> and CUHK03 <ref type="bibr" target="#b17">[18]</ref>. <ref type="figure" target="#fig_7">Figure 5</ref> shows several example person bounding box images. We adopted the standard person re-id setting including the training/test ID split and test protocol ( <ref type="table" target="#tab_1">Table 1)</ref>. For performance measure, we use the cumulative matching characteristic (CMC) and mean Average Precision (mAP) metrics.  <ref type="bibr" target="#b18">[19]</ref>, we use T = 4 regions for hard attention, e.g. a total of 4 local streams. In each stream, we fix the size of three levels of hard attention as 24×28, 12×14 and 6×7. For model optimisation, we use the ADAM <ref type="bibr" target="#b11">[12]</ref> algorithm at the initial learning rate 5×10 −4 with the two moment terms β 1 = 0.9 and β 2 = 0.999. We set the batch size to 32, epoch to 150, momentum to 0.9. Note, we do not adopt any data argumentation methods (e.g. scaling, rotation, flipping, and colour distortion), neither model pre-training. Existing deep re-id methods typically benefit significantly from these operations at the price of not only much higher computational cost but also notoriously difficult and time-consuming model tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparisons to State-of-the-Art Methods</head><p>Evaluation on Market-1501 We evaluated HA-CNN against 13 existing methods on Market-1501. Compared to Market-1501, person images from this benchmark have more variations in resolution and background due to wider camera views and more complex scene layout, therefore presenting a more challenging re-id task. <ref type="table">Table 3</ref> shows that HA-CNN again outperforms all compared stateof-the-arts with clear accuracy advantages, surpassing the 2 nd best SVDNet-ResNet50 (without attention modelling) by 3.8% (80.5-76.7) in Rank-1 and 7.0% (63. <ref type="bibr">8-56.8)</ref> in mAP. This suggests the importance of attention modelling in re-id and the efficacy of our attention joint learning approach in a more challenging re-id scenario. Importantly, the performance advantage by our method is achieved at a Evaluation on CUHK03 We evaluated HA-CNN on both manually labelled and auto-detected (more misalignment) person bounding boxes of the CUHK03 benchmark. We utilise the 767/700 identity split rather than 1367/100 since the former defines a more realistic and challenging reid task. In this setting, the training set is small with only about 7,300 images (versus 12,936/16,522 in Market-1501/DukeMCMT-ReID). This generally imposes a harder challenge to deep models, particularly when our HA-CNN does not benefit from any auxiliary data pre-training (e.g. ImageNet) nor data augmentation. <ref type="table" target="#tab_4">Table 4</ref> shows that HA-CNN still achieves the best re-id accuracy, outperforming hand-crafted feature based methods significantly and deep competitors less so. Our model achieves a small margin (+0.2% in Rank-1 and +1.3%) over the best alternative SVDNet-ResNet50 on the detected set. However, it is worth pointing out that SVDNet-ResNet50 benefits additionally from not only large ImageNet pre-training but also a much larger network and more complex training process. In contrast, HA-CNN is much more lightweight on parameter size with the advantage of easy training and fast deployment. This shows that our attention joint learning can be a better replacement of existing complex networks with timeconsuming model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Further Analysis and Discussions</head><p>Effect of Different Types of Attention We further evaluated the effect of each individual attention component in our HA model: Soft Spatial Attention (SSA), Soft Channel Attention (SCA), and Hard Regional Attention (HRA). <ref type="table">Table 5</ref> shows that: (1) Any of the three attention in isolation brings person re-id performance gain; (2) The combination of SSA and SCA gives further accuracy boost, which suggests the complementary information between the two soft attention discovered by our model; (3) When combining the hard and soft attention, another significant performance gain is obtained. This shows that our method is effective in identifying and exploiting the complementary information between coarse hard attention and fine-grained soft attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Cross-Attention Interaction Learning</head><p>We also evaluated the benefit of cross-attention interaction learning (CAIL) between global and local branches. <ref type="table" target="#tab_6">Table 6</ref> shows that CAIL has significant benefit to re-id matching,  DukeMTMC-ReID, respectively. This validates our design is rational that it is necessary to jointly learn the attended feature representations across soft and hard attention subject to the same re-id label constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Joint Local and Global Features</head><p>We evaluated the effect of joint local and global features by comparing their individual re-id performances against that of the joint feature.  Visualisation of Harmonious Attention We visualise both learned soft attention and hard attention at three different levels of HA-CNN. <ref type="figure">Figure 6</ref> shows: (1) Hard attention localises four body parts well at all three levels, approximately corresponding to head+shoulder (red), upper-body (blue), upper-leg (green) and lower-leg (violet).</p><p>(2) Soft attention focuses on the discriminative pixel-wise selections progressively in spatial localisation, e.g. attending hierarchically from the global whole body by the 1 st -level spatial SA (c) to local salient parts (e.g. object associations) by the 3 rd -level spatial SA (g). This shows compellingly the effectiveness of joint soft and hard attention learning. Model Complexity We compare the proposed HA-CNN model with four popular CNN architectures (Alexnet <ref type="bibr" target="#b14">[15]</ref>, VGG16 <ref type="bibr" target="#b25">[26]</ref>, GoogLeNet <ref type="bibr" target="#b29">[30]</ref>, and ResNet50 <ref type="bibr" target="#b7">[8]</ref>) in model size and complexity. <ref type="table" target="#tab_9">Table 8</ref> shows that HA-CNN has the smallest model size (2.7 million parameters) and the 2 nd smallest FLOPs (1.09 × 10 9 ) and yet, still retains the 2 nd deepest structure <ref type="bibr" target="#b38">(39)</ref>.</p><p>(a) (b) (c) (d) (e) (f) (g) <ref type="figure">Figure 6</ref>. Visualisation of our harmonious attention in person re-id. From left to right, (a) the original image, (b) the 1 st -level of HA, (c) the 1 st -level of SA, (d) the 2 nd -level of HA, (e) the 2 nd -level of SA, (f) the 3 rd level of HA, (g) the 3 rd -level of SA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we presented a novel Harmonious Attention Convolutional Neural Network (HA-CNN) for joint learning of person re-identification attention selection and feature representations in an end-to-end fashion. In contrast to most existing re-id methods that either ignore the matching misalignment problem or exploit stringent attention learning algorithms, the proposed model is capable of extracting/exploiting multiple complementary attention and maximising their latent complementary effect for person re-id in a unified lightweight CNN architecture. This is made possible by the Harmonious Attention module design in combination with a two-branches CNN architecture. Moreover, we introduce a cross-attention interaction learning mechanism to further optimise joint attention selection and reid feature learning. Extensive evaluations were conducted on three re-id benchmarks to validate the advantages of the proposed HA-CNN model over a wide range of state-of-theart methods on both manually labelled and more challenging auto-detected person images. We also provided detailed model component analysis and discussed HA-CNN's model complexity as compared to popular alternatives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of attention selection in auto-detected person bounding boxes used for person re-id matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The Harmonious Attention Convoluntional Neural Network. The symbol d l (l ∈ {1, 2, 3}) denotes the number of convolutional filter in the corresponding Inception unit at the l-th block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>whole soft attention (b) spatial attention (c) channel attention (d) hard attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>The structure of each Harmonious Attention module consists of (a) Soft Attention which includes (b) Spatial Attention (pixel-wise) and (c) Channel Attention (scale-wise), and (d) Hard</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Schematic comparison between (a) STN<ref type="bibr" target="#b10">[11]</ref> and (b) HA-CNN Hard Attention. Global feature and hard attention are jointly learned in a multi-task design. "H": Hard attention module; "Fg": Global feature module; "Fg": Local feature module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Example cross-view matched pairs from three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Implementation DetailsWe implemented our HA-CNN model in the Tensorflow<ref type="bibr" target="#b0">[1]</ref> framework. All person images are resized to 160×64. For HA-CNN architecture, we set the width of Inception units at the 1 st /2 nd /3 rd levels as: d 1 = 128, d 2 = 256 and d 3 = 384. Following</figDesc><table><row><cell cols="3">Re-id evaluation protocol. TS: Test Setting; SS: Single-</cell></row><row><cell cols="3">Shot; MS: Multi-Shot. SQ: Single-Query; MQ: Multi-Query.</cell></row><row><cell>Dataset</cell><cell cols="2"># ID # Train # Test# Image Test Setting</cell></row><row><cell>CUHK03</cell><cell>1,467 767 700 14,097</cell><cell>SS</cell></row><row><cell cols="3">Market-1501 1,501 751 750 32,668 SQ/MQ</cell></row><row><cell cols="2">DukeMCMT-ReID 1,402 702 702 36,411</cell><cell>SQ</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>shows</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Market-1501 evaluation. 1 st /2 nd best in red/blue.</figDesc><table><row><cell>Query Type</cell><cell cols="2">Single-Query</cell><cell cols="2">Multi-Query</cell></row><row><cell>Measure (%)</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell>XQDA[20]</cell><cell>43.8</cell><cell>22.2</cell><cell>54.1</cell><cell>28.4</cell></row><row><cell>SCS[2]</cell><cell>51.9</cell><cell>26.3</cell><cell>-</cell><cell>-</cell></row><row><cell>DNS[41]</cell><cell>61.0</cell><cell>35.6</cell><cell>71.5</cell><cell>46.0</cell></row><row><cell>CRAFT[4]</cell><cell>68.7</cell><cell>42.3</cell><cell>77.0</cell><cell>50.3</cell></row><row><cell>CAN[21]</cell><cell>60.3</cell><cell>35.9</cell><cell>72.1</cell><cell>47.9</cell></row><row><cell>S-LSTM[32]</cell><cell>-</cell><cell>-</cell><cell>61.6</cell><cell>35.3</cell></row><row><cell>G-SCNN[31]</cell><cell>65.8</cell><cell>39.5</cell><cell>76.0</cell><cell>48.4</cell></row><row><cell>HPN [22]</cell><cell>76.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SVDNet [28]</cell><cell>82.3</cell><cell>62.1</cell><cell>-</cell><cell>-</cell></row><row><cell>MSCAN [17]</cell><cell>80.3</cell><cell>57.5</cell><cell>86.8</cell><cell>66.7</cell></row><row><cell>DLPA [42]</cell><cell>81.0</cell><cell>63.4</cell><cell>-</cell><cell>-</cell></row><row><cell>PDC [27]</cell><cell>84.1</cell><cell>63.4</cell><cell>-</cell><cell>-</cell></row><row><cell>JLML [19]</cell><cell>85.1</cell><cell>65.5</cell><cell>89.7</cell><cell>74.5</cell></row><row><cell>HA-CNN</cell><cell>91.2</cell><cell>75.7</cell><cell>93.8</cell><cell>82.8</cell></row><row><cell cols="5">Table 3. DukeMTMC-ReID evaluation. 1 st /2 nd best in red/blue.</cell></row><row><cell cols="2">Measure (%)</cell><cell></cell><cell>R1</cell><cell>mAP</cell></row><row><cell cols="2">BoW+KISSME [37]</cell><cell></cell><cell>25.1</cell><cell>12.2</cell></row><row><cell cols="2">LOMO+XQDA [20]</cell><cell></cell><cell>30.8</cell><cell>17.0</cell></row><row><cell cols="2">ResNet50 [8]</cell><cell></cell><cell>65.2</cell><cell>45.0</cell></row><row><cell cols="2">ResNet50+LSRO [47]</cell><cell></cell><cell>67.7</cell><cell>47.1</cell></row><row><cell cols="2">JLML [19]</cell><cell></cell><cell>73.3</cell><cell>56.4</cell></row><row><cell cols="2">SVDNet-CaffeNet [28]</cell><cell></cell><cell>67.6</cell><cell>45.8</cell></row><row><cell cols="2">SVDNet-ResNet50 [28]</cell><cell></cell><cell>76.7</cell><cell>56.8</cell></row><row><cell>HA-CNN</cell><cell></cell><cell></cell><cell>80.5</cell><cell>63.8</cell></row><row><cell cols="5">lower model training and test cost through an much easier</cell></row><row><cell cols="5">training process. For example, the performance by SVD-</cell></row><row><cell cols="5">Net relies on the heavy ResNet50 CNN model (23.5 million</cell></row><row><cell cols="5">parameters) with the need for model pre-training on the Im-</cell></row><row><cell cols="5">ageNet data (1.2 million images), whilst HA-CNN has only</cell></row><row><cell cols="5">2.7 million parameters with no data augmentation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>CUHK03 evaluation. The setting is 767/700 training/test split. 1 st /2 nd best in red/blue.</figDesc><table><row><cell cols="2">Measure (%)</cell><cell cols="4">Labelled R1 mAP R1 mAP Detected</cell></row><row><cell cols="2">BoW+XQDA [37]</cell><cell>7.9</cell><cell>7.3</cell><cell>6.4</cell><cell>6.4</cell></row><row><cell cols="2">LOMO+XQDA [20]</cell><cell cols="4">14.8 13.6 12.8 11.5</cell></row><row><cell cols="2">IDE-C [48]</cell><cell cols="4">15.6 14.9 15.1 14.2</cell></row><row><cell cols="2">IDE-C+XQDA [48]</cell><cell cols="4">21.9 20.0 21.1 19.0</cell></row><row><cell cols="2">IDE-R [48]</cell><cell cols="4">22.2 21.0 21.3 19.7</cell></row><row><cell cols="2">IDE-R+XQDA [48]</cell><cell cols="4">32.0 29.6 31.1 28.2</cell></row><row><cell cols="2">SVDNet-CaffeNet [28]</cell><cell>-</cell><cell>-</cell><cell cols="2">27.7 24.9</cell></row><row><cell cols="2">SVDNet-ResNet50 [28]</cell><cell>-</cell><cell>-</cell><cell cols="2">41.5 37.3</cell></row><row><cell>HA-CNN</cell><cell></cell><cell cols="4">44.4 41.0 41.7 38.6</cell></row><row><cell cols="6">Table 5. Evaluating individual types of attention in our HA model.</cell></row><row><cell cols="6">Setting: SQ. SSA: Soft Spatial Attention; SCA: Soft Channel At-</cell></row><row><cell cols="3">tention; HRA: Hard Regional Attention.</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">Market-1501</cell><cell cols="3">DukeMTMC-ReID</cell></row><row><cell>Metric (%)</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell></cell><cell>mAP</cell></row><row><cell>No Attention</cell><cell>84.7</cell><cell>65.3</cell><cell>72.4</cell><cell></cell><cell>53.4</cell></row><row><cell>SSA</cell><cell>85.5</cell><cell>65.8</cell><cell>73.9</cell><cell></cell><cell>54.8</cell></row><row><cell>SCA</cell><cell>86.8</cell><cell>67.9</cell><cell>73.7</cell><cell></cell><cell>53.5</cell></row><row><cell>SSA+SCA</cell><cell>88.5</cell><cell>70.2</cell><cell>76.1</cell><cell></cell><cell>57.2</cell></row><row><cell>HRA</cell><cell>88.2</cell><cell>71.0</cell><cell>75.3</cell><cell></cell><cell>58.4</cell></row><row><cell>All</cell><cell>91.2</cell><cell>75.7</cell><cell>80.5</cell><cell></cell><cell>63.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Evaluating cross-attention interaction learning (CAIL). Setting: SQ.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Market-1501 DukeMTMC-ReID</cell></row><row><cell cols="2">Metric (%) R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell cols="4">w/o CAIL 86.6 66.2 74.0</cell><cell>55.4</cell></row><row><cell>w/ CAIL</cell><cell cols="3">91.2 75.7 80.5</cell><cell>63.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>shows: (1) Either feature represen-</cell></row><row><cell>tation alone is already very discriminative for person re-id.</cell></row><row><cell>For instance, the global HA-CNN feature outperforms the</cell></row><row><cell>best alternative JLML [19] (Table 2) by 4.8%(89.9-85.1) in</cell></row><row><cell>Rank-1 and by 7.0%(72.5-65.5) in mAP (SQ) on Market-</cell></row><row><cell>1501. (2) A further performance gain is obtained by joining</cell></row><row><cell>the two representations, yielding 6.1%(91.2-85.1) in Rank-</cell></row><row><cell>1 boost and 10.2%(75.7-65.5) in mAP increase. Similar</cell></row><row><cell>trends are observed on the DukeMCMT-ReID (Table 3).</cell></row><row><cell>These validate the complementary effect of jointly learning</cell></row><row><cell>local and global features in harmonious attention context by</cell></row><row><cell>our HA-CNN model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Evaluating global-level and local-level features. Setting: SQ.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Market-1501 DukeMTMC-ReID</cell></row><row><cell>Metric (%)</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell>Global</cell><cell cols="3">89.9 72.5 78.9</cell><cell>60.0</cell></row><row><cell>Local</cell><cell cols="3">88.9 71.7 77.3</cell><cell>59.5</cell></row><row><cell cols="4">Global+Local 91.2 75.7 80.5</cell><cell>63.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Comparisons of model size and complexity. FLOPs: the number of FLoating-point OPerations; PN: Parameter Number.</figDesc><table><row><cell>Model</cell><cell>FLOPs</cell><cell cols="2">PN (million) Depth</cell></row><row><cell>AlexNet [15]</cell><cell>7.25×10 8</cell><cell>58.3</cell><cell>7</cell></row><row><cell>VGG16 [26]</cell><cell>1.55×10 10</cell><cell>134.2</cell><cell>16</cell></row><row><cell>ResNet50 [8]</cell><cell>3.80×10 9</cell><cell>23.5</cell><cell>50</cell></row><row><cell>GoogLeNet [30]</cell><cell>1.57×10 9</cell><cell>6.0</cell><cell>22</cell></row><row><cell>JLML</cell><cell>1.54×10 9</cell><cell>7.2</cell><cell>39</cell></row><row><cell>HA-CNN</cell><cell>1.09×10 9</cell><cell>2.7</cell><cell>39</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This choice is independent of our model design and others can be readily considered such as AlexNet<ref type="bibr" target="#b14">[15]</ref>, ResNet<ref type="bibr" target="#b7">[8]</ref> and VggNet<ref type="bibr" target="#b25">[26]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Only a small number of methods (seeTable 3) have been evaluated and reported on DukeMTMC-ReID.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<title level="m">Tensorflow: A system for large-scale machine learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A multi-task deep network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Person reidentification by camera correlation aware feature augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introduction to algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning attention selection for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Person re-identification by deep joint learning of multi-loss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiscale deep learning architectures for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Person re-identification with correspondence structure learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Posedriven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dorsal and ventral attention systems: distinct neural circuits but collaborative roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vossel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Neuroscientist</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="159" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint learning of single-image and cross-image representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised learning of generative topic saliency for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Highly efficient regression for scalable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Person reidentification by discriminative selection in video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Person reidentification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
		<idno>ECCV. 2014. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Re-identification by relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="653" to="668" />
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4678" to="4686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
