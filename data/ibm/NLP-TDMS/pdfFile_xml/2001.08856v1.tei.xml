<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahia</forename><forename type="middle">Saeed</forename><surname>Assiri</surname></persName>
							<email>ys34747w@pace.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Pace University</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>plain CNNs</term>
					<term>data augmentation</term>
					<term>regularization</term>
					<term>MNIST</term>
					<term>state of the art</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks have been achieving the best possible accuracies in many visual pattern classification problems. However, due to the model capacity required to capture such representations, they are often oversensitive to overfitting and therefore require proper regularization to generalize well. In this paper, we present a combination of regularization techniques which work together to get better performance, we built plain CNNs, and then we used data augmentation, dropout and customized early stopping function, we tested and evaluated these techniques by applying models on five famous datasets, MNIST, CIFAR10, CIFAR100, SVHN, STL10, and we achieved three state-of-the-art-of (MNIST, SVHN, STL10) and very high-Accuracy on the other two datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Learning has the main rule of the improvement in the field of computer vision, resulting in state-of-the-art performance in many challenging tasks <ref type="bibr" target="#b3">[4]</ref>, such as object recognition <ref type="bibr" target="#b1">[2]</ref>, semantic segmentation <ref type="bibr" target="#b0">[1]</ref>, image captioning <ref type="bibr" target="#b10">[11]</ref>, and human pose estimation <ref type="bibr" target="#b12">[12]</ref>. Using the convolutional neural networks (CNNs) <ref type="bibr" target="#b9">[10]</ref> was the main reason for many of the significant accomplishments over the past few years because they can learn deep feature representations of images. However, as complexity increases, the resource utilization of such models also increases. Modern deep networks contain hundreds of millions of parameters which provide the necessary representational power for such tasks <ref type="bibr" target="#b2">[3]</ref>, as a result of the huge representational power the probability of overfitting increases and leads to poor generalization. To fight the overfitting, different regularization techniques can be applied, such as data augmentation, dropout, and early stopping. Data augmentation is a very famous technique due to its ease of implementation and effectiveness. Using image transforms such as rotation shearing scaling cropping, or flipping can be applied to create new data which can be used to improve accuracy <ref type="bibr" target="#b9">[10]</ref>. Large models can also be regularized by adding noise during the training process, whether it be added to the input <ref type="bibr" target="#b3">[4]</ref>, weights, or gradients. One of the most common uses of noise for improving model accuracy is dropout <ref type="bibr" target="#b7">[8]</ref>, which stochastically drops neuron activations during training and, as a result, discourages the co-adaptation of feature detectors. In this work, we consider applying different data augmentation techniques combined with dropout. These techniques encourage plain convolutional networks to achieve better generalization and get better results in the validation and testing phase.</p><p>In the remainder of this paper, we introduce collected optimization methods and demonstrate that using data augmentation and dropout can improve model robustness and lead to better model performance. We show that these simple methods work with a simple plain convolutional neural networks model and can also be combined with most regularization techniques, including learning rate and early stopping and other regularization techniques in a very simple manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Augmentation for Image</head><p>Data augmentation is an effective technique for improving the accuracy of modern image classifiers, and it has long been used in practice when training convolutional neural networks. LeCun et al. used many affine transformations, like horizontal and vertical translation, scaling, squeezing, and shearing, to improve their model's accuracy when training LeNet5 <ref type="bibr" target="#b9">[10]</ref>. Krizhevsky did apply image mirroring, cropping, as well as randomly adjusting color and intensity values based on ranges determined using principal component analysis on ImageNet dataset to improve the performance of AlexNet <ref type="bibr" target="#b7">[8]</ref> for the 2012 ImageNet Large Scale Visual Recognition Competition <ref type="bibr" target="#b3">[4]</ref>.</p><p>Bengio et al. <ref type="bibr" target="#b4">[5]</ref> did apply a large variety of transformations to a handwritten character dataset, for example, local elastic deformation, motion blur, Gaussian smoothing, Gaussian noise, salt and pepper noise, pixel permutation and affine transformations <ref type="bibr" target="#b3">[4]</ref>.</p><p>When training Deep Image <ref type="bibr" target="#b13">[13]</ref> on the ImageNet dataset, Wu et al. did apply a wide range of color casting, vignetting, rotation, and lens distortion, as well as horizontal and vertical stretching.</p><p>Lemley et al.came up with a learned end-to-end approach called Smart Augmentation <ref type="bibr" target="#b5">[6]</ref> instead of relying on hard-coded transformations. In this method, a neural network is trained to intelligently combine existing samples to generate additional data that is useful for the training process. <ref type="bibr" target="#b3">[4]</ref>.</p><p>DeVries et al. came up with a new technique called Cutout, which is <ref type="bibr" target="#b3">[4]</ref> closest to the occlusions technique. However, occlusions generally take the form of scratches, dots, or scribbles that overlay the target character, while cutout use zero-masking to completely obstruct an entire region. Cutout can be interpreted as applying a spatial dropout in input space, much in the same way that convolutional neural networks leverage information about spatial structure to improve performance over that of feedforward networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dropout in Neural Networks</head><p>The second common regularization technique that we used in our models is dropout <ref type="bibr" target="#b7">[8]</ref>, which was first introduced by Hinton et al., they used the method with a range of different neural networks on different problem types achieving improved results, including handwritten digit recognition (MNIST), photo classification (CIFAR-10), and speech recognition (TIMIT) <ref type="bibr" target="#b20">[20]</ref>. Dropout is implemented by setting hidden unit activations to zero with some fixed probability during training. All activations are kept when evaluating the network, but the resulting output is scaled according to the dropout probability. This technique has the effect of approximately averaging over an exponential number of smaller sub-networks and works well as a robust type of bagging, which discourages the co-adaptation of feature detectors within the network. <ref type="bibr" target="#b3">[4]</ref> Nitish Srivastava et al. <ref type="bibr" target="#b16">[16]</ref> used dropout on a wide range of computer vision, speech recognition, and text classification tasks and found that it consistently improved performance on each problem. George Dahl et al. <ref type="bibr" target="#b19">[19]</ref> used a deep neural network with rectified linear activation functions and dropout to achieve state-of-the-art results on a standard speech recognition task <ref type="bibr" target="#b20">[20]</ref>. Tompson et al. introduce Spatial Dropout <ref type="bibr">[14]</ref>, which randomly discards entire feature maps rather than individual pixels.</p><p>While dropout was found to be very effective at regularizing fully connected layers, we discovered that it does have the same powerful at convolutional neural when used at the best place and when used with the best rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Early Stopping</head><p>Early stopping is a form of regularization used to avoid overfitting when using some methods, such as gradient descent. Such methods update the learner to make it better fit the training data with each iteration. The early stopping rule is to stop the model training at a certain number of iterations to keep the validation dataset free of overfitting. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Implementation Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architecture Concept</head><p>One consideration here is the number of weights that can be set independently, the more of those we have, the greater the risk of overfitting, and the greater the training time. So, increasing this number makes training take longer and runs a higher risk of overfitting, but potentially increases the expressiveness of our neural network. We did want the number to be as small as possible without sacrificing accuracy. So, trying something small and increase it until the model stops getting improving in the accuracy, we also considered this concept when setting the dropout rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Architecture and Design</head><p>As a result of this concept, We built simple convolutional network models <ref type="bibr" target="#b8">[9]</ref> (plain convolutional neural networks with no residual blocks ), four layers for MNIST dataset, and 11 layers for Cifar10, Cifar100, SVHN datasets and 13 layers for STL10, The networks employ a homogeneous design utilizing 3×3 kernels for convolutional layer and 2×2 kernels for max-pooling operations, the number of model layers depends on the size of input image, for instance, MNIST has 28*28 grayscale images, this reduced the number of layers to only four Layers; however, STL10 has 96*96*3 in this case the number of layers is 13. Applying max-pooling (2*2) after every two convolutional layers helped the model not only to control the depth of the model but also reduced variance and reduced computation complexity (as 2*2 max-pooling/average pooling reduce 75% of data), and extract low-level features from neighborhood, Relu activation function was added before every max-pooling layer, and there is a dropout layer following every max-pooling and then end the model with wide classifier layer which is composed of two fully connected layers followed by dropout layer and then the last layer which is the softmax layer.</p><p>The learning rate is 0.01 and the batch sizes used are different upon the dataset's image size, for the MNIST dataset, we found that large batch size is very useful and reduced the fluctuating on the validation set, so the batch size for MNIST dataset was 256 and for Cifar10 Cifar100 and SVHN batch size was 128, however for STL10 the batch size was only 8, we considered the size of the image (96*96), so we used a tradeoff strategy, if the image size is very big the batch size is small however when the image size is small, the batch size becomes larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Our customized early stopping</head><p>We built a naive early stopping to watch the validation curves, we built it in order to control the fluctuating phenomenon by making a baseline point and then asking the model not to stop unless passing this point, and that helped the models to catch highest possible accuracy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Augmentation techniques</head><p>After applying the trial and error method, we combined five data augmentation techniques, <ref type="table">Table 6</ref>, and we found that flipping techniques did not help the model to generalize during training, they negatively affected the model; as a result, we did not use them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments on MNIST</head><p>MNIST dataset <ref type="bibr" target="#b9">Lecun et al. (1998)</ref>  <ref type="bibr" target="#b6">[7]</ref> consists of 70,000 28x28 grayscale images of handwritten digits 0 to 9, of which 60,000 are used for training, and 10,000 are used for testing.</p><p>After we formulated our model, we conducted our experiments on MNIST through two stages, The first stage is applying three paradigms of Dropout technique, <ref type="table">Table 1</ref>, in the first paradigm we applied regular dropout after the fully connected layer on our model without inserting any dropout layer in the convolutional part, second paradigm we applied spatial Dropout before each max-pooling layer in the part of the convolutional network without applying dropout after the fully connected layer, and finally, in the third paradigm, we applied mixed technique which is a combination of inserting Spatial Dropout before each max-pooling layer and also inserting regular dropout after the fully connected layer, and the number of the epoch is 2500 <ref type="figure" target="#fig_0">, Fig 1,</ref> we achieved very high accuracy on all three paradigms, <ref type="table">Table 1</ref>, however, the highest accuracy was achieved is by applying the Dropout only after the fully connected layer. <ref type="table">Table 1</ref>. classification error of 3 methods applied to Dropout on MNIST.  To test the robustness of Dropout we considered the results that we achieved through the first stage and chose the paradigm which only has the dropout layer after the fully connected layer, and then started the second stage which aimed to figure out what is the best ratio of regular dropout that we should use and what is the size of fully connected layer considered the best to be selected. We found there is a direct proportion of the size of the fully connected layer and the ratio of dropout out, so we conducted this stage by gradually and simultaneously increasing the size of the fully connected layer and the rate of dropout. We found the best size of the fully connected layer is 2048, and the best ratio of the dropout is 0.8, and to make sure of the experiment´s outcome, we tested this algorithm five times with the same parameters as shown in <ref type="table">Table 2</ref>. Our experiments achieved a new state of the art, <ref type="table">Table 3</ref>, of all five runs, the highest accuracy achieved was 99.83 % with error rate only 0.17 %.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments on other Data Sets</head><p>We used four datasets, in addition to MNIST, to evaluate our models SVHN, CIFAR-10, CIFAR-100, and STL10. These data sets include different image types and training set sizes. Our Models achieved state-of-the-art results on three datasets MNIST, SVHN, and STL10, <ref type="table" target="#tab_3">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">CIFAR10 AND CIFAR100</head><p>Both of the CIFAR datasets consist of 60,000 color images of size 32×32 pixels. CIFAR-10 has ten distinct classes, such as cat, dog, car, and boat. CIFAR-100 contains 100 classes <ref type="bibr" target="#b3">[4]</ref>. For those two datasets, we applied dropout to convolutional neural networks. The best architecture that we found has 11 convolutional layers, followed by two fully connected layers. All activation functions used are ReLUs. A max-pooling layer followed each convolutional layer, and dropout was applied after the max-pooling to all layers of the network with the probability of (0.25). <ref type="figure">Fig. 3</ref>. MNIST Accuracy-Where the customized early stopping happened for the 5th experiment (the error rate on the test set =0.17%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">SVHN</head><p>The Street View House Numbers (SVHN) dataset contains a total of 630,420 color images with a resolution of 32×32 pixels. Each image is centered on a number from one to ten, which needs to be identified. The official dataset split contains 73,257 training images and 26,032 test images, but there are also 531,131 additional training images available, we used both available training sets when training our models. For this data set, we applied regular dropout to the convolutional neural networks part, and we did not use the spatial dropout in this experiment. The best architecture that we found has 11 convolutional layers, followed by two fully connected layers. All activation units were ReLUs. A max-pooling layer followed each convolutional layer, and dropout was applied to all the layers of the network with the probability of (0.25); the fully connected layer size was 1024, followed by a dropout with ratio 0.4. We achieved new state the art for SVHN if only using Plain Convolutional Neural Networks, with no residual block, we achieved 98.50 % with an only error rate of 1.5%, and when comparing with residual neural networks, our networks achieved the fourth place.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">STL-10</head><p>The STL-10 dataset <ref type="bibr" target="#b3">[4]</ref> consists of a total of 113,000 color images with a resolution of 96×96 pixels. The training set only contains 5,000 images, while the test set consists of 8,000 images. All training and test set images belong to one of ten classes, such as airplane, bird, or horse. The remainder of the dataset is composed of 100,000 unlabeled images belonging to the target ten classes, plus additional but visually similar classes. While the main purpose of the STL-10 dataset is to test semi-supervised learning algorithms, we used it to observe how our collected optimization method performs when applied to higher resolution images with a small training set. For this reason, we did not use the unlabeled portion, which is the larger portion of the dataset, and only used the labeled training set. For this data set, we applied regular dropout to the convolutional neural networks part, and we did not use the spatial dropout in this experiment. The best architecture that we found has 13 convolutional layers, followed by two fully connected layers. All activation units were ReLUs. A max-pooling layer followed each convolutional layer, and dropout was applied to all the layers of the network with the probability of (0.25), the fully connected layer size was 1024, followed by a dropout with ratio 0.4. Our model was able to achieve the best accuracy ever on supervised classification of STL-10 and was able to defeat cut out <ref type="bibr" target="#b3">[4]</ref> without "cutout" tech, and without "residual block," the model achieved 88.08% with only error rate 11.92%, <ref type="table" target="#tab_4">Table 5</ref>.   <ref type="bibr" target="#b15">[15]</ref> (1.69%) <ref type="table">Table 6</ref>. Augmentation techniques used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>The Simple Plain Convolutional Neural Networks model is still able to achieve outstanding performance and to compete for the residual Neural networks technique,and when comparing the number of trainable parameters of both techniques we can recognize how valuable using the plain model, the parameters numbers reduced by %80 and CNNs, the smaller simple models, able to get the best performance for some datasets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>MNIST Accuracy -first 2000 epoch before the early stopping condition achieved (all three paradigms needed as minimum 2000 epoch).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>MNIST Accuracy-Where the customized early stopping happened for the regular Dropout experiment, the model strongly generalized with FC =2048 and dropout=0.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>SVHN Accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>STL10 Accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>results of all five experiments done on MNIST by using FC=2048 and Dropout= 0.8. Top MNIST Results.</figDesc><table><row><cell>Method</cell><cell>Error rate</cell></row><row><cell>DropConnectWan et al. (2013) [7]</cell><cell>0.21%</cell></row><row><cell>Multi-column DNN for Image Classification Ciregan</cell><cell>0.23%</cell></row><row><cell>et al. (2012) [18]</cell><cell></cell></row><row><cell>APAC Sato et al. (2015) [17]</cell><cell>0.23%</cell></row><row><cell>Generalizing Pooling Functions in CNN Lee et al.</cell><cell>0.29%</cell></row><row><cell>(2016) [15]</cell><cell></cell></row><row><cell>Stochastic Optimization of Plain Convolutional</cell><cell>0.17%</cell></row><row><cell>Neural Networks with simple methods</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Total experiments result in all five datasets.</figDesc><table><row><cell>Dataset</cell><cell>%Accu-</cell><cell>%error</cell><cell>The total of model params</cell></row><row><cell></cell><cell>racy</cell><cell></cell><cell></cell></row><row><cell>MNIST</cell><cell>99.83</cell><cell>0.17</cell><cell>&gt;1,400.000</cell></row><row><cell>Cifar10</cell><cell>94.29</cell><cell>5.71</cell><cell>4,252,298</cell></row><row><cell>Cifar100</cell><cell>72.96</cell><cell>27.04</cell><cell>&gt;4,252,298</cell></row><row><cell>SVHN</cell><cell>98.50</cell><cell>1.50</cell><cell>4,252,298</cell></row><row><cell>STL10</cell><cell>88.08</cell><cell>11.92</cell><cell>&gt;5,000,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Datasets of the new State of the art achieved.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Accuracy% % error</cell><cell>New state of</cell><cell>The previous state of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>the art?</cell><cell>the art (%error)</cell></row><row><cell>MNIST</cell><cell>99.83</cell><cell></cell><cell>0.17%</cell><cell>Yes</cell><cell>DropConnect Wan et al.[7] (2013)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(0.21%)</cell></row><row><cell>STL10</cell><cell>88.08</cell><cell>%</cell><cell>11.92</cell><cell>Yes</cell><cell>Improved Regu-larization with cutout Cutout. (2017)[4]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(12.74±0.23)%</cell></row><row><cell>SVHN</cell><cell>98.50</cell><cell></cell><cell>1.50%</cell><cell>Yes (in</cell><cell>Generalizing</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>plain Cnn)</cell><cell>Pooling Functions</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fourth over-</cell><cell>in Convolutional</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>all</cell><cell>Neural Networks:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mixed, Gated, and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tree(2016)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An analysis of deep neural network models for practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits &amp; Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improved Regularization of Convolutional Neural Networks with Cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learners benefit more from out-of-distribution examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chherawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="164" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Smart augmentation-learning an optimal data augmentation strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bazrafkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corcoran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Hasanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rouhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<title level="m">LET&apos;S KEEP IT SIMPLE, USING SIMPLE ARCHITECTURES TO OUTPERFORM DEEPER AND MORE COMPLEX ARCHITECTURES</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks.A. Toshev and C. Szegedy</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep image: Scaling up image recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.02876</idno>
		<idno>2015. 14</idno>
	</analytic>
	<monogr>
		<title level="j">J. Tompson, R. Goroshin, A. Jain, Y. LeCun and C. Bregler. In CVPR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="648" to="656" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalizing Pooling Functions in CNNs: Mixed, Gated, and Tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2017.2703082</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="863" to="875" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krizhevsky</forename><surname>Hinton H</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">APAC: Augmented PAttern Classification with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yokoi</surname></persName>
		</author>
		<ptr target="http://adsabs.harvard.edu/abs/2015arXiv150503229S" />
		<editor>Adsabs.harvard</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Idsia</surname></persName>
		</author>
		<ptr target="http://www.idsia.ch/~ciresan/data/cvpr2012.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for LVCSR using rectified linear units and dropout</title>
		<ptr target="http://www.cs.toronto.edu/~gdahl/papers/reluDropoutBN_icassp2013.pdf" />
	</analytic>
	<monogr>
		<title level="m">Cs.toronto</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Gentle Introduction to Dropout for Regularizing Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brownlee</surname></persName>
		</author>
		<ptr target="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning Mastery</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
