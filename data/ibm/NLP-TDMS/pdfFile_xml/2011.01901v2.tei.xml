<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING VISUAL REPRESENTATIONS FOR TRANS- FER LEARNING BY SUPPRESSING TEXTURE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlok</forename><surname>Mishra</surname></persName>
							<email>shlokm@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Shah</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Gwangju Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
							<email>abhinav@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jacobs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING VISUAL REPRESENTATIONS FOR TRANS- FER LEARNING BY SUPPRESSING TEXTURE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent literature has shown that features obtained from supervised training of CNNs may over-emphasize texture rather than encoding high-level information. In self-supervised learning in particular, texture as a low-level cue may provide shortcuts that prevent the network from learning higher level representations. To address these problems we propose to use classic methods based on anisotropic diffusion to augment training using images with suppressed texture. This simple method helps retain important edge information and suppress texture at the same time. We empirically show that our method achieves state-of-the-art results on object detection and image classification with eight diverse datasets in either supervised or self-supervised learning tasks such as MoCoV2 and Jigsaw. Our method is particularly effective for transfer learning tasks and we observed improved performance on five standard transfer learning datasets. The large improvements (up to 11.49%) on the Sketch-ImageNet dataset, DTD dataset and additional visual analyses with saliency maps suggest that our approach helps in learning better representations that better transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep convolutional neural networks (CNNs) can learn powerful visual features that have resulted in significant improvements on many computer vision tasks such as semantic segmentation <ref type="bibr" target="#b41">(Shelhamer et al., 2017)</ref>, object recognition <ref type="bibr" target="#b22">(Krizhevsky et al., 2012)</ref>, and object detection . However, CNNs often fail to generalize well across datasets under domain-shift due to varied lighting, sensor resolution, spectral-response etc. One of the reasons for this poor generalization is CNNs' over reliance on low-level cues like texture <ref type="bibr" target="#b13">(Geirhos et al., 2018)</ref>. These low-level cues and texture biases have been identified as grave challenges to various learning paradigms ranging from supervised learning <ref type="bibr" target="#b1">(Brendel &amp; Bethge, 2019;</ref><ref type="bibr" target="#b13">Geirhos et al., 2018;</ref><ref type="bibr" target="#b39">Ringer et al., 2019)</ref> to self-supervised learning (SSL) <ref type="bibr" target="#b34">(Noroozi &amp; Favaro, 2016;</ref><ref type="bibr" target="#b35">Noroozi et al., 2018;</ref><ref type="bibr" target="#b11">Doersch et al., 2015;</ref><ref type="bibr" target="#b2">Caron et al., 2018;</ref><ref type="bibr" target="#b10">Devlin et al., 2019)</ref>.</p><p>We focus on learning visual representation that are robust to changes in low-level information, like texture cues. Specifically, we propose to use classical tools to suppress texture in images, as a form of data augmentation, to encourage deep neural networks to focus more on learning representations that are less dependent on textural cues. We use the Perona-Malik non-linear diffusion method <ref type="bibr" target="#b36">(Perona &amp; Malik, 1990)</ref>, robust Anistropic diffusion <ref type="bibr" target="#b0">(Black et al., 1998)</ref>, and Bilateral filtering <ref type="bibr" target="#b43">(Tomasi &amp; Manduchi, 1998b)</ref> to augment our training data. These methods suppress texture while retaining structure, by preserving boundaries.</p><p>Our work is inspired by the observations that ImageNet pre-trained models fail to generalize well across datasets <ref type="bibr" target="#b13">(Geirhos et al., 2018;</ref><ref type="bibr" target="#b37">Recht et al., 2019)</ref>, due to over-reliance on texture and lowlevel features. Stylized-ImageNet <ref type="bibr" target="#b13">(Geirhos et al., 2018)</ref> attempted to modify the texture from images by using style-transfer to render images in the style of randomly selected paintings from the Kaggle paintings dataset. However, this approach offers little control over exactly which cues are removed from the image. The resulting images sometimes retain texture and distort the original shape. In our approach <ref type="figure">(Fig. 1)</ref>, we suppress the texture instead of modifying it. We empirically show that  <ref type="figure">Figure 1</ref>: An overview of our approach. We propose to augment the ImageNet dataset by a version of the dataset with Anisotropic diffused images. The use of this augmentation helps the network rely less on texture information and increases performance in diverse experiments.</p><p>Figure 2: Examples of images from Sketch-ImageNet. Images have very little or no texture, which implies texture will have little to no impact on object classification. this helps in learning better higher level representations and works better than CNN-based stylized augmentation. We compare our approach with Gaussian blur augmentation, recently used in <ref type="bibr" target="#b3">(Chen et al., 2020a;</ref>, and show that Anisotropic-filtering for texture suppression is better, because Gaussian blur can potentially suppress edges and other higher-level semantic information as well.</p><p>Our approach yields consistent improvements in both supervised and self-supervised learning settings for learning representations that generalize well across different datasets. For the supervised setting, we pre-train on ImageNet, and test on eight different datasets including ImageNet, Pascal VOC <ref type="bibr" target="#b12">(Everingham et al., 2009</ref>), DTD <ref type="bibr" target="#b33">(Newell &amp; Deng, 2020)</ref>, CIFAR 100 <ref type="bibr" target="#b16">(Hendrycks et al., 2019)</ref>, Sketch ImageNet <ref type="bibr" target="#b47">(Wang et al., 2019)</ref>, etc. For self-supervised setting, we used two learning frameworks: Jigsaw <ref type="bibr" target="#b34">(Noroozi &amp; Favaro, 2016)</ref>, and MoCoV2 <ref type="bibr" target="#b4">(Chen et al., 2020b)</ref>. MoCo <ref type="bibr" target="#b15">(He et al., 2019)</ref> and MoCoV2 <ref type="bibr" target="#b4">(Chen et al., 2020b)</ref> have achieved competitive performance on ImageNet classification and have outperformed supervised pre-trained counterparts on detection and segmentation tasks on the PASCAL VOC <ref type="bibr" target="#b12">(Everingham et al., 2009</ref>) and COCO datasets <ref type="bibr" target="#b25">(Lin et al., 2014)</ref>. Our texture-suppressing augmentation consistently outperforms MoCoV2, which uses Gaussian blurring, and Jigsaw on transfer learning experiments on VOC classification, detection, segmentation benchmarks, and also on classification tasks for other transfer learning datasets, including DTD <ref type="bibr" target="#b33">(Newell &amp; Deng, 2020)</ref>, Cars <ref type="bibr" target="#b21">(Krause et al., 2013)</ref>, Aircraft <ref type="bibr" target="#b30">(Maji et al., 2013)</ref>, etc.</p><p>Overall, we achieve significant improvements on several benchmarks:</p><p>• In a set of eight diverse datasets, our method exhibits substantial improvements (as high as +11.49% on Sketch ImageNet and 10.41% on the DTD dataset) in learning visual representations across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review relevant methods that aim to remove texture cues from images to reduce the dependency of CNNs on low-level cues. Since we also experiment with the application of our method in self-supervised learning, we review recent work in this area as well.</p><p>Reliance on Low-Level Texture Cues. Recent studies have highlighted that deep CNNs can leverage low-level texture information for classification on the ImageNet dataset. Contrary to the popular belief that CNNs capture shape information of objects using hierarchical representations <ref type="bibr" target="#b24">(LeCun et al., 2015)</ref>, the work in <ref type="bibr" target="#b13">(Geirhos et al., 2018)</ref> revealed that CNNs trained on ImageNet are more biased towards texture than shape information. This dependency on texture not only affects generalization, but it can also limit the performance of CNNs on energing real-world use-cases, like few-shot image classification <ref type="bibr" target="#b39">(Ringer et al., 2019)</ref>. <ref type="bibr" target="#b1">Brendel &amp; Bethge (2019)</ref> showed that a bag of CNNs with limited receptive field in the original image can still lead to excellent image classification performance. Intuitively, a small receptive field forces the CNNs to heavily rely on local cues vs. learning hierarchical shape representations. This evidence strongly suggests that texture alone can yield competitive performance on ImageNet and the fact that it's relatively easier to learn vs. hierarchical features may explain deep CNNs' bias towards texture. In order to reduce reliance on texture, Stylized-ImageNet <ref type="bibr" target="#b13">(Geirhos et al., 2018)</ref> modified the ImageNet images into different styles taken from the Kaggle Painter by Numbers dataset. While trying to remove texture, this approach could also significantly affect the shape. Also, there isn't an explicit control over the amount of removed texture. Moreover, this method may not be directly applicable to self-supervised learning because the fixed number of possible texture patterns result in images with strong low-level visual cues resulting in shortcuts. We show that the accuracy on downstream tasks, when MoCoV2 and Jigsaw are trained with Stylized-ImageNet, decreases dramatically <ref type="table" target="#tab_1">(Table 1 Supplementary)</ref>. We, on the other hand, use Perona-Malik's anisotropic diffusion <ref type="bibr" target="#b36">(Perona &amp; Malik, 1990</ref>) and bilateral filtering <ref type="bibr" target="#b43">(Tomasi &amp; Manduchi, 1998b)</ref> as ways of suppressing texture in images. These methods remove texture without degrading the edge information. Consequently, the shape information of the objects are better preserved. Also, these methods provide finer control over the level of texture suppression. Suppressing the texture in training images forces the CNN to build representations that put less emphasis on texture. We show that such data augmentation can lead to performance improvements in both supervised and self-supervised settings. We also distinguish our work from other data augmentation strategies like Auto-Augment <ref type="bibr" target="#b8">(Cubuk et al., 2018)</ref> which uses Reinforcement Learning to automatically search for improved data augmentation policies and introduces Patch Gaussian Augmentation, which allows the network to interpolate between robustness and accuracy <ref type="bibr" target="#b28">(Lopes et al., 2019)</ref>. The motivation behind our proposed approach is to suppress the reliance of CNNs on low-level cues and encourage CNNs to learn representations that are less dependent on texture.</p><p>Self-Supervised Learning. To demonstrate the importance of removing texture in the selfsupervised setting, we consider two pretext tasks. The first pretext task is Jigsaw <ref type="bibr" target="#b34">(Noroozi &amp; Favaro, 2016)</ref> which is a patch based self-supervised learning method that falls under the umbrella of visual permutation learning <ref type="bibr" target="#b7">(Cruz et al., 2017;</ref> . Some of the most recent self-supervised methods are contrastive learning based methods <ref type="bibr" target="#b15">(He et al., 2019;</ref><ref type="bibr" target="#b2">Caron et al., 2018;</ref><ref type="bibr" target="#b18">Hénaff et al., 2019;</ref><ref type="bibr" target="#b17">Hjelm et al., 2018;</ref><ref type="bibr" target="#b31">Misra &amp; van der Maaten, 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2020a;</ref>. In <ref type="bibr" target="#b2">Caron et al. (2018)</ref>, the authors have proposed using contrastive losses on patches, where they learn representations by predicting representations of one patch from another. In MoCo <ref type="bibr" target="#b15">(He et al., 2019)</ref>, a dynamic dictionary is built as a queue along with a moving average encoder. Every image will be used as a positive sample for a query based on a jittered version of the image. The queue will contain a batch of negative samples for the contrastive losses. MoCo has two encoder networks. The momentum encoder has weights updated through backpropagation on the contrastive loss and a momentum update. In MoCoV2, Gaussian blur and linear projection layers were added that further improve the representations. MoCo and MoCoV2 have shown competitive results on ImageNet classification and have outperformed supervised pre-trained counterparts on seven detection/segmentation tasks, including PASCAL VOC <ref type="bibr" target="#b12">(Everingham et al., 2009</ref>) and COCO <ref type="bibr" target="#b25">(Lin et al., 2014)</ref>.</p><p>Transfer Learning. Transfer learning is one of the most important problems in computer vision due to difficulty in collecting large datasets across all domains. In this work, we discuss transfer learning in context of ImageNet. A lot of early datasets were shown to be too small to generalize well to other datasets <ref type="bibr" target="#b44">(Torralba &amp; Efros, 2011)</ref>. Following this, many new large-scale datasets were released <ref type="bibr" target="#b9">(Deng et al., 2009;</ref><ref type="bibr" target="#b25">Lin et al., 2014)</ref>, which are believed to transfer better. However, recent results have shown that these datasets do not generalize well in all cases <ref type="bibr" target="#b37">(Recht et al., 2019;</ref><ref type="bibr" target="#b20">Kornblith et al., 2019)</ref>. <ref type="bibr" target="#b20">Kornblith et al. (2019)</ref> showed that ImageNet features generally transfer well, but do not transfer well to fine-grained tasks. We show results of transfer learning on some the datasets that were used by <ref type="bibr" target="#b20">Kornblith et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>Texture and other visual cues may bias CNNs towards over-fitting on these cues. This may lead to brittleness when these cues change in new domains. CNN-based classifiers have been shown to exploit textures rather than shapes for classification <ref type="bibr" target="#b13">(Geirhos et al., 2018;</ref><ref type="bibr" target="#b1">Brendel &amp; Bethge, 2019)</ref>. We aim to reduce the prominence of texture in images, and thus encourage networks trained on them to learn representations that capture better higher level representations.</p><p>Gaussian Blur. Gaussian blurring is the most popular smoothing methods in computer vision, and it has been recently proposed as data augmentation for SSL <ref type="bibr" target="#b3">(Chen et al., 2020a;</ref>. However, along with low-level texture, Gaussian filtering also blurs across boundaries, diminishing edges and structural information.</p><p>Anisotropic Diffusion. We propose to use Anisotropic Diffusion Filters or ADF <ref type="bibr" target="#b36">(Perona &amp; Malik, 1990)</ref>, which keep the shape information coherent and only alter low-level texture. We specifically use Perona-Malik diffusion <ref type="bibr" target="#b36">(Perona &amp; Malik, 1990)</ref>. These filters smooth the texture without degrading the edges and boundaries. Intuitively, it will encourage the network to extract high-level semantic features from the input patches. Interestingly, we find that a relatively modest amount of smoothing suffices to reduce texture shortcuts.</p><p>Perona-Malik diffusion smooths the image using the differential diffusion equation:</p><formula xml:id="formula_0">∂I ∂t = c(x, y, t)∆I + ∇c · ∇I (1) c ( ∇I ) = e −( ∇I /K) 2<label>(2)</label></formula><p>where I is the image, t is the time of evolution, ∇ is the Laplacian operator, and (x, y) is a location in the image. The amount of smoothing is modulated by the magnitude of the gradient in the image, through c. The larger the gradient, the smaller the smoothing at that location. Therefore, after applying Anisotropic diffusion we obtain images with blurred regions but edges are still prominent. <ref type="figure">Fig. 6</ref> shows some examples of the application of the filter. Since the ADF reduces the texture in the image without replacing it, the domain gap between images is not large, while in the case of Stylized ImageNet, the domain shift will be large. We also experiment with a few other texture removing methods like robust Anistropic diffusion <ref type="bibr" target="#b0">(Black et al., 1998)</ref> and Bilateral filtering <ref type="bibr" target="#b43">(Tomasi &amp; Manduchi, 1998b)</ref>. However, empirically we find that the most simple Anistropic diffusion method has the best results as discussed in Section 4.2. Recently, there has been some work on removing textures using deep learning as well <ref type="bibr" target="#b48">(Xu et al., 2014;</ref><ref type="bibr" target="#b26">Liu et al., 2016;</ref><ref type="bibr" target="#b29">Lu et al., 2018)</ref>. We find, though, that fast and simple classical methods work well on our tasks.</p><p>For all our experiments we create a dataset 'Anisotropic ImageNet' by combining ADF filtered ImageNet images with standard ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We start by briefly describing the datasets used in our experiments. We then show that ADF is particularly effective when there is domain shift, supporting our hypothesis that variation in texture is a significant effect of domain shift. We show this in both SSL and supervised settings. The effect is larger when we transfer from ImageNet to datasets such as Sketch Imagenet <ref type="bibr" target="#b47">(Wang et al., 2019)</ref> and DTD <ref type="bibr" target="#b5">(Cimpoi et al., 2014)</ref>, where the domain shift is larger. We also show that when there is no domain shift, our method is competitive with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>In all experiments, we use ImageNet training set as the source of our training data. For object detection and semantic segmentation, we evaluate on Pascal VOC 2007 and VOC 2012. For label corruption, we evaluate on CIFAR100. For the downstream task is classification we evaluate on DTD <ref type="bibr" target="#b33">(Newell &amp; Deng, 2020)</ref>, Sketch-ImageNet <ref type="bibr" target="#b47">(Wang et al., 2019)</ref>, Birds <ref type="bibr">(Wah et al., 2011)</ref>, Aircraft <ref type="bibr" target="#b30">(Maji et al., 2013)</ref>, Stanford Dogs <ref type="bibr" target="#b19">(Khosla et al., 2012)</ref>, Stanford Cars <ref type="bibr" target="#b21">(Krause et al., 2013)</ref>, and the ImageNet validation dataset.</p><p>Experimental Details. For SSL we build on MoCoV2 <ref type="bibr" target="#b4">(Chen et al., 2020b)</ref>. For supervised learning, we use the ResNet50  model, closely following <ref type="bibr" target="#b13">(Geirhos et al., 2018)</ref>. After training on Anisotropic ImageNet, we fine-tune our model on the standard ImageNet training set following the procedure of <ref type="bibr" target="#b13">(Geirhos et al., 2018)</ref>.</p><p>Hyper-parameters for Anisotropic Diffusion. We set the conduction coefficient (K) of Anisotropic Diffusion to 20 and a total of 20 iterations are used. We use the MedPy implementation. All other hyper-parameters are described in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TRANSFER LEARNING FOR SELF-SUPERVISED LEARNING</head><p>We first experiment with Anisotropic ImageNet on Self-Supverised methods. We have double the number of images (Anisotropic images + normal images) as compared to normal ImageNet. So for fair comparison, we only train our methods for half the number of epochs as compared to training with just ImageNet. We then fine-tune the network pre-trained on the Anisotropic ImageNet for the downstream tasks including image classification, object detection, and semantic segmentation on PASCAL VOC, and other transfer learning datasets. Since, we are removing low-level cues from the images, we expect to see better results when we are transferring to different datasets.</p><p>MoCo V2. We evaluate our method with MoCo V2 <ref type="bibr" target="#b4">(Chen et al., 2020b)</ref>, which is the state-of-theart in SSL. The original MoCoV2 used Gaussian blurring with 0.5 probability as data augmentation. In our case, we add Anisotropic diffusion on the images with 0.5 probability, and for the remaining 50% of the images we apply Gaussian blurring with 0.5 probability. So, in our setup every image has 0.5 probability of coming from Anisotropic ImageNet, 0.25 of Gaussian blurring, and 0.25 of being normal ImageNet. Also, the number of iterations on Anisotropic filtering is chosen randomly between 10 to 20. We conduct two sets of experiments on MoCoV2 for object detection. In the first setup, starting from a MoCoV2 initialization, we train a Faster R-CNN (Ren et al., 2015) with C4backbone, which is fine-tuned end-to-end. In the second setup, we again initialize Faster-RCNN from the MoCoV2-trained network, and only train the region proposal, classification, and boxregression layers, and keep the rest of the layers unchanged (the performance for this case is marked as AP* 50 in <ref type="table" target="#tab_1">Table 1</ref>). In both setups, training is done on VOC(07+12) trainval set and we evaluate on the VOC07 test.</p><p>For both setups, we achieve the state-of-the-art performance for object detection on VOC Dataset. In the first setup, we show improvements on COCO-based evaluation metrics (i.e., AP 50 , AP 0.05:0.05:0.95 , AP 75 ) as shown in first three columns of <ref type="table" target="#tab_1">Table 1</ref>, and achieve new state-of-the-art performance on object detection. In the second setup, <ref type="table" target="#tab_1">Table 1</ref> shows that the baseline detection accuracy for MoCoV2 trained on ImageNet is 66.5 mAP, and the one trained with our method is 67.3 mAP. We also observe improvement of 0.5 mean IoU on semantic segmentation <ref type="bibr" target="#b27">(Long et al., 2015)</ref> over MoCo V2 baseline. These results show that in case of transfer learning, we improve across different datasets. More details can be found in the supplementary material. Our method is not bound to a particular pretext task and in the supplementary material we show that our method leads to improvements with the Jigsaw <ref type="bibr" target="#b35">(Noroozi et al., 2018)</ref> task as well. We test on two types of metrics for object detection: first is COCO-based metrics as used in <ref type="bibr" target="#b4">Chen et al. (2020b)</ref> and the second metric AP* 50 uses frozen backbone . We achieve the state-of-the-art results on all metrics. We also improve performance over the baseline on the semantic segmentation (SS) task <ref type="bibr" target="#b27">(Long et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Epochs <ref type="formula">AP</ref>   These results suggest that training the network on the Anisotropic ImageNet dataset forces it to learn better representations. This is consistent with our hypothesis that Anisotropic diffusion leads to smoothing of texture in images. This forces the network to be less reliant on lower-level information to solve the pretext task and, hence, learn representations that focus on higher-level concepts.</p><p>Experiments with Stylized ImageNet on MoCoV2 and Jigsaw. We now show experiments that indicate that, while effective in a supervised setting, Stylized ImageNet does not help with SSL. We train a model with MoCoV2 and Jigsaw as pretext tasks on the Stylized-ImageNet (SIN) dataset <ref type="bibr" target="#b13">(Geirhos et al., 2018)</ref> and fine-tune on the downstream tasks of object detection and image classification on PASCAL VOC. In <ref type="table" target="#tab_3">Table 2 (and Table 2</ref> in supplementary), we show that there is a huge drop in performance. One reason for this failure using the SIN dataset could be that the model is able to memorize the textures in the stylized images since it only has 79,434 styles. This is not a problem in the original fully-supervised setting where the authors used SIN for supervised image classification. In that case, the network can learn to ignore texture to discriminate between classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TRANSFER LEARNING FOR SUPERVISED LEARNING</head><p>As shown in the last section, suppressing texture leads to performance improvements in the case of domain transfer with SSL. In this section, we also show improvements on supervised learning and domain transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">ACROSS DOMAINS</head><p>We hypothesize that learning the texture bias is the most harmful when it comes to domain transfer. Thus, we first design the challenging experimental setup for learning visual representation; learning it for different domains.</p><p>Sketch-ImageNet. For a cross-domain supervised learning setup, we chose to use the Sketch-ImageNet dataset. Sketch-ImageNet contains sketches collected by making Google image queries "sketch of X", where "X" is chosen from the standard class names of ImageNet. The sketches have very little to no texture, so performance on Sketch-ImageNet is a strong indicator of how well the model can perform when much less texture is present. Sketch-ImageNet has been collected in the same fashion as <ref type="bibr" target="#b37">Recht et al. (2019)</ref>, implying that validation set is different compared to the original ImageNet validation set. As shown in <ref type="table" target="#tab_4">Table 3</ref>, the difference between the Anisotropic model  DTD Dataset. To better demonstrate the effectiveness of less texture dependent representations, we used the dataset introduced by <ref type="bibr" target="#b33">(Newell &amp; Deng, 2020)</ref>. This dataset provides four variations in images: Texture, color, lighting, and viewpoint. It contains 480,000 training Images and 72,000 testing Images. In this dataset, we made sure that texture information during training and testing are completely different. So, the texture is not a cue when we use this dataset. We evaluated our Anisotropic model on this dataset and compared against the baseline normal ImageNet model. The Anisotropic model achieves a performance boost of 10.41% in classification which suggests that we are indeed able to learn texture agnostic feature representations.</p><p>Other Datasets -Aircraft, Birds, Dogs, and Cars. We further evaluate our method on image classification task on four different fine-grained classification datasets. We also observe improvement on image classification across five datasets in <ref type="table" target="#tab_3">Table 2</ref>. These results suggest that in case of domain shift, higher level semantics are more important and capturing them helps in better transfer learning performance.</p><p>Object Detection. The biggest improvement we observe on transfer learning is on object detection on Faster- <ref type="bibr">RCNN (Ren et al., 2015)</ref> as shown in <ref type="table" target="#tab_10">Table 9</ref>. This improvement on object detection suggests that we are able to capture more high-level semantics which helps us in transfer learning performance on object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">SAME DOMAIN</head><p>We also observe consistent performance improvements in the same domain setups.</p><p>ImageNet. In <ref type="table" target="#tab_10">Table 9</ref>, we show results by using Anisotropic ImageNet for supervised classification. We observe that Anisotropic ImageNet improves performance in both ImageNet classification and object detection. For Gaussian blurring experiments, we closely follow <ref type="bibr" target="#b4">Chen et al. (2020b)</ref> and add a Gaussian blur operator with random radius from 10 to 20 and train in a similar manner to Stylized ImageNet <ref type="bibr" target="#b13">(Geirhos et al., 2018)</ref>. So, this result shows that Anisotropic ImageNet is similar to Stylized ImageNet and is a better alternative to Gaussian blurring. We also observe that Gaussian blurring does not perform as well as Anisotropic ImageNet in terms of ImageNet top-1 and VOC object detection performance. Hence, blurring the image completely without respecting boundaries and edges does not improve performance as much as Anisotropic diffusion. Different Texture Removing Methods. We also provide results using different texture removing methods and different hyper-parameters for Anistropic diffusion in <ref type="table" target="#tab_10">Table 9</ref>. We observe that as we increase the number of iterations and remove more and more texture from images, performance starts to degrade possibly due to the difference that comes in the data distribution after removing texture information. The most simple texture removing method <ref type="bibr" target="#b36">(Perona &amp; Malik, 1990</ref>) has the best results. We also show results on the task of label corruption in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">VISUAL ANALYSIS BY SALIENCY MAPS</head><p>We now visually analyze the results by the saliency maps, which are produced by different networks. We use <ref type="bibr">GradCam (Selvaraju et al., 2016)</ref> to calculate the saliency maps. <ref type="figure">In Fig 7,</ref> we show the saliency maps produced by networks trained using the combined dataset and the original ImageNet dataset. We observe that Anisotropic ImageNet has saliency maps that spread out over a bigger area and that include the outlines of the objects. This suggests that it attends less to texture and more to overall holistic shape. In contrast, ImageNet trained models have narrower saliency maps that miss the overall shape and focus on localized regions, suggesting attention to texture.</p><p>In <ref type="figure">Fig. 7(a-e)</ref>, we present the examples where the Anisotropic model gives the correct prediction, and the ImageNet model fails. For example in <ref type="figure">Fig. 7</ref>(e), we observe that the network trained on ImageNet alone is not focusing on the whole bird and is only focusing on the body to make the decision; whereas the one trained with Anisotropic ImageNet is focusing on complete bird to make a decision.</p><p>We include more saliency maps on Sketch-ImageNet, and cases where ImageNet trained models are correct and our model fails in the supplementary material. We show more analysis about confidence of models and further analysis on transfer learning in the Supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose to help a CNN focus on high level cues instead of relying on texture by augmenting the ImageNet dataset with images filtered with Anisotropic diffusion, in which texture information is suppressed. Empirical results suggest that using the proposed data augmentation for pretraining self-supervised models and for training supervised models gives improvements across eight diverse datasets. Noticeably, the 11.4% improvement while testing the supervised model on Sketch Ima-geNet suggests that the network is indeed capturing more higher level representations as compared to the models trained on ImageNet alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>This work is supported in part by, by the US Defense Advanced Research Projects Agency (DARPA) Semantic Forensics (SemaFor) Program under grant HR001120C0124. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the DARPA.</p><p>7 TRAINING WITH STYLIZED IMAGENET <ref type="figure">Figure 6</ref> shows the training curves for both Stylized ImageNet and Standard ImageNet. We see that the model quickly saturates when using Stylized ImageNet. This leads to a low performance on downstream tasks. Our hypothesis is that the model rapidly learns to exploit some regularities in the texture introduced by the GANs to easily solve the MoCoV2 and Jigsaw tasks. This means that the self-supervised model has the tendency to take shortcuts in the presence of regular textures. The aim of our paper has been to investigate such short-coimings and provide appropriate solutions to the issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">JIGSAW TASK</head><p>In addition to MoCoV2 we also show results with Jigsaw pretext task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">METHOD</head><p>The goal in Jigsaw is to infer correct ordering of the given regions from an image. Following <ref type="bibr" target="#b34">Noroozi &amp; Favaro (2016)</ref>, the typical setting is to divide an image into nine non-overlapping square patches and randomly shuffle them. A CNN is trained to predict the original permutation of these patches in the image. Jigsaw++ <ref type="bibr" target="#b35">(Noroozi et al., 2018)</ref> extended this idea and replaced some patches with random patches. These patch based methods come with their own issues and there has been some recent effort to solve them. <ref type="bibr" target="#b32">Mundhenk et al. (2017)</ref> describe an easy short-cut CNNs take that utilizes the chromatic aberration produced due to different wavelengths exiting the lens at different angles. The authors provided a solution to the problem of chromatic aberration by removing cues from the images and also allowing the color pattern to be partially preserved. They also address the problem of true spatial extent that network sees in patch based methods by yoking the patch jitter to create a random crop effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">RESULTS USING JIGSAW PRETRAINING</head><p>The baseline model in the case of Jigsaw is pre-trained using the standard ImageNet dataset. Unlike <ref type="bibr" target="#b34">Noroozi &amp; Favaro (2016)</ref>, we use ResNet18 as our backbone instead of Alexnet <ref type="bibr" target="#b23">(Krizhevsky et al., 2017)</ref> to take advantage of deeper layers and capture better image representations. We obtained these results by building on top of a publicly available implementation 1 . <ref type="table" target="#tab_1">Table 12</ref> shows results for Jigsaw models trained and tested on different datasets. We observe that the Jigsaw model trained on the Cartoon dataset outperforms the baseline methods by 2.52 mAP and Anisotropic ImageNet outperforms the baseline methods by 1.8 mAP on the PASCAL VOC image classification dataset. On object detection Bilateral ImageNet outperforms the baseline Jigsaw model by 0.78 mAP. On semantic segmentation Anisotropic Imagenet outperforms the baseline Jigsaw models by 8.1 mAP. Traditionally semantic segmentation has been a difficult task for Self-Supervised methods <ref type="bibr" target="#b34">(Noroozi &amp; Favaro, 2016;</ref><ref type="bibr" target="#b2">Caron et al., 2018)</ref> and improvement of this order on semantic segmentation shows the effectiveness of removing texture.</p><p>We also show results on ImageNet classification as the downstream task. Due to its large scale, it is usually infeasible to fine-tune the whole network for the final task every time. Therefore, following prior work <ref type="bibr" target="#b2">(Caron et al., 2018)</ref>, we only fine-tune a linear classifier. The inputs to this classifier are the features from a convolution layer in the network. Note that while fine-tuning for the final task, we keep the backbone frozen. Therefore, the performance of the linear classifier can be seen as a direct representation of the quality of the features obtained from the CNN. We report the results of this experiment on ImageNet in <ref type="table" target="#tab_1">Table 11</ref>. Adding the Anisotropic ImageNet dataset to this model gives a further improvement of 0.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">JIGSAW USING ALEXNET AS BACKBONE</head><p>Our improvement when using Anisotropic ImageNet is not restricted to the backbone. Traditionally in Self-Supverised learning one of the most followed architectures is Alexnet <ref type="bibr" target="#b34">(Noroozi &amp; Favaro, 2016;</ref><ref type="bibr" target="#b11">Doersch et al., 2015;</ref><ref type="bibr" target="#b2">Caron et al., 2018)</ref>. Following these methods, we also show results on Alexnet backbone. In <ref type="table">Table.</ref> 7 we show results on VOC Classification when using Alexnet as the backbone. We obtain an improvement of 0.67 mAP over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">PATCH-WISE ANISOTROPIC DIFFUSION</head><p>In our best performing model, we considered all the patches for the jigsaw task to either come from the standard ImageNet or anisotropic diffusion filtered ImageNet. What if each of the 9 patches for the Jigsaw task could be either a standard patch or filtered patch? For this experiment we randomly choose a patch from the standard dataset or the filtered dataset, with equal probability. This is a much more extreme form of data augmentation and considerably increases the difficulty of the task. We got an improvement of 0.6 mAP over the baseline model for the classification task. However this is 1.1 mAP lower then the doing Anistropic Diffusion on whole image.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">ANISTROPIC IMAGES</head><p>We show some more examples of Anistropic images obtained by applying Anisotropic diffusion filiters to images from ImageNet <ref type="bibr" target="#b9">(Deng et al., 2009)</ref> in figures 9 and 10. Notice how the images lose texture information. This makes it more difficult for models to find shortcuts. This, in turn, leads to better semantic representations learned by the model which leads to higher performance on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">OTHER TEXTURE REMOVING METHODS</head><p>In this section we give details of other texture removing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">BILATERAL FILTERING</head><p>Bilateral Filtering <ref type="bibr" target="#b42">(Tomasi &amp; Manduchi, 1998a)</ref> is an efficient method of anisotropic diffusion. In Gaussian filtering, each pixel is replaced by an average of neighboring pixels, weighted by their spatial distance. Bilateral Filtering is its extension in which weights also depend on photometric distance. This also limits smoothing across edges, in which nearby pixels have quite different intensities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">CARTOONIZATION</head><p>A more extreme method of limiting texture is to create cartoon images. To convert an image into a cartoonish image we first apply bilateral filtering to reduce the color palette of the image. Then in the second step we convert the actual image to grayscale and apply a median filter to reduce noise in the grayscale image. After this we create an edge mask from the greyscale image using adaptive thresholding. Finally we combine these two images to produce cartoonish looking images (see <ref type="figure">Fig.  6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal ImageNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anisotropic Diffusion Filtered</head><p>Cartoon Images</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian Images</head><p>Bilateral Images <ref type="figure">Figure 5</ref>: The figure shows examples of the effect of anisotropic diffusion. The texture on the towel in the first figure and that on the leaves and the beetle is smoothed out. This effect helps in forcing the network to rely on higher level information rather than textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">SALIENCY MAPS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">SKETCH-IMAGENET SALIENCY MAPS</head><p>In <ref type="figure">Fig 11 we</ref> show some of saliency maps for Sketch-ImageNet images. We can see from saliency maps that Anistropic ImageNet has broader saliency map and has better coverage of the object as compared to ImageNet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">SALIENCY MAPS FOR ANISTROPIC IMAGENET AND STANDARD IMAGENET MODELS</head><p>We also show some addtional saliency maps in <ref type="figure" target="#fig_1">Figure 12, Figure 14</ref>, <ref type="figure" target="#fig_0">Figure 13</ref> and <ref type="figure">Figure 15</ref> corresponding to both the models. We can see from the figures that Anistropic ImageNet has in general diffused saliency maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">IMPLEMENTATION DETAILS</head><p>Training Details. With image classification as the downstream task, we train our network for 90,000 iterations with an initial learning rate of 0.003 following <ref type="bibr" target="#b2">(Caron et al., 2018)</ref>.</p><p>For object detection we report our results for Faster- <ref type="bibr">RCNN (Ren et al., 2015)</ref> using our pre-trained model as backbone. We tune hyper-parameters using the validation set. For object detection, we follow the details of  to train a model; 10 epochs with an initial learning rate of 0.001.</p><p>For semantic segmentation, we report our results on FCN <ref type="bibr" target="#b41">(Shelhamer et al., 2017)</ref> using our pretrained model as backbone. We train the FCN model for 30 epochs using an initial learning rate of 0.01.</p><p>ImageNet. We use ImageNet for all training and evaluation of image classification accuracy. For self-supervised learning, we follow <ref type="bibr" target="#b2">(Caron et al., 2018;</ref><ref type="bibr" target="#b15">He et al., 2019)</ref>; we train linear classifiers using features obtained from the final Residual block by freezing all convolutional layers. The performance of these linear classifiers is meant to evaluate the quality of the feature representations learnt by the convolutional layers, since the backbone is completely frozen and only fully-connected layers are being trained. We chose hyper-parameters using the validation set and report performance on the ImageNet validation set.</p><p>Note that since we use ImageNet to pre-train for self-supervised learning, there is no domain difference when we conduct inference on ImageNet, but with VOC there is. With the VOC results, we validate that the gain by our method is particularly large when there is domain shift.</p><p>Jigsaw task. In Jigsaw <ref type="bibr" target="#b34">(Noroozi &amp; Favaro, 2016)</ref> the image is divided into 9 non-overlapping square patches. We select 1,000 from the 9! possible permutations. All of our primary experiments on Jigsaw use ResNet18 as the backbone . We train the Jigsaw task for 90 epochs, with an initial learning rate of 0.01. The learning rate is reduced by a factor of 0.1 after <ref type="bibr">(30,</ref><ref type="bibr">30,</ref><ref type="bibr">20,</ref><ref type="bibr">10)</ref> epochs. We use the same data augmentation as in <ref type="bibr" target="#b34">(Noroozi &amp; Favaro, 2016)</ref>. In MoCo , we use ResNet50  as the backbone, following the same procedure as mentioned in <ref type="bibr" target="#b15">(He et al., 2019)</ref>.</p><p>PASCAL VOC. Following <ref type="bibr" target="#b2">(Caron et al., 2018)</ref> and <ref type="bibr" target="#b4">(Chen et al., 2020b)</ref>, we evaluate image classification and object detection on the PASCAL VOC dataset <ref type="bibr" target="#b12">(Everingham et al., 2009)</ref>. It contains about 5,000 images in the train-val set belonging to 20 classes. Note that the image classification task is multi-label. Therefore, the metric used for evaluating both image classification and object detection is the mean Average Precision (mAP).</p><p>Training Details for Object detection for COCO based metrics: We report object detection results on  C4 backbone which is finetuned end to end on VOC07+12 trainval dataset and evaluated on the VOC 07 test set using the COCO suite of metrics.</p><p>Other Details. We use 4 Nvidia GTX 1080 Ti for all experiments. Pretraining on Jigsaw takes 3 days on the standard ImageNet dataset. The SGD optimizer with momentum was used for all experiments with momentum of 0.9 and weight decay of 5 × 10 −4 . Cross-entropy loss was used for all experiments, mini-batch size was set to 256. Pretraining on MoCoV2 takes 6 days on 4 Nvidia P100 machines. We set all other hyperparamters following <ref type="bibr" target="#b4">Chen et al. (2020b)</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">LABEL CORRUPTION TASK</head><p>We also show results on the label corruption task in <ref type="table" target="#tab_9">Table 8</ref>. In this task we use CIFAR100 as our dataset and we augment the CIFAR100 dataset with Anisotropic diffused images. We create a dataset double the size of original CIFAR100 dataset and use it for the task of Label Corruption <ref type="bibr" target="#b16">(Hendrycks et al., 2019)</ref>. We can see from the results that as we consistenly have improvements compared to baseline. With increase in corruption probablity, our results improve even more which shows that focussing on higher level features also improve accuracy in label corruption task as well.</p><p>[t!]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14">CONFIDENCE OF MODELS</head><p>In this section we compare the confidence and entropy of Anistropic Model and ImageNet model when both the models have given correct predictions. To find confidence, we generate the probability scores of correct class. After this we calculated the mean of correct probability scores on both the models. As we can see from <ref type="table" target="#tab_1">Table 10</ref> that Anistropic ImageNet has larger mean which means that Anistropic ImageNet has better confidence as compared to Standard ImageNet. We also calculate the entropy of output probability distribution from both the models. We can see from <ref type="table" target="#tab_1">Table 10</ref> Anistropic ImageNet has lower entropy scores as compared to Standard ImageNet.</p><p>[t!]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15">SALIENCY MAPS</head><p>In <ref type="figure">Fig. 7</ref> we show the saliency maps produced by networks trained using the combined dataset and the original ImageNet dataset. We use <ref type="bibr">GradCam(Selvaraju et al., 2016)</ref> to calculate the saliency <ref type="figure">Figure 6</ref>: Training plots for the both Stylized ImageNet and Standard ImageNet. Both of these models used ResNet18 as the backbone. The plot shows that the model trained on Stylized ImageNet quickly overfits bit by finding shortcuts after around 6000 steps. Therefore, it gives poor performance on downstream tasks by relying on texture based shortcuts. Refer to the <ref type="table" target="#tab_1">Table 1</ref> from the main paper. maps. We can see that Anisotropic ImageNet has saliency maps that spread out over a bigger area and that include the outlines of the objects. This suggests that it attends less to texture and more to overall holistic shape. In contrast, ImageNet trained models have narrower saliency maps that miss the overall shape and focus on localized regions, suggesting an attention to texture. In <ref type="figure">Fig.  7</ref>(f-j) we show these for the case where the Anisotropic model gives the correct prediction and the ImageNet model fails. For example in <ref type="figure">Fig. 7(j)</ref>, we see that the network trained on ImageNet alone is not focusing on the whole bird and is only focusing on the body to make the decision whereas the one trained with Anisotropic ImageNet is focusing on complete bird to make a decision. We see a similar trend in the cases where both the models give the correct prediction ( <ref type="figure">Fig. 7(a-e)</ref>). In the case where Anisotropic model makes incorrect predictions and ImageNet model ( <ref type="figure">Fig. 7(k-o)</ref>) is correct we see the saliency maps are still diffused, but we fail to capture the whole object leading to incorrect predictions.  <ref type="figure">Figure 8</ref>: Saliency maps when our technique gives the correct prediction and baseline approach gives incorrect label. The top row gives the saliency maps for our model and the bottom one shows the corresponding saliency maps for the model trained on imagenet alone. We can see from saliency maps that Anisotropic model has bigger saliency maps which might be the reason for the correct prediction. <ref type="figure">Figure 9</ref>: Original images (left)and images obtained after anisotropic diffusion (right). Most of the texture information in the images has been smoothed out by the filter while retaining the shape information. This forces the network to capture higher-level semantics without relying on low-level texture cues <ref type="figure">Figure 10</ref>: Original images (left)and images obtained after anisotropic diffusion (right). Most of the texture information in the images has been smoothed out by the filter while retaining the shape information. This forces the network to capture higher-level semantics without relying on low-level texture cues Anisotropic ImageNet (a) (b) (c) (d) (e) <ref type="figure">Figure 11</ref>: Saliency maps on few randomly selected images from Sketch-ImageNet. We can see from saliency maps that Anisotropic model has bigger saliency maps which might be the reason for the correct prediction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Four different methods for reducing texture in images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Saliency maps using GradCam. The text on the left of the row indicates whether Anisotropic model or ImageNet model was used. The figure shows the saliency maps where Anisotropic model gave correct predictions and ImageNet model gave wrong predictions. The failure of ImageNet model might be due to it not attending to whole object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 12 :Figure 14 :Figure 15 :</head><label>121415</label><figDesc>Saliency maps when Anistropic Model had correct predictions and ImageNet model has wrong predictionsSaliency maps when both model have correct predictions. Saliency maps when ImageNet model has correct predictions and Anistropic model has wrong predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the state of the art methods in SSL. We note that using Anisotropic diffusion with MoCoV2 improves performance on VOC detection and Semantic Segmentation (SS).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>50 AP 0.50:0.05:0.95 AP 75 AP* 50 mIoU (SS)</figDesc><table><row><cell>Stylized ImageNet</cell><cell></cell><cell>43.5</cell><cell>28.80</cell><cell>59.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Supervised</cell><cell></cell><cell>81.3</cell><cell>53.5</cell><cell>58.8</cell><cell>70.1</cell><cell>53.5</cell></row><row><cell>MoCo V2 (Chen et al., 2020b)</cell><cell>200</cell><cell>82.4</cell><cell>57.0</cell><cell>63.6</cell><cell>66.5</cell><cell>55.5</cell></row><row><cell>MoCo V2 Anistropic (Ours)</cell><cell>200</cell><cell>82.8</cell><cell>57.4</cell><cell>64.2</cell><cell>67.3</cell><cell>56.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Transfer learning across different datasets. Note that our approach leads to improvements in both supervised and self-supervised learning set-up.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Aircraft Birds Dogs Cars DTD</cell></row><row><cell>Supervised (Reproduced)</cell><cell>90.88</cell><cell>90.3 85.35 92.1 72.66</cell></row><row><cell>Supervised Anistropic (Ours)</cell><cell>91.67</cell><cell>91.42 86.40 93.1 73.03</cell></row><row><cell>MoCo V2 (Chen et al., 2020b)</cell><cell>91.57</cell><cell>92.13 87.13 92.8 74.73</cell></row><row><cell>MoCo V2 Anistropic (Ours)</cell><cell>92.05</cell><cell>92.76 87.92 93.5 75.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experiments with Sketch-ImageNet. Use of Anisotropic ImageNet shows that our method is better at capturing representation that are less dependent on texture.</figDesc><table><row><cell>Method</cell><cell cols="2">Top-1 Accuracy Top-5 Accuracy</cell></row><row><cell>ImageNet Baseline</cell><cell>13.00</cell><cell>26.24</cell></row><row><cell>Stylized Baseline</cell><cell>16.36</cell><cell>31.56</cell></row><row><cell>Anisotropic (Ours)</cell><cell>24.49</cell><cell>41.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison using different texture removing methods, with different hyper-parameters for Anisotropic diffusion methods. We observe that the most simple<ref type="bibr" target="#b36">(Perona &amp; Malik, 1990)</ref> performs the best and removing more texture from images does not improve performance.</figDesc><table><row><cell>Method</cell><cell cols="4"># Iterations Top-1 Acc Top-5 Acc Obj. Det.</cell></row><row><cell>Baseline Supervised</cell><cell>-</cell><cell>76.13</cell><cell>92.98</cell><cell>70.7</cell></row><row><cell>Perona Malik (Perona &amp; Malik, 1990)</cell><cell>20</cell><cell>76.71</cell><cell>93.26</cell><cell>74.37</cell></row><row><cell>Perona Malik (Perona &amp; Malik, 1990)</cell><cell>50</cell><cell>76.32</cell><cell>92.96</cell><cell>73.80</cell></row><row><cell>Robust AD (Black et al., 1998)</cell><cell>20</cell><cell>76.58</cell><cell>92.96</cell><cell>73.33</cell></row><row><cell>Robust AD (Black et al., 1998)</cell><cell>50</cell><cell>76.64</cell><cell>93.09</cell><cell>73.57</cell></row><row><cell>Gaussian Blur</cell><cell>-</cell><cell>76.21</cell><cell>92.64</cell><cell>73.26</cell></row><row><cell>Cartoon ImageNet</cell><cell>-</cell><cell>76.22</cell><cell>93.12</cell><cell>72.31</cell></row><row><cell>Bilateral ImageNet</cell><cell>-</cell><cell>75.99</cell><cell>92.90</cell><cell>71.34</cell></row></table><note>and the baseline model is 11.49% for Top-1 accuracy, This result implies that our model captures representations that are less dependent on texture as compared to standard ImageNet and Stylized ImageNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of our approach with Jigsaw baseline methods. Using our best model, we improve 2.52 mAP in VOC classification , 0.78 mAP on VOC detection and 8.1 mAP on VOC semantic segmentation(SS) over the baseline models. Note that Stylized ImageNet performs poorly on VOC classification due to the visual shortcuts.</figDesc><table><row><cell>Method</cell><cell cols="3">Dataset Size VOC Cls. VOC Det.</cell><cell>SS</cell></row><row><cell>Baseline</cell><cell>1.2M</cell><cell>74.82</cell><cell>61.98</cell><cell>27.1</cell></row><row><cell>Stylized (Geirhos et al., 2018)</cell><cell>1.2M</cell><cell>13.81</cell><cell>28.13</cell><cell>10.12</cell></row><row><cell>Gaussian ImageNet</cell><cell>2×1.2M</cell><cell>75.49</cell><cell>62.39</cell><cell>27.9</cell></row><row><cell>Bilateral ImageNet</cell><cell>2×1.2M</cell><cell>74.55</cell><cell>62.74</cell><cell>28.9</cell></row><row><cell>Only Anisotropic</cell><cell>1.2M</cell><cell>74.52</cell><cell>61.85</cell><cell>32.7</cell></row><row><cell>Anisotropic ImageNet</cell><cell>2×1.2M</cell><cell>76.77</cell><cell>61.59</cell><cell>35.2</cell></row><row><cell>Cartoon ImageNet</cell><cell>2×1.2M</cell><cell>77.34</cell><cell>59.31</cell><cell>34.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>ImageNet classification by finetuning the last FC layer. Features from the conv layers are kept unchanged. This experiment helps evaluate the quality of features learnt by the convolutional layers.</figDesc><table><row><cell>Method</cell><cell cols="4">Dataset Size VOC Cls VOC Det. ImageNet Cls. Acc</cell></row><row><cell>Jigsaw Baseline</cell><cell>1.2M</cell><cell>74.82</cell><cell>61.98</cell><cell>26.17</cell></row><row><cell>Jigsaw anisotropic</cell><cell>2×1.2M</cell><cell>76.77</cell><cell>61.59</cell><cell>26.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Experiments with Alexnet as the backbone. Ideas of anisotropic diffusion filter can extend to other architectures like Alexnet.</figDesc><table><row><cell cols="2">The Anistropic ImageNet model improves over the baseline by</cell></row><row><cell>0.67 mAP</cell><cell></cell></row><row><cell>Method</cell><cell>VOC 2007 Classification</cell></row><row><cell>Jigsaw Baseline(Our Implementation)</cell><cell>65.21</cell></row><row><cell>Jigsaw anisotropic</cell><cell>65.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Results on Label corruption task. We can see that our model consistently outperforms the baseline with larger improvement upon increasing the corruption probability.</figDesc><table><row><cell cols="4">Corruption probability Gold Fraction Baseline(Error) Anistropic Model(Error)</cell></row><row><cell>0.2</cell><cell>0.05</cell><cell>29.61</cell><cell>28.75</cell></row><row><cell>0.2</cell><cell>0.1</cell><cell>28.31</cell><cell>27.84</cell></row><row><cell>0.4</cell><cell>0.05</cell><cell>31.92</cell><cell>30.5</cell></row><row><cell>0.4</cell><cell>0.1</cell><cell>32.59</cell><cell>31.48</cell></row><row><cell>0.6</cell><cell>0.05</cell><cell>39.04</cell><cell>38.52</cell></row><row><cell>0.6</cell><cell>0.1</cell><cell>36.75</cell><cell>34.98</cell></row><row><cell>0.8</cell><cell>0.05</cell><cell>54.23</cell><cell>52.6</cell></row><row><cell>0.8</cell><cell>0.1</cell><cell>44.31</cell><cell>43</cell></row><row><cell>1.0</cell><cell>0.05</cell><cell>75.05</cell><cell>71.21</cell></row><row><cell>1.0</cell><cell>0.1</cell><cell>51.19</cell><cell>45.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Comparison between Stylized ImageNet and our Anisotropic ImageNet. Following<ref type="bibr" target="#b13">(Geirhos et al., 2018)</ref>, we use ResNet50 as our backbone. We finetune our models on only the ImageNet dataset. We can see that on ImageNet classification and object detection, Anisotropic ImageNet and Stylized ImageNet have very similar performance.</figDesc><table><row><cell>Method</cell><cell cols="4">Finetune Top-1 Accuracy Top-5 Accuracy OBJ Detection</cell></row><row><cell>Stylized Imagenet</cell><cell>-</cell><cell>74.59</cell><cell>92.14</cell><cell>70.6</cell></row><row><cell>Stylized Imagenet</cell><cell>IN</cell><cell>76.72</cell><cell>93.28</cell><cell>75.1</cell></row><row><cell>Anisotropic Imagenet</cell><cell>-</cell><cell>68.38</cell><cell>87.19</cell><cell>-</cell></row><row><cell>Anisotropic Imagenet</cell><cell>IN</cell><cell>76.71</cell><cell>93.26</cell><cell>74.27</cell></row><row><cell>Cartoon Imagenet</cell><cell>IN</cell><cell>76.22</cell><cell>93.12</cell><cell>72.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard</figDesc><table><row><cell>ImageNet</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Entropy Mean Highest probability</cell></row><row><cell>Anistropic ImageNet</cell><cell>0.81</cell><cell>0.93</cell></row><row><cell>Standard ImageNet</cell><cell>1.88</cell><cell>0.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>ImageNet classification by finetuning the last FC layer. Features from the conv layers are kept unchanged. This experiment helps evaluate the quality of features learnt by the convolutional layers.</figDesc><table><row><cell>Method</cell><cell cols="4">Dataset Size VOC Cls. VOC Det. ImageNet Cls. Acc</cell></row><row><cell>Jigsaw Baseline</cell><cell>1.2M</cell><cell>74.82</cell><cell>61.98</cell><cell>26.17</cell></row><row><cell>Jigsaw anisotropic</cell><cell>2×1.2M</cell><cell>76.77</cell><cell>61.59</cell><cell>26.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Comparison of our approach with Jigsaw baseline methods. Using our best model, we improve 2.52 mAP in VOC classification , 0.78 mAP on VOC detection and 8.1 mAP on VOC semantic segmentation(SS) over the baseline models. Note that Stylized ImageNet performs poorly on VOC classification due to the visual shortcuts.</figDesc><table><row><cell>Method</cell><cell cols="3">Dataset Size VOC Cls. VOC Det.</cell><cell>SS</cell></row><row><cell>Baseline</cell><cell>1.2M</cell><cell>74.82</cell><cell>61.98</cell><cell>27.1</cell></row><row><cell>Stylized (Geirhos et al., 2018)</cell><cell>1.2M</cell><cell>13.81</cell><cell>28.13</cell><cell>10.12</cell></row><row><cell>Gaussian ImageNet</cell><cell>2×1.2M</cell><cell>75.49</cell><cell>62.39</cell><cell>27.9</cell></row><row><cell>Bilateral ImageNet</cell><cell>2×1.2M</cell><cell>74.55</cell><cell>62.74</cell><cell>28.9</cell></row><row><cell>Only Anisotropic</cell><cell>1.2M</cell><cell>74.52</cell><cell>61.85</cell><cell>32.7</cell></row><row><cell>Anisotropic ImageNet</cell><cell>2×1.2M</cell><cell>76.77</cell><cell>61.59</cell><cell>35.2</cell></row><row><cell>Cartoon ImageNet</cell><cell>2×1.2M</cell><cell>77.34</cell><cell>59.31</cell><cell>34.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Saliency maps on three different set of images. The text on the left of the row indicates whether Anisotropic model or ImageNet model was used. The first two rows show the saliency maps where both model gave correct predictions. We can see from saliency maps that the Anisotropic model has more diffused saliency maps. The second two rows show the saliency maps where Anisotropic model gave correct predictions and ImageNet model gave wrong predictions. The failure of ImageNet model might be due to it not attending to whole object. The last two rows show the saliency maps where Anisotropic model gives incorrect predictions and ImageNet model gives correct predictions. Even in this failure mode, the Anisotropic model gives diffused saliency maps.</figDesc><table><row><cell>Anisotropic</cell><cell>Correct</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ImageNet</cell><cell>Correct</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell></row><row><cell>Anisotropic</cell><cell>Correct</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ImageNet</cell><cell>Incorrect</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(f)</cell><cell>(g)</cell><cell>(h)</cell><cell>(i)</cell><cell>(j)</cell></row><row><cell cols="2">Anisotropic ImageNet Figure 7: (a) Incorrect Correct (k)</cell><cell>(l) (b)</cell><cell>(m) (c)</cell><cell>(n) (d)</cell><cell>(o) (e)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/bbrattoli/JigsawPuzzlePytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>• We also get improvements in same domain visual recognition tasks on ImageNet validation (+0.7%) and a label corruption task <ref type="bibr" target="#b16">(Hendrycks et al., 2019)</ref>.</p><p>• We achieve state-of-the-art results in self-supervised learning on VOC detection and other transfer learning tasks.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marimont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Society</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="421" to="453" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>IEEE transactions on image</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Approximating cnns with bag-of-local-features models works surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual permutation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3100" to="3114" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeppermnet: Visual permutation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6044" to="6052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno>abs/1805.09501</idno>
		<title level="m">Learning augmentation policies from data. ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<idno>abs/1811.12231</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1808.06670</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Novel dataset for fine-grained image categorization : Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2656" to="2666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning recursive filters for low-level vision via a hybrid neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Shan Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving robustness without sacrificing accuracy with patch gaussian augmentation. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin Dogus</forename><surname>Cubuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep texture and structure aware filtering network for image smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How useful is self-supervised pretraining for visual tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00737</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR42600.2020.00737.2" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles. ArXiv, abs/1603.09246</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Texture bias of cnns limits few-shot classification performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Ringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Macleod</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2572683</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2016.2572683.1" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sixth International Conference on Computer Vision (IEEE Cat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Manduchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
	<note>Bilateral filtering for gray and color images</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Iccv</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The Caltech-UCSD Birds</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning robust global representations by penalizing local predictive power. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">Chase</forename><surname>Lipton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scale-invariant convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1411.6369</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
