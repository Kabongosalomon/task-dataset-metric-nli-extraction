<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structural-RNN: Deep Learning on Spatio-Temporal Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
							<email>ashesh@cs.cornell.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Brain Of Things Inc. 3</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
							<email>zamir@cs.stanford.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Brain Of Things Inc. 3</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Brain Of Things Inc. 3</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
							<email>asaxena@cs.stanford.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structural-RNN: Deep Learning on Spatio-Temporal Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatiotemporal graphs are a popular tool for imposing such highlevel intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks (RNNs). We develop a scalable method for casting an arbitrary spatiotemporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Links: Web</head><p>Corresponding Structural-RNN Spatio-temporal graph representation Problem (e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activity)</head><p>Activity Affordance RNN Activity Affordance Activity Affordance</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The world we live in is inherently structured. It is comprised of components that interact with each other in space and time, leading to a spatio-temporal composition. Utilizing such structures in problem formulation allows domainexperts to inject their high-level knowledge in learning frameworks. This has been the incentive for many efforts in computer vision and machine learning, such as Logic Networks <ref type="bibr" target="#b45">[46]</ref>, Graphical Models <ref type="bibr" target="#b27">[28]</ref>, and Structured SVMs <ref type="bibr" target="#b25">[26]</ref>. Structures that span over both space and time (spatio-temporal) are of particular interest to computer vision and robotics communities. Primarily, interactions between humans and environment in real world are inherently t-1 t t+1 spatio-temporal in nature. For example, during a cooking activity, humans interact with multiple objects both in space and through time. Similarly, parts of human body (arms, legs, etc.) have individual functions but work with each other in concert to generate physically sensible motions. Hence, bringing high-level spatio-temporal structures and rich sequence modeling capabilities together is of particular importance for many applications. The notable success of RNNs has proven their capability on many end-to-end learning tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b65">66]</ref>. However, they lack a high-level and intuitive spatio-temporal structure though they have been shown to be successful at modeling long sequences <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref>. Therefore, augmenting a high-level structure with learning capability of RNNs leads to a powerful tool that has the best of both worlds. Spatio-temporal graphs (st-graphs) are a popular <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b21">22]</ref> general tool for representing such high-level spatio-temporal structures. The nodes of the graph typically represent the problem components, and the edges capture their spatio-temporal interactions. To achieve the above goal, we develop a generic tool for transforming an arbitrary st-graph into a feedforward mixture of RNNs, named structural-RNN (S-RNN). <ref type="figure" target="#fig_0">Figure 1</ref> schematically illustrates this process, where a sample spatio-temporal problem is shown at the bottom, the corresponding st-graph representation is shown in the middle, and our RNN mixture counterpart of the st-graph is shown at the top.</p><p>In high-level steps, given an arbitrary st-graph, we first roll it out in time and decompose it into a set of contributing factor components. The factors identify the independent components that collectively determine one decision and are derived from both edges and nodes of the st-graph. We then semantically group the factor components and represent each group using one RNN, which results in the desired RNN mixture. The main challenges of this transformation problem are: 1) making the RNN mixture as rich as possible to enable learning complex functions, yet 2) keeping the RNN mixture scalable with respect to size of the input st-graph. In order to make the resulting RNN mixture rich, we liberally represent each spatio-temporal factor (including node factors, temporal edge factors, and spatiotemporal edge factors) using one RNN. On the other hand, to keep the overall mixture scalable but not lose the essential learning capacity, we utilize "factor sharing" (aka clique templates <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53]</ref>) and allow the factors with similar semantic functions to share an RNN. This results in a rich and scalable feedforward mixture of RNNs that is equivalent to the provided st-graph in terms of input, output, and spatiotemporal relationships. The mixture is also fully differentiable, and therefore, can be trained jointly as one entity.</p><p>The proposed method is principled and generic as the transformation is based on a set of well defined steps and it is applicable to any problem that can be formulated as st-graphs (as defined in Section 3). Several previous works have attempted solving specific problems using a collection of RNNs <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5]</ref>, but they are almost unanimously task-specific. They also do not utilize mechanisms similar to factorization or factor sharing in devising their architecture to ensure richness and scalability.</p><p>S-RNN is also modular, as it is enjoying an underlying high-level structure. This enables easy high-level manipulations which are basically not possible in unstructured (plain-vanilla) RNNs (e.g., we will experimentally show forming a feasible hybrid human motion by mixing parts of different motion styles -Sec 4.2 ). We evaluate the proposed approach on a diverse set of spatio-temporal problems (human pose modeling and forecasting, human-object interaction, and driver decision making), and show significant improvements over the state of the art on each problem. We also study complexity and convergence properties of S-RNN and provide further experimental insights by visualizing its memory cells that reveals some cells interestingly represent certain semantic operations. The code of the entire framework that accepts a st-graph as the input and yields the output RNN mixture is available at the http://asheshjain.org/srnn. The contribution of this paper are: 1) a generic method for casting an arbitrary st-graph as a rich, scalable, and jointly trainable RNN mixture, 2) in defence of structured approaches, we show S-RNN significantly outperforms its unstructured (plain-vanilla) RNN counterparts, 3) in defence of RNNs, we show on several diverse spatio-temporal problems that modeling structure with S-RNN outperforms the non-deep learning based structured counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We give a categorized overview of the related literature. In general, three main characteristics differentiate our work from existing techniques: being generic and not restricted to a specific problem, providing a principled method for transforming a st-graph into a scalable rich feedforward RNN mixture, and being jointly trainable.</p><p>Spatio-temporal problems. Problems that require spatial and temporal reasoning are very common in robotics and computer vision. Examples include human activity recognition and segmentation from videos <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36]</ref>, context-rich human-object interactions <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>, modeling human motion <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b55">56]</ref> etc. Spatio-temporal reasoning also finds application in assistive robots, driver understanding, and object recognition <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b10">11]</ref>. In fact most of our daily activities are spatio-temporal in nature. With growing interests in rich interactions and robotics, this form of reasoning will become even more important. We evaluate our generic method on three context-rich spatio-temporal problems: (i) Human motion modeling <ref type="bibr" target="#b13">[14]</ref>; (ii) Human-object interaction understanding <ref type="bibr" target="#b32">[33]</ref>; and (iii) Driver maneuver anticipation <ref type="bibr" target="#b21">[22]</ref>.</p><p>Mixtures of deep architectures. Several previous works build multiple networks and wire them together in order to capture some complex structure (or interactions) in the problem with promising results on applications such as activity detection, scene labeling, image captioning, and object detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b60">61]</ref>. However, such architectures are mostly hand designed for specific problems, though they demonstrate the benefit in using a modular deep architecture. Recursive Neural Networks <ref type="bibr" target="#b16">[17]</ref> are, on the other hand, generic feedforward architectures, but for problems with recursive structure such as parsing of natural sentences and scenes <ref type="bibr" target="#b47">[48]</ref>. Our work is a remedy for problems expressed as spatio-temporal graphs. For a new spatiotemporal problem in hand, all a practitioner needs to do is to express their intuition about the problem as an st-graph.</p><p>Deep learning with graphical models. Many works have addressed deep networks with graphical models for structured prediction tasks. <ref type="bibr">Bengio</ref>   have addressed end-to-end image segmentation with fully connected CRF <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40]</ref>. Several works follow a two-stage approach and decouple the deep network from CRF. They have been applied to multiple problems including image segmentation, pose estimation, document processing <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3]</ref> etc. All of these works advocate and well demonstrate the benefit in exploiting the structure in the problem together with rich deep architectures. However, they largely do not address spatio-temporal problems and the proposed architectures are task-specific.</p><p>Conditional Random Fields (CRF) model dependencies between the outputs by learning a joint distribution over them. They have been applied to many applications <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b44">45]</ref> including st-graphs which are commonly modeled as spatio-temporal CRF <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b10">11]</ref>. In our approach, we adopt st-graphs as a general graph representation and embody it using an RNN mixture architecture. Unlike CRF, our approach is not probabilistic and is not meant to model the joint distribution over the outputs. S-RNN instead learns the dependencies between the outputs via structural sharing of RNNs between the outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Structural-RNN architectures</head><p>In this section we describe our approach for building structural-RNN (S-RNN) architectures. We start with a st-graph, decompose it into a set of factor components, then represent each factor using a RNN. The RNNs are interconnected in a way that the resulting architecture captures the structure and interactions of the st-graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Representation of spatio-temporal graphs</head><p>Many applications that require spatial and temporal reasoning are modeled using st-graphs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b21">22]</ref>. We represent a st-graph with G = (V, E S , E T ), whose structure (V, E S ) unrolls over time through edges E T . <ref type="figure" target="#fig_1">Figure 2a</ref> shows an example st-graph capturing human-object interactions during an activity. The nodes v ∈ V and edges e ∈ E S ∪ E T of the st-graph repeats over time. In particular, <ref type="figure" target="#fig_1">Figure 2b</ref> shows the same st-graph unrolled through time. In the unrolled st-graph, the nodes at a given time step t are connected with undirected spatio-temporal edge e = (u, v) ∈ E S , and the nodes at adjacent time steps (say the node u at time t and the node v at time t + 1) are con- <ref type="bibr" target="#b0">1</ref> Given a st-graph and the feature vectors associated with the nodes x t v and edges x t e , as shown in <ref type="figure" target="#fig_1">Figure 2b</ref>, the goal is to predict the node labels (or real value vectors) y t v at each time step t. For instance, in human-object interaction, the node features can represent the human and object poses, and edge features can their relative orientation; the node labels represent the human activity and object affordance. Label y t v is affected by both its node and its interactions with other nodes (edges), leading to an overall complex system. Such interactions are commonly parameterized with a factor graph that conveys how a (complicated) function over the st-graph factorizes into simpler functions <ref type="bibr" target="#b34">[35]</ref>. We derive our S-RNN architecture from the factor graph representation of the st-graph. Our factor graph representation has a factor function Ψ v (y v , x v ) for each node and a pairwise factor Ψ e (y e(1) , y e(2) , x e ) for each edge. <ref type="figure" target="#fig_1">Figure 2c</ref> shows the factor graph corresponding to the st-graph in 2a. <ref type="bibr" target="#b1">2</ref> Sharing factors between nodes. Each factor in the stgraph has parameters that needs to be learned. Instead of learning a distinct factor for each node, semantically similar nodes can optionally share factors. For example, all "object nodes" {u,w} in the st-graph can share the same node factor and parameters. This modeling choice allows enforcing parameter sharing between similar nodes. It further gives the flexibility to handle st-graphs with more nodes without increasing the number of parameters. For this purpose, we partition the nodes as C V = {V 1 , .., V P } where V p is a set of semantically similar nodes, and they all use the same node factor Ψ Vp . In <ref type="figure" target="#fig_2">Figure 3a</ref> we re-draw the st-graph and assign same color to the nodes sharing node factors.</p><formula xml:id="formula_0">nected with undirected temporal edge iff (u, v) ∈ E T .</formula><p>Partitioning nodes on their semantic meanings leads to a natural semantic partition of the edges,</p><formula xml:id="formula_1">C E = {E 1 , .., E M },</formula><p>where E m is a set of edges whose nodes form a semantic pair. Therefore, all edges in the set E m share the same edge factor Ψ Em . For example all "human-object  <ref type="figure" target="#fig_3">Figure 4</ref> we show the detailed layout of this forward-pass.</p><formula xml:id="formula_2">v involve RNNs R E 1 , R E 3 and R V 1 . In</formula><formula xml:id="formula_3">Input features into R E 1 is sum of human-object edge features xu,v + xv,w. (d) The forward-pass for object node w involve RNNs R E 1 , R E 2 , R E 4 and R V 2 .</formula><p>In this forward-pass, the edgeRNN R E 1 only processes the edge feature xv,w. (Best viewed in color) edges" {(v, u), (v, w)} are modeled with the same edge factor. Sharing factors based on semantic meaning makes the overall parametrization compact. In fact, sharing parameters is necessary to address applications where the number of nodes depends on the context. For example, in human-object interaction the number of object nodes vary with the environment. Therefore, without sharing parameters between the object nodes, the model cannot generalize to new environments with more objects. For modeling flexibility, the edge factors are not shared across the edges in E S and E T . Hence, in <ref type="figure" target="#fig_2">Figure 3a</ref></p><formula xml:id="formula_4">, object-object (w, w) ∈ E T temporal edge is colored differently from object-object (u, w) ∈ E S spatio-temporal edge.</formula><p>In order to predict the label of node v ∈ V p , we consider its node factor Ψ Vp , and the edge factors connected to v in the factor graph. We define a node factor and an edge factor as neighbors if they jointly affect the label of some node in the st-graph. More formally, the node factor Ψ Vp and edge factor Ψ Em are neighbors, if there exist a node v ∈ V p such that it connects to both Ψ Vp and Ψ Em in the factor graph. We will use this definition in building S-RNN architecture such that it captures the interactions in the st-graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Structural-RNN from spatio-temporal graphs</head><p>We derive our S-RNN architecture from the factor graph representation of the st-graph. The factors in the st-graph operate in a temporal manner, where at each time step the factors observe (node &amp; edge) features and perform some computation on those features. In S-RNN, we represent each factor with an RNN. We refer the RNNs obtained from the node factors as nodeRNNs and the RNNs obtained from the edge factors as edgeRNNs. The interactions represented by the st-graph are captured through connections between the nodeRNNs and the edgeRNNs.</p><p>We denote the RNNs corresponding to the node factor Ψ Vp and the edge factor Ψ Em as R Vp and R Em respectively. In order to obtain a feedforward network, we connect the edgeRNNs and nodeRNNs to form a bipartite graph G R = ({R Em }, {R Vp }, E R ). In particular, the edgeRNN R Em is connected to the nodeRNN R Vp iff the factors Ψ Em and Ψ Vp are neighbors in the st-graph, i.e. they jointly af-</p><formula xml:id="formula_5">Algorithm 1 From spatio-temporal graph to S-RNN Input G = (V, E S , E T ), C V = {V 1 , ..., V P } Output S-RNN graph G R = ({R Em }, {R Vp }, E R ) 1: Semantically partition edges C E = {E 1 , ..., E M } 2: Find factor components {Ψ Vp , Ψ Em } of G 3: Represent each Ψ Vp with a nodeRNN R Vp 4: Represent each Ψ Em with an edgeRNN R Em 5: Connect {R Em } and {R Vp } to form a bipartite graph. (R Em , R Vp ) ∈ E R iff ∃v ∈ V p , u ∈ V s.t. (u, v) ∈ E m Return G R = ({R Em }, {R Vp }, E R )</formula><p>fect the label of some node in the st-graph. To summarize, in Algorithm 1 we show the steps for constructing S-RNN architecture. <ref type="figure" target="#fig_2">Figure 3b</ref> shows the S-RNN for the human activity represented in <ref type="figure" target="#fig_2">Figure 3a</ref>. The nodeRNNs combine the outputs of the edgeRNNs they are connected to (i.e. its neighbors in the factor graph), and predict the node labels. The predictions of nodeRNNs (eg. R V1 and R V2 ) interact through the edgeRNNs (eg. R E1 ). Each edgeRNN handles a specific semantic interaction between the nodes connected in the st-grap and models how the interactions evolve over time. In the next section, we explain the inputs, outputs, and the training procedure of S-RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training structural-RNN architecture</head><p>In order to train the S-RNN architecture, for each node of the st-graph the features associated with the node are fed into the architecture. In the forward-pass for node v ∈ V p , the input into edgeRNN R Em is the temporal sequence of edge features x t e on the edge e ∈ E m , where edge e is incident to node v in the st-graph. The nodeRNN R Vp at each time step concatenates the node feature x t v and the outputs of edgeRNNs it is connected to, and predicts the node label. At the time of training, the errors in prediction are backpropagated through the nodeRNN and edgeRNNs involved during the forward-pass. That way, S-RNN non-linearly combines the node and edge features associated with the nodes in order to predict the node labels. <ref type="figure" target="#fig_2">Figure 3c</ref> shows the forward-pass through S-RNN for the human node. <ref type="figure" target="#fig_3">Figure 4</ref> shows a detailed architecture lay- out of the same forward-pass. The forward-pass involves the edgeRNNs R E1 (human-object edge) and R E3 (humanhuman edge). Since the human node v interacts with two object nodes {u,w}, we pass the summation of the two edge features as input to R E1 . The summation of features, as opposed to concatenation, is important to handle variable number of object nodes with a fixed architecture. Since the object count varies with environment, it is challenging to represent variable context with a fixed length feature vector. Empirically, adding features works better than mean pooling. We conjecture that addition retains the object count and the structure of the st-graph, while mean pooling averages out the number of edges. The nodeRNN R V1 concatenates the (human) node features with the outputs from edgeRNNs, and predicts the activity at each time step. Parameter sharing and structured feature space. An important aspect of S-RNN is sharing of parameters across the node labels. Parameter sharing between node labels happen when an RNN is common in their forward-pass. For example in <ref type="figure" target="#fig_2">Figure 3c</ref> and 3d, the edgeRNN R E1 is common in the forward-pass for the human node and the object nodes. Furthermore, the parameters of R E1 gets updated through back-propagated gradients from both the object and human nodeRNNs. In this way, R E1 affects both the human and object node labels.</p><p>Since the human node is connected to multiple object nodes, the input into edgeRNN R E1 is always a linear combination of human-object edge features. This imposes an structure on the features processed by R E1 . More formally, the input into R E1 is the inner product s T F, where F is the feature matrix storing the edge features x e s.t. e ∈ E 1 . Vector s captures the structured feature space. Its entries are in {0,1} depending on the node being forward-passed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We present results on three diverse spatio-temporal problems to ensure generic applicability of S-RNN, shown in   <ref type="figure" target="#fig_6">Figure 5</ref>. The applications include: (i) modeling human motion <ref type="bibr" target="#b13">[14]</ref> from motion capture data <ref type="bibr" target="#b20">[21]</ref>; (ii) human activity detection and anticipation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref>; and (iii) maneuver anticipation from real-world driving data <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Human motion modeling and forecasting</head><p>Human body is a good example of separate but well related components. Its motion involves complex spatiotemporal interactions between the components (arms, legs, spine), resulting in sensible motion styles like walking, eating etc. In this experiment, we represent the complex motion of humans over st-graphs and learn to model them with S-RNN. We show that our structured approach outperforms the state-of-the-art unstructured deep architecture <ref type="bibr" target="#b13">[14]</ref> on motion forecasting from motion capture (mocap) data. Several approaches based on Gaussian processes <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b62">63]</ref>, Restricted Boltzmann Machines (RBMs) <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b50">51]</ref>, and RNNs <ref type="bibr" target="#b13">[14]</ref> have been proposed to model human motion. Recently, Fragkiadaki et al. <ref type="bibr" target="#b13">[14]</ref> proposed an encoder-RNN-decoder (ERD) which gets state-of-the-art forecasting results on H3.6m mocap data set <ref type="bibr" target="#b20">[21]</ref>. S-RNN architecture for human motion. Our S-RNN architecture follows the st-graph shown in <ref type="figure" target="#fig_6">Figure 5a</ref>. According to the st-graph, the spine interacts with all the body parts, and the arms and legs interact with each other. The st-graph is automatically transformed to S-RNN following Section 3.2. The resulting S-RNN have three nodeRNNs, one for each type of body part (spine, arm, and leg), four edgeRNNs for modeling the spatio-temporal interactions between them, and three edgeRNNs for their temporal connections. For edgeRNNs and nodeRNNs we use FC(256)-FC(256)-LSTM(512) and LSTM(512)-FC(256)-FC(100)-FC(·) architectures, respectively, with skip input and output connections <ref type="bibr" target="#b17">[18]</ref>. The outputs of nodeRNNs are skeleton joints of different body parts, which are concatenated to reconstruct the complete skeleton. In order to model human motion, we train S-RNN to predict the mocap frame at time t + 1 given the frame at time t. Similar to <ref type="bibr" target="#b13">[14]</ref>, we gradually add noise to the mocap frames during training. This simulates curriculum learning <ref type="bibr" target="#b1">[2]</ref> and helps in keeping the forecasted motion close to the manifold of human motion. As node features we use the raw joint values expressed as exponential map <ref type="bibr" target="#b13">[14]</ref>, and edge features are concatenation of the node features. We train all RNNs jointly to minimize the Euclidean loss between the predicted mocap frame and the ground truth. See supplementary material on the project web page <ref type="bibr" target="#b23">[24]</ref> for training details. Evaluation setup. We compare S-RNN with the stateof-the-art ERD architecture <ref type="bibr" target="#b13">[14]</ref> on H3.6m mocap data set <ref type="bibr" target="#b20">[21]</ref>. We also compare with a 3 layer LSTM architecture (LSTM-3LR) which <ref type="bibr" target="#b13">[14]</ref> use as a baseline. <ref type="bibr" target="#b2">3</ref> For motion forecasting we follow the experimental setup of <ref type="bibr" target="#b13">[14]</ref>. We downsample H3.6m by two and train on 6 subjects and test on subject S5. To forecast, we first feed the architectures with (50) seed mocap frames, and then forecast the future (100) frames. Following <ref type="bibr" target="#b13">[14]</ref>, we consider walking, eating, and smoking activities. In addition to these three, we also consider discussion activity.</p><p>Forecasting is specially challenging on activities with complex aperiodic human motion. In H3.6m data set, significant parts of eating, smoking, and discussion activities are aperiodic, while walking activity is mostly periodic. Our evaluation demonstrates the benefits of having an underlying structure in three important ways: (i) We present visualizations and quantitative results on complex aperiodic activities ( <ref type="bibr" target="#b13">[14]</ref> evaluates only on periodic walking motion); (ii) We forecast human motion for almost twice longer than state-of-the-art <ref type="bibr" target="#b13">[14]</ref>. This is very challenging for aperiodic activities; and finally (iii) We show S-RNN interestingly  <ref type="bibr" target="#b13">[14]</ref> does not offer such modularity.</p><p>Qualitative results on motion forecasting. <ref type="figure" target="#fig_7">Figure 6</ref> shows forecasting 1000ms of human motion on "eating" activity -the subject drinks while walking. S-RNN stays close to the ground-truth in the short-term and generates human like motion in the long-term. On removing edgeRNNs, the parts of human body become independent and stops interacting through parameters. Hence without edgeRNNs the skeleton freezes to some mean position. LSTM-3LR suffers with a drifting problem. On many test examples it drifts to the mean position of walking human ( <ref type="bibr" target="#b13">[14]</ref> made similar observations about LSTM-3LR). The motion generated by ERD <ref type="bibr" target="#b13">[14]</ref> stays human-like in the short-term but it drifts away to non-human like motion in the long-term. This was a common outcome of ERD on complex aperiodic activities, unlike S-RNN. Furthermore, ERD produced human motion was non-smooth on many test examples. See the video on the project web page for more examples <ref type="bibr" target="#b23">[24]</ref>. Quantitative evaluation. We follow the evaluation metric of Fragkiadaki et al. <ref type="bibr" target="#b13">[14]</ref> and present the 3D angle error between the forecasted mocap frame and the ground truth in <ref type="table" target="#tab_1">Table 1</ref>. Qualitatively, ERD models human motion better than LSTM-3LR. However, in the short-term, it does not mimic the ground-truth as well as LSTM-3LR. Fragkiadaki et al. <ref type="bibr" target="#b13">[14]</ref> also note this trade-off between ERD and LSTM-3LR. On the other hand, S-RNN outperforms both LSTM-3LR and ERD on short-term motion forecasting on all activities. S-RNN therefore mimics the ground truth in the short-term and generates human like motion in the long term. In this way it well handles both short and long term forecasting. Due to stochasticity in human motion, longterm forecasts (&gt; 500ms) can significantly differ from the ground truth but still depict human-like motion. For this reason, the long-term forecast numbers in <ref type="table" target="#tab_1">Table 1</ref> are not a fair representative of algorithms modeling capabilities. We also observe that discussion is one of the most challenging  aperiodic activity for all algorithms. User study. We asked users to rate the motions on a Likert scale of 1 to 3. S-RNN performed best according to the user study. See supplementary material for the results. To summarize, unstructured approaches like LSTM-3LR and ERD struggles to model long-term human motion on complex activities. S-RNN's good performance is attributed to its structural modeling of human motion through the underlying st-graph. S-RNN models each body part separately with nodeRNNs and captures interactions between them with edgeRNNs in order to produce coherent motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Going deeper into structural-RNN</head><p>We now present several insights into S-RNN architecture and demonstrate the modularity of the architecture which enables it to generate hybrid human motions.</p><p>Visualization of memory cells. We investigated if S-RNN memory cells represent meaningful semantic submotions. Semantic cells were earlier studied on other problems <ref type="bibr" target="#b26">[27]</ref>, we are the first to present it on a vision task and human motion. In <ref type="figure" target="#fig_9">Figure 7</ref> (left) we show a cell in the leg nodeRNN learns the semantic motion of moving the leg forward. The cell fires positive (red color) on the forward movement of the leg and negative (blue color) on its backward movement. As the subject walks, the cell alternatively fires for the right and the left leg. Longer activations in the right leg corresponds to the longer steps taken by the subject with the right leg. Similarly, a cell in the arm nodeRNN learns the concept of moving hand close to the face, as shown in <ref type="figure" target="#fig_9">Figure 7</ref> (right). The same cell fires whenever the subject moves the hand closer to the face during eating or smoking. The cell remains active as long as the hand stays close to the face. See the video <ref type="bibr" target="#b23">[24]</ref>.</p><p>Generating hybrid human motion. We now demonstrate the flexibility of our modular architecture by generating novel yet meaningful motions which are not in the data set. Such modularity is of interest and has been explored to generate diverse motion styles <ref type="bibr" target="#b54">[55]</ref>. As a result of having an underlying high-level structure, our approach allows us to exchange RNNs between the S-RNN architectures trained on different motion styles. We leverage this to create a novel S-RNN architecture which generates a hybrid motion of a human jumping forward on one leg, as shown in <ref type="figure" target="#fig_10">Figure 8</ref> (Left). For this experiment we modeled the left and right leg with different nodeRNNs. We trained two independent S-RNN models -a slower human and a faster human (by down sampling data) -and swapped the left leg nodeRNN of the trained models. The resulting faster human, with a slower left leg, jumps forward on the left leg to keep up with its twice faster right leg. <ref type="bibr" target="#b3">4</ref> Unstructured architectures like ERD <ref type="bibr" target="#b13">[14]</ref> does not offer this kind of flexibility. <ref type="figure" target="#fig_10">Figure 8</ref> (Right) examines the test and train error with iterations. Both S-RNN and ERD converge to similar training error, however S-RNN generalizes better with a smaller test error for next step prediction. Discussion in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Human activity detection and anticipation</head><p>In this section we present S-RNN for modeling human activities. We consider the CAD-120 <ref type="bibr" target="#b28">[29]</ref> data set where the activities involve rich human-object interactions. Each activity consist of a sequence of sub-activities (e.g. moving, drinking etc.) and objects affordance (e.g., reachable, drinkable etc.), which evolves as the activity progresses. Detecting and anticipating the sub-activities and affordance enables personal robots to assist humans. However, the problem is challenging as it involves complex interactions -humans interact with multiple objects during an activity, and objects also interact with each other (e.g. pouring water from "glass" into a "container"), which makes it a particularly good fit for evaluating our method. Koppula et al. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29]</ref> represents such rich spatio-temporal interactions with the st-graph shown in <ref type="figure" target="#fig_6">Figure 5b</ref>, and models it with a spatio-temporal CRF. In this experiment, we show that modeling the same st-graph with S-RNN yields superior results. We use the node and edges features from <ref type="bibr" target="#b28">[29]</ref>. <ref type="figure" target="#fig_2">Figure 3b</ref> shows our S-RNN architecture to model the st-graph. Since the number of objects varies with environment, factor sharing between the object nodes and the human-object edges becomes crucial. In S-RNN, R V2 and R E1 handles all the object nodes and the human-object edges respectively. This allows our fixed S-RNN architecture to handle varying size st-graphs. For edgeRNNs we use a single layer LSTM of size 128, and for nodeRNNs we use LSTM(256)-softmax(·). At each time step, the human nodeRNN outputs the sub-activity label (10 classes), and the object nodeRNN outputs the affordance (12 classes). <ref type="table">Table 2</ref>: Maneuver Anticipation on 1100 miles of real-world driving data. S-RNN is derived from the st-graph shown in <ref type="figure" target="#fig_6">Figure 5c</ref>. Jain et al. <ref type="bibr" target="#b21">[22]</ref> use the same st-graph but models it in a probabilistic frame with AIO-HMM. The table shows average precision, recall and time-to-maneuver. Time-to-maneuver is the interval between the time of algorithm's prediction and the start of the maneuver. Algorithms are compared on the features from <ref type="bibr" target="#b21">[22]</ref>.  <ref type="table">Table 3</ref>: Results on CAD-120 <ref type="bibr" target="#b28">[29]</ref>. S-RNN architecture derived from the st-graph in <ref type="figure" target="#fig_6">Figure 5b</ref> outperforms Koppula et al. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29]</ref> which models the same st-graph in a probabilistic framework. S-RNN in multi-task setting (joint detection and anticipation) further improves the performance. Having observed the st-graph upto time t, the goal is to detect the sub-activity and affordance labels at the current time t, and also anticipate their future labels of the time step t+1. For detection we train S-RNN on the labels of the current time step. For anticipation we train the architecture to predict the labels of the next time step, given the observations upto the current time. We also train a multi-task version of S-RNN, where we add two softmax layers to each nodeRNN and jointly train for anticipation and detection. <ref type="table">Table 3</ref> shows the detection and anticipation F1-scores averaged over all the classes. S-RNN significantly improves over Koppula et al. on both anticipation <ref type="bibr" target="#b30">[31]</ref> and detection <ref type="bibr" target="#b28">[29]</ref>. On anticipating object affordance S-RNN F1-score is 44% more than <ref type="bibr" target="#b30">[31]</ref>, and 7% more on detection. S-RNN does not have any Markov assumptions like spatio-temporal CRF, and therefore, it better models the long-time dependencies needed for anticipation. The table also shows the importance of edgeRNNs in handling spatio-temporal components. EdgeRNN transfers the information from the human to objects, which helps is predicting the object labels. Therefore, S-RNN without the edgeRNNs poorly models the objects. This signifies the importance of edgeRNNs and also validates our design. Finally, training S-RNN in a multi-task manner works best in majority of the cases. In <ref type="figure" target="#fig_11">Figure 9</ref> we show the visualization of an eating activity. We show one representative frame from each sub-activity and our corresponding predictions. S-RNN complexity. In terms of complexity, we discuss two aspects as a function of the underlying st-graph: (i) the number of RNNs in the mixture; and (ii) the complexity of forward-pass. The number of RNNs depends on the number of semantically similar nodes in the st-graph. The overall S-RNN architecture is compact because the edgeRNNs are shared between the nodeRNNs, and the number of semantic categories are usually few in context-rich applications. Furthermore, because of factor sharing the number of RNNs does not increase if more semantically similar nodes are added to the st-graph. The forward-pass complexity depends on the number of RNNs. Since the forward-pass through all edgeRNNs and nodeRNNs can happen in parallel, in practice, the complexity only depends on the cascade of two neural networks (edgeRNN followed by nodeRNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Driver maneuver anticipation</head><p>We finally present S-RNN for another application which involves anticipating maneuvers several seconds before they happen. Jain et al. <ref type="bibr" target="#b21">[22]</ref> represent this problem with the stgraph shown in <ref type="figure" target="#fig_6">Figure 5c</ref>. They model the st-graph as a probabilistic Bayesian network (AIO-HMM <ref type="bibr" target="#b21">[22]</ref>). The stgraph represents the interactions between the observations outside the vehicle (eg. the road features), the driver's maneuvers, and the observations inside the vehicle (eg. the driver's facial features). We model the same st-graph with S-RNN architecture using the node and edge features from Jain et al. <ref type="bibr" target="#b21">[22]</ref>. <ref type="table">Table 2</ref> shows the performance of different algorithms on this task. S-RNN performs better than the state-of-the-art AIO-HMM <ref type="bibr" target="#b21">[22]</ref> in every setting. See supplementary material for the discussion and details <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a generic and principled approach for combining high-level spatio-temporal graphs with sequence modeling success of RNNs. We make use of factor graph, and factor sharing in order to obtain an RNN mixture that is scalable and applicable to any problem expressed over st-graphs. Our RNN mixture captures the rich interactions in the underlying st-graph. We demonstrated significant improvements with S-RNN on three diverse spatiotemporal problems including: (i) human motion modeling; (ii) human-object interaction; and (iii) driver maneuver anticipation. By visualizing the memory cells we showed that S-RNN learns certain semantic sub-motions, and demonstrated its modularity by generating novel human motion. <ref type="bibr" target="#b4">5</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>From st-graph to S-RNN for an example problem. (Bottom) Shows an example activity (human microwaving food). Modeling such problems requires both spatial and temporal reasoning. (Middle) Stgraph capturing spatial and temporal interactions between the human and the objects. (Top) Schematic representation of our structural-RNN architecture automatically derived from st-graph. It captures the structure and interactions of st-graph in a rich yet scalable manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An example spatio-temporal graph (st-graph) of a human activity. (a) st-graph capturing human-object interaction. (b) Unrolling the st-graph through edges E T . The nodes and edges are labelled with the feature vectors associated with them. (c) Our factor graph parameterization of the st-graph. Each node and edge in the st-graph has a corresponding factor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Corresponding S-RNN (c) Forward-pass for human node (d) Forward-pass for object node nodeRNNs edgeRNNs (a) Spatio-temporal graph with colors indicating sharing of factors An example of st-graph to S-RNN. (a) The st-graph from Figure 2 is redrawn with colors to indicate sharing of nodes and edge factors. Nodes and edges with same color share factors. Overall there are six distinct factors: 2 node factors and 4 edge factors. (b) S-RNN architecture has one RNN for each factor. EdgeRNNs and nodeRNNs are connected to form a bipartite graph. Parameter sharing between the human and object nodes happen through edgeRNN R E 1 . (c) The forward-pass for human node</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Forward-pass for human node v. Shows the architecture layout corresponding to the Figure 3c on unrolled st-graph. (View in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>In the example above F = [x v,u x v,w ] T . For the human node v, s = [1 1] T , while for the object node u, s = [1 0] T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Human motion modeling (b) Activity detection and anticipation (c) Maneuver anticipation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Diverse spatio-temporal tasks. We apply S-RNN to the following three diverse spatio-temporal problems. (View in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Forecasting eating activity on test subject. On aperiodic activities, ERD and LSTM-3LR struggle to model human motion. S-RNN, on the other hand, mimics the ground truth in the short-term and generates human like motion in the long term. Without (w/o) edgeRNNs the motion freezes to some mean standing position. See the video<ref type="bibr" target="#b23">[24]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>(</head><label></label><figDesc>A) Cell #496 fires in response to "moving the leg forward" Cell #419 fires in response to "moving arm close to the face" One quick puff of smoke with right arm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>S-RNN memory cell visualization. (Left) A cell of the leg nodeRNN fires (red) when "putting the leg forward". (Right) A cell of the arm nodeRNN fires for "moving the hand close to the face". We visualize the same cell for eating and smoking activities. (See the video [24])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>IterationsFigure 8 :</head><label>8</label><figDesc>(Left) Generating hybrid motions (See the video<ref type="bibr" target="#b23">[24]</ref>). We demonstrate flexibility of S-RNN by generating a hybrid motion of a "human jumping forward on one leg". (Right) Train and test error. S-RNN generalizes better than ERD with a smaller test error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative result on eating activity on CAD-120. Shows multi-task S-RNN detection and anticipation results. For the sub-activity at time t, the labels are anticipated at time t−1. (Zoom in to see the image)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Motion forecasting angle error. {80, 160, 320, 560, 1000} msecs after the seed motion. The results are averaged over 8 seed motion sequences for each activity on the test subject.</figDesc><table><row><cell>Methods</cell><cell cols="5">Short-term forecast 80ms 160ms 320ms 560ms Long-term forecast 1000ms</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Walking activity</cell></row><row><cell cols="2">ERD [14] 1.30</cell><cell>1.56</cell><cell>1.84</cell><cell>2.00</cell><cell>2.38</cell></row><row><cell cols="2">LSTM-3LR 1.18</cell><cell>1.50</cell><cell>1.67</cell><cell>1.81</cell><cell>2.20</cell></row><row><cell cols="2">S-RNN 1.08</cell><cell>1.34</cell><cell>1.60</cell><cell>1.90</cell><cell>2.13</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Eating activity</cell></row><row><cell cols="2">ERD [14] 1.66</cell><cell>1.93</cell><cell>2.28</cell><cell>2.36</cell><cell>2.41</cell></row><row><cell cols="2">LSTM-3LR 1.36</cell><cell>1.79</cell><cell>2.29</cell><cell>2.49</cell><cell>2.82</cell></row><row><cell cols="2">S-RNN 1.35</cell><cell>1.71</cell><cell>2.12</cell><cell>2.28</cell><cell>2.58</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Smoking activity</cell></row><row><cell cols="2">ERD [14] 2.34</cell><cell>2.74</cell><cell>3.73</cell><cell>3.68</cell><cell>3.82</cell></row><row><cell cols="2">LSTM-3LR 2.05</cell><cell>2.34</cell><cell>3.10</cell><cell>3.24</cell><cell>3.42</cell></row><row><cell cols="2">S-RNN 1.90</cell><cell>2.30</cell><cell>2.90</cell><cell>3.21</cell><cell>3.23</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Discussion activity</cell></row><row><cell cols="2">ERD [14] 2.67</cell><cell>2.97</cell><cell>3.23</cell><cell>3.47</cell><cell>2.92</cell></row><row><cell cols="2">LSTM-3LR 2.25</cell><cell>2.33</cell><cell>2.45</cell><cell>2.48</cell><cell>2.93</cell></row><row><cell cols="2">S-RNN 1.67</cell><cell>2.03</cell><cell>2.20</cell><cell>2.39</cell><cell>2.43</cell></row><row><cell cols="6">learns semantic concepts, and demonstrate its modularity</cell></row><row><cell cols="6">by generating hybrid human motion. Unstructured deep ar-</cell></row><row><cell>chitectures like</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For simplicity, the example st-graph inFigure 2aconsiders temporal edges of the form (v, v) ∈ E T .<ref type="bibr" target="#b1">2</ref> Note that we adopted factor graph as a tool for capturing interactions and not modeling the overall function. Factor graphs are commonly used in probabilistic graphical models for factorizing joint probability distributions. We consider them for general st-graphs and do not establish relations to its probabilistic and function decomposition properties.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We reproduce ERD and LSTM-3LR architectures following<ref type="bibr" target="#b13">[14]</ref>. The authors implementation were not available at the time of submission.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Imagine your motion forward if someone holds your right leg and runs!</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b4">5</ref> <p>We acknowledge NRI #1426452, ONR-N00014-14-1-0156, MURI-WF911NF-15-1-0479 and Panasonic Center grant #122282.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally trained handwritten word recognizer using spatial representation, convolutional neural networks, and hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global training of document processing systems using graph transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal graphs of human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M A L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mosift: Recognizing human actions in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A spatio-temporal probabilistic model for multi-sensor multi-class object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Observing humanobject interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Car that knows before you do: Anticipating maneuvers via learning temporal driving models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond geometric path planning: Learning context-driven user preferences via suboptimal feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">S-rnn supplementary video and material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<ptr target="http://asheshjain.org/srnn" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ob-jects2action: Classifying and localizing actions without any video example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="27" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Anticipatory planning for humanrobot teams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISER</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal structure from rgb-d videos for human activity detection and anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.5644</idno>
		<title level="m">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Factor graphs and the sum-product algorithm. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Kschischang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-A</forename><surname>Loeliger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>IEEE Trans</publisher>
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Track to the future: Spatio-temporal video segmentation with long-range motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Key object driven multi-category object recognition, localization and tracking using spatio-temporal context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01013</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Factorie: Probabilistic programming via imperatively defined factor graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7753</idno>
		<title level="m">Learning longer memory in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recognizing object affordances in terms of spatio-temporal object-object relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pieropan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE-RAS Intl. Conf. on Humanoid Robots</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Conditional random fields for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ML</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human action segmentation and recognition using discriminative semi-markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Parsing Natural Scenes and Natural Language with Recursive Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Active: Activity concept transitions in video event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The recurrent temporal restricted boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An introduction to conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Discriminative probabilistic models for relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Factored conditional restricted boltzmann machines for modeling motion style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dynamical binary latent variable models for 3d human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Topologically-constrained latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Conditional random fields for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Vail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4729</idno>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Overtaking vehicle detection using a spatio-temporal crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IVS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
