<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Feature Alignment for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Chen</surname></persName>
							<email>cqchen94@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="laboratory">Fujian Key Laboratory of Sensing and Computing for Smart City</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="laboratory">Fujian Key Laboratory of Sensing and Computing for Smart City</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<email>hwenbing@126.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<email>yu.rong@hotmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="laboratory">Fujian Key Laboratory of Sensing and Computing for Smart City</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
							<email>huangyue05@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="laboratory">Fujian Key Laboratory of Sensing and Computing for Smart City</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
							<email>tingyangxu@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<email>jzhuang@uta.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive Feature Alignment for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) transfers knowledge from a label-rich source domain to a fullyunlabeled target domain. To tackle this task, recent approaches resort to discriminative domain transfer in virtue of pseudo-labels to enforce the class-level distribution alignment across the source and target domains. These methods, however, are vulnerable to the error accumulation and thus incapable of preserving cross-domain category consistency, as the pseudo-labeling accuracy is not guaranteed explicitly. In this paper, we propose the Progressive Feature Alignment Network (PFAN) to align the discriminative features across domains progressively and effectively, via exploiting the intra-class variation in the target domain. To be specific, we first develop an Easyto-Hard Transfer Strategy (EHTS) and an Adaptive Prototype Alignment (APA) step to train our model iteratively and alternatively. Moreover, upon observing that a good domain adaptation usually requires a non-saturated source classifier, we consider a simple yet efficient way to retard the convergence speed of the source classification loss by further involving a temperature variate into the soft-max function. The extensive experimental results reveal that the proposed PFAN exceeds the state-of-the-art performance on three UDA datasets. * indicates equal contributions.</p><p>† Corresponding authors</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Hiving large-scale labeled datasets is one of the reasons for the recent success of deep convolutional neural networks (CNNs) <ref type="bibr" target="#b13">[14]</ref>. Nevertheless, the collection and annotation of numerous samples in various domains is an extremely expensive and time-consuming process. Meanwhile, traditional CNNs trained on one large dataset show low gen- There is intra-class variation in the target domain. eralization ability on another due to the data bias or shift <ref type="bibr" target="#b37">[38]</ref>. Unsupervised domain adaptation (UDA) methods tackle the mentioned problem by transferring knowledge from a label-rich source domain to a fully unlabeled target domain <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref>. The deep UDA methods have achieved remarkable performance <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16]</ref>, which usually seek to jointly achieve small source generalization error and cross-domain distribution discrepancy.</p><p>Most prior efforts focus on matching global source and target data distributions to learn domain-invariant representations. However, the learned representations may not only bring the source and target domains closer, but also mix samples with different class labels together. Recent studies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b40">41]</ref> started to consider learning discriminative representations for the target domain. Specifically, some of them <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref> proposed to use pseudo-labels to learn target discriminative representations, which encourages a low-density separation between classes in the target domain <ref type="bibr" target="#b19">[20]</ref>. Despite their efficacy, these approaches faces two critical limitations. Firstly, they require a strong pre-assumption that the correctly-pseudolabeled samples can reduce the bias caused by falselypseudo-labeled samples. Nevertheless, it is challenged to satisfy the assumption, especially when the domain discrepancy is large. The learned classifiers might be incapable of confidently distinguishing target samples, or precisely pseudo-label them with an expected accuracy requirement. Secondly, they backpropagate the category loss for target samples based on pseudo-labeled samples, which makes the target performance vulnerable to the error accumulation.</p><p>During the exploration, we empirically observe the distinct data patterns in the target domain. The motivation is demonstrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The intra-class distribution variance exists in the target domain. Some target samples, which we call easy samples, are very likely to be classified correctly since they are sufficiently close to the source domain, and we can directly assign pseudo-labels to them without any adaptation. Some target samples, which we call hard samples, lay far away from the source domain and they are ambiguous for the classification boundaries. Moreover, some easy samples, which we call false-easy samples, lay in the support of non-corresponding source classes and are prone to be falsely pseudo-labeled with high confidence. These false-labeled samples introduce wrong information in the category alignment and potentially result in the error accumulation. Thus it is prerequisite to alleviate their negative influences in the context of UDA.</p><p>In this paper, we propose a Progressive Feature Alignment Network (PFAN), which largely extends the ability of prior discriminative representations-based approaches by explicitly enforcing the category alignment in a progressive manner. Firstly, an Easy-to-Hard Transfer Strategy (EHTS) progressively selects reliable pseudo-labeled target samples with cross-domain similarity measurements. However, the selected samples may include some misclassified target samples with high confidence. Then, to suppress the negative influence of falsely-labeled samples, we propose an Adaptive Prototype Alignment (APA) to align the source and target prototypes for each category. Rather than backpropagating the category loss for target samples based on pseudo-labeled samples, our work statistically align the cross-domain class distributions based on the source samples and the selected pseudo-labeled target samples.</p><p>The EHTS and APA update iteratively and alternatively, where EHTS boosts the robustness of APA by providing reliable pseudo-labeled samples, and the cross-domain category alignment learned by APA can effectively alleviate those falsely-labeled samples introduced by the EHTS. Moreover, upon observing that a good adaptation model usually requires a non-saturated source classifier, we consider a simple yet efficient way to retard the convergence speed of the source classification loss by further involving a temperature variate into the soft-max function. The experimental results reveal that the proposed PFAN exceeds the state-of-the-art performance on three UDA datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We summarize the work most relevant to our proposed approach. We focus primarily on deep UDA methods due to their empirical superiority in this problem. Inspired by the recent success of generative adversarial networks (GAN) <ref type="bibr" target="#b10">[11]</ref>, deep adversarial domain adaptation has received increasing attention in learning domaininvariant representations to reduce the domain discrepancy and provide remarkable results <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45]</ref>. These methods try to find a feature space such that confusion between the source and the target distributions in that space is maximal. For example, <ref type="bibr" target="#b8">[9]</ref> proposed a gradient reversal layer to train a feature extractor that produces features which maximize the domain binary classifier loss, while simultaneously minimizing the label predictor loss.</p><p>Many approaches utilize a distance metric to measure the domain discrepancy between the source and target domains, such as maximum mean discrepancy (MMD), KLdivergence or Wasserstein distance <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b5">6]</ref>. Most prior efforts intend to achieve domain alignment by matching P (X s ) and P (X t ). However, an exact domainlevel alignment does not imply a fine-grained class-to-class overlap. Thus, it is important to pursue the category-level alignment under the absence of target true labels. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b40">41]</ref> utilize the pseudo-labels to compensate the lack of categorical information in the target domain. <ref type="bibr" target="#b22">[23]</ref> jointly matched both the marginal distribution and conditional distribution using a revised MMD. <ref type="bibr" target="#b31">[32]</ref> utilized an asymmetric tri-training strategy to learn discriminative representations for the target domain. <ref type="bibr" target="#b43">[44]</ref> iteratively selected pseudo-labeled target samples based on the classifier from the previous training epoch and re-trained the model by using the enlarged training set. <ref type="bibr" target="#b40">[41]</ref> proposed to assign pseudo-labels to all target samples and utilize them to achieve semantic alignment across domains. However, these approaches highly relied on the hypothesis that correctly-pseudo-labeled samples can reduce the bias caused by falsely-pseudo-labeled samples. They do not explicitly alleviate those falsely-pseudo-labeled samples. When the falsely-pseudo-labeled samples take the prominent position, their performances will be limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Progressive Feature Alignment Network</head><p>In this section, we first provide the details of the proposed PFAN and then theoretically investigate the effectiveness of our approach. The overall architecture of PFAN is depicted in <ref type="figure">Fig. 2</ref>, which consists of three components, EHTS, APA, and the soft-max function with a temperature variate. EHTS provides reliable pseudo-labeld samples <ref type="figure">Figure 2</ref>: The overall structure of the proposed PFAN. We separate the network into three modules: feature extractor G, label predictor F , domain discriminator D and with associated parameters θ g , θ f , θ d . Left: The easy-to-hard strategy (ETHS). Right: The network structure: the dotted lines in PFAN denote weight-sharing. from easy to hard by iterations and APA explicitly enforces the cross-domain category alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task Formulation</head><p>In UDA, we are given a source domain <ref type="bibr" target="#b27">[28]</ref>. The source and target domains are drawn from the joint probability distributions P (X s , Y s ) and Q(X t , Y t ) respectively, and P = Q. We assume that the source and target domains contain the same object classes, and we consider C classes in all.</p><formula xml:id="formula_0">D s = {(x s i , y s i )} ns i=1 (x s i ∈ X s , y s i ∈ Y s ) of n s labeled sam- ples and given a target domain D t = {x t j } nt j=1 (x j t ∈ X t ) of n t unlabeled samples</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Easy-to-Hard Transfer Strategy</head><p>The EHTS is biased to favor easier samples and this bias helps to avoid including the hard samples which are more likely to be given false pseudo-labels. In our approach, the easy samples are increasing progressively. Thus the "hard" samples will potentially be selected in further steps. The selected pseudo-labeled samples by EHTS can be used to align with their corresponding source categories as described in Section 3.3.</p><p>The EHTS first computes a D-dimensional prototype c S k ∈ R D of each class in the source domain. The source prototype is a mean vector of the embedded source samples in each class through an embedding function G (i.e. the feature extractor in <ref type="figure">Fig. 2</ref>) with trainable parameters θ g ,</p><formula xml:id="formula_1">c S k = 1 N k s (x s i ,y s i )∈D k s G(x s i ),<label>(1)</label></formula><p>where D k s denotes the set of samples labeled with class k in the source domain and N k s is the number of corresponding samples. Then, a set of prototypes {c S k } C k=1 are obtained. The embedded target samples are supposed to gather around the source prototypes in the latent feature space. Thus, we use a similarity measurement ψ to cluster j-th unlabeled target sample, x t j , to the corresponding source prototypes, where ψ is computed as follows,</p><formula xml:id="formula_2">ψ(x t j ) = CS(G(x t j ), c S k ), k = {1, 2, ..., C}<label>(2)</label></formula><p>where CS(., .) denotes the cosine similarity function between two vectors. x t j is added into the target domain of the class</p><formula xml:id="formula_3">D k t with a pseudo-labelŷ t j = k where k = arg max k ψ k (x t j ). Then, the unlabeled target samples D t are partitioned into C classes (i.e. D t = {D k t } C k=1 = {x t j ,ŷ t j } nt j=1</formula><p>) and each sample is scored by its similarity. To obtain the "easy" samples, we constrain that the similarity scores should above a certain threshold τ . During the training process, the values of the similarity ψ increase continuously because the source samples and the target samples become closer to each other in the hidden space as training proceeds. "Hard" samples in the earlier stages may be selected as "easy" in the later stages. However, the constant threshold will turn too much "hard" samples into "easy" samples in each step. To control the growth rate of the "easy" samples, we gradually adjust the threshold step by step as follows,</p><formula xml:id="formula_4">τ = 1 1 + e −µ·(m+1) − 0.01,<label>(3)</label></formula><p>where µ is a constant and m (m = {0, 1, 2, ...}) denotes the training step. Therefore, the sample selection function is formulated as follows,</p><formula xml:id="formula_5">∀xj ∈ D k t | C k=1 , wj = 1 if ψ ≥ τ 0 if ψ &lt; τ<label>(4)</label></formula><p>where w j = 1 indicates x j to be selected; otherwise, w j = 0 indicates x j not to be selected. Finally, we obtain a selected pseudo-labeled target domainD t = {x t j ,ŷ t j }n t j=1 , wheren t denotes the number of selected samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Prototype Alignment</head><p>In this section, we introduce the proposed APA, which considers the pairwise semantic similarity across domains to explicitly alleviate the negative influence of those falseeasy samples and enforce the cross-domain category consistency. It can be implemented by aligning the prototype of source and selected target samples for each category. We measure the distance between two prototypes as follows,</p><formula xml:id="formula_6">d(c S k , c T k ) = c S k − c T k 2 ,<label>(5)</label></formula><p>where c S k and c T k represent the source and target prototypes, respectively. We opt for the squared Euclidean distance as the distance measure function. The justification is that the cluster mean yields optimal cluster representatives when a Bregman divergence (e.g. squared Euclidean distance and Mahalanobis distance) is used <ref type="bibr" target="#b35">[36]</ref>. An optional approach for prototype alignment is to compute and align the local prototypes based on the mini-batch sampled from D s and D t at each iteration. However, this approach is in a position of weakness because the categorical information in each mini-batch is expected to be insufficient, even one falsely labeled sample in the target mini-batch may cause huge bias between the computed prototype and true prototype.</p><p>To overcome the aforementioned problems, we propose to adaptively align the global prototypes. The APA first computes the initial global prototypes based on the selected pseudo-labeled target samplesD t as follows,</p><formula xml:id="formula_7">c T k(0) = 1 D k t (x t j ,y t j )∈D k t G(x t j ).<label>(6)</label></formula><p>In each iteration, we compute a set of local prototypes {c t k } C k=1 using the mini-batch samples. The accumulated prototypes are computed as the average of all previous local prototypes in each iteration,</p><formula xml:id="formula_8">c t k(I) = 1 I I i=1 c t k (i) ,<label>(7)</label></formula><p>where I denotes the iteration times in the current training step. Then, the new c T k are updated as follows,</p><formula xml:id="formula_9">ρt = CS(c t k(I) , c T k(I−1) ), c T k(I) ← ρt 2 c t k(I) + (1 − ρt 2 )c T k(I−1) ,<label>(8)</label></formula><p>where CS(., .) is the cosine distance which was defined in Eq. (2) and ρ is the trade-off parameters. let c S k(I) be analogously updated for the source domain. To this end, the APA loss is formulated as follows,</p><formula xml:id="formula_10">Lapa(θg) = C k=1 d(c S k(I) , c T k(I) ).<label>(9)</label></formula><p>The motivations of APA is intuitive: 1) the accumulated prototypes are introduced to estimate the accumulated shift caused by the falsely labeled samples, and then we can use their similarity with the previous global prototypes to decide the new global prototypes c T k ; and 2) we statistically align the cross-domain category distributions which can alleviate the error accumulation of the pseudo-labels.</p><p>Algorithm 1 Progressive Feature Alignment Network, m = {0, 1, ...} denotes the training step, I denotes the iteration times, B s and B t denote the mini-batch training sets. </p><formula xml:id="formula_11">Require: labeled source samples D s = {(x s i , y s i )} ns i=1 , un- labeled target samples D t = {x t j } nt j=1 Ensure: θ g , θ d ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Losses</head><p>In this work, we empirically found that a good adaptor needs a non-saturated source classifier. This empirical result is supported by the theoretical analysis described in Section 3.5. The justification is that the adaptation model is biased towards minimizing the source classification loss, which usually converges rapidly since the available of the source true labels. However, this bias may lead the overfitting to the source samples and resulting in a limited target performance. Inspired by <ref type="bibr" target="#b14">[15]</ref>, we propose to add a high temperature variate T (T &gt; 1) to the source classifier (as depicted in <ref type="figure">Fig. 2</ref>). By that means we can retard the convergence speed of the source classification loss and effectively guides the adaptor to a better adaptation performance. We achieve this behavior via the following softmax function,</p><formula xml:id="formula_12">qi = exp(zi/T ) j exp(zj/T ) ,<label>(10)</label></formula><p>where q i denotes the class probabilities for a source samples and z is the logit that produced by source classifier. Using a higher value for T produces a softer output and naturally retards the convergence speed. Adversarial learning has been successfully introduced to UDA by extracting domain-invariant features to achieve domain alignment <ref type="bibr" target="#b8">[9]</ref>. However, the learned representations can not ensure category alignment, which is the main source of performance reduction. Therefore, our work simultaneously considers domain-level and category-level alignment.</p><p>In our PFAN, the input x is first embedded by G to a Ddimensional feature vector f ∈ R D , i.e. f = G(x; θ g ). In order to make f domain-invariant, the parameters θ g of feature extractor G are expected to be optimized by maximizing the loss of the domain discriminator D, while the parameters θ d of domain discriminator D are trained by minimizing the loss of the domain discriminator, the discriminator is optimized following a standard classification loss:</p><formula xml:id="formula_13">L d (θg, θ d ) = Ex∼D s [logD(G(x))] + E x∼D t [logD(1 − G(x))],<label>(11)</label></formula><p>In addition, we also need to simultaneously minimize the loss of the label predictor F for the labeled source samples and the APA loss. Formally, our ultimate goal is to optimize the following minimax objective:</p><formula xml:id="formula_14">min θg ,θ f max θ d ns i=1 Lc(F (G(x s i ; θg); θ f ), y s i ) +λL d (θg, θ d ) + γLapa(θg)<label>(12)</label></formula><p>where L c is the standard cross-entropy loss, λ and γ are weights that control the interaction among the source classification loss, the domain confusion loss and the APA loss. The pseudo-code of training PFAN is shown in Algorithm 1, the EHTS and APA work alternatively and iteratively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Theoretical Analysis</head><p>In this section, we theoretically show that our approach improves the boundary of the expected error on the target samples, making use of the theory of domain adaptation <ref type="bibr" target="#b0">[1]</ref>. Formally, let H be the hypothesis class and given two domains S and T , the probabilistic bound of the error of hypothesis h on the target domain is defined as,</p><formula xml:id="formula_15">∀h ∈ H, R T (h) ≤ R S (h) + 1 2 d H∆H (S, T ) + C (13)</formula><p>where the expected error on the target samples, R T (h), are bounded by three terms: (1) the expected error on the source domain, R S (h); (2) d H∆H (S, T ) is the domain divergence measured by a discrepancy distance between two distributions S and T w.r.t. a hypothesis set H; (3) the shared error of the ideal joint hypothesis, C.</p><p>In Inequality <ref type="formula" target="#formula_1">(13)</ref>, R S (h) is expected to be small and prone to be optimized by a deep network since we have source labels. On the other hand, prior efforts <ref type="bibr" target="#b8">[9]</ref> seeks to minimize d H∆H (S, T ) by the domain classifier-based adversarial learning. However, A small d H∆H (S, T ) and a small R S (h) do not guarantee small R T (h). It is possible that C tends to be large when the cross-domain category alignment is not be explicitly enforced (i.e. the marginal distribution is well aligned, but the class conditional distribution is not guaranteed). Therefore, C needs to be bounded as well. Unfortunately, we cannot directly measure C due to the absence of target true labels. Thus, we resort to the pseudo-labels to give the approximate evaluation and minimization.</p><p>Definition 1. If R T (·) denotes the expected risk on the selected pseudo-labeled target setD t , the ideal joint hypothesis is the hypothesis which minimizes the combined error</p><formula xml:id="formula_16">h * = arg min h∈H R S (h, f S ) + R T (h, f T ),</formula><p>and the combined error of the ideal hypothesis is</p><formula xml:id="formula_17">C = R S (h * , f S ) + R T (h * , f T ),<label>(14)</label></formula><p>where f S and f T are the labeling functions for the source and target domains, respectively.</p><p>To bound the combined error of the ideal hypothesis, the following inequality holds: Theorem 1. Let fT be the pseudo-labeling function. Given R T (f S , fT ) and R T (f T , fT ) as the minimum shared error and the degree to which the target samples are falsely labeled onD t , respectively. We have</p><formula xml:id="formula_18">C ≤ min h∈H RS (h, fS )+R T (h, fT )+2R T (fS , fT )+R T (fT , fT ).<label>(15)</label></formula><p>We show the derivation of Theorem 1 in the Supplementary Material. It is easy to respectively find a suitable h in H to approximate the f S and fT since we have the source labels and target pseudo-labels. However, we assume that when the category alignment has not been achieved, there exists an optimality gap between f S and fT <ref type="figure" target="#fig_2">(Fig. 3(a)</ref>). While most existing methods do not consider such phenomenon and directly minimizing R S (h, f S ), which leads the overfitting to source samples. <ref type="figure">h, fT )</ref>). The proposed softmax function with a temperature variate alleviates the overfitting to source samples (i.e. enforcing a non-saturated source classifier) by retarding the convergence speed of R S (h, f S ). This guides the adaptation model to a better target performance, i.e., a smaller R S (h, f S ) + R T (h, fT ). Note that when the cross-domain category distributions is well aligned, the aforementioned optimality gap is removed ( <ref type="figure" target="#fig_2">Fig. 3(b)</ref>).</p><formula xml:id="formula_19">Remark 1 (Minimizing R S (h, f S ) + R T (</formula><p>Recall that the labeling function f can be decomposed into the feature extractor G and label classifier F . By considering the 0-1 loss function σ for R T , we have</p><formula xml:id="formula_20">R T (f S , fT ) = E x∼T [σ(F S (G(x)), FT (G(x)))] = E x∼T [|σ(F S (G(x)), y 1 ) − σ(FT (G(x)), y 2 )|]<label>(16)</label></formula><p>where</p><formula xml:id="formula_21">|σ(F S (G(x)), y 1 ) − σ(FT (G(x)), y 2 )| = 1 if y 1 = y 2 0 if y 1 = y 2<label>(17)</label></formula><p>Remark 2 (Minimizing shared error). The proposed approach aims to progressively align feature in category-level, i.e., it aligns the kth class in source domain D k s with the same pseudo-labeled target classD k t . When the categories are aligned, it is safe to assume that y 1 = y 2 . Thus, R T (f S , fT ) is expected to be minimized. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Baselines</head><p>Office-31 <ref type="bibr" target="#b30">[31]</ref> is a popular benchmark for evaluation on domain adaptation. It contains 4110 images of 31 categories in total, which are collected from three domains, including Amazon (A) comprising 2817 images downloaded from online merchants, Webcam (W) involving 795 low resolution images acquired from webcams, and DSLR (D) containing 498 high resolution images of digital SLRs. We try all 6 combinations of two domains for evaluation.</p><p>ImageCLEF-DA <ref type="bibr" target="#b3">[4]</ref> originally used for the ImageCLEF 2014 domain adaptation challenge consists of twelve common classes from three domains: ImageNet ILSVRC 2012 (I), Pascal VOC 2012 (P), and Caltech-256 (C). Each doamin has 600 images in total and contains 50 images per class. We test 6 tasks by using all domain combinations.</p><p>MNIST <ref type="bibr" target="#b18">[19]</ref>, SVHN <ref type="bibr" target="#b25">[26]</ref> and USPS <ref type="bibr" target="#b6">[7]</ref> contain digital images of 10 classes. In particular, the images in MNIST and SVHN are grey, and are of size 28 × 28 and 16 × 16, respectively; USPS consists of color images of size 32 × 32, and there are often more than one digit in one image. Following previous works, we consider the three transfer tasks: MNIST→SVHN, SVHN→MNIST and MNIST→USPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Joining previous practices, we instantiate our backbone by AlexNet that has been pre-trained on ImageNet for Office-31 and ImageCLEF-DA, and employ the CNN architecture by <ref type="bibr" target="#b38">[39]</ref> for the digital datasets. As suggested by <ref type="bibr" target="#b24">[25]</ref>, we fine-tune the feature extractor G upon the backbone and train the predictor F from the scratch via back propagation. We utlize stochastic gradient descent (SGD) for the training with a momentum of 0.9 and a annealing learning rate (lr) given by lr p = lr0 (1+αp) β , where p is increased linearly from 0 to 1 as the training proceeds, lr 0 = 0.01, α = 10, and β = 0.75. In order to suppress noisy signal especially for the initial training steps, we use the similar schedule method as <ref type="bibr" target="#b8">[9]</ref> to adaptively change the values of λ and γ in Eq. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with State-of-the-Arts</head><p>State-of-the-arts. We compare our approach with various state-of-the-art UDA methods, including AlexNet <ref type="bibr" target="#b17">[18]</ref>, Deep Domain Confusion (DDC) <ref type="bibr" target="#b39">[40]</ref>, Deep Adaptation Network (DAN) <ref type="bibr" target="#b21">[22]</ref>, Residual Transfer Network (RTN) <ref type="bibr" target="#b23">[24]</ref> , Reverse Gradient (RevGrad) <ref type="bibr" target="#b8">[9]</ref>, Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b38">[39]</ref>, Joint Adaptation Networks (JAN) <ref type="bibr" target="#b24">[25]</ref>, Asymmetric Tri-Training (ATT) <ref type="bibr" target="#b31">[32]</ref> , Multi-Adversarial Domain Adaptation (MADA) <ref type="bibr" target="#b28">[29]</ref>, and Moving Semantic Transfer Network (MSTN) <ref type="bibr" target="#b40">[41]</ref>. For all above methods, we summarize the results reported in their original papers. For similarity, we term our method as PFAN hereafter. <ref type="table" target="#tab_0">Table 1</ref> displays the results on Office-31. The proposed PFAN outperforms all compared methods in general and improves the state-of-the-art result from 79.1% to 80.4% on average. If we focus more on the hard transfer tasks (e.g. A → W and A → D), PFAN substantially exhibits better transferring ability than others. In contrast to JAN, MADA and MSTN, our PFAN additionally considers both the target intra-class variation and the non-saturated source classifier. Our better performance over them could indicate the effectiveness of these two components. RevGrad has also taken the domain adversarial adaptation into account, but its results are still inferior to ours. The advantage of our model compared to RevGrad is that, we furhter perform EHTS and APA, which as supported by our experiments can explicitly enforce the cross-domain category alignment, hence delivering better performance.</p><p>The results of ImageCLEF-DA are reported in <ref type="table" target="#tab_1">Table 2</ref>. Our approach outperforms all comparison methods on most transfer tasks, which reveals that PFAN is scalable for different datasets.</p><p>The results of digit classification are reported in <ref type="table" target="#tab_2">Table 3</ref>. We follow the training protocol established in <ref type="bibr" target="#b38">[39]</ref>. For adaptation between MNIST and USPS, we randomly sample 2000 images from MNIST and 1800 from USPS. For   adaptation between SVHN and MNIST, we use the full training sets. For the hard transfer task MNIST→SVHN, we reproduced the MSTN <ref type="bibr" target="#b40">[41]</ref> but were unable to get it to converge, since the performance of this approach depends strongly on the accuracy of the pseudo-labeled samples which was lower on this task. In contrast, our approach significantly outperforms the suboptimal result by +4.8%, which clearly demonstrates the effect of our approach on selecting reliable pseudo-labeled samples and alleviating the negative influence of falsely-labeled samples on the challenging scenario. For the easier tasks SVHN→MNIST and MNIST→USPS, our approach also shows superiority.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Empirical Analysis</head><p>Ablation Study. To isolate the contribution of our work, we perform ablation study by evaluating several variants of PFAN: (1) PFAN (Random), which randomly selects the target samples instead of using the easy-to-hard order; (2) PFAN (Full), which uses all target samples at the training period; (3) PFAN (woAPA), which denotes training completely without the APA (i.e. γ = 0 in Eq. (12)); (4) PFAN (woA), which denotes aligning the prototypes based on the current mini-batch without considering the global and accumulated prototypes; (5) PFAN (woT), which removes the temperature from our model (i.e. T = 1 in Eq. (10)). The results are shown in <ref type="table" target="#tab_3">Table 4</ref>. We can observe that all the components are designed reasonably and when any one of these components is removed, the performance degrades. It is noteworthy that PFAN outperforms both PFAN (Random) and PFAN (Full), which reveals that the EHTS can provide more reliable and informative target samples for the crossdomain category alignment.</p><p>Pseudo-labeling Accuracy. We show the relationship between the pseudo-labeling accuracy and test accuracy in <ref type="figure" target="#fig_7">Fig. 5</ref>. We found that (1) the pseudo-labeling accuracy keeps higher and stable throughout as training proceeds, which thanks to the EHTS by selecting reliable pseudolabeled samples;</p><p>(2) the test accuracy increases with the increasing of labeled samples, which implies that the number of correctly and falsely labeled samples are both proportionally increasing, but our approach can explicitly alleviate the negative influence of the falsely-labeled samples.   Non-saturated source classifier. To further verify our hypothesis about the non-saturated source classifier, we investigate the source classification loss in different temperature setting. The results are reported in <ref type="figure" target="#fig_5">Fig. 4(a)</ref>. The T = 1 model converges faster than T = 1.8 especially at the beginning of training. However, such difference gradually decreases as training proceeds. The justification is that we use a higher T to retard the convergence speed of the source classification loss (i.e. alleviating the adaptor overfitting to the source samples), thus showing better adaptation. Distribution Discrepancy. The domain adaptation theory <ref type="bibr" target="#b0">[1]</ref> suggests that A-distance can be used as a measure of domain discrepancy. The way of estimating empirical Adistance was defined as d A = 2(1 − ), where is the generalization error of a classifier trained to discriminate the source and target features. We utilize a kernel SVM to estimate the A-distance. <ref type="figure" target="#fig_5">Fig. 4(b)</ref> demonstrates the A-distance calculated with the features from AlexNet, RevGrad and PFAN on tasks A → W and W → D. We can observe that our method significantly reduces the A-distance compared with the AlexNet. However, when compared with RevGrad, PFAN shows smaller improvement with respect to A-distance, but improves the performance by large margin, which demonstrates that a low domain divergence does not imply better performance in the target domain. This phenomenon is consistent with the analysis in Section 3.5.</p><p>Feature Visualization. We utilize t-SNE <ref type="bibr" target="#b7">[8]</ref> to visualize the deep feature of the network activations on task A → W (randomly selected 8 classes) learned by RevGrad (the bottleneck layer) and PFAN (the bottleneck layer). As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>(c)-4(d), we can see that the RevGrad features on target domain can not be discriminated very well, some categories have been mixed up in the feature space. By contrast, PFAN can learn more discriminative representations, which jointly enlarges the inter-class dispersion and reduces the intra-class variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel approach called Progressive Feature Alignment Network, to take advantage of target domain intra-class variance and cross-domain category consistency for addressing UDA problems. The proposed EHTS and APA complement each other in selecting reliable pseudo-labeled samples and alleviating the bias caused by the falsely-labeled samples. The performance is further improved by retarding the convergence speed of the source classification loss. The extensive experiments reveal that our approach outperforms state-of-the-art UDA approaches on three domain adaptation datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(Best viewed in color.) Motivations of the proposed work (SVHN→MNIST). The classification boundaries are first drawn by the fully labeled source domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>θ f 1 : m = 0 2 : 4 : 5 : while not converge do 6 :m = m + 1 7 : 8 : 9 :</head><label>12456789</label><figDesc>Stage-1: 3: Initialize G and F using D s , output: model 0 Stage-2: Run the EHTS based on model m−1 , output:D t Calculate the initial global prototypes c S k(0) and c T k(0) using D s andD t based on model m−1 for I = 1 to max iter do 10: Derive B s and B t sampled from D s andD t 11: Calculate local prototypes c s k(I) and c t k(I) 12: Update: c S k(I) , c T k(I) by using Eq. 7 and Eq. 8 13: Train model m fine-tuned from model m−1 using B s and B t by optimizing 12, output: model m 14: end for 15:D t = ∅ 16: end while</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Before category alignment: there exists an optimality gap. (b) After category alignment: the optimality gap does not exist any more.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Remark 3 (</head><label>3</label><figDesc>Minimizing the degree to which the target samples are falsely labeled onD t ). The proposed EHTS aims to select reliable pseudo-labeled samples in the target domain which minimizes R T (f T , fT ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(12) by computing λ = γ = 2 1+exp(−δp) − 1 with δ = 10. We set T = 1.8 in Eq. (10) and µ = 0.8 in Eq. (3) for all experiments. The batch size is selected as 128. The means and standard derivations of all results are obtained over 5 random runs. All experiments are implemented by the Caffe framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>(a) The convergence speed of the source classification loss in different temperature setting. (b) Distribution Discrepancy. (c)-(d) The t-SNE visualization of network activations on target domain W generated by RevGrad and PFAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of the pseudo-labeling accuracy and the test accuracy on transfer task A → W. The pseudolabeling accuracy is computed using (the number of correctly labeled samples)/(the number of labeled samples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>AlexNet-based approaches on Office-31 (%)</figDesc><table><row><cell>Method</cell><cell>A → W</cell><cell>D → W</cell><cell>W → D</cell><cell>A → D</cell><cell>D → A</cell><cell>W → A</cell><cell>Avg</cell></row><row><cell>AlexNet [18]</cell><cell>61.5±0.5</cell><cell>95.1±0.3</cell><cell>99.0±0.2</cell><cell>64.4±0.5</cell><cell>48.8±0.3</cell><cell>47.0±0.4</cell><cell>69.3</cell></row><row><cell>DDC [40]</cell><cell>61.8±0.4</cell><cell>95.0±0.5</cell><cell>98.5±0.4</cell><cell>64.4±0.3</cell><cell>52.1±0.6</cell><cell>52.2±0.4</cell><cell>70.6</cell></row><row><cell>DAN [22]</cell><cell>68.5±0.4</cell><cell>96.0±0.3</cell><cell>99.0±0.2</cell><cell>67.0±0.4</cell><cell>54.0±0.4</cell><cell>53.1±0.3</cell><cell>72.9</cell></row><row><cell>RTN [24]</cell><cell>73.3±0.3</cell><cell>96.8±0.2</cell><cell>99.6±0.1</cell><cell>71.0±0.2</cell><cell>50.5±0.3</cell><cell>51.0±0.1</cell><cell>73.7</cell></row><row><cell>RevGrad [9]</cell><cell>73.0±0.5</cell><cell>96.4±0.3</cell><cell>99.2±0.3</cell><cell>72.3±0.3</cell><cell>53.4±0.4</cell><cell>51.2±0.5</cell><cell>74.3</cell></row><row><cell>JAN [25]</cell><cell>74.9±0.3</cell><cell>96.6±0.2</cell><cell>99.5±0.2</cell><cell>71.8±0.2</cell><cell>58.3±0.3</cell><cell>55.0±0.4</cell><cell>76.0</cell></row><row><cell>MADA [29]</cell><cell>78.5±0.2</cell><cell>99.8±0.1</cell><cell>100.0±.0</cell><cell>74.1±0.1</cell><cell>56.0±0.2</cell><cell>54.5±0.3</cell><cell>77.1</cell></row><row><cell>MSTN [41]</cell><cell>80.5±0.4</cell><cell>96.9±0.1</cell><cell>99.9±0.1</cell><cell>74.5±0.4</cell><cell>62.5±0.4</cell><cell>60.0±0.6</cell><cell>79.1</cell></row><row><cell>PFAN</cell><cell>83.0±0.3</cell><cell>99.0±0.2</cell><cell>99.9±0.1</cell><cell>76.3±0.3</cell><cell>63.3±0.3</cell><cell>60.8±0.5</cell><cell>80.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="4">: AlexNet-based approaches on ImageCLEF-DA (%)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>I → P</cell><cell>P → I</cell><cell>I → C</cell><cell>C → I</cell><cell>C → P</cell><cell>P → C</cell><cell>Avg</cell></row><row><cell>AlexNet [18]</cell><cell>66.2±0.2</cell><cell>70.0±0.2</cell><cell>84.3±0.2</cell><cell>71.3±0.4</cell><cell>59.3±0.5</cell><cell>84.5±0.3</cell><cell>73.9</cell></row><row><cell>DAN [22]</cell><cell>67.3±0.2</cell><cell>80.5±0.3</cell><cell>87.7±0.3</cell><cell>76.0±0.3</cell><cell>61.6±0.3</cell><cell>88.4±0.2</cell><cell>76.9</cell></row><row><cell>RevGrad [9]</cell><cell>66.5±0.5</cell><cell>81.8±0.4</cell><cell>89.0±0.5</cell><cell>79.8±0.5</cell><cell>63.5±0.4</cell><cell>88.7±0.4</cell><cell>78.2</cell></row><row><cell>JAN [25]</cell><cell>67.2±0.5</cell><cell>82.8±0.4</cell><cell>91.3±0.5</cell><cell>80.0±0.5</cell><cell>63.5±0.4</cell><cell>91.0±0.4</cell><cell>79.3</cell></row><row><cell>MADA [29]</cell><cell>68.3±0.3</cell><cell>83.0±0.1</cell><cell>91.0±0.2</cell><cell>80.7±0.2</cell><cell>63.8±0.2</cell><cell>92.2±0.3</cell><cell>79.8</cell></row><row><cell>MSTN [41]</cell><cell>67.3±0.3</cell><cell>82.8±0.2</cell><cell>91.5±0.1</cell><cell>81.7±0.3</cell><cell>65.3±0.2</cell><cell>91.2±0.2</cell><cell>80.0</cell></row><row><cell>PFAN</cell><cell>68.5±0.5</cell><cell>84.4±0.4</cell><cell>92.2±0.6</cell><cell>82.3±0.4</cell><cell>66.3±0.3</cell><cell>91.7±0.2</cell><cell>80.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy on the digit classification task. (%)</figDesc><table><row><cell>Source</cell><cell>MNIST</cell><cell>SVHN</cell><cell>MNIST</cell></row><row><cell>Target</cell><cell>SVHN</cell><cell>MNIST</cell><cell>USPS</cell></row><row><cell>Source Only</cell><cell>33.0±1.2</cell><cell>60.1±1.1</cell><cell>75.2±1.6</cell></row><row><cell>RevGrad [9]</cell><cell>35.7</cell><cell>73.9</cell><cell>77.1±1.8</cell></row><row><cell>ADDA [39]</cell><cell>-</cell><cell>76.0±1.8</cell><cell>89.4±0.2</cell></row><row><cell>ATT [32]</cell><cell>52.8</cell><cell>85.0</cell><cell>-</cell></row><row><cell>MSTN [41]</cell><cell>did not converge</cell><cell>91.7±1.5</cell><cell>92.9±1.1</cell></row><row><cell>PFAN</cell><cell>57.6±1.8</cell><cell>93.9±0.8</cell><cell>95.0±1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation of PFAN on different transfer tasks. (%)</figDesc><table><row><cell>Model</cell><cell>A→W</cell><cell>I→P</cell><cell>SVHN→MNIST</cell></row><row><cell>Source Only</cell><cell>61.6</cell><cell>66.2</cell><cell>60.1</cell></row><row><cell>PFAN (Random)</cell><cell>77.0</cell><cell>67.0</cell><cell>87.2</cell></row><row><cell>PFAN (Full)</cell><cell>81.9</cell><cell>68.0</cell><cell>92.5</cell></row><row><cell>PFAN (woAPA)</cell><cell>76.4</cell><cell>67.1</cell><cell>82.0</cell></row><row><cell>PFAN (woA)</cell><cell>82.2</cell><cell>68.1</cell><cell>93.0</cell></row><row><cell>PFAN (woT)</cell><cell>80.6</cell><cell>67.9</cell><cell>92.1</cell></row><row><cell>PFAN</cell><cell>83.0</cell><cell>68.5</cell><cell>93.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A theory of learning from different domains. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation problems: A dasvm classification technique and a circular validation strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Marconcini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="770" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Overview and analysis of the results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Novi</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neda</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Suzanüsküdarlı</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="192" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cotraining for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2456" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Re-weighted adversarial adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Wassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7976" to="7985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural network recognizer for hand-written zip code digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John S Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="323" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation with conditional transferable components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2839" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Associative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Frerix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain invariant and class discriminative feature learning for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="4260" to="4273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Element</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8004" to="8013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02560</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning transferrable representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2110" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08735</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5419" to="5428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Importance weighted adversarial nets for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8156" to="8164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3801" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
