<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RGB-D Salient Object Detection with Cross-Modality Modulation and Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of ICE</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RGB-D Salient Object Detection with Cross-Modality Modulation and Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an effective method to progressively integrate and refine the cross-modality complementarities for RGB-D salient object detection (SOD). The proposed network mainly solves two challenging issues: 1) how to effectively integrate the complementary information from RGB image and its corresponding depth map, and 2) how to adaptively select more saliency-related features. First, we propose a cross-modality feature modulation (cmFM) module to enhance feature representations by taking the depth features as prior, which models the complementary relations of RGB-D data. Second, we propose an adaptive feature selection (AFS) module to select saliency-related features and suppress the inferior ones. The AFS module exploits multi-modality spatial feature fusion with the self-modality and cross-modality interdependencies of channel features are considered. Third, we employ a saliency-guided position-edge attention (sg-PEA) module to encourage our network to focus more on saliency-related regions. The above modules as a whole, called cmMS block, facilitates the refinement of saliency features in a coarse-to-fine fashion. Coupled with a bottom-up inference, the refined saliency features enable accurate and edge-preserving SOD. Extensive experiments demonstrate that our network outperforms stateof-the-art saliency detectors on six popular RGB-D SOD benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Depth maps provide useful cues such as depth of field, shape, and boundary to complement RGB images for SOD <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref>. However, depth maps are inherently noisy and the cues provided can be inconsistent or misaligned with the RGB modality. The issues make designing an RGB-D algorithm challenging. Contemporary RGB-D SOD detectors, CPFP <ref type="bibr" target="#b44">[45]</ref>  <ref type="figure" target="#fig_0">(Fig. 1(d)</ref>) and A2dele <ref type="bibr" target="#b32">[33]</ref> ( <ref type="figure" target="#fig_0">Fig. 1(e)</ref>), could still miss salient objects due to cluttered backgrounds or yield incomplete or serrated boundaries of saliency maps.  <ref type="bibr" target="#b44">[45]</ref> and A2dele (CVPR'20) <ref type="bibr" target="#b32">[33]</ref>, respectively. (f) are our results. Compared with the latest CPFP and A2dele, our method can yield more complete, sharp, and edge-preserving saliency detection results by effectively intergrating crossmodality complementaries and adaptively selecting saliency-related features.</p><p>In this work, we consider addressing the aforementioned problem through more careful investigation on the integration of cross-modality complementaries from RGB image and depth map as well as the selection of saliency-related features. To this end, we present an effective network that achieves complete, sharp, and edge-preserving saliency detection, as shown in <ref type="figure" target="#fig_0">Fig. 1(f)</ref>.</p><p>First, we propose a cross-modality feature modulation (cmFM) module that enhances RGB feature representations by taking the corresponding depth features as prior. This is in contrast to popular strategies that perform either input fusion <ref type="bibr" target="#b29">[30]</ref>, early fusion <ref type="bibr" target="#b18">[19]</ref>, or late fusion <ref type="bibr" target="#b17">[18]</ref>, that crudely concatenate or add the multi-modality information. The proposed modulation design enables effective integration of multi-modality information through feature transformation, distinctly models the inseparable cross-modality relations, and reduces the interference caused by the inherent inconsistency of multi-modality data.</p><p>Second, we devise an adaptive feature selection (AFS) module that highlights the importance of different channel features in self-and cross-modalities, while fusing multi-modality spatial features in a gated manner. This is different from previous RGB-D SOD algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b44">45]</ref> that treat channel features from different modalities equally and independently. Relaxing such assumptions allows our method to adaptively select more saliency-related features and suppress the inferior ones from both spatial features and channel features. It also mitigates the negative influence of poorly captured depth maps. Hence, our network equips additional flexibility in dealing with different information. We also emphasize the saliency-related positions and edges by introducing a saliency-guided positionedge attention (sg-PEA) module, which collects its attention weights from the predicted saliency maps and saliency edge maps.</p><p>Our method is unique in that the feature modulation and attention mechanism are closely coupled in a coarse-to-fine manner. Specifically, fusion is first performed by the cmFM module to provide rich features representations. Coordinated with our AFS module, saliency-related features are emphasized while redundant features are suppressed. The saliency-related features are further refined by the sg-PEA module. A careful design to place the cmFM, AFS, and sg-PEA modules allows the cross-modality complementarities to go through modulation, selection, and refinement in a coarse-to-fine fashion, providing our network with precise saliency features. Coupled with a bottom-up inference, the precise saliency features enable us to perform more accurate and robust SOD. Contributions. We present an effective approach for RGB-D SOD. Crossmodality complementarities are effectively integrated and saliency-related features are adaptively selected. This is made possible by designing a coarse-to-fine fusion that consists of 1) a cross-modality feature modulation module that enhances RGB feature representations by taking the corresponding depth features as prior, and 2) an adaptive feature selection module that progressively emphasizes the importance of channel features in self-and cross-modalities while fusing the significant multi-modality spatial features. Our method consistently outperforms state-of-the-art SOD methods on six popular RGB-D SOD benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Salient Object Detection. SOD methods range from bottom-up <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">41]</ref> to top-down models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46]</ref>. In addition to the color appearance, depth maps can provide useful cues such as depth of field, shape, and boundary. The depth map is implicitly used in the unsupervised methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47]</ref>. Whereas for the supervised methods, the discriminative and complementary features are learned from RGB-D images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>. Our work differs from recent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, mainly in two aspects: 1) we use depth features as prior to learn optimal affine transformation parameters, which can flexibly modulate multi-level RGB features, and 2) we consider both self-modality and cross-modality channel features as well as multi-modality spatial features, thus effectively capturing relations among different modalities. Feature Modulation. Inspired by FiLM <ref type="bibr" target="#b30">[31]</ref> that first applies linear feature modulation for visual reasoning, feature modulation has been used in few-shot learning <ref type="bibr" target="#b27">[28]</ref> and image super-resolution <ref type="bibr" target="#b38">[39]</ref>. In our studies, we modulate the multi-level feature representations conditioned on the corresponding depth features. Besides, we design the cross-modality feature modulation in a pixel-wise manner, which provides elaborate and fine-grained control to the features. Attention Mechanism. Attention mechanism is increasingly applied in diverse forms such as spatial attention <ref type="bibr" target="#b6">[7]</ref>, dual-attention <ref type="bibr" target="#b14">[15]</ref>, self-attention <ref type="bibr" target="#b37">[38]</ref>, multi-level attention <ref type="bibr" target="#b39">[40]</ref>, and channel attention <ref type="bibr" target="#b43">[44]</ref>. In contrast, we employ the attention mechanism in our adaptive feature selection module, which explores the interdependencies of channel features in the self-and cross-modalities while fusing the significant multi-modality spatial features in a gated manner. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>We first present an overview of our network architecture. Then, we describe the key components including the cross-modality feature modulation module, adaptive feature selection module, and saliency-guided position-edge attention module. At last, we introduce the loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of Network Architecture</head><p>The overview of our network architecture is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. After the topdown features extraction from VGG-16 backbone <ref type="bibr" target="#b35">[36]</ref>, the multi-level RGB features and depth features are fed to a convolutional layer for halving the number of feature maps, respectively. Then, the dimension reduced RGB-D features are forwarded to the corresponding cmMS block. In each cmMS block, the RGB-D features go through cmFM module, AFS module, and sg-PEA module for feature modulation, selection, and refinement, respectively. Specifically, we introduce modulated features by using our proposed cross-modality feature modulation (cmFM) module. The purpose of cmFM module is to effectively integrate the cross-modality complementarities in a flexible and trainable fashion. After that, RGB features, depth features, modulated features, and up-sampled features from the higher level (if any) are independently forwarded to our proposed adaptive feature selection (AFS) module for selectively emphasizing the informative channel features and fusing the significant spatial features. The AFS module models the relations between different levels and accelerates task-oriented feature integration. Meanwhile, the concatenation of RGB features, depth features, modulated features, and up-sampled features (if any) is applied to predict the saliency edge map via a saliency edge prediction (E-Pre) unit. Then, with the saliency map up-sampled from the higher level (if any) and saliency edge map, we highlight the saliency position and edge regions of the features after the AFS module. After that, we predict the saliency map in the current level via a saliency map prediction (S-Pre) unit by using the refined features. At last, in the bottom-up inference, we progressively integrate and highlight multi-level features to predict the fine-scaled saliency map (i.e., the Smap 1 in <ref type="figure" target="#fig_1">Fig. 2</ref>). We adopt 3×3 kernels for all convolutional layers in our network, except the cmFM module that employs the multi-scale convolutions to enlarge receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-modality Feature Modulation (cmFM)</head><p>Inspired by the unsupervised RGB-D SOD algorithms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref> which take the depth map as prior information to enrich the saliency cues, we propose a cmFM module conditioned on the depth features. The cmFM module learns pixel-wise affine transformation parameters from the conditioning depth features then modulates the corresponding RGB feature representations in each level of our network. The detailed cmFM module is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Features Modulated Features</head><p>Feature Extractor</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth Map</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Image</head><p>Cross-modality Feature Modulation (cmFM)  Given the dimension halved RGB features F rgb L ∈ R N ×H×W and depth features F depth L ∈ R N ×H×W , the cmFM module learns a mapping function M conditioned on the depth features to yield a set of affine transformation parameters (γ L ,β L ) ∈ R N ×H×W . Here, N is the number of feature maps; H and W are the height and width of the feature maps, respectively. It can be expressed as:</p><formula xml:id="formula_0">(γ L , β L ) = M(F depth L ),<label>(1)</label></formula><p>where the superscript indicates the modality while the subscript represents the level. The mapping function M is built on two parallel stacked convolutional layers as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. With the estimated affine transformation parameters (γ L ,β L ), we conduct pixel-wise scaling and shifting on the RGB feature representations, which can be expressed as:</p><formula xml:id="formula_1">F mod L = F rgb L ⊗ γ L ⊕ β L ,<label>(2)</label></formula><p>where F mod L represent the modulated features; ⊗ and ⊕ indicate the elementwise multiplication and element-wise addition, respectively. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the cluttered backgrounds of RGB features become clear and the salient object is highlighted with the modulation of depth features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive Feature Selection (AFS)</head><p>To make our network focus more on informative features, we propose an AFS module to progressively re-scale channel-wise features. Simultaneously, the AFS module fuses significant spatial features of multi-modalities. To be specific, we first explore the interdependencies of channel features in the self-modality, then further determine the relevance in the cross-modality. After squeezing by a convolutional layer that reduces the redundant features, we achieve the channel attention-on-channel attention features. Such a self-modality and cross-modality channel attention mechanism can model relations of the channel features among different modalities well and adaptively select the informative channel features. The advantages of our channel attention-on-channel attention than the conventional channel attention are verified in the ablation studies.</p><p>We simultaneously fuse the multi-modality features to achieve the enhanced feature representations based on a gated spatial fusion mechanism, where the pixel-wise confidence map for each input feature is calculated. In this way, the significant multi-modality spatial features are preserved. As a result, we achieve saliency-related features and filter out irrelevant or misleading features from both spatial and channel aspects. The detail of AFS module is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>Given the features (F rgb L , F depth L , F mod L , Fs up L+1 ), we first perform global average pooling on each set of features separately, leading to a channel descriptor z ∈ R N ×1 for each one, which is an embedded global distribution of channel-wise feature responses. Fs up L+1 indicate 2× up-sampled features from the L+1 level by using the 'Up' block that consists of one 2× linear interpolation followed by two convolutional layers, where each convolutional layer is followed by the ReLU activation and outputs n feature maps. The k-th entry of z is expressed as:  where k ∈ [1, N ]. Then, a self-gating mechanism is used to fully capture channelwise dependencies s ∈ R N ×1 :</p><formula xml:id="formula_2">z k = 1 H × W H i W j F k (i, j),<label>(3)</label></formula><formula xml:id="formula_3">s = σ(W 2 * (δ(W 1 * z))),<label>(4)</label></formula><p>where σ(·) represents the Sigmoid activation, δ(·) represents the ReLU activation, * denotes the convolution operation, and W 1 and W 2 are the weights of two fully-connected layers with their numbers of output channels being N 16 and N , respectively. At last, these weights are applied to each set of input features F to generate re-scaled features U ∈ R N ×H×W : U = F ⊗ s. This processing is mathematically expressed as an SE mapping function in this paper and can also be implemented by the squeeze-and-excitation network <ref type="bibr" target="#b19">[20]</ref>. However, the highlighted channel features may become relatively useless among all channel attention results from multi-modalities.</p><p>To emphasize the informative channel features, we first halve the number of feature maps in each channel attention result by a convolutional layer, then concatenate them:</p><formula xml:id="formula_4">V L = Cat{U rgb L , U depth L , U mod L , Us up L+1 }.</formula><p>After that, we further explore the interdependencies of channel features by Y L = SE(V L ). We finally squeeze the number of channel features by a convolutional layer and achieve the results of channel attention-on-channel attention Y caca L . Meanwhile, we fuse the multi-modality input features to achieve enhanced spatial feature representations. First, the input features are concatenated F cat L = Cat{F rgb L , F depth L , F mod L , Fs up L+1 }, and fed to a plain CNN network (indicated as G) to estimate their pixel-wise confidence maps:</p><formula xml:id="formula_5">(C rgb L , C depth L , C mod L , C up L+1 ) = G(F cat L ),<label>(5)</label></formula><p>where C rgb L , C depth L , C mod L , and C up L+1 ∈ R N ×H×W represent the confidence maps. The G is built on six stacked convolutional layers as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. The achieved features in the level L can be expressed as: Then, we pass these features to a convolutional layer and achieve the gated fusion features F gated L . At last, we combine the enhanced spatial feature representations with the enhanced channel feature representations by:</p><formula xml:id="formula_6">F gated L = F rgb L ⊗ C rgb L ⊕ F depth L ⊗ C depth L ⊕ F mod L ⊗ C mod L ⊕ Fs up L+1 ⊗ C up L+1 (6) Depth Map RGB Image GT</formula><formula xml:id="formula_7">F AF S L = Cat{F gated L , Y caca L },<label>(7)</label></formula><p>where the final results F AF S L enjoy the most informative features towards saliency detection, called saliency-related features in this paper. The visual examples are presented in <ref type="figure" target="#fig_4">Fig. 5</ref>. As shown, the saliency-related spatial features and channel features are preserved and highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Saliency-Guided Position-Edge Attention (sg-PEA)</head><p>After selecting the saliency-related features, we also encourage the network to focus on those positions and edges most essential to the nature of salient objects. The benefits are illustrated as follows: 1) the saliency position attention can better locate the salient objects and accelerate the network convergence, and 2) the saliency edge attention can alleviate the problem of edge blur caused by the repeated pooling operations, which is vital for the pixel-wise saliency prediction.</p><p>To the end, we propose a saliency-guided position-edge attention (sg-PEA) module to locate and sharpen salient objects. The sg-PEA module further includes a saliency map prediction (S-Pre) unit and a saliency edge prediction (E-Pre) unit as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The details are provided in <ref type="figure" target="#fig_5">Fig. 6</ref>, where S-Pre unit and E-Pre unit share the same structure, but different weights. Position Attention. We employ the up-sampled saliency map from the higher level as the attention weights. Here, the up-sampling is implemented by the simple 2× linear interpolation. In our method, the saliency map is predicted by the S-Pre unit in each level in a supervised learning manner. The benefits of such a side supervision manner lie in four aspects: 1) the convolutional layers in each level have explicit objective towards saliency detection, 2) the side supervision can accelerate gradient back-propagation, 3) the predicted saliency map works as a guidance and can steer the convolutional layers of lower level to focus more on saliency positions in a low-computational manner, and 4) the multiple side outputs can provide diverse choices based on accuracy and inference speed. We provide more analysis on the side outputs in the supplementary material. </p><formula xml:id="formula_8">Fs L = F poa L ⊕ F poa L ⊗ Sedge L ,<label>(9)</label></formula><p>where Sedge L is the predicted saliency edge map in the level L. We call Fs L as the refined features. At last, with the refined features, the final result (i.e., Smap 1 ) with the same size as the input RGB image can be achieved in a bottomup manner. In <ref type="figure" target="#fig_5">Fig. 6</ref>, we present the changes of features before and after sg-PEA module. As shown, the features increasingly focus on the saliency position and edge details, while the cluttered backgrounds are concurrently reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Function</head><p>We employ the standard cross-entropy (SCE) loss <ref type="bibr" target="#b0">[1]</ref> to jointly optimize our network for the saliency prediction and saliency edge prediction:</p><formula xml:id="formula_9">Loss = L i=1 (λ i SCE SP re i + η i SCE EP re i ),<label>(10)</label></formula><p>where L indicates the level, SCE SP re i and SCE EP re i represent the losses for predicting the saliency map and saliency edge map in the level i, respectively. λ and η are the corresponding weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmark Datasets and Evaluation Metrics</head><p>We conduct experiments on six popular RGB-D SOD datasets, including NJUD <ref type="bibr" target="#b20">[21]</ref> (1985 RGB-D images), NLPR <ref type="bibr" target="#b29">[30]</ref> (1000 RGB-D images), STEREO <ref type="bibr" target="#b26">[27]</ref> (797 RGB-D images), LFSD <ref type="bibr" target="#b23">[24]</ref> (100 RGB-D images), SSD <ref type="bibr" target="#b22">[23]</ref> (80 RGB-D images), and DUT <ref type="bibr" target="#b31">[32]</ref> (1200 RGB-D images). For quantitative evaluations, Precision-Recall (P-R) curve, F-measure <ref type="bibr" target="#b1">[2]</ref>, MAE score <ref type="bibr" target="#b7">[8]</ref>, and S-measure <ref type="bibr" target="#b10">[11]</ref> are employed. P-R curve depicts the different combinations of precision and recall scores; the closer the P-R curve is to <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b0">1)</ref>, the better the performance of the method. F-measure is the weighted harmonic mean of precision and recall; it is a comprehensive measurement, with a larger value indicating a better performance. MAE score measures the difference between the continuous saliency map and ground truth; a smaller value indicates a smaller gap hence better. S-measure calculates the structural similarity between the saliency map and ground truth; a larger value indicates a better performance. Additionally, we compare the model sizes of different methods in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We adopt the same training, validation, and testing sets as described in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. The ground truth of saliency edge map prediction is obtained by using the Canny edge detector on the saliency mask. We implement our network with TensorFlow on a PC with an Nvidia Tesla V100 GPU. During training, the batch size is set to 4, the filter weights of each layer are initialized by Gaussian distribution, and the bias is initialized as a constant. We use ADAM and fix the learning rate to 1e −4 . The weight λ 1 for predicting the final saliency map is set to 1.2 while other weights are set to 1 in Eq. <ref type="bibr" target="#b9">(10)</ref>. For a pair of RGB-D images of size 224×224, the average runtime of our method is 0.037s on the aforementioned PC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with State-of-the-art Methods</head><p>We compare our method with 12 state-of-the-art learning-based SOD methods, including two latest RGB-induced SOD methods (i.e., PoolNet <ref type="bibr" target="#b25">[26]</ref> and EGNet <ref type="bibr" target="#b45">[46]</ref>), and ten RGB-D SOD methods (i.e., DF <ref type="bibr" target="#b34">[35]</ref>, CTMF <ref type="bibr" target="#b17">[18]</ref>, MMCI <ref type="bibr" target="#b5">[6]</ref>, PCFN <ref type="bibr" target="#b2">[3]</ref>, TAN <ref type="bibr" target="#b3">[4]</ref>, CPFP <ref type="bibr" target="#b44">[45]</ref>, DCFF <ref type="bibr" target="#b4">[5]</ref>, DMRA <ref type="bibr" target="#b31">[32]</ref>, ASIF-Net <ref type="bibr" target="#b21">[22]</ref>, and A2dele <ref type="bibr" target="#b32">[33]</ref>).</p><p>Visual comparisons are shown in <ref type="figure" target="#fig_6">Fig. 7</ref>. Our method achieves more competitive performance than the compared methods. First, the salient objects in our results are more complete and accurate, and the object boundaries are sharper. In the first image, only our method can accurately and completely detect the    salient toy in front, while the competing methods incorrectly reserve the background regions (e.g., Android doll and checkerboard). In the fourth image that comes with an unsatisfactory depth map, our method can still accurately locate salient target with a complete structure and clear boundaries. Second, our method preserves more details in the saliency result. In the sixth image, more <ref type="table">Table 1</ref>: Quantitative comparisons on six testing datasets. The bold numbers are performance of our method, also the best across all datasets  details of plant leaves are better conserved. Third, our method can address some challenging cases, such as a complex background and small object. In the third image, the cat dolls in the back row are successfully suppressed by our method, the detected salient boundaries are sharper, and the structure is more complete.</p><formula xml:id="formula_10">STEREO Dataset NLPR-Test Dataset NJUD-Test Dataset F β ↑ MAE ↓ Sm ↑ F β ↑ MAE ↓ Sm ↑ F β ↑ MAE ↓ Sm ↑ PoolNet</formula><formula xml:id="formula_11">.9040 LFSD Dataset SSD Dataset DUT-Test Dataset F β ↑ MAE ↓ Sm ↑ F β ↑ MAE ↓ Sm ↑ F β ↑ MAE ↓ Sm ↑ PoolNet</formula><p>In the fifth image illustrating a case of complex background, our method can still completely detect a small salient object (i.e., the human).</p><p>The P-R curves of different methods are shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. Our method (i.e., the red solid line) achieves the highest precision compared to other methods on all datasets. The numerical results are reported in <ref type="table">Table 1</ref>  gain reaches 2.0% for F-measure, 14.1% for MAE score, and 2.5% for S-measure. All these measures demonstrate the superiority and effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>To verify the impact of our key modules, we conduct experiments on the STEREO dataset and DUT-Test dataset. The quantitative results are shown in <ref type="table" target="#tab_4">Table 2</ref>. An example of visual comparison is illustrated in <ref type="figure" target="#fig_11">Fig. 9</ref>. Cross-Modality Feature Modulation (cmFM). We compare three variants: w/o cmFM, w/ cmFA, and w/ cmFC. In <ref type="figure" target="#fig_11">Fig. 9</ref>, the baseline w/o cmFM cannot effectively detect the salient object while the baselines w/ cmFA and w/ cmFC achieve the similar detection result. The same quantitative trend also reflects in <ref type="table" target="#tab_4">Table 2</ref>. Compared with the full model, the results indicate that the proposed cmFM module is important for improving the SOD performance. Besides, the simple addition and concatenation can only boost a little performance. Adaptive Feature Selection (AFS). We compare with four baselines: w/o AFS, w/o GFF, w/o CACA, and w/ CA. Observing <ref type="figure" target="#fig_11">Fig. 9</ref> and <ref type="table" target="#tab_4">Table 2</ref>, we found that the performance of the baseline w/o AFS is obviously worse than the baselines w/o GFF, w/o CACA, and w/ CA. The visual results reflect that the baseline w/o GFF produces incomplete salient object while the baseline w/o CACA yields the result with an unclear boundary. Collectively, these results un-  Saliency-guided Position-Edge Attention (sg-PEA). We compare with three baselines: w/o PEA, w/o PA, and w/o EA. In <ref type="figure" target="#fig_11">Fig. 9</ref>, the baseline w/o PEA fails to highlight the position and edge of salient object. The baseline w/o PA has a sharper boundary of partial complete object while the baseline w/o PE shows a more complete object but unclear boundary. In contrast, our full model achieves better performance than these three baselines as presented in <ref type="table" target="#tab_4">Table 2</ref>. In summary, the ablation studies demonstrate the effectiveness and advantages of the proposed three modules qualitatively and quantitatively. In addition, the ablation studies also demonstrate that careful feature modulation, selection, and refinement can effectively improve the performance of RGB-D SOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose an RGB-D SOD network equipped with cross-modality feature modulation and adaptive feature selection. The former effectively integrates the multi-modality complementarities while the latter adaptively highlights saliencyrelated features. We demonstrate that both elaborate integration of cross-modality features and adaptive selection of multi-modality spatial and channel features can boost the performance of SOD. Experiment results also demonstrate that our method achieves new state-of-the-art performance on six benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Two motivating examples of SOD. (a)-(c) represent the input images, the corresponding depth maps, and the ground truth (GT), respectively. (d) and (e) are the results of state-of-the-art RGB-D SOD detectors CPFP (CVPR'19)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of our network architecture. The inputs are the RGB image and its depth map. The cmMS block consists of a cmFM module, an AFS module, and an sg-PEA module. Here, the sg-PEA module further contains an S-Pre unit and an E-Pre unit. 'Conv n' represents the convolutional layer that outputs n feature maps, where n is the half number of input feature maps. 'A', 'M', and 'C' represent element-wise addition, element-wise multiplication, and concatenation along with the channel dimension, respectively. 'Up' represents the up-sampling block. Pink line indicates 2× linear interpolation. Fs represent the refined features after the cmMS block while Fs up are the up-sampled Fs by the 'Up' block. In this figure, each convolutional layer is followed by the ReLU activation. Our network finally produces five saliency maps (Smap L ) and five saliency edge maps (Sedge L ) with the resolutions, ranging from 14×14 to 224×224 by a scale of 2. L indicates the level. We treat Smap 1 as the final result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The proposed cmFM module. For the estimation of both γ and β, the kernels of convolutional layers are 7×7, 5×5, 3×3, and 3×3. The feature extractor represents VGG-16 backbone. The feature maps are illustrated as heatmaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The detail of AFS module. 'Cat' represents the concatenation operation. 'SE-Net' is the squeeze-and-excitation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Visual results of the intermediate features in our AFS module. 'CA-on-CA Features' indicates the features after our channel selection while 'Gated Fusion Features' represents the features after our spatial selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Visual results of sg-PEA module. Left panel shows the structure of S-Pre/E-Pre unit, and the predicted saliency maps and saliency edge maps in different levels. Right panel shows the intermediate features before and after the sg-PEA module. After the sg-PEA module, the background of features are suppressed, and the edge and position details are assigned more focuses. To be specific, with the saliency-related features F AF S L and the up-sampled saliency map Smap up L+1 , the position attention results F poa L can be expressed as: treating all positions of saliency features equally, the position attention can quickly and efficiently employ the saliency property of higher level and enhance the saliency representations of the current level. To avoid gradient diffusion induced by successive attention (the values of feature maps are close to zero), we adopt an identical mapping manner as shown in Eq. (8).Edge Attention. To obtain the edge attention weights, we first concatenate the RGB-D features, the modulated features, and up-sampled features, then forward them to the E-Pre unit to predict the saliency edge map in each level. The saliency edge maps, also estimated by supervised learning, can be used to emphasize the salient edges of the features by simple element-wise multiplication. For level L, the output features of edge attention can be expressed as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Visual examples of different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>P-R curves of different methods on the testing datasets. (a)-(f) correspond to STEREO, NLPR-Test, NJUD-Test, LFSD, SSD, and DUT-Test datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc><ref type="bibr" target="#b25">[26]</ref> 0.8757 0.0655 0.8359 0.8627 0.0448 0.8573 0.8740 0.0676 0.8600 EGNet<ref type="bibr" target="#b45">[46]</ref> 0.8717 0.0671 0.8363 0.8452 0.0504 0.8497 0.8667 0.0704 0.8562 DF [35] 0.6961 0.1738 0.6279 0.6480 0.1079 0.6710 0.6355 0.1987 0.5930 CTMF [18] 0.8265 0.1023 0.8230 0.8407 0.0561 0.8549 0.8572 0.0847 0.8493 PCFN [3] 0.8838 0.0606 0.8722 0.8635 0.0437 0.8592 0.8875 0.0592 0.8768 MMCI [6] 0.8610 0.0796 0.8504 0.8412 0.0591 0.8524 0.8684 0.0789 0.8588 TAN [4] 0.8865 0.0591 0.8701 0.8765 0.0410 0.8736 0.8882 0.0605 0.8785 CPFP [45] 0.8856 0.0537 0.8702 0.8878 0.0359 0.8760 0.7994 0.0794 0.7984 DCFF [5] 0.8867 0.0638 0.8706 0.8779 0.0439 0.8695 0.8910 0.0646 0.8774 DMRA [32] 0.8953 0.0474 0.8778 0.8870 0.0339 0.8646 0.9003 0.0529 0.8804 ASIF-Net [22] 0.8939 0.0493 0.8686 0.9002 0.0298 0.8844 0.9007 0.0471 0.8887 A2dele [33] 0.8997 0.0431 0.8713 0.8976 0.0285 0.8770 0.8939 0.0510 0.8704 ours 0.9084 0.0422 0.8895 0.9137 0.0273 0.8999 0.9149 0.0442 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc><ref type="bibr" target="#b25">[26]</ref> 0.8474 0.0945 0.8217 0.7644 0.1099 0.7491 0.8828 0.0669 0.8392 EGNet<ref type="bibr" target="#b45">[46]</ref> 0.8445 0.0871 0.8300 0.7040 0.1351 0.7072 0.8876 0.0641 0.8439 DF [35] 0.8534 0.1424 0.7791 0.7631 0.1511 0.7422 0.7747 0.1455 0.7051 CTMF [18] 0.8147 0.1202 0.7883 0.7550 0.1003 0.7757 0.8417 0.0971 0.8226 PCFN [3] 0.8290 0.1118 0.7919 0.8447 0.0627 0.8427 0.8094 0.0999 0.7878 MMCI [6] 0.8128 0.1318 0.7793 0.8230 0.0820 0.8133 0.8044 0.1125 0.7818 TAN [4] 0.8275 0.1108 0.7935 0.8350 0.0629 0.8393 0.8236 0.0926 0.7948 CPFP [45] 0.8495 0.0881 0.8200 0.8014 0.0818 0.8067 0.7866 0.0995 0.7335 DCFF [5] 0.8220 0.1191 0.7917 0.8388 0.0769 0.8316 0.8141 0.1014 0.7835 DMRA [32] 0.8723 0.0754 0.8391 0.8579 0.0583 0.8569 0.9082 0.0477 0.8637 ASIF-Net [22] 0.8584 0.0896 0.8144 0.8633 0.0562 0.8566 0.8574 0.0725 0.8141 A2dele [33] 0.8577 0.0740 0.8306 0.8248 0.0691 0.8093 0.9145 0.0426 0.8611 ours 0.8882 0.0720 0.8465 0.8650 0.0524 0.8615 0.9328 0.0366 0.8853</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 :</head><label>9</label><figDesc>Visual comparison with different baselines. (1) The baseline w/o cmFM represents our full model without the cmFM module (i.e., no modulated features); and the baselines w/ cmFA and w/ cmFC refer to that the cmFM module is replaced by the cmFA or cmFC module (i.e., the depth and RGB features are integrated by the element-wise addition or concatenation). (2) The baseline w/o AFS represents our full model without the AFS module (i.e., the features after cmFM module are directly concatenated with the up-sampled saliency-related features); the baselines w/o GFF and w/o CACA correspond to removing the fused spatial features and the channel attention-on-channel attention features, respectively; and the baseline w/ CA refers to that the AFS module is replaced by the conventional channel attention module<ref type="bibr" target="#b19">[20]</ref>.(3) The baselines w/o PEA, w/o PA, and w/o PE correspond to our full model without the sg-PEA module, the position attention unit, and the edge attention unit, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>F β ↑ MAE ↓ Sm ↑ F β ↑ MAE ↓ Sm ↑ full model 0.9084 0.0422 0.8895 0.9328 0.0366 0.8853 cmFM w/o cmFM 0.8727 0.0722 0.8573 0.8968 0.0616 0.8599 w/ cmFA 0.9020 0.0479 0.8820 0.9237 0.0429 0.8771 w/ cmFC 0.8995 0.0480 0.8825 0.9221 0.0617 0.8789 AFS w/o AFS 0.8990 0.0546 0.8762 0.9165 0.0503 0.8666 w/o GFF 0.9012 0.0690 0.8826 0.9212 0.0458 0.8777 w/o CACA 0.9017 0.0517 0.8797 0.9276 0.0470 0.8742 w/ CA 0.9027 0.0503 0.8780 0.9216 0.0468 0.8747 sg-PEA w/o PEA 0.9057 0.0450 0.8854 0.9205 0.0427 0.8796 w/o PA 0.9064 0.0442 0.8857 0.9234 0.0409 0.8827 w/o PE 0.9065 0.0481 0.8862 0.9296 0.0385 0.8806 derscore the importance of progressive self-modality and cross-modality channel attention while fusing important spatial features of multi-modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Conv n ReLU Conv n ReLU Conv n ReLU Conv n ReLU Conv n ReLU Conv n Conv n ReLU Conv n ReLU Conv n ReLU Conv n Feature Extractor</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. Our method achieves the best quantitative results across all metrics, outperforming all competing methods. Compared with the second best method on the NJUD-Test dataset, the percentage gain reaches 1.6% for F-measure, 6.2% for MAE score, and 1.7% for S-measure. On the DUT-test dataset, the minimum percentage</figDesc><table><row><cell>RGB Image</cell><cell>Depth Map</cell><cell>GT</cell><cell>Full Model</cell><cell>w/o cmFM</cell><cell>w/ cmFA</cell><cell>w/ cmFC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>cmFM module</cell><cell></cell></row><row><cell>w/o AFS</cell><cell>w/o GFF</cell><cell>w/o CACA</cell><cell>w/ CA</cell><cell>w/o PEA</cell><cell>w/o PA</cell><cell>w/o PE</cell></row><row><cell></cell><cell cols="2">AFS module</cell><cell></cell><cell></cell><cell>sg-PEA module</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparisons of ablated models</figDesc><table><row><cell>Modules Baselines</cell><cell>STEREO Dataset</cell><cell>DUT-Test Dataset</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A tutorial on the crossentropy method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T D</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative cross-modal transfer learning and densely cross-level feedback fusion for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multiscale multi-path and cross-modal interactions for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">SCA-CNN: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Review of visual saliency detectioin with comprehensive information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2941" to="2959" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Going from RGB to RGBD saliency: A depth-guided transformation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">BBS-Net: RGB-D salient object detection with a bifurcated backbone strategy network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local background enclosure for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2343" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1623" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">JL-DCF: Joint learning and denselycooperative fusion framework for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3052" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Edge-aware convolutional neural network based salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett. pp</title>
		<imprint>
			<biblScope unit="page" from="114" to="118" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CNNs-based RGB-D saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3171" to="3183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection using anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Process.: Image Commun</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="115" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ASIF-Net: Attention steered interweave fusion network for RGBD salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A three-pathway psychobiological framework of salient object detection using stereoscopic technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCVW</publisher>
			<biblScope unit="page" from="3008" to="3014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2806" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2976" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TADAM: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="721" to="731" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Salient object detection via structured matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="818" to="832" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">RGBD salient object detection: A benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">FiLM: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7254" to="7263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A2dele: Adaptive and attentive depth distiller for efficient RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9060" to="9069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">BASNet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RGBD salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Recovering realistic texture in image superresolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-level attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4709" to="4717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reversion correction and regularized random walk ranking for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1311" to="1322" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">UC-Net: Uncertainty inspired RGB-D saliency detection via conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="8582" to="8591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Select,supplement and focus for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3472" to="3481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for RGBD salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">EGNet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A multilayer backpropagation saliency detection algorithm and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="25181" to="25197" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
