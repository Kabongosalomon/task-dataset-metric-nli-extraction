<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADVANTAGE-WEIGHTED REGRESSION: SIMPLE AND SCALABLE OFF-POLICY REINFORCEMENT LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><forename type="middle">Bin</forename><surname>Peng</surname></persName>
							<email>xbpeng@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
							<email>aviralk@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Zhang</surname></persName>
							<email>grace.zhang@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
							<email>svlevine@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADVANTAGE-WEIGHTED REGRESSION: SIMPLE AND SCALABLE OFF-POLICY REINFORCEMENT LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters. (Video 1 ) 1 Supplementary video: xbpeng.github.io/projects/AWR/ 1 arXiv:1910.00177v3 [cs.LG] 7 Oct 2019 for training the policy via weighted regression. The complete algorithm is shown in Algorithm 1. AWR can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. Despite its simplicity, we find that AWR achieves competitive results when compared to commonly used on-policy and off-policy RL algorithms, and can effectively incorporate fully off-policy data, which has been a challenge for other RL algorithms. Our derivation of AWR presents an interpretation of our method as a constrained policy optimization procedure, and provides a theoretical analysis of the use of off-policy data with experience replay.</p><p>We first revisit the original formulation of reward-weighted regression, an on-policy RL method that utilizes supervised learning to perform policy updates, and then propose a number of new design decisions that significantly improve performance on a suite of standard continuous control benchmark tasks. We then provide a theoretical analysis of AWR, including the capability to incorporate off-policy data with experience replay. Although the design of AWR involves only a few simple design decisions, we show experimentally that these additions provide for a large improvement over previous methods for regression-based policy search, such as reward-weighted regression (RWR) <ref type="bibr" target="#b28">(Peters &amp; Schaal, 2007)</ref>, while also being substantially simpler than more modern methods, such as MPO (Abdolmaleki et al., 2018). We show that AWR achieves competitive performance when compared to several well-established state-of-the-art on-policy and off-policy algorithms. We further demonstrate our algorithm on challenging control tasks with complex simulated characters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Model-free reinforcement learning can be a general and effective methodology for training agents to acquire sophisticated behaviors with minimal assumptions on the underlying task <ref type="bibr" target="#b20">(Mnih et al., 2015;</ref><ref type="bibr" target="#b13">Heess et al., 2017;</ref><ref type="bibr" target="#b26">Pathak et al., 2017)</ref>. However, reinforcement learning algorithms can be substantially more complex to implement and tune than standard supervised learning methods. Arguably the simplest reinforcement learning methods are policy gradient algorithms <ref type="bibr" target="#b37">(Sutton et al., 2000)</ref>, which directly differentiate the expected return and perform gradient ascent. Unfortunately, these methods can be notoriously unstable and are typically on-policy (or nearly on-policy), often requiring a substantial number of samples to learn effective behaviors. Our goal is to develop a reinforcement learning algorithm that is simple, easy to implement, and can readily incorporate off-policy experience data.</p><p>In this work, we propose advantage-weighted regression (AWR), a simple off-policy algorithm for model-free RL. Each iteration of the AWR algorithm simply consists of two supervised regression steps: one for training a value function baseline via regression onto cumulative rewards, and another </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In reinforcement learning, the objective is to learn a control policy that enables an agent to maximize its expected return for a given task. At each time step t, the agent observes the state of the environment s t ∈ S, and samples an action a t ∈ A from a policy a t ∼ π(a t |s t ). The agent then applies that action, which results in a new state s t+1 and a scalar reward r t = r(s t , a t ). The goal is to learn an optimal policy that maximizes the agent's expected discounted return J(π), J(π) = E τ ∼pπ(τ ) </p><p>where p π (τ ) represents the likelihood of a trajectory τ = {(s 0 , a 0 , r 0 ) , (s 1 , a 1 , r 1 ) , ...} under a policy π, and γ ∈ [0, 1) is the discount factor. d π (s) = ∞ t=0 γ t p(s t = s|π) represents the unnormalized discounted state distribution induced by the policy π <ref type="bibr" target="#b36">(Sutton &amp; Barto, 1998)</ref>, and p(s t = s|π) is the likelihood of the agent being in state s after following π for t timesteps.</p><p>Our proposed AWR algorithm builds on ideas from reward-weighted regression (RWR) <ref type="bibr" target="#b29">(Peters et al., 2010)</ref>, a policy search algorithm based on an expectation-maximization framework, which solves the following supervised regression problem at each iteration: π k+1 = arg max π E s∼dπ k (s) E a∼π k (a|s) log π(a|s) exp 1 β R s,a .</p><p>(2) π k represents the policy at the kth iteration of the algorithm, and R s,a = ∞ t=0 γ t r t is the return. The RWR update can be interpreted as solving a maximum likelihood problem that fits a new policy π k+1 to samples collected under the current policy π k , where the likelihood of each action is weighted by the exponentiated return received for that action, with a temperature parameter β &gt; 0.</p><p>As an alternative to the EM framework, a similar algorithm can also be derived using the dual formulation of a constrained policy search problem <ref type="bibr" target="#b29">(Peters et al., 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ADVANTAGE-WEIGHTED REGRESSION</head><p>In this work, we propose advantage-weighted regression (AWR), a simple off-policy RL algorithm based on reward-weighted regression. We first provide an overview of the complete advantageweighted regression algorithm, and then describe its theoretical motivation and analyze its properties. The complete AWR algorithm is summarized in Algorithm 1. Each iteration k of AWR consists of the following simple steps. First, the current policy π k (a|s) is used to sample a batch of trajectories {τ i } that are then stored in the replay buffer D, which is structured as a first-in first-out (FIFO) Algorithm 1 Advantage-Weighted Regression 1: π 1 ← random policy 2: D ← ∅ 3: for iteration k = 1, ..., k max do 4:</p><formula xml:id="formula_1">add trajectories {τ i } sampled via π k to D 5: V D k ← arg min V E s,a∼D R D s,a − V (s) 2 6: π k+1 ← arg max π E s,a∼D log π(a|s) exp 1 β R D s,a − V D k (s) 7: end for</formula><p>queue, as is common for off-policy reinforcement learning algorithms <ref type="bibr" target="#b20">(Mnih et al., 2015;</ref>. Then, the entire buffer D is used to fit a value function V D k (s) to the trajectories in the replay buffer, which can be done with simple Monte Carlo return estimates R D s,a = T t=0 γ t r t . Finally, the same buffer is used to fit a new policy using advantage-weighted regression, where each state-action pair in the buffer is weighted according to the exponentiated advantage exp( 1 β A D (s, a)), with the advantage given by A D (s, a) = R D s,a − V D (s) and β is a hyperparameter. AWR uses only supervised regression as learning subroutines, making the algorithm very simple to implement. In the following subsections, we first motivate the algorithm as an approximation to a constrained policy search problem, and then extend our analysis to incorporate experience replay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DERIVATION</head><p>In this section, we derive the AWR algorithm as an approximate optimization of a constrained policy search problem. Our goal is to find a policy that maximizes the expected improvement η(π) = J(π) − J(µ) over a sampling policy µ(a|s). We first derive AWR for the setting where the sampling policy is a single Markovian policy. Then, in the next section, we extend our result to the setting where the data is collected from multiple policies, as in the case of experience replay that we use in practice. The expected improvement η(π) can be expressed in terms of the advantage A µ (s, a) = R µ s,a − V µ (s) with respect to the sampling policy µ <ref type="bibr" target="#b15">(Kakade &amp; Langford, 2002;</ref><ref type="bibr" target="#b33">Schulman et al., 2015)</ref>:</p><formula xml:id="formula_2">η(π) = E s∼dπ(s) E a∼π(a|s) [A µ (s, a)] = E s∼dπ(s) E a∼π(a|s) R µ s,a − V µ (s) ,<label>(3)</label></formula><p>where R µ s,a denotes the return obtained by performing action a in state s and following µ for the following timesteps, and V µ (s) = a µ(a|s)R a s da corresponds to the value function of µ. This objective differs from the ones used in the derivations of related algorithms, such as RWR and REPS <ref type="bibr" target="#b28">(Peters &amp; Schaal, 2007;</ref><ref type="bibr" target="#b29">Peters et al., 2010;</ref><ref type="bibr" target="#b0">Abdolmaleki et al., 2018)</ref>, which maximize the expected return J(π) instead of the expected improvement. The expected improvement directly gives rise to an objective that involves the advantage. We will see later that this yields weights for the policy update that differ in a subtle but important way from standard reward-weighted regression. As we show in our experiments, this difference results in a large empirical improvement.</p><p>The objective in Equation 3 can be difficult to optimize due to the dependency between d π (s) and π, as well as the need to collect samples from π. Following <ref type="bibr" target="#b33">Schulman et al. (2015)</ref>, we can instead optimize an approximationη(π) of η(π) using the state distribution of µ:</p><formula xml:id="formula_3">η(π) = E s∼dµ(s) E a∼π(a|s) R µ s,a − V µ (s) .<label>(4)</label></formula><p>Here,η(π) matches η(π) to first order <ref type="bibr" target="#b15">(Kakade &amp; Langford, 2002)</ref>, and provides a good estimate of η if π and µ are close in terms of the KL-divergence <ref type="bibr" target="#b33">(Schulman et al., 2015)</ref>. Using this objective, we can formulate the following constrained policy search problem:</p><formula xml:id="formula_4">arg max π s d µ (s) a π(a|s) R µ s,a − V µ (s) da ds (5) s.t. s d µ (s)D KL (π(·|s)||µ(·|s)) ds ≤ .<label>(6)</label></formula><p>The constraint in Equation 6 ensures that the new policy π is close to the data distribution of µ, and therefore the surrogate objectiveη(π) remains a reasonable approximation to η(π). We refer the reader to <ref type="bibr" target="#b33">Schulman et al. (2015)</ref> for a detailed derivation and an error bound.</p><p>We can derive AWR as an approximate solution to this constrained optimization. This derivation follows a similar procedure as <ref type="bibr" target="#b29">Peters et al. (2010)</ref>, and begins by forming the Langrangian of the constrained optimization problem presented above,</p><formula xml:id="formula_5">L(π, β) = s d µ (s) a π(a|s) R µ s,a − V µ (s) da ds + β − s d µ (s)D KL (π(·|s)||µ(·|s)) ds ,<label>(7)</label></formula><p>where β is a Lagrange multiplier. Differentiating L(π, β) with respect to π(a|s) and solving for the optimal policy π * results in the following expression for the optimal policy π * (a|s) = 1</p><formula xml:id="formula_6">Z(s) µ(a|s) exp 1 β R µ s,a − V µ (s) ,<label>(8)</label></formula><p>with Z(s) being the partition function. A detailed derivation is available in Appendix A. If π is represented by a function approximator (e.g., a neural network), a new policy can be obtained by projecting π * onto the manifold of parameterized policies,</p><formula xml:id="formula_7">arg min π E s∼D [D KL (π * (·|s)||π(·|s))] (9) = arg max π E s∼dµ(s) E a∼µ(a|s) log π(a|s) exp 1 β R µ s,a − V µ (s) .<label>(10)</label></formula><p>While this derivation for AWR largely follows the derivations used in prior work <ref type="bibr" target="#b29">(Peters et al., 2010;</ref><ref type="bibr" target="#b0">Abdolmaleki et al., 2018)</ref>, our expected improvement objective introduces a baseline V µ (s) to the policy update, which as we show in our experiments, is a crucial component for an effective algorithm. We next extend AWR to incorporate experience replay for off-policy training, where the sampling policy is no longer a single policy, but rather a mixture of policies from past iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EXPERIENCE REPLAY AND OFF-POLICY LEARNING</head><p>A crucial design decision of AWR is the choice of sampling policy µ(a|s). Standard implementations of RWR typically follow an on-policy approach, where the sampling policy is selected to be the current policy µ(a|s) = π k (a|s) at iteration k. This can be sample inefficient, as data collected at each iteration of the algorithms are discarded after a single update iteration. Importance sampling can be incorporated into RWR to reuse data from previous iterations, but at the cost of larger variance from the importance sampling estimator <ref type="bibr" target="#b16">(Kober &amp; Peters, 2009</ref>). Instead, we can improve sample efficiency of AWR by incorporating experience replay and explicitly accounting for training data from a mixture of multiple prior policies. As described in Algorithm 1, at each iteration, AWR collects a batch of data using the latest policy π k , and then stores this data in a replay buffer D, which also contains data collected from previous policies {π 1 , · · · , π k }. The value function and policy are then updated using samples drawn from D. This replay strategy is analogous to modeling the sampling policy as a mixture of policies from previous iterations µ k (τ ) = k i=1 w i π i (τ ), where π i (τ ) = p(τ |π i ) represents the likelihood of a trajectory τ under a policy π i from the ith iteration, and the weights i w i = 1 specify the probabilities of selecting each policy π i .</p><p>We now extend the derivation from the previous section to the off-policy setting with experience replay, and show that Algorithm 1 indeed optimizes the expected improvement over a sampling policy modeled by the replay buffer. Given a replay buffer consisting of trajectories from past policies, the joint state-action distribution of µ is given by µ(s, a) = k i=1 w i d πi (s)π i (a|s), and similarly for the marginal state distribution d µ (s) = k i=1 w i d πi (s). The expected improvement can now be expressed with respect to the set of sampling policies in the replay buffer: η(π) = J(π)− i w i J(π i ). Similar to Equation 3, η(π) can be expressed in terms of the advantage A πi (s, a) = R πi s,a − V πi (s) of each sampling policies,</p><formula xml:id="formula_8">η(π) = J(π) − i w i J(π i ) = E s∼dπ(s) E a∼π(a|s) i w i A πi (s, a) .<label>(11)</label></formula><p>As before, we can optimize an approximationη(π) of η(π) using the state distribution of µ,</p><formula xml:id="formula_9">η(π) = E s∼dµ(s) E a∼π(a|s) [A πi (s, a)] = k i=1 w i E s∼dπ i (s) E a∼π(a|s) [A πi (s, a)]<label>(12)</label></formula><p>In Appendix B, we show that the update procedure in Algorithm 1 optimizes the following objective:</p><formula xml:id="formula_10">arg max π k i=1 w i E s∼dπ i (s) E a∼π(a|s) [A πi (s, a)] (13) s.t. E s∼dµ(s) [D KL (π(·|s)||µ(·|s))] ≤ ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_11">µ(a|s) = µ(s,a) dµ(s) = i widπ i (s)πi(a|s) j wj dπ j (s)</formula><p>represents the conditional action distribution defined by the replay buffer. This objective can be solved via the Lagrangian to yield the following update:</p><formula xml:id="formula_12">arg max π k i=1 w i E s∼dπ i (s) E a∼πi(a|s) log π(a|s) exp 1 β R πi s,a − j w j d πj (s)V πj (s) j w j d πj (s) ,<label>(15)</label></formula><p>where the expectations can be approximated by simply sampling from D following Line 6 of Algorithm 1. A detailed derivation is available in Appendix B. Note, the baseline in the exponent now consists of an average of the value functions of the different policies. One approach for estimating this quantity would be to fit separate value functions V πi for each policy. However, if only a small amount of data is available from each policy, then V πi could be highly inaccurate . Therefore, instead of learning separate value functions, we fit a single mean value functionV (s) that directly estimates the weighted average of V πi 's,</p><formula xml:id="formula_13">V = arg min V i w i E s,∼dπ i (s) E a∼πi(a|s) ||R πi s,a − V (s)|| 2<label>(16)</label></formula><p>This loss can also be approximated by simply sampling from the replay buffer following Line 5 of Algorithm 1. The optimal solutionV (s)</p><formula xml:id="formula_14">= i widπ i (s)V π i (s) j wj dπ j (s)</formula><p>is exactly the baseline in Equation 15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">IMPLEMENTATION DETAILS</head><p>Finally, we discuss several design decisions that are important for a practical implementation of AWR. An overview of AWR is provided in Algorithm 1. The policy update in Equation 10 requires sampling states from the discounted state distribution d µ (s). However, we found that simply sampling states uniformly from D was also effective, and simpler to implement. This is a common strategy used in standard implementations of RL algorithms . When updating the value function and policy, Monte Carlo estimates can be used to approximate the expected return R D s,a of samples in D, but this can result in a high-variance estimate. Instead, we opt to approximate R D s,a using TD(λ) to obtain a lower-variance estimate <ref type="bibr" target="#b36">(Sutton &amp; Barto, 1998)</ref>. TD(λ) is applied by bootstrapping with the value function V D k−1 (s) from the previous iteration. A simple Monte Carlo return estimator can also be used though, as shown in our experiments, but this produces somewhat worse results. To further simplify the algorithm, instead of adaptively updating the Lagrange multiplier β, as is done in previous methods <ref type="bibr" target="#b28">(Peters &amp; Schaal, 2007;</ref><ref type="bibr" target="#b29">Peters et al., 2010;</ref><ref type="bibr" target="#b0">Abdolmaleki et al., 2018)</ref>, we find that simply using a fixed constant for β is also effective. The weights ω D s,a = exp 1 β R D s,a − V D (s) used to update the policy can occasionally assume excessively large values, which can cause gradients to explode. We therefore apply weight clippinĝ ω D s,a = min ω D s,a , ω max with a threshold ω max to mitigate issues due to exploding weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Existing RL methods can be broadly categorized into on-policy and off-policy algorithms <ref type="bibr" target="#b36">(Sutton &amp; Barto, 1998)</ref>. On-policy algorithms generally update the policy using data collected from the same policy. A popular class of on-policy algorithms is policy gradient methods <ref type="bibr" target="#b39">(Williams, 1992;</ref><ref type="bibr" target="#b37">Sutton et al., 2000)</ref>, which have been shown to be effective for a diverse array of complex tasks <ref type="bibr" target="#b13">(Heess et al., 2017;</ref><ref type="bibr" target="#b26">Pathak et al., 2017;</ref><ref type="bibr" target="#b27">Peng et al., 2018;</ref><ref type="bibr" target="#b32">Rajeswaran et al., 2018)</ref>. However, on-policy algorithms are typically data inefficient, requiring a large number of interactions with the environment, making it impractical to directly apply these techniques to domains where environmental interactions can be costly (e.g. robotics and other real world applications). Off-policy algorithms improve sample efficiency by enabling a policy to be trained using data from other sources, such as data collected from different agents or data from previous iterations of the algorithm. Importance sampling is a simple strategy for incorporating off-policy data <ref type="bibr" target="#b36">(Sutton &amp; Barto, 1998;</ref><ref type="bibr" target="#b19">Meuleau et al., 2000;</ref><ref type="bibr" target="#b11">Hachiya et al., 2009</ref>), but can introduce optimization instabilities due to the potentially large variance of the importance sampling estimator. Dynamic programming methods based on Q-function learning can also leverage off-policy data <ref type="bibr" target="#b31">(Precup et al., 2001;</ref><ref type="bibr" target="#b20">Mnih et al., 2015;</ref><ref type="bibr" target="#b8">Gu et al., 2016;</ref><ref type="bibr" target="#b10">Haarnoja et al., 2018b)</ref>. But these methods can be notoriously unstable, and in practice, require a variety of stabilization techniques to ensure more consistent performance <ref type="bibr" target="#b12">(Hasselt et al., 2016;</ref><ref type="bibr" target="#b38">Wang et al., 2016;</ref><ref type="bibr" target="#b14">Hessel et al., 2017;</ref><ref type="bibr" target="#b6">Fujimoto et al., 2018;</ref><ref type="bibr" target="#b22">Nachum et al., 2018;</ref>. Furthermore, it can be difficult to apply these methods to learn from fully off-policy data, where an agent is unable to collect additional environmental interactions <ref type="bibr" target="#b7">(Fujimoto et al., 2019;</ref>.</p><p>Alternatively, policy search can also be formulated under an expectation-maximization framework. This approach has lead to a variety of EM-based RL algorithms <ref type="bibr" target="#b29">(Peters et al., 2010;</ref><ref type="bibr" target="#b24">Neumann, 2011;</ref><ref type="bibr" target="#b0">Abdolmaleki et al., 2018)</ref>, an early example of which is reward-weighted regression (RWR) <ref type="bibr" target="#b28">(Peters &amp; Schaal, 2007)</ref>. RWR presents a simple on-policy RL algorithm that casts policy search as a supervised regression problem. A similar algorithm, relative entropy policy search (REPS) <ref type="bibr" target="#b29">(Peters et al., 2010)</ref>, can also be derived from the dual formulation of a constrained policy search problem. RWR has a number appealing properties: it has a very simple update rule, and since each iteration corresponds to supervised learning, it can be more stable and easier to implement than many of the previously mentioned RL methods. Despite these advantages, RWR has not been shown to be an effective RL algorithm when combined with neural network function approximators, as demonstrated in prior work and our own experiments <ref type="bibr" target="#b33">(Schulman et al., 2015;</ref><ref type="bibr" target="#b4">Duan et al., 2016)</ref>. In this work, we propose a number of modifications to the formulation of RWR to produce an effective off-policy deep RL algorithm, while still retaining much of the simplicity of previous methods.</p><p>The most closely related prior works to our method are REPS <ref type="bibr" target="#b29">(Peters et al., 2010)</ref> and MPO <ref type="bibr" target="#b0">(Abdolmaleki et al., 2018)</ref>, both of which are based on a constrained policy search formulation, and perform policy updates using supervised regression. The optimization problem being solved in REPS is similar to AWR, but REPS optimizes the expected return instead of the expected improvement. The weights in REPS also contains a Bellman error term that superficially resembles advantages, but are computed using a linear value function derived from a feature matching constraint. Learning the REPS value function requires minimization of a dual function, which is a complex function of the Bellman error, while the value function in AWR can be learned with simple supervised regression. REPS can in principle leverage off-policy data, but the policy iteration procedure proposed for REPS models the sampling policy using only the latest policy, and does not incorporate experience replay with data from previous iterations. More recently, <ref type="bibr" target="#b0">Abdolmaleki et al. (2018)</ref> proposed MPO, a deep RL variant of REPS, which applies a partial EM algorithm for policy optimization. The method first fits a Q-function of the current policy via bootstrapping, and then performs a policy improvement step with respect to this Q-function under a trust region constraint that penalizes large policy changes. MPO uses off-policy data for training a Q-function critic via bootstrapping and employs Retrace(λ) for off-policy correction . In contrast, AWR is substantially simpler, as it can simply fit a value function to the observed returns in a replay buffer, and performs weighted supervised regression on the actions to fit the policy. <ref type="bibr" target="#b25">Neumann &amp; Peters (2009)</ref> proposed LAWER, a kernel-based fitted Q-iteration algorithm where the Bellman error is weighted by the normalized advantage of each state-action pair. This was then followed by a soft-policy improvement step. A similar soft-policy update has also been used in more recent algorithms such as soft actor-critic <ref type="bibr" target="#b10">(Haarnoja et al., 2018b)</ref>. Similarly to <ref type="bibr" target="#b25">(Neumann &amp; Peters, 2009</ref>), our method also uses exponentiated advantages to weight the policy update, but does not perform fitted Q-iteration, and instead utilizes off-policy data in a simple constrained policy search procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Our experiments aim to comparatively evaluate the performance of AWR to commonly used onpolicy and off-policy deep RL algorithms. We evaluate our method on the OpenAI Gym benchmarks <ref type="bibr" target="#b1">(Brockman et al., 2016)</ref>, consisting of discrete and continuous control tasks. We also evaluate our method on complex motion imitation tasks with high-dimensional simulated characters, including a 34 DoF humanoid and 82 DoF dog <ref type="bibr" target="#b27">(Peng et al., 2018)</ref>. We then demonstrate the effectiveness of AWR on fully off-policy learning, by training on static datasets of demonstrations collected from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">BENCHMARKS</head><p>We compare AWR to a number of state-of-the-art RL algorithms, including on-policy algorithms, such as TRPO <ref type="bibr" target="#b33">(Schulman et al., 2015)</ref> and PPO , off-policy algorithms, such as DDPG , TD3 <ref type="bibr" target="#b6">(Fujimoto et al., 2018)</ref>, and SAC <ref type="bibr" target="#b9">(Haarnoja et al., 2018a)</ref>, as well as RWR <ref type="bibr" target="#b28">(Peters &amp; Schaal, 2007)</ref>, which we include for comparison due to its similarity to AWR. 2 TRPO and PPO use the implementations from OpenAI baselines . DDPG, TD3, and SAC uses the implementations from RLkit <ref type="bibr" target="#b30">(Pong, 2019)</ref>. RWR is a custom implementation following the algorithm described by <ref type="bibr" target="#b28">Peters &amp; Schaal (2007)</ref>.</p><p>Snapshots of the AWR policies are shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Learning curves comparing the different algorithms on the OpenAI Gym benchmarks are shown in <ref type="figure">Figure 3</ref>, and <ref type="table" target="#tab_1">Table 1</ref> summarizes the average returns of the final policies across 5 training runs initialized with different random seeds. Overall, AWR shows competitive performance with the state-of-the-art deep RL algorithms. It significantly outperforms on-policy methods such as PPO and TRPO in both sample efficiency and asymptotic performance. While it is not yet as sample efficient as current state-of-the-art off-policy methods, such SAC and TD3, it is generally able to achieve a similar asymptotic performance, despite using only simple supervised regression for both policy and value function updates. The complex Humanoid-V2 task proved to be the most challenging case for AWR, and its performance still lags well behind SAC. Note that RWR generally does not perform well on any of these tasks. This suggests that, although AWR is simple and easy to implement, the particular modifications it makes compared to standard RWR are critical for effective performance. To illustrate AWR's generality on tasks with discrete actions, we compare AWR to TRPO, PPO, and RWR on LunarLander-v2. DDPG, TD3, and SAC are not easily applicable to discrete action spaces due to their need to backpropagate from the Q-function to the policy. On this discrete control task, AWR also shows strong performance compared to the other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ABLATION EXPERIMENTS</head><p>To determine the effects of various design decisions, we evaluate the performance of AWR when key components of the algorithm have been removed. The experiments include an on-policy version of AWR (On-Policy), where only data collected from the latest policy is used to perform updates. We <ref type="figure">Figure 3</ref>: Learning curves of the various algorithms when applied to OpenAI Gym tasks. Results are averaged across 5 random seeds. AWR is generally competitive with the best current methods.  also compare with a version of AWR without the baseline V (s) (No Baseline), which corresponds to using the standard RWR weights ω s,a = exp( 1 β R s,a ), and another version that uses Monte Carlo return estimates instead of TD(λ) (No TD(λ)). The effects of these components are illustrated in <ref type="figure">Figure 4</ref>. Overall, these design decisions appear to be vital for an effective algorithm, with the most crucial components being the use of experience replay and a baseline. Updates using only on-policy data can lead to instabilities and result in noticeable degradation in performance, which may be due to overfitting on a smaller dataset. This issue might be mitigated by collecting a larger batch of on-policy data per iteration, but this can also negatively impact sample efficiency. Removing the baseline also noticeably hampers performance. Using simple Monte Carlo return estimates instead of TD(λ) seems to be a viable alternative, and the algorithm still achieves competitive performance on some tasks. When combined, these different components yield substantial performance gains over standard RWR.</p><p>To better evaluate the effect of experience replay on AWR, we compare the performance of policies trained with different capacities for the replay buffer. <ref type="figure">Figure 4</ref> illustrates the learning curves for buffers of size 5k, 20k, 50k, 100k, and 500k, with 50k being the default buffer size in our experiments. The size of the replay buffer appears to have a significant impact on overall performance. Smaller buffer sizes can result in instabilities during training, which again may be an effect of overfitting to a smaller dataset. As the buffer size increases, AWR remains stable even when the dataset is dominated by off-policy data from previous iterations. In fact, performance over the course of training appears more stable with larger replay buffers, but progress can also become slower. Since the sampling policy µ(a|s) is modeled by the replay buffer, a larger buffer can limit the rate at which µ changes by maintaining older data for more iterations. Due to the trust region penalty in Equation 7, a slower changing µ also prevents the policy π from changing quickly. The replay buffer therefore provides a simple mechanism to trade-off between stability and learning speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MOTION IMITATION</head><p>The Gym benchmarks present relatively low-dimensional tasks. In this section, we study how AWR can solve higher-dimensional tasks with complex simulated characters, including a 34 DoF humanoid and 82 DoF dog. The objective of the tasks is to imitate reference motion clips recorded using motion capture from real world subjects. The experimental setup follows the motion imitation framework proposed by <ref type="bibr" target="#b27">Peng et al. (2018)</ref>. Motion clips are collected from publicly available datasets (CMU; SFU; <ref type="bibr" target="#b40">Zhang et al., 2018)</ref>. The skills include highly dynamics motions, such as spinkicks and canters (i.e. running), and motions that requires more coordinated movements of the character's body, such as a cartwheel. Snapshots of the behaviors learned by the AWR policies are available in <ref type="figure">Figure 5</ref>. <ref type="table">Table 2</ref> compares the performance of AWR to RWR and the highly-tuned PPO implementation from <ref type="bibr" target="#b27">Peng et al. (2018)</ref>. Learning curves for the different algorithms are shown in <ref type="figure">Figure 6</ref>. AWR performs well across the set of challenging skills, consistently achieving comparable or better performance than PPO. RWR struggles with controlling the humanoid, but exhibits stronger performance on the dog. This performance difference may be due to the more dynamic and acrobatic skills of the humanoid, compared to the more standard locomotion skills of the dog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">OFF-POLICY LEARNING WITH STATIC DATASETS</head><p>Since AWR is an off-policy RL algorithm, it has the advantage of being able to leverage data from other sources. This not only accelerates the learning process on standard tasks, as discussed above, but also allows us to apply AWR in a fully off-policy setting, where the algorithm is provided with a static dataset of transitions, and then tasked with learning the best possible policy. To evaluate our method in this setting, we use the off-policy tasks proposed by . The objective of these tasks is to learn policies solely from static datasets, without collecting any additional data from the policy that is being trained. The dataset consists of trajectories τ = {(s 0 , a 0 , r 0 ) , (s 1 , a 1 , r 1 ) , ...} from rollouts of a demo policy. Unlike standard imitation learning tasks, which only observes the states and actions from the demo policy, the dataset also provides the reward received by the demo policy at each step. The demo policies are trained using SAC on various OpenAI Gym tasks. A dataset of 1 million timesteps is collected for each task.</p><p>For AWR, we simply treat the dataset as the replay buffer D and directly apply the algorithm without additional modifications. <ref type="figure">Figure 7</ref> compares AWR with other algorithms when applied to the datasets. We include comparisons to the performance of the original demo policy used to generate the dataset (Demo) and a behavioral cloning policy (BC). The comparisons also include recent off-policy methods: batch-constrained Q-learning (BCQ) <ref type="bibr" target="#b7">(Fujimoto et al., 2019)</ref> and bootstrapping error accumulation reduction (BEAR) , which have shown strong performance on off-policy learning with static datasets. Note that both of these prior methods are modifications to existing off-policy RL methods, such as TD3 and SAC, which are already quite complex. In  <ref type="table">Table 2</ref>: Performance statistics of algorithms on the motion imitation tasks. Returns are normalized between the minimum and maximum possible returns per episode. <ref type="figure">Figure 6</ref>: Learning curves on motion imitation tasks. On these challenging tasks, AWR generally learns faster than PPO and RWR. <ref type="figure">Figure 7</ref>: Performance of various algorithms on off-policy learning tasks with static datasets. AWR is able to learn policies that are comparable or better than the original demo policies.</p><p>contrast, AWR is simple and requires no modifications for the fully off-policy setting. Despite not collecting any additional data, AWR is able to learn effective policies from these fully off-policy datasets, achieving comparable or better performance than the original demo policies. On-policy methods, such as PPO performs poorly in this off-policy setting. Q-function based methods, such as TD3 and SAC, can in principle handle off-policy data but, as discussed in prior work, tend to struggle in this setting in practice <ref type="bibr" target="#b7">(Fujimoto et al., 2019;</ref>. Indeed, standard behavioral cloning (BC) often outperforms these standard RL methods. In this fully off-policy setting, AWR can be interpreted as an advantage-weighted form of behavioral cloning, which assigns higher likelihoods to demonstration actions that receive higher advantages. Unlike Q-function based methods, AWR is less susceptible to issues from out-of-distribution actions as the policy is always trained on observed actions from the behaviour data . AWR also shows comparable performance to BEAR and BCQ, which are specifically designed for this off-policy setting and introduce considerable algorithmic overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND FUTURE WORK</head><p>We presented advantage-weighted regression, a simple off-policy reinforcement learning algorithm, where policy updates are performed using standard supervised learning methods. Despite its simplicity, our algorithm is able to solve challenging control tasks with complex simulated agents, and achieve competitive performance on standard benchmarks compared to a number of well-established RL algorithms. Our derivation introduces several new design decisions, and our experiments verify the importance of these components. AWR is also able to learn from fully off-policy datasets, demonstrating comparable performance to state-of-the-art off-policy methods. While AWR is effective for a diverse suite of tasks, it is not yet as sample efficient as the most efficient off-policy algorithms. We believe that exploring techniques for improving sample efficiency and performance on fully off-policy learning can open opportunities to deploy these methods in real world domains. We are also interested in exploring applications that are particularly suitable for these regression-based RL algorithms, as compared to other classes of RL techniques. A better theoretical understanding of the convergence properties of these algorithms, especially when combined with experience replay, could also be valuable for the development of future algorithms.</p><p>DeepDrive, Sony Interactive Entertainment America, Google, NVIDIA, Amazon, and the DARPA Assured Autonomy program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A AWR DERIVATION</head><p>In this section, we derive the AWR algorithm as an approximate optimization of a constrained policy search problem. Our goal is to find a policy that maximize the expected improvement η(π) = J(π) − J(µ) over a sampling policy µ(a|s). We start with a lemma from <ref type="bibr" target="#b15">Kakade &amp; Langford (2002)</ref>, which shows that the expected improvement can be expressed in terms of the advantage A µ (s, a) = R µ s,a − V µ (s) with respect to the sampling policy µ, where R µ s,a denotes the return obtained by performing action a in state s and following µ for the following timesteps, and V µ (s) = a µ(a|s)R a s da corresponds to the value function of µ,</p><formula xml:id="formula_15">E τ ∼pπ(τ ) ∞ t=0 γ t A µ (s t , a t )<label>(17)</label></formula><formula xml:id="formula_16">= E τ ∼pπ(τ ) ∞ t=0 γ t (r(s t , a t ) + γV µ (s t+1 ) − V µ (s t )) (18) = E τ ∼pπ(τ ) −V µ (s 0 ) + ∞ t=0 γ t r(s t , a t ) (19) = −E s0∼p(s0) [V µ (s 0 )] + E τ ∼pπ(τ ) ∞ t=0 γ t r(s t , a t ) (20) = −J(µ) + J(π)<label>(21)</label></formula><p>We can rewrite Equation 22 with an expectation over states instead of trajectories: </p><formula xml:id="formula_17">η(π) = E τ ∼pπ(τ ) ∞ t=0 γ t A µ (s t , a t )<label>(22)</label></formula><p>where d π (s) = ∞ t=0 γ t p(s t = s|π) represents the unnormalized discounted state distribution induced by the policy π <ref type="bibr" target="#b36">(Sutton &amp; Barto, 1998)</ref>, and p(s t = s|π) is the likelihood of the agent being in state s after following π for t timesteps.</p><p>The objective in Equation 25 can be difficult to optimize due to the dependency between d π (s) and π, as well as the need to collect samples from π. Following <ref type="bibr" target="#b33">Schulman et al. (2015)</ref>, we can optimize an approximationη(π) of η(π) using the state distribution of µ,</p><formula xml:id="formula_19">η(π) = s d µ (s) a π(a|s) R µ s,a − V µ (s) da ds.<label>(26)</label></formula><p>η(π) matches η(π) to first order <ref type="bibr" target="#b15">(Kakade &amp; Langford, 2002)</ref>, and provides a reasonable estimate of η if π and µ are similar. Using this objective, we can formulate the following constrained policy search problem:</p><formula xml:id="formula_20">arg max π s d µ (s) a π(a|s) R µ s,a − V µ (s) da ds<label>(27)</label></formula><p>s.t. D KL (π(·|s)||µ(·|s)) ≤ , ∀ s (28) a π(a|s) da = 1, ∀ s.</p><p>Since enforcing the pointwise KL constraint in Equation 28 at all states is intractable, we relax the constraint by enforcing it only in expectation s d µ (s)D KL (π(·|s)||µ(·|s)) ds ≤ . To further simplify the optimization problem, we relax the hard KL constraint by converting it into a soft constraint with coefficient β,</p><formula xml:id="formula_22">arg max π s d µ (s) a π(a|s) R µ s,a − V µ (s) da ds + β − s d µ (s)D KL (π(·|s)||µ(·|s)) ds s.t.</formula><p>a π(a|s) da = 1, ∀ s.</p><p>(30) Next we form the Lagrangian,</p><formula xml:id="formula_23">L(π, β, α) = s d µ (s) a π(a|s) R µ s,a − V µ (s) da ds + β − s d µ (s)D KL (π(·|s)||µ(·|s)) ds + s α s 1 − a π(a|s)da ds,<label>(31)</label></formula><p>with β and α = {α s | ∀s ∈ S} corresponding to the Lagrange multipliers. Differentiating L(π, β, α) with respect to π(a|s) results in</p><formula xml:id="formula_24">∂L ∂π(a|s) = d µ (s) R µ s,a − V µ (s) − β d µ (s) logπ(a|s) + βd µ (s)logµ(a|s) − βd µ (s) − α s .<label>(32)</label></formula><p>Setting to zero and solving for π(a|s) gives</p><formula xml:id="formula_25">logπ(a|s) = 1 β R µ s,a − V µ (s) + logµ(a|s) − 1 − 1 d µ (s) α s β (33) π(a|s) = µ(a|s)exp 1 β R µ s,a − V µ (s) exp − 1 d µ (s) α s β − 1<label>(34)</label></formula><p>Since a π(a|s) da = 1, the second exponential term is the partition function Z(s) that normalizes the conditional action distribution,</p><formula xml:id="formula_26">Z(s) = exp 1 d µ (s) α s β + 1 = a µ(a |s) exp 1 β R µ s,a − V µ (s) da .<label>(35)</label></formula><p>The optimal policy is therefore given by,</p><formula xml:id="formula_27">π * (a|s) = 1 Z(s) µ(a|s) exp 1 β R µ s,a − V µ (s)<label>(36)</label></formula><p>If π is represented by a function approximator, the optimal policy π * can be projected onto the manifold of parameterized policies by solving the following supervised regression problem arg min π E s∼dµ(s) [D KL (π * (·|s)||π(·|s))]</p><formula xml:id="formula_28">= arg min π E s∼dµ(s) D KL 1 Z(s) µ(a|s) exp 1 β R µ s,a − V µ (s) π(·|s)<label>(37)</label></formula><p>= arg max π E s∼dµ(s) E a∼µ(a|s) log π(a|s) exp</p><formula xml:id="formula_30">1 β R µ s,a − V µ (s) ,<label>(39)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B AWR DERIVATION WITH EXPERIENCE REPLAY</head><p>In this section, we extend the derivation presented in Appendix A to incorporate experience replay using a replay buffer containing data from previous policies. To recap, the sampling distribution is a mixture of k past policies {π 1 , · · · , π k }, where the mixture is performed at the trajectory level. First, we define the trajectory distribution µ(τ ), marginal state-action distribution µ(s, a), and marginal state distribution d µ (s) of the replay buffer according to:</p><formula xml:id="formula_31">µ(τ ) = k i=1 w i d πi (τ ), µ(s, a) = k i=1 w i d πi (s)π i (a|s), d µ (s) = k i=1 w i d πi (s)<label>(40)</label></formula><p>where the weights i w i = 1 specify the probabilities of selecting each policy π i . The conditional action distribution µ(a|s) induced by the replay buffer is given by:</p><formula xml:id="formula_32">µ(a|s) = µ(s, a) d µ (s) = k i=1 w i d πi (s)π i (a|s) k j=1 w j d πj (s) .<label>(41)</label></formula><p>Next, using Lemma 6.1 from <ref type="bibr" target="#b15">Kakade &amp; Langford (2002)</ref> (also derived in Appendix A), the expected improvement of π over each policy π i satisfies</p><formula xml:id="formula_33">J(π) = J(π i ) + E s∼dπ(s),a∼π(a|s) [A πi (s, a)]<label>(42)</label></formula><p>The expected improvement over the mixture can then be expressed with respect to the individual policies,</p><formula xml:id="formula_34">η(π) = J(π) − J(µ) (43) = J(π) − k i=1 w i J(π i ) (44) = k i=1 w i (J(π) − J(π i )) (45) = k i=1 w i E s∼dπ(s),a∼π(a|s) [A πi (s, a)]<label>(46)</label></formula><p>In order to ensure that the policy π is similar to the past policies, we constrain π against the conditional action distributions of the replay buffer,</p><formula xml:id="formula_35">E s∼µ(s) D KL π(a|s) µ(a|s) ≤ ε.<label>(47)</label></formula><p>Note that constraining π against µ(a|s) has a number of desirable properties. First, the constraint prevents the policy π from choosing actions that are vastly different from all of the policies {π 1 , · · · , π k }. Second, the mixture weight assigned to each π i in the definition of µ depends on the marginal state density d πi (s) for the particular policy. This property is desirable as the policy π is now constrained to be similar to π i only at states that are likely to be visited by π i . This then yields the following constrained objective:</p><formula xml:id="formula_36">arg max π k i=1 w i E s∼dπ i (s) E a∼π(a|s) R πi s,a − V πi (s)<label>(48)</label></formula><formula xml:id="formula_37">s.t. E s∼dµ(s) [D KL (π(·|s)||µ(·|s))] ≤ ,<label>(49)</label></formula><p>a π(a|s) da = 1, ∀ s.</p><p>The Lagrangian of the above objective is given by:</p><formula xml:id="formula_39">L(π, β, α) = i w i E s∼dπ i (s) E a∼π(a|s) R πi s,a − V πi (s) + β − E s∼dµ(s) D KL π(·|s) k i=1 w i d πi (s)π i (·|s) k j=1 w j d πj (s) + s α s 1 − a π(a|s)da ds,<label>(51)</label></formula><p>Solving the Lagrangian following the same procedure as Appendix A leads to an optimal policy of the following form: </p><p>Finally, if π is represented by a function approximator, the optimal policy π * can be projected onto the manifold of parameterized policies by solving the following supervised regression problem arg min π E s,∼dµ(s) [D KL (π * (·|s)||π(·|s))] </p><p>One of the challenges of optimizing the objective in Equation 54 is that computing the expected return in the exponent requires rolling out multiple policies starting from the same state, which would require the environment to be resettable to any given state. Therefore, to obtain a more practical objective, we approximate the expected return across policies using a single rollout from the replay buffer, </p><p>This single-sample estimator results in a biased estimate of the exponentiated advantage, because the expectation with respect to the mixture weights appears in the exponent. But in practice, we find this biased estimator to be effective for our experiments. Therefore, the objective used in practice is given by:</p><formula xml:id="formula_44">arg max π k i=1 w i E s∼dπ i (s) E a∼πi(a|s) log π(a|s) exp 1 β R πi s,a − j w j d πj (s)V πj (s) j w j d πj (s) ,<label>(56)</label></formula><p>where the expectations can be approximated by simply sampling from D following Line 6 of Algorithm 1. Note, the baseline in the exponent now consists of an average of the value functions of the different policies. One approach for estimating this quantity would be to fit separate value functions V πi for each policy. However, if only a small amount of data is available from each policy, then V πi could be highly inaccurate. Therefore, instead of learning separate value functions, we fit a single mean value functionV (s) that directly estimates the weighted average of V πi 's, V = arg min </p><p>This loss can also be approximated by simply sampling from the replay buffer following Line 5 of Algorithm 1. The optimal solutionV (s) = i widπ i (s)V π i (s) j wj dπ j (s)</p><p>is exactly the baseline in Equation 56.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL SETUP</head><p>In our experiments, the policy is represented by a fully-connected network with 2 hidden layers consisting of 128 and 64 ReLU units respectively <ref type="bibr" target="#b23">(Nair &amp; Hinton, 2010)</ref>, followed by a linear output layer. The value function is modeled by a separate network with a similar architecture, but consists of a single linear output unit for the value. Stochastic gradient descent with momentum is used to update both the policy and value function. The stepsize of the policy and value function are 5×10 −5 and 1×10 −4 respectively, and a momentum of 0.9 is used for both. The temperature is set to β = 0.05 for all experiments, and λ = 0.95 is used for TD(λ). The weight clipping threshold ω max is set to 20. At each iteration, the agent collects a batch of approximately 2000 samples, which are stored in the replay buffer D along with samples from previous iterations. The replay buffer stores 50k of the most recent samples. Updates to the value function and policy are performed by uniformly sampling minibatches of 256 samples from D. The value function is updated with 200 gradient steps per iteration, and the policy is updated with 1000 gradient steps. D LEARNING CURVES <ref type="figure">Figure 8</ref>: Learning curves of the various algorithms when applied to OpenAI Gym tasks. Results are averaged over 5 random seeds. AWR is generally competitive with the best current methods. <ref type="figure">Figure 9</ref>: Learning curves on motion imitation tasks. On these challenging tasks, AWR generally learns faster than PPO and RWR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Complex simulated character trained using advantage-weighted regression. Left: Humanoid performing a spinkick. Right: Dog performing a canter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>γ</head><label></label><figDesc>t r t = E s∼dπ(s) E a∼π(a|s) [r(s, a)] ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Snapshots of AWR policies trained on OpenAI Gym tasks. Our simple algorithm learns effective policies for a diverse set of discrete and continuous control tasks. demo policies. Behaviors learned by the policies are best seen in the supplementary video 1 . Code for our implementation of AWR is available at xbpeng.github.io/projects/AWR/. At each iteration, the agent collects a batch of approximately 2000 samples, which are stored in the replay buffer D along with samples from previous iterations. The replay buffer stores 50k of the most recent samples. Updates to the value function and policy are performed by uniformly sampling minibatches of 256 samples from D. The value function is updated with 200 gradient steps per iteration, and the policy is updated with 1000 steps. Detailed hyperparameter settings are provided in Appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Left: Learning curves comparing AWR with various components removed. Each component appears to contribute to improvements in performance, with the best performance achieved when all components are combined. Right: Learning curves comparing AWR with different capacity replay buffers. AWR remains stable with large replay buffers containing primarily off-policy data from previous iterations of the algorithm. Snapshots of 34 DoF humanoid and 82 DoF dog trained with AWR to imitate reference motion recorded from real world subjects. AWR is able to learn sophisticated skills with characters with large numbers of degrees of freedom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) R µ s,a − V µ (s) da ds,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>π</head><label></label><figDesc>* (a|s) = 1 Z(s) µ(a|s) exp 1 β i w i d πi (s) R πi s,a − V πi (s) j w j d πj (s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>d πi (s) R πi s,a − V πi (s) j w j d πj (s) π(·|s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>i</head><label></label><figDesc>w i d πi (s)R πi s,a j w j d πj (s) ≈ R Ds,a such that (s, a) ∈ D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>s,∼dπ i (s) E a∼πi(a|s) ||R πi s,a − V (s)|| 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Final returns for different algorithms on the OpenAI Gym tasks, with ± corresponding to one standard deviation of the average return across 5 random seeds. In terms of final performance, AWR generally performs comparably or better than prior methods.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">While we attempted to compare to MPO<ref type="bibr" target="#b0">(Abdolmaleki et al., 2018)</ref>, we were unable to find source code for an implementation that reproduces results comparable to those reported by<ref type="bibr" target="#b0">Abdolmaleki et al. (2018)</ref>, and could not implement the algorithm such that it achieves similar performance to those reported by the authors.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Abhishek Gupta and Aurick Zhou for insightful discussions. This research was supported an NSERC Postgraduate Scholarship, a Berkeley Fellowship for Graduate Study, Berkeley</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1ANxQW0b" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Openai gym</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cmu graphics lab motion capture database</title>
		<ptr target="http://mocap.cs.cmu.edu/" />
		<imprint/>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<ptr target="https://github.com/openai/baselines" />
		<editor>Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Benchmarking deep reinforcement learning for continuous control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/duan16.html" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diagnosing bottlenecks in deep qlearning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/fu19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Addressing function approximation error in actor-critic methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Herke Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meger</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/fujimoto18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Off-policy deep reinforcement learning without exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/fujimoto19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Continuous deep q-learning with model-based acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/gu16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>Maria Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/haarnoja18b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/haarnoja18b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive importance sampling for value function approximation in off-policy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotaka</forename><surname>Hachiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Akiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiayma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2009.01.002</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neunet.2009.01.002" />
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1399" to="1410" />
			<date type="published" when="2009-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double qlearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3016100.3016191" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, AAAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2094" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Emergence of locomotion behaviours in rich environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Dhruva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Lemmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno>abs/1707.02286</idno>
		<ptr target="http://arxiv.org/abs/1707.02286" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<idno>abs/1710.02298</idno>
		<ptr target="http://arxiv.org/abs/1710.02298" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approximately optimal approximate reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<idno>1-55860- 873-7</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=645531.656005" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Conference on Machine Learning, ICML &apos;02</title>
		<meeting>the Nineteenth International Conference on Machine Learning, ICML &apos;02<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Policy search for motor primitives in robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">R</forename><surname>Peters</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Stabilizing off-policy q-learning via bootstrapping error reduction. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.00949" />
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<title level="m">Continuous control with deep reinforcement learning. ICLR</title>
		<meeting><address><addrLine>Nicolas Manfred Otto Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Off-policy policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Meuleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Peshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kee</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
		<idno>00280836</idno>
		<ptr target="http://dx.doi.org/10.1038/nature14236" />
	</analytic>
	<monogr>
		<title level="j">Ioannis Antonoglou</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
	<note>Nature</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Safe and efficient offpolicy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<idno>978-1-5108-3881-9</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=3157096" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1054" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning gaussian policies from smoothed action value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>978-1-60558-907-7</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=3104322.3104425" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML&apos;10</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning, ICML&apos;10<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational inference for policy search in changing situations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
		<idno>978-1-4503-0619-5</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=3104482.3104585" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fitted q-iteration by advantage weighted regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">R</forename><surname>Peters</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/3501-fitted-q-iteration-by-advantage-weighted-regression.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Curiosity-driven exploration by self-supervised prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepmimic: Exampleguided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/3197517.3201311</idno>
		<ptr target="http://doi.acm.org/10.1145/3197517.3201311" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reinforcement learning by reward-weighted regression for operational space control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1273496.1273590</idno>
		<idno>978-1-59593-793-3. doi: 10.1145/ 1273496.1273590</idno>
		<ptr target="http://doi.acm.org/10.1145/1273496.1273590" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
		<meeting>the 24th International Conference on Machine Learning, ICML &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="745" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relative entropy policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Mülling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altün</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2898607.2898863" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI&apos;10</title>
		<meeting>the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI&apos;10</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1607" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitchyr</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rlkit</surname></persName>
		</author>
		<ptr target="https://github.com/vitchyr/rlkit" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Off-policy temporal difference learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<idno>1-55860-778-1</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=645530" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems (RSS)</title>
		<meeting>Robotics: Science and Systems (RSS)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/schulman15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms. CoRR, abs/1707.06347</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1707.06347" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sfu motion capture database</title>
		<ptr target="http://mocap.cs.sfu.ca/" />
		<imprint/>
		<respStmt>
			<orgName>SFU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Introduction to Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">0262193981</biblScope>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. A. Solla, T. K. Leen, and K. Müller</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Sample efficient actor-critic with experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<idno>abs/1611.01224</idno>
		<ptr target="http://arxiv.org/abs/1611.01224" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00992696</idno>
		<idno>0885-6125. doi: 10.1007/ BF00992696</idno>
		<ptr target="https://doi.org/10.1007/BF00992696" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mode-adaptive neural networks for quadruped motion control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/3197517.3201366</idno>
		<ptr target="http://doi.acm.org/10.1145/3197517.3201366" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
