<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Recurrent Neural Network Grammars</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
							<email>yoonkim@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University ‡ University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<email>srush@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University ‡ University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
							<email>leiyu@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University ‡ University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
							<email>akuncoro@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University ‡ University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University ‡ University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
							<email>melisgl@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University ‡ University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Recurrent Neural Network Grammars</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural network grammars (RNNG) are generative models of language which jointly model syntax and surface structure by incrementally generating a syntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs achieve strong language modeling and parsing performance, but require an annotated corpus of parse trees. In this work, we experiment with unsupervised learning of RNNGs. Since directly marginalizing over the space of latent trees is intractable, we instead apply amortized variational inference. To maximize the evidence lower bound, we develop an inference network parameterized as a neural CRF constituency parser. On language modeling, unsupervised RNNGs perform as well their supervised counterparts on benchmarks in English and Chinese. On constituency grammar induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural network grammars (RNNGs) <ref type="bibr" target="#b22">(Dyer et al., 2016)</ref> model sentences by first generating a nested, hierarchical syntactic structure which is used to construct a context representation to be conditioned upon for upcoming words. Supervised RNNGs have been shown to outperform standard sequential language models, achieve excellent results on parsing <ref type="bibr" target="#b22">(Dyer et al., 2016;</ref><ref type="bibr" target="#b46">Kuncoro et al., 2017)</ref>, better encode syntactic properties of language , and correlate with electrophysiological responses in the human brain . However, these all require annotated syntactic trees for training. In this work, we explore unsupervised learning of recurrent neural network grammars for language modeling and grammar induction.</p><p>Work done while the first author was an intern at DeepMind. Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model p θ (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequence), and maximize the log marginal likelihood log p θ (x) = log z p θ (x, z). Successful approaches to unsupervised parsing have made strong conditional independence assumptions (e.g. context-freeness) and employed auxiliary objectives <ref type="bibr" target="#b44">(Klein and Manning, 2002)</ref> or priors <ref type="bibr" target="#b37">(Johnson et al., 2007)</ref>. These strategies imbue the learning process with inductive biases that guide the model to discover meaningful structures while allowing tractable algorithms for marginalization; however, they come at the expense of language modeling performance, particularly compared to sequential neural models that make no independence assumptions.</p><p>Like RNN language models, RNNGs make no independence assumptions. Instead they encode structural bias through operations that compose linguistic constituents. The lack of independence assumptions contributes to the strong language modeling performance of RNNGs, but make unsupervised learning challenging. First, marginalization is intractable. Second, the biases imposed by the RNNG are relatively weak compared to those imposed by models like PCFGs. There is little pressure for non-trivial tree structure to emerge during unsupervised RNNG (URNNG) learning.</p><p>In this work, we explore a technique for handling intractable marginalization while also injecting inductive bias. Specifically we employ amortized variational inference <ref type="bibr" target="#b42">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b69">Rezende et al., 2014;</ref><ref type="bibr" target="#b59">Mnih and Gregor, 2014)</ref> with a structured inference network. Variational inference lets us tractably optimize a lower bound on the log marginal likelihood, while employing a structured inference network encourages non-trivial structure. In particular, a con-ditional random field (CRF) constituency parser <ref type="bibr" target="#b25">(Finkel et al., 2008;</ref><ref type="bibr" target="#b20">Durrett and Klein, 2015)</ref>, which makes significant independence assumptions, acts as a guide on the generative model to learn meaningful trees through regularizing the posterior <ref type="bibr" target="#b27">(Ganchev et al., 2010)</ref>.</p><p>We experiment with URNNGs on English and Chinese and observe that they perform well as language models compared to their supervised counterparts and standard neural LMs. In terms of grammar induction, they are competitive with recently-proposed neural architectures that discover tree-like structures through gated attention <ref type="bibr" target="#b71">(Shen et al., 2018)</ref>. Our results, along with other recent work on joint language modeling/structure learning with deep networks <ref type="bibr" target="#b71">(Shen et al., 2018</ref><ref type="bibr" target="#b72">(Shen et al., , 2019</ref><ref type="bibr" target="#b86">Wiseman et al., 2018;</ref><ref type="bibr" target="#b39">Kawakami et al., 2018)</ref>, suggest that it is possible to learn generative models of language that model the underlying data well (i.e. assign high likelihood to held-out data) and at the same time induce meaningful linguistic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Unsupervised Recurrent Neural Network Grammars</head><p>We use x = [x 1 , . . . , x T ] to denote a sentence of length T , and z ∈ Z T to denote an unlabeled binary parse tree over a sequence of length T , represented as a binary vector of length 2T − 1. Here 0 and 1 correspond to SHIFT and REDUCE actions, explained below. 1 <ref type="figure">Figure 1</ref> presents an overview of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative Model</head><p>An RNNG defines a joint probability distribution p θ (x, z) over sentences x and parse trees z. We consider a simplified version of the original RNNG <ref type="bibr" target="#b22">(Dyer et al., 2016</ref>) by ignoring constituent labels and only considering binary trees. The RNNG utilizes an RNN to parameterize a stack data structure <ref type="bibr" target="#b21">(Dyer et al., 2015)</ref> of partiallycompleted constituents to incrementally build the parse tree while generating terminals. Using the current stack representation, the model samples an action (SHIFT or REDUCE): SHIFT generates a terminal symbol, i.e. word, and shifts it onto the stack, 2 REDUCE pops the last two elements off 1 The cardinality of ZT ⊂ {0, 1} 2T −1 is given by the (T − 1)-th Catalan number, |ZT | = (2T −2)! T !(T −1)! . 2 A better name for SHIFT would be GENERATE (as in <ref type="bibr" target="#b22">Dyer et al. (2016)</ref>), but we use SHIFT to emphasize similarity with the shift-reduce parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Overview of our approach. The inference network q φ (z | x) (left) is a CRF parser which produces a distribution over binary trees (shown in dotted box). Bij are random variables for existence of a constituent spanning i-th and j-th words, whose potentials are the output from a bidirectional LSTM (the global factor ensures that the distribution is only over valid binary trees). The generative model p θ (x, z) (right) is an RNNG which consists of a stack LSTM (from which actions/words are predicted) and a tree LSTM (to obtain constituent representations upon REDUCE). Training involves sampling a binary tree from q φ (z | x), converting it to a sequence of shift/reduce actions (z = [SHIFT, <ref type="bibr">SHIFT, SHIFT, REDUCE, REDUCE, SHIFT, REDUCE]</ref> in the above example), and optimizing the log joint likelihood log p θ (x, z). the stack, composes them, and shifts the composed representation onto the stack.</p><p>Formally, let S = [(0, 0)] be the initial stack. Each item of the stack will be a pair, where the first element is the hidden state of the stack LSTM, and the second element is an input vector, described below. We use top(S) to refer to the top pair in the stack. The push and pop operations are defined imperatively in the usual way. At each time step, the next action z t (SHIFT or REDUCE) is sampled from a Bernoulli distribution parameterized in terms of the current stack representation. Letting (h prev , g prev ) = top(S), we have</p><formula xml:id="formula_0">z t ∼ Bernoulli(p t ), p t = σ(w h prev + b).</formula><p>Subsequent generation depend on z t :</p><p>• If z t = 0 (SHIFT), the model first generates a terminal symbol via sampling from a categorical distribution whose parameters come from an affine transformation and a softmax,</p><formula xml:id="formula_1">x ∼ softmax(Wh prev + b).</formula><p>Then the generated terminal is shifted onto the stack using a stack LSTM,</p><formula xml:id="formula_2">h next = LSTM(e x , h prev ), push(S, (h next , e x )),</formula><p>where e x is the word embedding for x.</p><p>• If z t = 1 (REDUCE), we pop the last two elements off the stack,</p><formula xml:id="formula_3">(h r , g r ) = pop(S), (h l , g l ) = pop(S),</formula><p>and obtain a new representation that combines the left/right constituent representations using a tree LSTM <ref type="bibr" target="#b79">(Tai et al., 2015;</ref><ref type="bibr" target="#b92">Zhu et al., 2015)</ref>, g new = TreeLSTM(g l , g r ).</p><p>Note that we use g l and g r to obtain the new representation instead of h l and h r . <ref type="bibr">3</ref> We then update the stack using g new ,</p><formula xml:id="formula_4">(h prev , g prev ) = top(S), h new = LSTM(g new , h prev ), push(S, (h new , g new ))</formula><p>.</p><p>The generation process continues until an end-ofsentence symbol is generated. The parameters θ of the generative model are w, b, W, b, and the parameters of the stack/tree LSTMs. For a sentence x = [x 1 , . . . , x T ] of length T , the binary parse tree is given by the binary vector z = [z 1 , . . . , z 2T −1 ]. <ref type="bibr">4</ref> The joint log likelihood decomposes as a sum of terminal/action log likelihoods,</p><formula xml:id="formula_5">log p θ (x, z) = T t=1 log p θ (x t | x &lt;t , z &lt;n(t) ) log p θ (x | z) + 2T −1 j=1 log p θ (z j | x &lt;m(j) , z &lt;j ) log p θ (z | x&lt;z) ,<label>(1)</label></formula><p>where z &lt;n(t) refers to all actions before generating the t-th word, and similarly x &lt;m(j) refers to all words generated before taking the j-th action. For brevity, from here on we will use log p θ (x | z) to refer to the first term (terminal log likelihood) and log p θ (z | x &lt;z ) to refer to the second term (action log likelihood) in the above decomposition. 5 3 The update equations for the tree LSTM (and the stack LSTM) also involve cell states in addition to the hidden states. To reduce notational clutter we do not explicitly show the cell states and instead subsume them into g. If one (or both) of the inputs to the tree LSTM is a word embedding, the associated cell state is taken to be zero. See <ref type="bibr" target="#b79">Tai et al. (2015)</ref> for the exact parameterization. <ref type="bibr">4</ref> As it stands, the support of z is {0, 1} 2T −1 , all binary vectors of length 2T − 1. To restrict our distribution to ZT (binary vectors which describe valid trees), we constrain zt to be valid at each time step, which amounts to deterministically choosing zt = 0 (SHIFT) if there are fewer than two elements (not counting the initial zero tuple) on the stack. <ref type="bibr">5</ref> The action log likelihood is the sum of log conditional priors, which is obviously different from the unconditional log prior log p θ (z) = log x p θ (x, z).</p><p>In the supervised case where ground-truth z is available, we can straightforwardly perform gradient-based optimization to maximize the joint log likelihood log p θ (x, z). In the unsupervised case, the standard approach is to maximize the log marginal likelihood,</p><formula xml:id="formula_6">log p θ (x) = log z ∈Z T p θ (x, z ).</formula><p>However this summation is intractable because z t fully depends on all previous actions [z 1 , . . . , z t−1 ]. Even if this summation were tractable, it is not clear that meaningful latent structures would emerge given the lack of explicit independence assumptions in the RNNG (e.g. it is clearly not context-free). We handle these issues with amortized variational inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Amortized Variational Inference</head><p>Amortized variational inference (Kingma and Welling, 2014) defines a trainable inference network φ that parameterizes q φ (z | x), a variational posterior distribution, in this case over parse trees z given the sentence x. This distribution is used to form an evidence lower bound (ELBO) on the log marginal likelihood,</p><formula xml:id="formula_7">ELBO(θ, φ; x) = E q φ (z | x) log p θ (x, z) q φ (z | x) .</formula><p>We maximize the ELBO with respect to both model parameters θ and inference network parameters φ. The ELBO is still intractable to calculate exactly, but this formulation will allow us to obtain unbiased gradient estimators based on Monte Carlo sampling.</p><p>Observe that rearranging the ELBO gives the following optimization problem,</p><formula xml:id="formula_8">max θ,φ log p θ (x) − KL[q φ (z | x) p θ (z | x)].</formula><p>Thus, φ is trained to match the variational posterior q φ (z | x) to the true posterior p θ (z | x), but θ is also trained to match the true posterior to the variational posterior. Indeed, there is some evidence to suggest that generative models trained with amortized variational inference (i.e. variational autoencoders) learn posterior distributions that are close to the variational family <ref type="bibr" target="#b17">(Cremer et al., 2018)</ref>.</p><p>We can use this to our advantage with an inference network that injects inductive bias. We propose to do this by using a context-free model for the inference network, in particular, a neural CRF parser <ref type="bibr" target="#b20">(Durrett and Klein, 2015)</ref>. This choice can seen as a form of posterior regularization that limits posterior flexibility of the overly powerful RNNG generative model. <ref type="bibr">6,</ref><ref type="bibr">7</ref> The parameterization of span scores is similar to recent works <ref type="bibr" target="#b82">(Wang and Chang, 2016;</ref><ref type="bibr" target="#b43">Kitaev and Klein, 2018)</ref>: we add position embeddings to word embeddings and run a bidirectional LSTM over the input representations to obtain the forward</p><formula xml:id="formula_9">[ − → h 1 , . . . , − → h T ] and backward [ ← − h 1 , . . . , ← − h T ] hidden states.</formula><p>The score s ij ∈ R for a constituent spanning x i to x j is given by,</p><formula xml:id="formula_10">s ij = MLP([ − → h j+1 − − → h i ; ← − h i−1 − ← − h j ]).</formula><p>Letting B be the binary matrix representation of a tree (B ij = 1 means there is a constituent spanning x i and x j ), the CRF parser defines a distribution over binary trees via the Gibbs distribution,</p><formula xml:id="formula_11">q φ (B | x) = 1 Z T (x) exp i≤j B ij s ij , where Z T (x) is the partition function, Z T (x) = B ∈B T exp i≤j B ij s ij ,</formula><p>and φ denotes the parameters of the inference network (i.e. the bidirectional LSTM and the MLP). Calculating Z T (x) requires a summation over an exponentially-sized set B T ⊂ {0, 1} T ×T , the set of all binary trees over a length T sequence. However we can perform the summation in O(T 3 ) using the inside algorithm <ref type="bibr" target="#b4">(Baker, 1979)</ref>, shown in 6 While it has a similar goal, this formulation differs the from posterior regularization as formulated by <ref type="bibr" target="#b27">Ganchev et al. (2010)</ref>, which constrains the distributional family via linear constraints on posterior expectations. In our case, the conditional independence assumptions in the CRF lead to a curved exponential family where the vector of natural parameters has fewer dimensions than the vector of sufficient statistics of the full exponential family. This curved exponential family is a subset of the marginal polytope of the full exponential family, but it is an intersection of both linear and nonlinear manifolds, and therefore cannot be characterized through linear constraints over posterior expectations. <ref type="bibr">7</ref> In preliminary experiments, we also attempted to learn latent trees with a transition-based parser (which does not make explicit independence assumptions) that looks at the entire sentence. However we found that under this setup, the inference network degenerated into a local minimum whereby it always generated left-branching trees despite various optimization strategies. <ref type="bibr" target="#b84">Williams et al. (2018)</ref> observe a similar phenomenon in the context of learning latent trees for classification tasks. However <ref type="bibr" target="#b48">Li et al. (2019)</ref> find that it is possible use a transition-based parser as the inference network for dependency grammar induction, if the inference network is constrained via posterior regularization <ref type="bibr" target="#b27">(Ganchev et al., 2010)</ref> based on universal syntactic rules <ref type="bibr" target="#b61">(Naseem et al., 2010)</ref>.</p><p>Algorithm 1 Inside algorithm for calculating ZT (x) 1: procedure INSIDE(s) scores sij for i ≤ j 2:</p><p>for i := 1 to T do length-1 spans 3: β[i, i] = exp(sii) 4:</p><p>for := 1 to T − 1 do span length 5:</p><p>for i := 1 to T − do span start 6:</p><formula xml:id="formula_12">j = i + span end 7: β[i, j] = j−1 k=i exp(sij) · β[i, k] · β[k + 1, j] 8: return β[1, T ] return partition function ZT (x)</formula><p>Algorithm 1. This computation is itself differentiable and amenable to gradient-based optimization. Finally, letting f : B T → Z T be the bijection between the binary tree matrix representation and a sequence of SHIFT/REDUCE actions, the inference network defines a distribution over</p><formula xml:id="formula_13">Z T via q φ (z | x) q φ (f −1 (z) | x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization</head><p>For optimization, we use the following variant of the ELBO,</p><formula xml:id="formula_14">E q φ (z | x) [log p θ (x, z)] + H[q φ (z | x)], where H[q φ (z | x)] = E q φ (z | x) [− log q φ (z | x)]</formula><p>is the entropy of the variational posterior. A Monte Carlo estimate for the gradient with respect to θ is</p><formula xml:id="formula_15">∇ θ ELBO(θ, φ; x) ≈ 1 K K k=1 ∇ θ log p θ (x, z (k) ), with samples z (1) , . . . , z (K) from q φ (z | x)</formula><p>. Sampling uses the intermediate values calculated during the inside algorithm to sample split points recursively <ref type="bibr" target="#b29">(Goodman, 1998;</ref><ref type="bibr" target="#b26">Finkel et al., 2006)</ref>, as shown in Algorithm 2. The gradient with respect to φ involves two parts. </p><formula xml:id="formula_16">E q φ (z | x) [log p θ (x, z)]</formula><p>is obtained via the score function gradient estimator <ref type="bibr" target="#b28">(Glynn, 1987;</ref><ref type="bibr" target="#b85">Williams, 1992)</ref>,</p><formula xml:id="formula_17">∇ φ E q φ (z | x) [log p θ (x, z)] = E q φ (z | x) [log p θ (x, z)∇ φ log q φ (z | x)] ≈ 1 K K k=1 log p θ (x, z (k) )∇ φ log q φ (z (k) | x).</formula><p>Algorithm 2 Top-down sampling a tree from q φ (z | x)</p><formula xml:id="formula_18">1: procedure SAMPLE(β) β from running INSIDE(s) 2: B = 0 binary matrix representation of tree 3: Q = [(1, T )] queue of constituents 4:</formula><p>while Q is not empty do 5:</p><formula xml:id="formula_19">(i, j) = pop(Q) 6: τ = j−1 k=i β[i, k] · β[k + 1, j] 7:</formula><p>for k := i to j − 1 do get distribution over splits 8:</p><formula xml:id="formula_20">w k = (β[i, k] · β[k + 1, j])/τ 9: k ∼ Cat([wi, . . . , wj−1])</formula><p>sample a split point 10:</p><formula xml:id="formula_21">B i,k = 1, B k+1,j = 1 update B 11:</formula><p>if k &gt; i then if left child has width &gt; 1 12:</p><p>push(Q, (i, k)) add to queue 13:</p><p>if k + 1 &lt; j then if right child has width &gt; 1 14:</p><p>push(Q, (k + 1, j)) add to queue 15: z = f (B) f : BT → ZT maps matrix representation of tree to sequence of actions. 16:</p><p>return z</p><p>The above estimator is unbiased but typically suffers from high variance. To reduce variance, we use a control variate derived from an average of the other samples' joint likelihoods <ref type="bibr" target="#b60">(Mnih and Rezende, 2016)</ref>, yielding the following estimator,</p><formula xml:id="formula_22">1 K K k=1 (log p θ (x, z (k) ) − r (k) )∇ φ log q φ (z (k) | x), where r (k) = 1 K−1 j =k log p θ (x, z (j)</formula><p>). This control variate worked better than alternatives such as estimates of baselines from an auxiliary network <ref type="bibr" target="#b59">(Mnih and Gregor, 2014;</ref><ref type="bibr" target="#b18">Deng et al., 2018)</ref> or a language model <ref type="bibr" target="#b90">(Yin et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>For English we use the Penn Treebank <ref type="bibr">(Marcus et al., 1993, PTB)</ref> with splits and preprocessing from <ref type="bibr" target="#b22">Dyer et al. (2016)</ref> which retains punctuation and replaces singleton words with Berkeley parser's mapping rules, resulting in a vocabulary of 23,815 word types. 10 Notably this is much larger than the standard PTB LM setup from <ref type="bibr" target="#b58">Mikolov et al. (2010)</ref> which uses 10K types. 11 Also different from the LM setup, we model each sentence separately instead of carrying information across sentence boundaries, as the RNNG is a generative model of sentences. Hence our perplexity numbers are not comparable to the PTB LM results <ref type="bibr" target="#b56">(Melis et al., 2018;</ref><ref type="bibr" target="#b57">Merity et al., 2018;</ref>.</p><p>Since the PTB is rather small, and since the URNNG does not require annotation, we also test our approach on a subset of the one billion word for l := 1 to T − 1 do span length 5:</p><p>for i := 1 to T − l do span start 6:</p><formula xml:id="formula_23">j = i + l span end 7: τ = j−1 u=i β[i, u] · β[u + 1, j] 8: for u := i to j − 1 do 9: wu = (β[i, u] · β[u + 1, j])/τ 10: H[i, j] = j−1 u=i (H[i, u] + H[u + 1, j] 11: − log wu) · wu 12: return H[1, T ] return tree entropy H[q φ (z | x)]</formula><p>corpus <ref type="bibr" target="#b11">(Chelba et al., 2013)</ref>. We randomly sample 1M sentences for training and 2K sentences for validation/test, and limit the vocabulary to 30K word types. While still a subset of the full corpus (which has 30M sentences), this dataset is two orders of magnitude larger than PTB. Experiments on Chinese utilize version 5.1 of the Chinese Penn Treebank (CTB) <ref type="bibr" target="#b87">(Xue et al., 2005)</ref>, with the same splits as in <ref type="bibr" target="#b12">Chen and Manning (2014)</ref>. Singleton words are replaced with a single UNK token, resulting in a vocabulary of 17,489 word types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training and Hyperparameters</head><p>The stack LSTM has two layers with input/hidden size equal to 650 and dropout of 0.5. The tree LSTM also has 650 units. The inference network uses a one-layer bidirectional LSTM with 256 hidden units, and the MLP (to produce span scores s ij for i ≤ j) has a single hidden layer with a ReLU nonlinearity followed by layer normalization <ref type="bibr" target="#b3">(Ba et al., 2016)</ref> and dropout of 0.5. We share word embeddings between the generative model and the inference network, and also tie weights between the input/output word embeddings <ref type="bibr" target="#b67">(Press and Wolf, 2016)</ref>. Optimization of the model itself required standard techniques for avoiding posterior collapse in VAEs. <ref type="bibr">12</ref> We warm-up the ELBO objective by linearly annealing (per batch) the weight on the conditional prior log p θ (z | x &lt;z ) and the entropy H[q φ (z | x)] from 0 to 1 over the first two epochs (see equation (1) for definition of log p θ (z | x &lt;z )). This is analogous to KL-annealing in VAEs with continuous latent variables <ref type="bibr" target="#b6">(Bowman et al., 2016;</ref><ref type="bibr" target="#b75">Sønderby et al., 2016)</ref>. We train for 18 epochs (enough for convergence for all models) with a batch size of 16 and K = 8 samples for the Monte Carlo gradient estimators. The generative model is optimized with SGD with learning rate equal to 1, except for the affine layer that produces a distribution over the actions, which has learning rate 0.1. Gradients of the generative model are clipped at 5. The inference network is optimized with Adam (Kingma and <ref type="bibr" target="#b41">Ba, 2015)</ref> with learning rate 0.0001, β 1 = 0.9, β 2 = 0.999, and gradient clipping at 1. As Adam converges significantly faster than SGD (even with a much lower learning rate), we stop training the inference network after the first two epochs. Initial model parameters are sampled from U[−0.1, 0.1]. The learning rate starts decaying by a factor of 2 each epoch after the first epoch at which validation performance does not improve, but this learning rate decay is not triggered for the first eight epochs to ensure adequate training. We use the same hyperparameters/training setup for both PTB and CTB. For experiments on (the subset of) the one billion word corpus, we use a smaller dropout rate of 0.1. The baseline RNNLM also uses the smaller dropout rate.</p><p>All models are trained with an end-of-sentence token, but for perplexity calculation these tokens are not counted to be comparable to prior work <ref type="bibr" target="#b22">(Dyer et al., 2016;</ref><ref type="bibr" target="#b46">Kuncoro et al., 2017;</ref><ref type="bibr" target="#b9">Buys and Blunsom, 2018)</ref>. To be more precise, the inference network does not make use of the end-of-sentence token to produce parse trees, but the generative model is trained to generate the end-of-sentence token after the final REDUCE operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We compare the unsupervised RNNG (URNNG) against several baselines: (1) RNNLM, a standard RNN language model whose size is the same as URNNG's stack LSTM; (2) Parsing Reading Predict Network (PRPN) <ref type="bibr" target="#b71">(Shen et al., 2018)</ref>, a neural language model that uses gated attention layers to embed soft tree-like structures into a neural network (and among the current state-of-the-art in grammar induction from words on the full corpus); (3) RNNG with trivial trees (left branching, right branching, random); (4) supervised RNNG trained on unlabeled, binarized gold trees. 13 Note that the supervised RNNG also trains a discriminative parser q φ (z | x) (alongside the generative model p θ (x, z)) in order to sample parse forests for perplexity evaluation (i.e. importance sampling). This discriminative parser has the same ar- <ref type="bibr">13</ref> We use right branching binarization- <ref type="bibr" target="#b55">Matsuzaki et al. (2005)</ref> find that differences between various binarization schemes have marginal impact. Our supervised RNNG therefore differs the original RNNG, which trains on nonbinarized trees and does not ignore constituent labels. </p><formula xml:id="formula_24">(x) ≈ log 1 K K k=1 log p(x,z (k) ) q φ (z (k) | x)</formula><p>. During evaluation only, we also flatten q φ (z | x) by dividing span scores sij by a temperature term 2.0 before feeding it to the CRF. <ref type="bibr">15</ref> Using the code from https://github.com/yikangshen/ PRPN, we tuned model size, initialization, dropout, learning rate, and use of batch normalization. <ref type="bibr">16</ref> RNNG is trained to maximize log p θ (x, z) while URNNG is trained to maximize (a lower bound on) the language modeling objective log p θ (x). modeling of syntax helps generalization even with richly-parameterized neural models. Encouraged by these observations, we also experiment with a hybrid approach where we train a supervised RNNG first and continue fine-tuning the model (including the inference network) on the URNNG objective (RNNG → URNNG in <ref type="table">Table 1</ref>). 17 This approach results in nontrivial perplexity improvements, and suggests that it is potentially possible to improve language models with supervision on parsed data. In <ref type="figure" target="#fig_0">Figure 2</ref> we show perplexity by sentence length. We find that a standard language model (RNNLM) is better at modeling short sentences, but underperforms models that explicitly take into account structure (RNNG/URNNG) when the sentence length is greater than 10. Table 2 (top) compares our results against prior work on this version of the PTB, and <ref type="table" target="#tab_4">Table 2</ref> (bottom) shows the results on a 1M sentence subset of the one billion word corpus, which is two orders of magnitude larger than PTB. On this larger dataset URNNG still improves upon the RNNLM. We also trained an RNNG (and RNNG → URNNG) on this dataset by parsing the training set with the self-attentive parser from <ref type="bibr" target="#b43">Kitaev and Klein (2018)</ref>. <ref type="bibr">18</ref> These models improve upon the RNNLM but not the URNNG, potentially highlighting the limitations of using predicted trees for supervising RNNGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Grammar Induction</head><p>Table 1 also shows the F 1 scores for grammar induction. Note that we induce latent trees directly from words on the full dataset. 19 For 17 We fine-tune for 10 epochs and use a smaller learning rate of 0.1 for the generative model. <ref type="bibr">18</ref> To parse the training set we use the benepar en2 model from https://github.com/nikitakit/self-attentive-parser, which obtains an F1 score of 95.17 on the PTB test set. <ref type="bibr">19</ref> Past work on grammar induction usually train/evaluate on short sentences and also assume access to gold POS tags <ref type="bibr" target="#b44">(Klein and Manning, 2002;</ref><ref type="bibr" target="#b74">Smith and Eisner, 2004;</ref><ref type="bibr" target="#b5">Bod, 2006)</ref>. However more recent works train directly on words <ref type="bibr" target="#b36">(Jin et al., 2018;</ref><ref type="bibr" target="#b71">Shen et al., 2018;</ref><ref type="bibr" target="#b19">Drozdov et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PTB PPL</head><p>KN 5-gram <ref type="bibr" target="#b22">(Dyer et al., 2016)</ref> 169.3 RNNLM <ref type="bibr" target="#b22">(Dyer et al., 2016)</ref> 113.4 Original RNNG <ref type="bibr" target="#b22">(Dyer et al., 2016)</ref> 102.4 Stack-only RNNG <ref type="bibr" target="#b46">(Kuncoro et al., 2017)</ref> 101.2 Gated-Attention RNNG <ref type="bibr" target="#b46">(Kuncoro et al., 2017)</ref> 100.9 Generative Dep. Parser <ref type="bibr" target="#b8">(Buys and Blunsom, 2015)</ref> 138.6 RNNLM <ref type="bibr" target="#b9">(Buys and Blunsom, 2018)</ref> 100.7 Sup. Syntactic NLM <ref type="bibr" target="#b9">(Buys and Blunsom, 2018)</ref> 107.6 Unsup. Syntactic NLM <ref type="bibr" target="#b9">(Buys and Blunsom, 2018)</ref>    <ref type="bibr" target="#b71">Shen et al. (2018)</ref>, whose hyperparameters were tuned by us. RNNG ‡ is trained on predicted parse trees from the selfattentive parser from <ref type="bibr" target="#b43">Kitaev and Klein (2018)</ref>.</p><p>RNNG/URNNG we obtain the highest scoring tree from q φ (z | x) through the Viterbi inside (i.e. CKY) algorithm. <ref type="bibr">20</ref> We calculate unlabeled F 1 using evalb, which ignores punctuation and discards trivial spans (width-one and sentence spans). <ref type="bibr">21</ref> Since we compare F 1 against the original, non-binarized trees (per convention), F 1 scores of models using oracle binarized trees constitute the upper bounds. We confirm the replication study of <ref type="bibr" target="#b33">Htut et al. (2018)</ref> and find that PRPN is a strong model for grammar induction. URNNG performs on par with PRPN on English but PRPN does better on Chinese; both outperform right branching base-20 Alternatively, we could estimate arg maxz p θ (z | x) by sampling parse trees from q φ (z | x) and using p θ (x, z) to rerank the output, as in <ref type="bibr" target="#b22">Dyer et al. (2016)</ref>. <ref type="bibr">21</ref> Available at https://nlp.cs.nyu.edu/evalb/. We evaluate with COLLINS.prm parameter file and LABELED option equal to 0. We observe that the setup for grammar induction varies widely across different papers: lexicalized vs. unlexicalized; use of punctuation vs. not; separation of train/test sets; counting sentence-level spans for evaluation vs. ignoring them; use of additional data; length cutoff for training/evaluation; corpus-level F1 vs. sentence-level F1; and, more. In our survey of twenty or so papers, almost no two papers were identical in their setup. Such variation makes it difficult to meaningfully compare models across papers. Hence, we report grammar induction results mainly for the models and baselines considered in the present work.   lines. <ref type="table" target="#tab_6">Table 3</ref> further analyzes the learned trees and shows the F 1 score of URNNG trees against other trees (left), and the recall of URNNG/PRPN trees against ground truth constituents (right). We find that trees induced by URNNG and PRPN are quite different; URNNG is more sensitive to SBAR and VP, while PRPN is better at identifying NP. While left as future work, this naturally suggests a hybrid approach wherein the intersection of constituents from URNNG and PRPN is used to create a corpus of partially annotated trees, which can be used to guide another model, e.g. via posterior regularization <ref type="bibr" target="#b27">(Ganchev et al., 2010)</ref> or semisupervision <ref type="bibr" target="#b34">(Hwa, 1999)</ref>. Finally, <ref type="table" target="#tab_7">Table 4</ref> compares our results using the same evaluation setup as in <ref type="bibr" target="#b19">Drozdov et al. (2019)</ref>, which differs considerably from our setup. <ref type="table" target="#tab_9">Table 5</ref> shows some standard metrics related to the learned generative model/inference network. The "reconstruction" perplexity based on</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Distributional Metrics</head><formula xml:id="formula_25">E q φ (z | x) [log p θ (x | z)</formula><p>] is much lower than regular perplexity, and further, the Kullback-Leibler divergence between the conditional prior and the variational posterior, given by   <ref type="formula" target="#formula_5">(1)</ref> for definitions of log p θ (x | z) and log p θ (z | x &lt;z )). This indicates that the latent space is being used in a meaningful way and that there is no posterior collapse <ref type="bibr" target="#b6">(Bowman et al., 2016)</ref>. As expected, the entropy of the variational posterior is much lower than the entropy of the conditional prior, but there is still some uncertainty in the posterior.</p><formula xml:id="formula_26">E q φ (z | x) log q φ (z | x) p θ (z | x &lt;z ) ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Syntactic Evaluation</head><p>We perform a syntactic evaluation of the different models based on the setup from <ref type="bibr" target="#b54">Marvin and Linzen (2018)</ref>: the model is given two minimally different sentences, one grammatical and one ungrammatical, and must identify the grammatical sentence by assigning it higher probability. 22 <ref type="table">Table 6</ref> shows the accuracy results. Overall the supervised RNNG significantly outperforms the other models, indicating opportunities for further work in unsupervised modeling. While the URNNG does slightly outperform an RNNLM, the distribution of errors made from both models are similar, and thus it is not clear whether the outperformance is simply due to better perplexity or learning different structural biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Limitations</head><p>There are several limitations to our approach. For one, the URNNG takes considerably more time/memory to train than a standard language model due to the O(T 3 ) dynamic program in the inference network, multiple samples to obtain low-variance gradient estimators, and dynamic  <ref type="table">Table 6</ref>: Syntactic evaluation based on the setup from <ref type="bibr" target="#b54">Marvin and Linzen (2018)</ref>. Subj. is subject-verb agreement in sentential complement, across prepositional phrase/subjective relative clause, and VP coordination; Obj. Rel. refers to subject-verb agreement in/across an objective relative clause; Refl. refers to reflexive pronoun agreement with antecedent; NPI is negative polarity items. computation graphs that make efficient batching nontrivial. <ref type="bibr">23</ref> The model is sensitive to hyperparameters and required various optimization strategies (e.g. separate optimizers for the inference network and the generative model) to avoid posterior collapse. Finally, the URNNG also seemed to rely heavily on punctuation to identify constituents and we were unable to improve upon a right-branching baseline when training the URNNG on a version of PTB where punctuation is removed. 24</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There has been much work on incorporating tree structures into deep models for syntax-aware language modeling, both for unconditional <ref type="bibr" target="#b23">(Emami and Jelinek, 2005;</ref><ref type="bibr" target="#b8">Buys and Blunsom, 2015;</ref><ref type="bibr" target="#b22">Dyer et al., 2016)</ref> and conditional <ref type="bibr" target="#b89">(Yin and Neubig, 2017;</ref><ref type="bibr" target="#b1">Alvarez-Melis and Jaakkola, 2017;</ref><ref type="bibr" target="#b68">Rabinovich et al., 2017;</ref><ref type="bibr" target="#b0">Aharoni and Goldberg, 2017;</ref><ref type="bibr" target="#b24">Eriguchi et al., 2017;</ref><ref type="bibr" target="#b83">Wang et al., 2018;</ref><ref type="bibr" target="#b30">Gu et al., 2018)</ref> cases. These approaches generally rely on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs. Prior work on combining language modeling and unsupervised tree learning typically embed soft, tree-like structures as hidden layers of a deep net- <ref type="bibr">23</ref> The main time bottleneck is the dynamic compution graph, since the dynamic programming algorithm can be batched (however the latter is a significant memory bottleneck). We manually batch the SHIFT and REDUCE operation as much as possible, though recent work on auto-batching  could potentially make this easier/faster. <ref type="bibr">24</ref> Many prior works that induce trees directly from words often employ additional heuristics based on punctuation <ref type="bibr" target="#b70">(Seginer, 2007;</ref><ref type="bibr" target="#b66">Ponvert et al., 2011;</ref><ref type="bibr" target="#b76">Spitkovsky et al., 2013;</ref><ref type="bibr" target="#b64">Parikh et al., 2014)</ref>, as punctuation (e.g. comma) is usually a reliable signal for start/end of constituent spans. The URNNG still has to learn to rely on punctuation, similar to recent works such as depth-bounded PCFGs <ref type="bibr" target="#b36">(Jin et al., 2018)</ref> and DIORA <ref type="bibr" target="#b19">(Drozdov et al., 2019)</ref>. In contrast, PRPN <ref type="bibr" target="#b71">(Shen et al., 2018)</ref> and Ordered Neurons <ref type="bibr" target="#b72">(Shen et al., 2019)</ref> induce trees by directly training on corpus without punctuation. We also reiterate that punctuation is used during training but ignored during evaluation (except in <ref type="table" target="#tab_7">Table 4</ref>).</p><p>work <ref type="bibr" target="#b13">(Cho et al., 2014;</ref><ref type="bibr" target="#b15">Chung et al., 2017;</ref><ref type="bibr" target="#b71">Shen et al., 2018</ref><ref type="bibr" target="#b72">Shen et al., , 2019</ref>. In contrast, <ref type="bibr" target="#b9">Buys and Blunsom (2018)</ref> make Markov assumptions and perform exact marginalization over latent dependency trees. Our work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers <ref type="bibr" target="#b40">(Kim et al., 2017;</ref><ref type="bibr" target="#b7">Bradbury and Socher, 2017;</ref><ref type="bibr" target="#b80">Tran and Bisk, 2018;</ref><ref type="bibr" target="#b65">Peng et al., 2018;</ref><ref type="bibr" target="#b63">Niculae et al., 2018;</ref>, policy gradient-based approaches <ref type="bibr" target="#b91">(Yogatama et al., 2017;</ref><ref type="bibr" target="#b84">Williams et al., 2018;</ref><ref type="bibr" target="#b32">Havrylov et al., 2019)</ref>, or differentiable relaxations <ref type="bibr" target="#b14">(Choi et al., 2018;</ref><ref type="bibr" target="#b52">Maillard and Clark, 2018)</ref>.</p><p>The variational approximation uses amortized inference <ref type="bibr" target="#b42">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b59">Mnih and Gregor, 2014;</ref><ref type="bibr" target="#b69">Rezende et al., 2014)</ref>, in which an inference network is used to obtain the variational posterior for each observed x. Since our inference network is structured (i.e., a CRF), it is also related to CRF autoencoders <ref type="bibr" target="#b2">(Ammar et al., 2014)</ref> and structured VAEs <ref type="bibr" target="#b38">(Johnson et al., 2016;</ref><ref type="bibr" target="#b45">Krishnan et al., 2017)</ref>, which have been used previously for unsupervised <ref type="bibr" target="#b10">(Cai et al., 2017;</ref><ref type="bibr" target="#b19">Drozdov et al., 2019;</ref><ref type="bibr" target="#b48">Li et al., 2019)</ref> and semi-supervised <ref type="bibr" target="#b90">(Yin et al., 2018;</ref><ref type="bibr" target="#b16">Corro and Titov, 2019)</ref> parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>It is an open question as to whether explicit modeling of syntax significantly helps neural models. <ref type="bibr" target="#b78">Strubell et al. (2018)</ref> find that supervising intermediate attention layers with syntactic heads improves semantic role labeling, while <ref type="bibr" target="#b73">Shi et al. (2018)</ref> observe that for text classification, syntactic trees only have marginal impact. Our work suggests that at least for language modeling, incorporating syntax either via explicit supervision or as latent variables does provide useful inductive biases and improves performance. Finally, in modeling child language acquisition, the complex interaction of the parser and the grammatical knowledge being acquired is the object of much investigation <ref type="bibr" target="#b81">(Trueswell and Gleitman, 2007)</ref>; our work shows that apparently grammatical constraints can emerge from the interaction of a constrained parser and a more general grammar learner, which is an intriguing but underexplored hypothesis for explaining human linguistic biases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Perplexity of the different models grouped by sentence length on PTB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The entropy term H[q φ (z | x)] can be calculated exactly in O(T 3 ), again using the intermediate values from the inside algorithm (see Algorithm 3). 8 Since each step of this dynamic program is differentiable, we can obtain the gradient ∇ φ H[q φ (z | x)] using automatic differentation. 9 An estimator for the gradient with respect to</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 3 Calculating the tree entropy H[q φ (z | x)]</figDesc><table><row><cell cols="2">1: procedure ENTROPY(β)</cell><cell>β from running INSIDE(s)</cell></row><row><cell>2:</cell><cell>for i := 1 to T do</cell><cell>initialize entropy table</cell></row><row><cell>3:</cell><cell>H[i, i] = 0</cell><cell></cell></row><row><cell>4:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: (Top) Comparison of this work as a language model</cell></row><row><cell>against prior works on sentence-level PTB with preprocess-</cell></row><row><cell>ing from Dyer et al. (2016). Note that previous versions</cell></row><row><cell>of RNNG differ from ours in terms of parameterization and</cell></row><row><cell>model size. (Bottom) Results on a subset (1M sentences)</cell></row><row><cell>of the one billion word corpus. PRPN</cell></row></table><note>† is the model from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>(Left) F1 scores of URNNG against other trees.</figDesc><table><row><cell cols="3">"Self" refers to another URNNG trained with a different ran-</cell></row><row><cell cols="3">dom seed. (Right) Recall of constituents by label for URNNG</cell></row><row><cell cols="3">and PRPN. Recall for a particular label is the fraction of</cell></row><row><cell cols="3">ground truth constituents of that label that were identified by</cell></row><row><cell>the model (as in Htut et al. (2018)).</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">F1 +PP</cell></row><row><cell>PRPN-UP  ‡</cell><cell>39.8</cell><cell>45.4</cell></row><row><cell>PRPN-LM  ‡</cell><cell>42.8</cell><cell>42.4</cell></row><row><cell>ON-LSTM  ‡ (Shen et al., 2019)</cell><cell>49.4</cell><cell>−</cell></row><row><cell cols="2">DIORA  ‡ (Drozdov et al., 2019) 49.6</cell><cell>56.2</cell></row><row><cell>PRPN (tuned)</cell><cell>49.0</cell><cell>49.9</cell></row><row><cell>URNNG</cell><cell>52.4</cell><cell>52.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>PTB F1 scores using the same evaluation setup as<ref type="bibr" target="#b19">Drozdov et al. (2019)</ref>, which evaluates against binarized trees, counts punctuation and trivial spans, and uses sentencelevel F1. +PP indicates a post-processing heuristic which directly attaches trailing punctuation to the root. This does not change URNNG results since it learns to do so anyway. Results with ‡ are copied fromTable 1of<ref type="bibr" target="#b19">Drozdov et al. (2019)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Metrics related to the generative model/inference network for RNNG/URNNG. For the supervised RNNG we take the "inference network" to be the discriminative parser trained alongside the generative model (see §3.3). Recon. PPL is the reconstruction perplexity based onE q φ (z | x) [log p θ (x | z)], and KL is the Kullback-Leibler divergence. Prior entropy is the entropy of the conditional prior p θ (z | x&lt;z), and uniform entropy is the entropy of the uniform distribution over all binary trees. The KL/entropy metrics are averaged across sentences.</figDesc><table /><note>is highly nonzero. (See equation</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We adapt the algorithm for calculating tree entropy in PCFGs from<ref type="bibr" target="#b35">Hwa (2000)</ref> to the CRF case.9 ∇ φ H[q φ (z | x)]can also be computed using the insideoutside algorithm and a second-order expectation semiring<ref type="bibr" target="#b49">(Li and Eisner, 2009</ref>), which has the same asymptotic runtime complexity but generally better constants.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/clab/rnng 11 Both versions of the PTB data can be obtained from http: //demo.clab.cs.cmu.edu/cdyer/ptb-lm.tar.gz.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Posterior collapse in our context means that q φ (z | x) always produced trivial (always left or right branching) trees.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">We modify the publicly available dataset from https:// github.com/BeckyMarvin/LM syneval to only keep sentence pairs that did not have any unknown words with respect to our vocabulary, resulting in 80K sentence pairs for evaluation. Further, we train on a much smaller corpus, and hence our results are not directly comparable.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the members of the DeepMind language team for helpful feedback. YK is supported by a Google Fellowship. AR is supported by NSF Career 1845664.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards String-to-Tree Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Treestructured Decoding with Doubly-Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conditional Random Field Autoencoders for Unsupervised Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trainable Grammars for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">K</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Spring Conference of the</title>
		<meeting>the Spring Conference of the</meeting>
		<imprint>
			<publisher>Acoustical Society of America</publisher>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An All-Subtrees Approach to Unsupervised Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating Sentences from a Continuous Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards Neural Machine Translation with Latent Tree Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing</title>
		<meeting>the 2nd Workshop on Structured Prediction for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative Incremental Dependency Parsing with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Syntactic Generative Models with Exact Marginalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CRF Autoencoder for Unsupervised Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Thorsten Brants, Phillipp Koehn, and Tony Robinson</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
	</analytic>
	<monogr>
		<title level="m">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Fast and Accurate Dependency Parser using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to Compose Task-Specific Tree Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical Multiscale Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a Structured Variational Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caio</forename><surname>Corro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inference Suboptimality in Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Latent Alignment and Variational Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Yadev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural CRF Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transition-Based Dependency Parsing with Stack Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Neural Syntactic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="195" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to Parse and Translate Improves Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient, Feature-based, Conditional Random Field Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kleeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Solving the Problem of Cascading Errors: Approximate Bayesian Inference for Linguistic Annotation Pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Posterior Regularization for Structured Latent Variable Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Likelihood Ratio Gradient Estimation: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Glynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Winter Simulation Conference</title>
		<meeting>Winter Simulation Conference</meeting>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Parsing Inside-Out. PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Topdown Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jetic</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><forename type="middle">S</forename><surname>Shavarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Finding Syntax in Human Encephalography with Beam Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Brennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cooperative Learning of Disjoint Syntax and Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serhii</forename><surname>Havrylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grammar Induction with Neural Language Models: An Unusual Replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Phu Mon Htut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Supervised Grammar Induction Using Training Data with Limited Constituent Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sample Selection for Statistical Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised Grammar Induction with Depth-bounded PCFG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TACL</title>
		<meeting>TACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bayesian Inference for PCFGs via Markov chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><forename type="middle">R</forename><surname>Datta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09353</idno>
		<title level="m">Unsupervised Word Discovery with Segmental Neural Language Models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structured Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Constituency Parsing with a Self-Attentive Encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Generative Constituent-Context Model for Improved Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structured Inference Networks for Nonlinear State Space Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">What Do Recurrent Neural Network Grammars Learn About Syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dependency Grammar Induction with a Neural Variational Transition-based Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">First-and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Structured Alignment Networks for Matching Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning Structured Text Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TACL</title>
		<meeting>TACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Latent Tree Learning with Differentiable Parsers: Shift-Reduce Parsing and Chart Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</title>
		<meeting>the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Targeted Syntactic Evaluation of Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Probabilistic CFG with Latent Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the State of the Art of Evaluation in Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Regularizing and Optimizing LSTM Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network Based Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IN-TERSPEECH</title>
		<meeting>IN-TERSPEECH</meeting>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
	<note>Cernocky, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Variational Inference for Monte Carlo Objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Using Universal Linguistic Knowledge to Guide Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">On-the-fly Operation Batching in Dynamic Computation Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07860</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Towards Dynamic Computation Graphs via Sparse Latent Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Spectral Unsupervised Parsing with Additive Tree Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Backpropagating through Structured Argmax using a SPIGOT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Simpled Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elis</forename><surname>Ponvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Using the Output Embedding to Improve Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Abstract Syntax Networks for Code Generation and Semantic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Shakir Mohamed, and Daan Wierstra</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Fast Unsupervised Incremental Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Seginer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Neural Language Modeling by Jointly Learning Syntax and Lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">On Tree-based Neural Sentence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Annealing Techniques for Unsupervised Statistical Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Ladder Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Casper Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Søren Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A Minimal Span-Based Neural Constituency Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Linguistically-Informed Self-Attention for Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Inducing Grammars with and for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Learning to Parse and its Implications for Language Acquisition. The Oxford Handbook of Psycholinguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lila</forename><forename type="middle">R</forename><surname>Trueswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gleitman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Graph-based Dependency Parsing with Bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A Tree-based Decoder for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Pengcheng Yin, and Graham Neubig</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Do Latent Tree Learning Models Identify Meaningful Structure in Sentences?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TACL</title>
		<meeting>TACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Simple Statistical Gradientfollowing Algorithms for Connectionist Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning Neural Templates for Text Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">The Penn Chinese Treebank: Phrase Structure Annotation of a Large Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Fu Dong Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Breaking the Softmax Bottleneck: A High-Rank RNN Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A Syntactic Neural Model for General-Purpose Code Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Learning to Compose Words into Sentences with Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory Over Tree Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
