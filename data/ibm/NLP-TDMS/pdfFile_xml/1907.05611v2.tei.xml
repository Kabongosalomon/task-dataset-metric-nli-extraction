<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
							<email>dinggg@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguang</forename><surname>Lou</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusen</forename><surname>Zhang</surname></persName>
							<email>yusenzhang95@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borje</forename><surname>Karlsson</surname></persName>
							<email>borje.karlsson@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology(BNRist) School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The dominant approaches for named entity recognition (NER) mostly adopt complex recurrent neural networks (RNN), e.g., long-short-term-memory (LSTM). However, RNNs are limited by their recurrent nature in terms of computational efficiency. In contrast, convolutional neural networks (CNN) can fully exploit the GPU parallelism with their feedforward architectures. However, little attention has been paid to performing NER with CNNs, mainly owing to their difficulties in capturing the long-term context information in a sequence. In this paper, we propose a simple but effective CNN-based network for NER, i.e., gated relation network (GRN), which is more capable than common CNNs in capturing long-term context. Specifically, in GRN we firstly employ CNNs to explore the local context features of each word. Then we model the relations between words and use them as gates to fuse local context features into global ones for predicting labels. Without using recurrent layers that process a sentence in a sequential manner, our GRN allows computations to be performed in parallel across the entire sentence. Experiments on two benchmark NER datasets (i.e., CoNLL-2003 and Ontonotes 5.0)  show that, our proposed GRN can achieve state-of-the-art performance with or without external knowledge. It also enjoys lower time costs to train and test. We have made the code publicly available at https://github.com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Named Entity Recognition (NER) is one of the fundamental tasks in natural language processing (NLP). It is designed to locate a word or a phrase that references a specific entity, like person, organization, location, etc., within a text sentence. It plays a critical role in NLP systems for question answering, information retrieval, relation extraction, etc. And many efforts have been dedicated to the field.</p><p>Traditional NER systems mostly adopt machine learning models, such as Hidden Markov Model (HMM) <ref type="bibr" target="#b0">(Bikel et al. 1997)</ref> and Conditional Random Field (CRF) <ref type="bibr" target="#b10">(McCallum and Li 2003)</ref>. Although these systems can achieve high performance, they heavily rely on hand-crafted features or taskspecific resources <ref type="bibr" target="#b10">(Ma and Hovy 2016)</ref>, which are expensive to obtain and hard to adapt to other domains or languages.</p><p>With the development of deep learning, recurrent neural network (RNN) along with its variants have brought great success to the NLP fields, including machine translation, syntactic parsing, relation extraction, etc. RNN has proven to be powerful in learning from basic components of text sentences, like words and characters <ref type="bibr" target="#b21">(Tran, MacKinlay, and Yepes 2017)</ref>. Therefore, currently the vast majority of state-of-the-art NER systems are based on RNNs, especially long-short-term-memory (LSTM) (Hochreiter and Schmidhuber 1997) and its variant Bi-directional LSTM (BiLSTM). For example, <ref type="bibr" target="#b7">Huang et al. (2015)</ref> firstly used a BiLSTM to enhance words' context information for NER and demonstrated its effectiveness.</p><p>However, RNNs process the sentence in a sequential manner, because they typically factor the computation along the positions of the input sequence. As a result, the computation at the current time step is highly dependent on those at previous time steps. This inherently sequential nature of RNNs precludes them from fully exploiting the GPU parallelism on training examples, and thus can lead to higher time costs to train and test.</p><p>Unlike RNNs, convolutional neural network (CNN) can deal with all words in a feed-forward fashion, rather than composing representations incrementally over each word in a sentence. This property enables CNNs to well exploit the GPU parallelism. But in the NER community, little attention has been paid to performing NER with CNNs. It is mainly due to the fact that CNNs have the capacity of capturing local context information but they are not as powerful as LSTMs in capturing the long-term context information. Although the receptive field of CNNs can be expanded by stacking multiple convolution layers or using dilated convolution layers, the global context capturing issue still remains, especially for variant-sized text sentences, which hinders CNNs obtaining a comparable performance as LSTMs for NER.</p><p>In this paper, we propose a CNN-based network for NER, i.e., Gated Relation Network (GRN), which is more powerful than common CNNs for capturing long-term context information. Different from RNNs that capture the long-term dependencies in a recurrent component, our proposed GRN aims to capture the dependencies within a sentence by modelling the relations between any two words. Modelling word relations permits GRN to compose global context features without regard to the limited receptive fields of CNNs, enabling it to capture the global context information. This allows GRN to reach comparable performances in NER versus LSTM-based models. Moreover, without any recurrent layers, GRN can be trained by feeding all words concurrently into the neural network at one time, which can generally improve efficiency in training and test.</p><p>Specifically, the proposed GRN is customized into 4 layers, i.e., the representation layer, the context layer, the relation layer and the CRF layer. In the representation layer, like previous works, a word embedding vector and a character embedding vector extracted by a CNN are used as word features. In the context layer, CNNs with various kernel sizes are employed to transform the word features from the embedding space to the latent space. The various CNNs can capture the local context information at different scales for each word. Then, the relation layer is built on top of the context layer, which aims to compose a global context feature for a word via modelling its relations with all words in the sentence. Finally, we adopt a CRF layer as the loss function to train GRN in an end-to-end manner.</p><p>To verify the effectiveness of the proposed GRN, we conduct extensive experiments on two benchmark NER datasets, i.e., <ref type="bibr">CoNLL-2003</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Traditional NER systems mostly rely on hand-crafted features and task-specific knowledge. In recent years, deep neural networks have shown remarkable success in the NER task, as they are powerful in capturing the syntactic dependencies and semantic information for a sentence. They can also be trained in an end-to-end manner without involving subtle hand-crafted features, thus relieving the efforts of feature engineering. LSTM-based NER System. Currently, most state-of-theart NER systems employ LSTM to extract the context infor-mation for each word. <ref type="bibr" target="#b7">Huang et al. (2015)</ref> firstly proposed to apply a BiLSTM for NER and achieved a great success. Later <ref type="bibr" target="#b10">Ma and Hovy (2016)</ref> and <ref type="bibr" target="#b2">Chiu and Nichols (2016)</ref> introduced character-level representation to enhance the feature representation for each word and gained further performance improvement. <ref type="bibr" target="#b21">MacKinlay et al. (2017)</ref> proposed to stack BiLSTMs with residual connections between different layers of BiLSTM to integrate low-level and high-level features. <ref type="bibr" target="#b9">Liu et al. (2018)</ref> further proposed to enhance the NER model with a task-aware language model. Though effective, the inherently recurrent nature of RNNs/LSTMs makes them hard to be trained with full parallelization. And thus here we propose a CNN-based network, i.e., gated relation network (GRN), to dispense with the recurrence issue. And we show that the proposed GRN can obtain comparable performance as those state-of-theart LSTM-based NER models while enjoying lower training and test time costs.</p><p>Leveraging External Knowledge. It has been shown that external knowledge can greatly benefit NER models. External knowledge can be obtained by means of external vocabulary resources or pretrained knowledge representation, etc. <ref type="bibr" target="#b2">Chiu and Nichols (2016)</ref>  Non-Recurrent Networks in NLP. The efficiency issue of RNNs has started to attract attention from the NLP community. Several effective models have also been proposed to replace RNNs. <ref type="bibr" target="#b5">Gehring et al. (2017)</ref> proposed a convolutional sequence-to-sequence model and achieved significant improvement in both performance and training speed. <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref> used self-attention mechanism for machine translation and obtained remarkable translation performance. Our proposed GRN is also a trial to investigate whether CNNs can get comparable NER performances as LSTM-based models with lower time costs for training and test. And different from <ref type="bibr" target="#b5">(Gehring et al. 2017;</ref><ref type="bibr" target="#b22">Vaswani et al. 2017</ref>), we do not adopt the attention mechanism here, though GRN is a general model and can be customized into the attention mechanism easily. Iterated dilated CNN (ID-CNN), proposed by <ref type="bibr" target="#b18">Strubell et al. (2017)</ref>, also aims to improve the parallelization of NER models by using CNNs, sharing similar ideas to ours. However, although ID-CNN uses dilated CNNs and stacks layers of them, its capacity of modelling the global context information for a variant-sized sentence is still limited, and thus its performance is substantially inferior to those of the state-of-the-art LSTM-based models. In contrast, our proposed GRN can enhance the CNNs with much more capacity to capture global context information, which is mainly attributed to that the relation modelling approach in GRN allows to model long-term dependencies between words without regard to the limited receptive fields of CNNs. And thus GRN can achieve significantly superior performance than ID-CNN.</p><formula xml:id="formula_0">obtained F 1 =91.62%</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Model</head><p>In this section, we discuss the overall NER system utilizing the proposed GRN in detail. To ease the explanation, we organize our system with 4 specific layers, i.e., the representation layer, the context layer, the relation layer and the CRF layer. We will elaborate on these layers from bottom to top in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation Layer</head><p>Representation layer aims to provide informative features for the upper layers. The quality of features has great impacts on the system's performance. Traditionally, features are hand-crafted obeying some elaborative rules that may not be applicable to other domains. Therefore, currently many state-of-the-art approaches tend to employ deep neural networks for automatic feature engineering.</p><p>As previous works like <ref type="bibr" target="#b25">(Ye and Ling 2018)</ref>, the representation layer in GRN is comprised of only word-level features and character-level features. In this paper, we use pretrained static word embeddings, i.e., GloVe 1 <ref type="bibr" target="#b12">(Pennington, Socher, and Manning 2014)</ref>, as the initialized word-level feature. And during training, they will be fine-tuned. Here we denote the input sentence s as s = {s 1 , s 2 , ..., s T }, where s i with i = 1, 2, . . . , T denotes the ith word in the sentence, and T is the length of the sentence. We also use y = {y 1 , y 2 , ..., y T } to denote the corresponding entity labels for all words, i.e., y i corresponding to s i . With each word s i represented as a one-hot vector, its word-level feature w i is extracted as below:</p><formula xml:id="formula_1">w i = E(s i )<label>(1)</label></formula><p>where E is the word embedding dictionary, initialized by the GloVe embeddings and fine-tuned during training. Furthermore, we augment the word representation with the character-level feature, which can contribute to ease the out-of-vocabulary problem <ref type="bibr" target="#b14">(Rei, Crichton, and Pyysalo 2016)</ref>. Same as <ref type="bibr" target="#b10">(Ma and Hovy 2016)</ref>, here we adopt a CNN to extract the character-level feature for each word s i , as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Specifically, the j-th character in the word s i containing n characters is firstly represented as an embedding vector c i j in a similar manner as Eq. 1, except that the character embedding dictionary is initialized randomly. Then we use a convolutional layer to involve the information of neighboring characters for each character, which is critical to exploiting n-gram features. Finally, we perform a max-over-time pooling operation to reduce the convolution results into a single embedding vector c i :  where k is the kernel size of the convolutional layer. Here we fix k = 3 as <ref type="bibr" target="#b25">(Ye and Ling 2018)</ref>. Note that RNNs, especially LSTMs/BiLSTMs are also suitable to model the character-level feature. However, as revealed in <ref type="bibr" target="#b23">(Yang, Liang, and Zhang 2018)</ref>, CNNs are as powerful as RNNs in modelling the character-level feature. Besides, CNNs can probably enjoy higher training and test speed than RNNs. Therefore, in this paper we just adopt a CNN to model the character-level feature.</p><formula xml:id="formula_2">c i j = conv([c i j−k/2 , ..., c i j , ..., c i j+k/2 ]) c i = pooling([c i 0 , ...,c i j , ...,c i n ])<label>(2)</label></formula><p>We regard c i as the character-level feature for the word s i . then we concatenate it to the word-level feature w i to derive the final word feature</p><formula xml:id="formula_3">z i = [c i , w i ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Layer</head><p>Context layer aims to model the local context information among neighboring words for each word. The local context is critical for predicting labels, regarding that there could exist strong dependencies among neighboring words in a sentence. For example, a location word often co-occurs with prepositions like in, on, at. Therefore, it is of necessity to capture the local context information for each word.</p><p>And it is obvious that the local dependencies are not limited within a certain distance. Therefore, we should enable the context layer to be adaptive to different scales of local information. Here, like InceptionNet <ref type="bibr" target="#b19">(Szegedy et al. 2015)</ref>, we design the context layer with different branches, each being comprised of one certain convolutionaly layer. <ref type="figure" target="#fig_2">Figure 2</ref> shows the computational process of the context layer.</p><p>Specifically, we use three convolutional layers with the kernel size being 1, 3, 5, respectively. After obtaining the word feature Z = {z 1 , z 2 , ..., z T } of a sentence s, each branch firstly extracts the local informationz k i within a window-size k for each word s i . Then a max-pooling operation is employed to select the strongest channel-wise signals from all branches. To add the non-linear characteristic, we also apply tanh after each branch.</p><formula xml:id="formula_4">z k i = conv k ([z i−k/2 , ..., z i , ..., c i+k/2 ]) x i = pooling([tanh(z 1 i ), tanh(z 3 i ), tanh(z 5 i )])<label>(3)</label></formula><p>where k ∈ {1, 3, 5} is the kernel size. For each k, we use k/2 zero-paddings to ensure that each word can get the corresponding context feature. Here, we consider the output x i of the context layer as the context feature for word s i . Although with various kernel sizes, the context layer can capture different kinds of local context information, it still struggles to capture the global one. However, we will show that with the gated relation layer described in the following subsection, the global context information can be realized by a fusion of the local one, thus tackling the shortcoming of the context layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Layer</head><p>It has been shown that both short-term and long-term context information in a sequence is very critical in sequence learning tasks. LSTMs leverage the memory and the gating mechanism (Hochreiter and Schmidhuber 1997) to capture both context information and gain significant success. However, conventional CNNs cannot well capture the long-term context information owing to the limited receptive fields, and thus existing CNN-based NER models cannot achieve comparable performance as LSTM-based ones.</p><p>In this subsection, we introduce the gated relation layer in our proposed GRN, which aims to enhance the conventional CNNs with global context information. Specifically, it models the relations between any two words in the sentence. Then, with the gating mechanism, it composes a global context feature vector by weighted-summing up the relation scores with their corresponding local context feature vectors, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Similar to the attention mechanism, our proposed GRN is effective in modelling long-term dependencies without regard to the limited CNN receptive fields. And importantly, GRN can allow computations to be performed in parallel across the entire sentence, which can generally help to reduce the time costs for training and test.</p><p>Given the local context features x = {x 1 , x 2 , ..., x T } from the context layer for a sentence s, the relation layer firstly computes the relation score vector r ij between any two words s i and s j , which is of the same dimension as any x i . Specifically, it firstly concatenates the corresponding context features x i and x j , and then uses a linear function with the weight matrix W rx and the bias vector b rx to obtain r ij : <ref type="bibr" target="#b16">(Santoro et al. 2017)</ref>, we can directly average these relation score vectors as follows:</p><formula xml:id="formula_5">r ij = W rx [x i ; x j ] + b rx (4) Like</formula><formula xml:id="formula_6">r i = 1 T T j=1 r ij<label>(5)</label></formula><p>where r i is the fused global context feature vector for the word s i by the direct feature fusion operation, i.e., averaging in Eq. 5. However, considering that non-entity words generally take up the majority of a sentence, this operation may introduce much noise and mislead the label prediction. To tackle that, we further introduce the gating mechanism, and enable the relation layer to learn to select other dependent words adaptively. Specifically, for the word s i , we firstly normalize all its relation score vectors r ij with a sigmoid function to reduce their biases. Then we sum up the normalized relation score vectors r ij with the corresponding local context feature vector x j ∈ x = {x 1 , x 2 , ..., x T } of any other word s j . And similar to Eq. 5, finally we normalize the sum by the length of the sentence, i.e., T .</p><formula xml:id="formula_7">r i = 1 T T j=1 σ(r ij ) x j<label>(6)</label></formula><p>where σ is a gate using sigmoid function, and means element-wise multiplication. Note that r ij is asymmetrical and different from r ji , and the relation vector w.r.t s i itself, i.e., r ii , is also incorporated in the equation above. Therefore, with r i consisting of all the information of other words in the sentence, it can be seen as the global context feature vector for s i .</p><p>In a way, GRN can be seen as a channel-wise attention mechanism <ref type="bibr" target="#b1">(Chen et al. 2017</ref>). However, instead of using a softmax function, we leverage the gating mechanism on the relation score vectors to decide how all the words play a part in predicting the label for the word s i . We can also customize Eq. 6 to the formula of attention with gating mechanism, where a gate is used to compute the attention weight for a word:</p><formula xml:id="formula_8">α ij = σ(W x [x i ; x j ] + b x ) r i = 1 T T j=1 α ij * x j<label>(7)</label></formula><p>where α ij ∈ R 1 is an attention weight rather than a vector. To distinguish from the proposed GRN (i.e., Eq. 6), we name Eq. 5 as Direct Fusion Network (DFN) and Eq. 7 as Gated Attention Network (GAttN). We will consider DFN and GAttN as two of our baseline models to show the superiority of the proposed GRN.</p><p>Here we also add a non-linear function for r i as follows.</p><formula xml:id="formula_9">p i = tanh(r i )<label>(8)</label></formula><p>And we define p i as the final predicting feature for word s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRF Layer</head><p>Modelling label dependencies is crucial for NER task <ref type="bibr" target="#b10">(Ma and Hovy 2016;</ref><ref type="bibr" target="#b9">Liu et al. 2018)</ref>. Following <ref type="bibr" target="#b10">(Ma and Hovy 2016;</ref><ref type="bibr" target="#b7">Huang, Xu, and Yu 2015)</ref>, we employ a conditional random field (CRF) layer to model the label dependencies and calculate the loss for training GRN. Formally, for a given sentence s = {s 1 , s 2 , ..., s T } and its generic sequence of labels y = {y 1 , y 2 , ..., y T }, we firstly use Y(s) to denote the set of all possible label sequences for s. The CRF model defines a family of conditional probability p(y|s) over all possible label sequences y given s:</p><formula xml:id="formula_10">p(y|s) = T i=1 φ i (y i−1 , y i , s) y ∈Y(s) T i=1 φ i (y i−1 , y i , s) (9)</formula><p>where φ i (y i−1 , y i , s) = exp(f (s i , y , y)) with f being a function that maps words into labels:</p><formula xml:id="formula_11">f (s i , y , y) = W y p i + b y ,y<label>(10)</label></formula><p>where p i is derived as Eq. 8, W y is the predicting weights w.r.t y and b y ,y is the transition weight from y to y. Both W y and b y ,y are parameters to be learned. Loss of the CRF layer is formulated as follows.</p><formula xml:id="formula_12">L = − s log p(y|s)<label>(11)</label></formula><p>And for decoding, we aim to find the label sequence y * with the highest conditional probability: y * = arg max y∈Y(s) p(y|s)</p><p>which can be efficiently derived via Viterbi decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>To verify the effectiveness of the proposed GRN, we conduct extensive experiments on two benchmark NER datasets: CoNLL-2003 English NER <ref type="bibr" target="#b20">(Tjong Kim Sang and De Meulder 2003)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Training</head><p>We implement our proposed GRN with the Pytorch library <ref type="bibr" target="#b11">(Paszke et al. 2017</ref>). And we set the parameters below following <ref type="bibr" target="#b10">(Ma and Hovy 2016)</ref>. Word Embeddings. The dimension of word embedding is set as 100. And as mentioned, we initialize it with Stanford's publicly available GloVe 100-dimensional embeddings. We include all words of GloVe when building the vocabulary, besides those words appearing at least 3 times in the training set. For words out of the vocabulary (denoted as UNK) or those not in GloVe, we initialize their embeddings with kaiming uniform initialization <ref type="bibr" target="#b6">(He et al. 2015)</ref>.</p><p>Character Embeddings. We set the dimension of character embeddings as 30, and also initialize them with kaiming uniform initialization.</p><p>Weight Matrices and Bias Vectors. All weight matrices in linear functions and CNNs are initialized with kaiming uniform initialization, while bias vectors are initialized as 0.</p><p>Optimization. We employ mini-batch stochastic gradient descent with momentum to train the model. The batch size is set as 10. The momentum is set as 0.9 and the initial learning rate is set as 0.02. We use learning rate decay strategy to update the learning rate during training. Namely, we update the learning rate as 0.02 1+ρ * t at the t-th epoch with ρ = 0.02. We train each model on training sets with 200 epochs totally, using dropout = 0.5. For evaluation, we select its best version with the highest performance on the development set and report the corresponding performance on the test set. To reduce the model bias, we carry out 5 runs for each model and report the average performance and the standard deviation.</p><p>Network Structure. The output channel number of the CNN in Eq. 2 and Eq. 3 is set as 30 and 400, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Comparison</head><p>Here we first focus on the NER performance comparison between the proposed GRN and the existing state-of-the-art approaches.</p><p>CoNLL-2003. We compare GRN with various state-ofthe-art LSTM-based NER models, including <ref type="bibr" target="#b9">(Liu et al. 2018;</ref><ref type="bibr" target="#b25">Ye and Ling 2018)</ref>, etc. We also compare GRN with ID-CNN <ref type="bibr" target="#b18">(Strubell et al. 2017)</ref>, which also adopts CNNs without recurrent layers for NER. Furthermore, considering that some state-of-the-art NER models exploit external Model Mean(±std) F 1 Max F 1 Mean P/R <ref type="bibr" target="#b3">(Collobert et al. 2011</ref><ref type="bibr">) 88.67 (Luo et al. 2015</ref> 89.90 <ref type="bibr" target="#b2">(Chiu and Nichols 2016)</ref> 90.91 ± 0.20 90.75 / 91.08 <ref type="bibr" target="#b26">(Zhuo et al. 2016)</ref> 88.12 <ref type="bibr" target="#b14">(Rei, Crichton, and Pyysalo 2016)</ref> 84.09 <ref type="bibr" target="#b7">(Lample et al. 2016)</ref> 90.94 <ref type="bibr" target="#b10">(Ma and Hovy 2016)</ref> 91.21 91.35 / 91.06 <ref type="bibr" target="#b15">(Rei 2017)</ref> 86.26 <ref type="bibr" target="#b27">(Zukov-Gregoric et al. 2017)</ref> 89.83 <ref type="bibr" target="#b8">(Liu, Baldwin, and Cohn 2017)</ref> 89.5 <ref type="bibr" target="#b12">(Peters et al. 2017)</ref> 90.87 <ref type="bibr" target="#b9">(Liu et al. 2018)</ref> 91.24 ± 0.12 91.35 <ref type="bibr" target="#b25">(Ye and Ling 2018)</ref> 91.38 ± 0.10 91.53 ID-CNN <ref type="bibr" target="#b18">(Strubell et al. 2017)</ref> 90   <ref type="table" target="#tab_5">Table 2</ref>, which also includes the max F 1 scores, mean precision and recall values if available. Note that CNN-BiLSTM-CRF is our re-implementation of (Ma and Hovy 2016), and we obtain comparable performance as that reported in the paper. Therefore, by default we directly compare GRN with the reported performance of compared baselines. It should also be noticed that, since the relation layer in GRN can be related to the attention mechanism, here we also include some attention-based baselines, i.e.,, <ref type="bibr" target="#b14">(Rei, Crichton, and Pyysalo 2016)</ref> and <ref type="bibr" target="#b27">(Zukov-Gregoric et al. 2017</ref>), and we further introduce a new baseline termed CNN-BiLSTM-Att-CRF, which adds a self-attention layer for CNN-BiLSTM-CRF as <ref type="bibr" target="#b27">(Zukov-Gregoric et al. 2017)</ref>. As shown in <ref type="table" target="#tab_5">Table 2</ref>, compared with those LSTM-based NER models, the proposed GRN can obtain comparable or even slightly superior performance, with or without the external knowledge, which well demonstrates the effectiveness of GRN. And compared with ID-CNN, our proposed GRN can defeat it at a great margin in terms of F 1 score. We also try to add ELMo to the latest state-of-the-art model of <ref type="bibr" target="#b25">(Ye and Ling 2018)</ref> based on their published codes, and we find that the corresponding F 1 score is 91.79 ± 0.08, which is substantially lower than that of GRN.</p><p>OntoNotes 5.0. On OntoNotes 5.0, we compare the proposed GRN with NER models that also reported performance on it, including <ref type="bibr" target="#b2">(Chiu and Nichols 2016;</ref><ref type="bibr">Shen et al. Model</ref> Mean(±std) F 1 Mean P/R <ref type="bibr" target="#b2">(Chiu and Nichols 2016)</ref> 86.28 ± 0.26 86.04 / 86.53 <ref type="bibr" target="#b17">(Shen et al. 2017)</ref> 86.63 ± 0.49 <ref type="bibr" target="#b4">(Durrett and Klein 2014)</ref> 84.04 85.22 / 82.89 <ref type="bibr">(Passos, Kumar, and McCallum 2014) 82.30 (?)</ref> 83.45 CNN-BiLSTM-Att-CRF 87.25±0.17 ID-CNN <ref type="bibr" target="#b18">(Strubell et al. 2017)</ref> 86.84 ± 0.19 GRN 87.67 ± 0.17 87.79 / 87.56    <ref type="table" target="#tab_6">Table 3</ref>, GRN can obtain the state-of-the-art NER performance on OntoNotes 5.0, which further demonstrates its effectiveness. Overall, the comparison results on both CoNLL-2003 and OntoNotes 5.0 well indicate that our proposed GRN can achieve state-of-the-art NER performance with or without external knowledge. It demonstrates that, using GRN, CNNbased models can compete with LSTM-based ones for NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Here we study the impact of each layer on GRN. Firstly, we analyze the context layer by introducing two baseline models: (1) GRN w/o context: wiping out the context layer and building the relation layer on top of the representation layer directly; (2) GRN w/ branch 3 : removing branches in the context layer, except the one with kernel size = 3. Then to analyze the relation layer and the importance of gating mechanism in it, we compare GRN with: (1) GRN w/o relation: wiping out the relation layer and directly building the CRF layer on top of the context feature; (2) DFN (see Eq. 5);</p><p>(3) GAttN (see Eq. 7). All compared baselines use the same experimental settings as GRN. <ref type="table" target="#tab_7">Table 4</ref> and <ref type="table" target="#tab_8">Table 5</ref> report the experimental results on both datasets, where the last column shows the absolute performance drops compared to GRN.</p><p>As shown in <ref type="table" target="#tab_7">Table 4</ref> and <ref type="table" target="#tab_8">Table 5</ref>, when reducing the number of branches in the context layer, GRN w/o context and GRN w/ branch 3 drop significantly, which indicates that modelling different scales of local context information plays Compared with GRN w/o relation, DFN and GAttN, the proposed GRN defeats them at a substantial margin in terms of F 1 score, which demonstrates that the proposed gated relation layer is beneficial to the performance improvement. The comparison also reveals that the channel-wise gating mechanism in GRN is more powerful than the gated attention approach (i.e., Eq. 7) and the direct fusion approach (i.e., Eq. 5) under the same experimental settings for NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training/Test Time Comparison</head><p>In this section, we further compare the training and test time costs of the proposed GRN with those of CNN-BiLSTM-CRF, which is the most basic LSTM-based NER model achieving high performance. We conduct our experiments on a physical machine with Ubuntu 16.04, 2 Intel Xeon E5-2690 v4 CPUs, and a Tesla P100 GPU. For fair comparison, we keep the representation layer and the CRF layer the same for both models, so that the input and output dimensions for the "BiLSTM layer" in CNN-BiLSTM-CRF would be identical to those of the "context layer + relation layer" in GRN. We train both models with random initialization for a total of 30 epochs, and after each epoch, we evaluate the learned model on the test set. For both training and test, batch size is set as 10 as before. And here we use the average training time per epoch and the average test time to calculate speedups.</p><p>As shown in <ref type="table" target="#tab_10">Table 6</ref>, GRN can obtain a speedup of more than 1.15 during training and around 1.10 during test on both datasets. The speedup may seem not so significant, because the time costs reported here also include those consumed by common representation layer, CRF layer, etc. For reference, the fast ID-CNN with a CRF layer (i.e., ID-CNN-CRF) <ref type="bibr" target="#b18">(Strubell et al. 2017</ref>) was reported to have a test-time speedup of 1.28 over the basic BiLSTM-CRF model on CoNLL-2003. Compared to ID-CNN-CRF, GRN sacrifices some speedup for better performance, and the speedup gap between both is still reasonable. We can also see that the speedup on CoNLL-2003 is larger than that on OntoNotes 5.0, which can be attributed to that the average sentence length of <ref type="bibr">CoNLL-2003 (∼ 14)</ref> is smaller than that of OntoNotes 5.0 (∼ 18) and thus the relation layer in GRN would cost less time for the former. The results above demonstrate that the proposed GRN can generally bring ef-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Relation Visualization</head><p>Since the proposed GRN aims to boost the NER performance by modelling the relations between words, especially long-term ones, we can visualize the gating output in the relation layer to illustrate the interpretability of GRN. Specifically, we utilize the L2 norm of r ij to indicate the extent of relations between the word s i and the word s j . Then we further normalize the values into [0, 1] to build a heat map. <ref type="figure" target="#fig_4">Figure 4</ref> shows a visualization sample. We can find out that the entity words (y-axis) are more related to other entity words as well, even though they may be "far away" from each other in the sentence, like the 1st word "Sun" and the 8th word "Sidek" in the sample. Note that "Sun" and "Sidek" are not in an identical receptive field of any CNN used in our experiments, but their strong correlation can still be exploited with the relation layer in GRN. That concretely illustrates that, by introducing the gated relation layer, GRN is able to capture the long-term dependency between words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a CNN-based network, i.e., gated relation network (GRN), for named entity recognition (NER). Unlike the dominant LSTM-based NER models which process a sentence in a sequential manner, GRN can process all the words concurrently with one forward operation and thus can fully exploit the GPU parallelism for potential efficiency improvement. Besides, compared with common CNNs, GRN has a better capacity of capturing long-term context information. Specifically, GRN introduces a gated relation layer to model the relations between any two words, and utilizes gating mechanism to fuse local context features into global ones for all words. Experiments on CoNLL-2003 English NER and Ontonotes 5.0 datasets show that, GRN can achieve state-of-the-art NER performance with or without external knowledge, meaning that using GRN, CNN-based models can compete with LSTMbased models for NER. Experimental results also show that GRN can generally bring efficiency improvement for training and test.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>on CoNLL-2003 by integrating gazetteers. Peters et al. (2017) adopted a character-level language model pretrained on a large external corpus and gained substantial performance improvement. More recently, Peters et al. (2018) proposed ELMo, a deep language model trained with billions of words, to generate dynamic contextual word features, and gained the latest state-of-the-art performance on CoNLL-2003 by incorporating it into a BiLSTM-based model. Our proposed GRN can also incorporate external knowledge. Specifically, experiments show that, with ELMo incorporated, GRN can obtain even slightly superior performance on the same dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>CNN to extract the character-level feature for a word. Best see in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Branches with various convolutions for extracting the local context feature for words. Best see in colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Gated relation layer in GRN for composing the global context feature for each word. r ij denotes the relation score vector between word s i and word s j . Best see in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Word relation visualization: the x-axis shows the sentence and the y-axis shows the entity words in it. Regions with deeper color means stronger relations between the corresponding pair of words. a crucial role for NER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>=91.44 without external knowledge and F 1 =92.34 with ELMo (Peters et al. 2018) simply incorporated) and Ontonotes 5.0 (F 1 =87.67), meaning that using GRN, CNNbased models can compete with LSTM-based ones for NER. Moreover, GRN can also enjoy lower time costs for training and test, compared to the most basic LSTM-based model.Our contributions are summarized as follows.</figDesc><table><row><cell>English NER and</cell></row><row><cell>OntoNotes 5.0. Experimental results indicate that GRN</cell></row><row><cell>can achieve state-of-the-art performance on both CoNLL-</cell></row><row><cell>2003 (F 1 • We propose a CNN-based network, i.e., gated relation net-</cell></row><row><cell>work (GRN) for NER. GRN is a simple but effective CNN</cell></row><row><cell>architecture with a more powerful capacity of capturing</cell></row><row><cell>the global context information in a sequence than com-</cell></row><row><cell>mon CNNs.</cell></row><row><cell>• We propose an effective approach for GRN to model the</cell></row><row><cell>relations between words, and then use them as gates to</cell></row><row><cell>fuse local context features into global ones for incorpo-</cell></row><row><cell>rating long-term context information.</cell></row><row><cell>• With extensive experiments, we demonstrate that the pro-</cell></row><row><cell>posed CNN-based GRN can achieve state-of-the-art NER</cell></row><row><cell>performance comparable to LSTM-based models, while</cell></row><row><cell>enjoying lower training and test time costs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Statistics of CoNLL-2003 and Ontonotes 5.0.Table 1shows some statistics of both datasets. Following (Ma and Hovy 2016), we use the BIOES sequence labelling scheme instead of BIO for both datasets to train models. As for test, we convert the prediction results back to the BIO scheme and use the standard CoNLL-2003 evaluation script to measure the NER performance, i.e., F 1 scores, etc.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Performance comparison on CoNLL-2003. * in-</cell></row><row><cell>dicates models utilizing external knowledge beside the</cell></row><row><cell>CoNLL-2003 training set and pre-trained word embeddings.</cell></row><row><cell>P/R denotes precision and recall.</cell></row><row><cell>knowledge to boost their performance, here we also report</cell></row><row><cell>the performance of GRN with ELMo (Peters et al. 2018) in-</cell></row><row><cell>corporated as the external knowledge. Note that ELMo is</cell></row><row><cell>trained on a large corpus of text data and can generate dy-</cell></row><row><cell>namic contextual features for words in a sentence. Here we</cell></row><row><cell>simply concatenate the output ELMo features to the word</cell></row><row><cell>feature in GRN. The experimental results are reported in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">: Performance comparison on OntoNotes 5.0. P/R de-</cell></row><row><cell cols="2">notes precision and recall.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell cols="2">Mean(±std) F 1 F 1 Drop</cell></row><row><cell>context</cell><cell>GRN w/o context GRN w/ branch 3</cell><cell>88.36 ± 0.21 90.88 ± 0.22</cell><cell>3.08 0.56</cell></row><row><cell></cell><cell>GRN w/o relation</cell><cell>90.13 ± 0.28</cell><cell>1.31</cell></row><row><cell>relation</cell><cell>DFN</cell><cell>90.72 ± 0.06</cell><cell>0.72</cell></row><row><cell></cell><cell>GAttN</cell><cell>87.11 ± 0.25</cell><cell>4.33</cell></row><row><cell>Full</cell><cell>GRN</cell><cell>91.44 ± 0.16</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study onCoNLL-2003.    </figDesc><table><row><cell></cell><cell>Model</cell><cell cols="2">Mean(±std) F 1 F 1 Drop</cell></row><row><cell>context</cell><cell>GRN w/o context GRN w/ branch 3</cell><cell>82.21 ± 0.23 86.66 ± 0.21</cell><cell>5.46 1.01</cell></row><row><cell></cell><cell>GRN w/o relation</cell><cell>85.87 ± 0.16</cell><cell>1.8</cell></row><row><cell>relation</cell><cell>DFN</cell><cell>85.81 ± 0.14</cell><cell>1.86</cell></row><row><cell></cell><cell>GAttN</cell><cell>79.83 ± 0.37</cell><cell>7.83</cell></row><row><cell>Full</cell><cell>GRN</cell><cell>87.67 ± 0.17</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on OntoNotes 5.0.</figDesc><table /><note>2017; Durrett and Klein 2014), etc. As shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Training/test speedup of GRN compared with CNN-BiLSTM-CRF.ficiency improvement over LSTM-based methods for NER, via fully exploiting the GPU parallelism.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Natural Science Foundation of China (No. 61571269).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nymble: a high-performance learning name-finder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bikel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ANLP</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6298" to="6306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichols</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>and Nichols</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="477" to="490" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Durrett and Klein</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gehring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu ;</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
	<note>Neural architectures for named entity recognition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Capturing long-range contextual dependencies with memory-enhanced conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baldwin</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="555" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empower Sequence Labeling with Task-Aware Neural Language Model</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="188" to="191" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum ; Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Socher</forename><surname>Manning ; Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attending to characters in neural sequence labeling models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Crichton</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pyysalo ; Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Crichton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2121" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Santoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep active learning for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kronrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Representation Learning for NLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="252" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast and accurate entity recognition with iterated dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strubell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2670" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Tjong Kim</forename><surname>Meulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Meulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Named entity recognition with stack residual lstm and trainable bias decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mackinlay</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yepes ;</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="566" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in neural sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang ; Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3879" to="3889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salakhutdinov</forename><surname>Cohen ; Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06345</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hybrid semi-markov crf for neural sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="235" to="240" />
		</imprint>
	</monogr>
	<note>and</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Segment-level sequence modeling using gated recursive semi-markov conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1413" to="1423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural named entity recognition using a self-attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zukov-Gregoric</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">ICTAI</biblScope>
			<biblScope unit="page" from="652" to="656" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
