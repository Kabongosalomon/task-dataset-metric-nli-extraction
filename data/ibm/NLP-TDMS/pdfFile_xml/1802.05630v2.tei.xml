<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CNN+LSTM Architecture for Speech Emotion Recognition with Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Etienne</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIMSI</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Paris-Sud University</orgName>
								<address>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Saclay University</orgName>
								<address>
									<postCode>F-91405</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">DreamQuark</orgName>
								<address>
									<addrLine>29 rue de Courcelles</addrLine>
									<postCode>75008</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Fidanza</surname></persName>
							<email>guillaume.fidanza@dreamquark.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">DreamQuark</orgName>
								<address>
									<addrLine>29 rue de Courcelles</addrLine>
									<postCode>75008</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Petrovskii</surname></persName>
							<email>andrei.petrovskii@dreamquark.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">DreamQuark</orgName>
								<address>
									<addrLine>29 rue de Courcelles</addrLine>
									<postCode>75008</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Devillers</surname></persName>
							<email>laurence.devillers@limsi.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIMSI</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Paris-Sud University</orgName>
								<address>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Saclay University</orgName>
								<address>
									<postCode>F-91405</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Schmauch</surname></persName>
							<email>benoit.schmauch@dreamquark.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">DreamQuark</orgName>
								<address>
									<addrLine>29 rue de Courcelles</addrLine>
									<postCode>75008</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CNN+LSTM Architecture for Speech Emotion Recognition with Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we design a neural network for recognizing emotions in speech, using the IEMOCAP dataset. Following the latest advances in audio analysis, we use an architecture involving both convolutional layers, for extracting high-level features from raw spectrograms, and recurrent ones for aggregating long-term dependencies. We examine the techniques of data augmentation with vocal track length perturbation, layer-wise optimizer adjustment, batch normalization of recurrent layers and obtain highly competitive results of 64.5% for weighted accuracy and 61.7% for unweighted accuracy on four emotions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Providing high quality interaction between a human and a machine is a very challenging and active field of research with numerous applications. An important part of this domain is recognition of human speech emotions by computer systems. In the last years, impressive progress has been achieved in speech recognition by means of deep learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. These achievements also include significant results on speech emotion recognition (SER), see e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. In this work we build a neural network for SER on the IEMOCAP dataset <ref type="bibr" target="#b7">[8]</ref> and achieve the result highly competitive to the state of the art. <ref type="bibr" target="#b0">1</ref> When treating a SER problem with deep learning, one either creates hand-crafted acoustic features (MFCC, pitch, energy, ZCR...), which are used as inputs to a neural network, or sends the data, after some preprocessing (e.g. Fourier transform), directly to a neural network. We apply the second strategy by transforming the audio signal to a spectrogram, which is then used as an input to convolutional layers, followed by recurrent ones. Such a choice of an architecture, which has recently demonstrated very competitive performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>, assumes two main interpretations. On one hand, adding few convolutional layers in the beginning of the network is an efficient way to reduce dimensionality of the data and can significantly simplify <ref type="bibr" target="#b0">1</ref> To our knowledge, the present state of the art has been achieved in <ref type="bibr" target="#b5">[6]</ref>. However the cross-validation procedure performed in this paper (as in other works presenting the results obtained on the IEMOCAP dataset) includes only five folds of the dataset out of the ten possible. On the other hand, our experiments showed (see section 3) that the performance strongly depends on the part of the data which is used for measuring the scores. As a consequence the results obtained by 5-fold cross-validation without clarification what data has been used for the measurement are not possible to compare with. Therefore we propose to use 10-fold cross validation as the correct way for measuring the scores on IEMOCAP dataset and present our results correspondingly. the training procedure. On the other hand, it is also possible to use a deep CNN for extracting high-level features, which are then fed to a RNN for final time aggregation. We test a variety of architectures with different depths for the convolutional (1-6 layers) and recurrent modules (1-4 Bi-LSTM layers), achieving the best scores with a 4+1 scenario 2 .</p><p>To address challenges of class imbalance and data scarcity, we explored a vocal tract length perturbation for the purpose of data augmentation, and showed that it significantly improves the performance. In line with <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> we examined batch normalization applied to the recurrent layers of the network. Finally, we noticed that parameters of convolutional and Bi-LSTM layers are trained at a very different pace. We tried to take advantage of this observation by per-layer adjustment of the update rule parameters, but unfortunately were not able to make a definite conclusion in favor of this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Dataset description</head><p>IEMOCAP (Interactive Emotional Dyadic Motion Capture), collected at the University of Southern California (USC) <ref type="bibr" target="#b7">[8]</ref>, is one of the standard datasets for emotion recognition. It consists of twelve hours of audio and video recordings performed by 10 professional actors (five women and five men) and organized in 5 sessions of dialogues between two actors of different genders, either playing a script or improvising. Each sample of the audio set is an utterance assigned with an emotion label. Labeling was made by six students of USC, three at a time for each utterance. The annotators were allowed to assign multiple labels if necessary. The final true label for each utterance was chosen by majority vote if the emotion category with the highest vote was unique. Since the annotators reached consensus more often when labeling improvised utterances (83.1%) than scripted ones (66.9%) <ref type="bibr" target="#b7">[8]</ref>, we concentrate only on the improvised part of the dataset. For the sake of comparison with the prior stateof-the-art approaches, we predict four of the most represented emotions: neutral, sadness, anger and happiness, which leave us 2280 utterances in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data augmentation</head><p>The IEMOCAP dataset has two main drawbacks: class imbalance (see <ref type="figure" target="#fig_0">Fig. 1</ref>) and small size. To cope with both obstacles, we examined data augmentation by means of vocal tract length perturbation (VTLP), at the same time oversampling the least represented classes of the dataset: happiness and anger. VTLP is based on the speaker normalization technique considered in <ref type="bibr" target="#b12">[13]</ref>, where it was implemented to reduce interspeaker variability. The difference in human's vocal tract length can be modeled by rescaling the peaks of significant formants along the frequency axis with a factor α taking values in the approximate range (0.9, 1.1). Therefore, in order to get rid of this variablility, one should estimate the factor for each speaker and accordingly normalize the spectrograms. Applied inversely, the same idea can be used for data augmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>: in order to generate new samples, one simply has to perform rescaling of the original spectrograms along the frequency axis while keeping the scaling factor in the range (0.9, 1.1). Both approaches, normalization and augmentation, pursue the same objective: to enforce the invariance of the model to speaker-dependent features, since they are not relevant to the classification criterion. Augmentation, however, is easier to implement because we don't need to estimate the scaling factor of each speaker, and therefore we stick to this option.</p><p>Rescaling of frequencies has been performed as follows <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_0">G(f ) = αf 0 ≤ f ≤ f0 fmax−αf 0 fmax−f 0 (f − f0) + αf0 f0 ≤ f ≤ fmax,<label>(1)</label></formula><p>where fmax is the upper cut-off frequency and f0 is defined to be larger than the highest significant formants (we took f 0 fmax = 0.9). Therefore, we rescale the frequencies below f0 with α ∈ (0.9, 1.1), and then rescale the rest to ensure that the considered diapason stays constant.</p><p>We tried two strategies of data augmentation. In the first one, a single uniformly distributed value α ∈ (0.9, 1.1) was sampled at each epoch and used to rescale all training examples, and no rescaling was applied to the validation set. In the second strategy, each spectrogram was rescaled with an individually generated α for the training, as well as for the validation sets. For evaluation, we used the majority vote of the model predictions on eleven copies of the test set with α = 0.9, 0.92, 0.94, ..., 1.1. We present the scores obtained with the second augmentation strategy, which provided the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model description and experiments</head><p>As it has been mentioned above, the IEMOCAP dataset consists of five sessions, each being a conversation between a man and a woman, giving 10 speakers in total. In order to see how well the model can generalize to different speakers, we took the validation and test sets to correspond to two different speakers of one of the sessions. The training set was composed of the four remaining sessions. In the course of experiments, we observed that the performance strongly depends on which speakers are chosen for the test set (see Tab. 2). Therefore we choose 10-fold cross-validation strategy, in order to average over all possible choices of the dataset splitting. Interestingly, to the best of our knowledge, all the other results reported on the IEMOCAP dataset were obtained by 5-fold cross-validation. In this case the choice of the validation and test sets is not rigorously defined 3 and the scores obtained in this way are not possible to compare with.</p><p>For evaluating the model performance, we chose weighted (WA) and unweighted (UA) accuracies. WA is the standard accuracy computed over the whole test set. UA is an average over accuracies computed for each emotion separately. First, we compute the metrics for each fold and then present the scores as the average over all the folds. Since for imbalanced datasets UA is a more relevant characteristic, we rather concentrated our efforts on getting a high UA, in line with most of the other works on IEMOCAP.</p><p>We considered architectures with 1-6 convolutional layers, 1-4 Bi-LSTM layers and a dense layer with softmax nonlinearity on top of the network (see <ref type="figure" target="#fig_1">Fig. 2</ref>). As an optimization procedure, we used stochastic gradient descent with momentum and the batch size of 16 <ref type="bibr" target="#b3">4</ref> . For the regularization of weights we used L2-regularization.</p><p>Due to the significant variety of the data samples in the time length (from 21 to 909 time steps for window size N = 64ms and shift S = 32ms) we performed zero-padding of the samples along the time axis. In order to avoid the aggregation of the artificially added time steps by Bi-LSTM, we put a masking layer between the convolutional and Bi-LSTM modules. The size of the mask has been derived from the temporal size of the corresponding spectrogram and action of the convolutional strides on it.</p><p>Finally we normalized the samples according to the general statistics of the dataset:</p><formula xml:id="formula_1">x n = x −x √ σ 2 + ,<label>(2)</label></formula><p>wherex and σ are the average and standard deviation of the spectrogram pixels computed over the whole dataset along both time and frequency axes. Such normalization significantly improves the convergence time of the model. However, applied to networks of small depth (≤ 2 convolutional layers), it results in strong overfitting. As we have mentioned above, we conducted a variety of experiments with different depths of convolutional and Bi-LSTM modules. The presence of pooling layers alternating with the convolutions noticeably decreased the performance and has been discarded in the beginning of the experiments. We examined different scenarios: "shallow CNN + deep Bi-LSTM", "deep CNN + shallow Bi-LSTM" and "deep CNN + deep Bi-LSTM", In Tab. 1 we present the results of the best model and also contributions to the performance of the techniques we applied. One can see that oversampling allowed to increase UA by 2.1%, but resulted in 2.9% decrease of WA. Data augmentation with VTLP led to increase of both metrics by 1.1% and 0.7% for UA and WA correspondingly. Considering a larger range of the frequencies (8kHz) increased the UA by 0.8%. Finaly in Tab. 2 we present the results per fold, the scores obtained by averaging our 5 best folds and the results obtained in the other works by 5 fold cross-validation.</p><p>We also tried out batch normalization implemented for the Bi-LSTM layers of the network. During the experiments we observered that the data of interest are sensitive to normalization. Therefore we choose the most conservative normalizing strategy which implies averaging the samples over all the axes:</p><formula xml:id="formula_2">π n s,t,f = π s,t,f −π √ σ 2 + ,<label>(3)</label></formula><p>wherê</p><formula xml:id="formula_3">π = 1 b tf s,t,f π s,t,f , σ = 1 b tf s,t,f (π s,t,f −π) 2 .<label>(4)</label></formula><p>Here, s, t and f are the batch, temporal and frequency index respectively, π is preactivation and b tf is a product of the sum of the sample time lengths over the batch and the feature number. Then batch normalization is applied only to the input contribution to the hidden state:</p><formula xml:id="formula_4">ht = a(W h ht−1 + BN (Wxxt)),<label>(5)</label></formula><p>where BN stands for the standard batch normalization operation <ref type="bibr" target="#b16">[17]</ref>, a(π), ht, xt are activation, hidden state and input, and W h , Wx are the corresponding weights. We examined batch normalization applied to the architectures with 4 convolutional and 1-4 Bi-LSTM layers. The experiments with the initial batch size of 16 demonstrated faster overfitting and degradation of the performance compared to the baseline. The further experiments with larger batch size showed that it strongly influences the performance (see Tab. 3), despite the fact that the normalization has been performed along all the axes of the batch. One can see that the scores obtained with the batch size of 64 almost reaches the performance of our best model 1. Therefore, it is possible that further augmenting the batch size would lead to even better results. Unfortunately, due to GPU memory restrictions, we could not verify it. Monitoring the gradient of the network parameters, we observed that the gradient with respect to the weights of the convolutional layers is much larger than with respect to the weights of Bi-LSTM (see <ref type="figure" target="#fig_2">Fig. 3</ref>). This observation allows an interpretation that regarding the convolutional weights the loss surface should be steeper and deeper than regarding the weights of the Bi-LSTM. Therefore it gave us a nudge that it might be interesting to consider different update parameters, namely learning rate and momentum, for convolutional and recurrent modules. Apart from varying the conventional update parameters of the momentum optimizer we also considered its modification by introducing the new parameter β:</p><formula xml:id="formula_5">vt = vt−1γ + η∇wJ(w), w = w − βvt,<label>(6)</label></formula><p>where γ, η and vt stands for the momentum, learning rate and velosity correspondingly. The coefficient β brought into use in this way does not accumulate in the velocity expression and provide better control of the momentum term of the optimizer. Unfortunately, from our experiments we were not able to draw any definite conclusion in favor of layer-wise adjustment of η, γ or β. Nevertheless, we find that this is an interesting direction to persue and more thorough experiments might give more preferable result. It also might be interesting to test the update rule modification introduced in eq. (6) in the other settings in order to see whether it can provide an actual improvement of the momentum optimizer.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this work we built a neural network for recognizing emotions in speech, using the IEMOCAP dataset. Unlike the prior results, in order to measure the model performance we performed 10fold cross-validation, which is more appropriate for this dataset.</p><p>To adress the issues of scarcity and class imbalance we employed data augmentation by means of VTLP and minor class oversampling.</p><p>Following the modern trends in speech analysis, we used a mixed CNN-LSTM architecture, exploiting the capacity of convolutional layers to extract high-level representations from raw inputs. Interestingly, we noticed that parameters of convolutional and LSTM layers are trained at a very different pace. We tried to take advantage of this observation by per-layer adjustment of the update rule parameters, but unfortunately were not able to make a definite conclusion in favor of this idea. Nevertheless, we find that this is an interesting direction to persue and more thorough experiments might give more preferable result.</p><p>We also investigated the effect of batch normalization, an indispensable tool in most image recognition tasks. In order to preserve the signal structure as much as possible we performed the normalization layer-wise as well as batch-wise. Nevertheless, we did not manage to increase performance compared to the baseline, which might be caused by the small batch size we had to use in order to fit into the available GPU memory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Class distribution of the utterances in the improvised part of the IEMOCAP dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Network architecture mostly concentrating on the second option. The best results has been achieved with a choice of 4 convolutional and 1 Bi-LSTM layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Per-layer gradient evolution 3.1. Difference in the gradient scaling of the convolutional and recurrent layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>10-cross validation scores depending on the techniques applied (for each experiment we present the results corresponding to its best run).</figDesc><table><row><cell></cell><cell>Baseline</cell><cell></cell><cell></cell><cell>Best model</cell></row><row><cell>Augmentation during training</cell><cell>-</cell><cell>-</cell><cell>+</cell><cell>+</cell></row><row><cell>Oversampling (×2) of happiness and anger</cell><cell>-</cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell>Frequency range (kHz)</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>8</cell></row><row><cell>Weighted accuracy</cell><cell>66.4</cell><cell cols="2">63.5 64.2</cell><cell>64.5</cell></row><row><cell>Unweighted accuracy</cell><cell>57.7</cell><cell cols="2">59.8 60.9</cell><cell>61.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The performance of the best model per fold and comparison to the other works. The gender column indicates which speaker is used as test set in the fold.</figDesc><table><row><cell cols="5">Fold Session Gender WA (%) UA (%)</cell></row><row><cell>1</cell><cell>1</cell><cell>F</cell><cell>64.1</cell><cell>66.4</cell></row><row><cell>2</cell><cell>1</cell><cell>M</cell><cell>68.8</cell><cell>67.7</cell></row><row><cell>3</cell><cell>2</cell><cell>F</cell><cell>70.3</cell><cell>71.3</cell></row><row><cell>4</cell><cell>2</cell><cell>M</cell><cell>62</cell><cell>67.6</cell></row><row><cell>5</cell><cell>3</cell><cell>F</cell><cell>64.8</cell><cell>52.1</cell></row><row><cell>6</cell><cell>3</cell><cell>M</cell><cell>66.4</cell><cell>56</cell></row><row><cell>7</cell><cell>4</cell><cell>F</cell><cell>68.5</cell><cell>59.7</cell></row><row><cell>8</cell><cell>4</cell><cell>M</cell><cell>64.3</cell><cell>67.3</cell></row><row><cell>9</cell><cell>5</cell><cell>F</cell><cell>64.8</cell><cell>64.2</cell></row><row><cell>10</cell><cell>5</cell><cell>M</cell><cell>51</cell><cell>44.2</cell></row><row><cell cols="3">10 fold cross-valid.</cell><cell>64.5</cell><cell>61.7</cell></row><row><cell></cell><cell>5 best folds</cell><cell></cell><cell>66.9</cell><cell>65.3</cell></row><row><cell cols="3">[6] (5 fold cross-valid.)</cell><cell>62.9</cell><cell>63.9</cell></row><row><cell cols="3">[7] (5 fold cross-valid.)</cell><cell>67.3</cell><cell>62.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The performance of the best model equipped with the batch normalization (for each experiment we present the results corresponding to its best run).</figDesc><table><row><cell>Minibatch size</cell><cell>16</cell><cell>32</cell><cell>64</cell></row><row><cell>Weighted</cell><cell cols="3">63.6 65.1 65.4</cell></row><row><cell>Unweighted</cell><cell>58.9</cell><cell>59</cell><cell>60.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">4 convolutional and 1 Bi-LSTM layers arXiv:1802.05630v2 [cs.SD] 11 Sep 2018</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For instance, one could systematically use female speakers as validation and male speakers as test, or inversely<ref type="bibr" target="#b3">4</ref> We chose the small batch size in order to achieve high variability in the gradient descent directions</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving english conversational telephone speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Medennikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prudnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zatvornitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA -International Speech Communication Association</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The ibm 2016 english conversational telephone speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA -International Speech Communication Association</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="11" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Letterbased speech recognition with gated convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno>abs/1712.09444</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning for robust feature generation in audiovisual emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E. Mower</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3687" to="3691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High-level feature representation using recurrent neural network for speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA -International Speech Communication Association</title>
		<meeting><address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech 2015</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient emotion recognition from speech using deep learning on spectrograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Satt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rozenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2017</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>ISCA -International Speech Communication Association</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Iemocap: interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4580" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalized recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1603.09025</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A frequency warping approach to speaker normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vocal tract length perturbation (VTLP) improves speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data augmentation for deep neural network acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Khoskits lezvi chanachum khory usutsman metvodnerov, BS thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanogh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
