<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Roynard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Robotics</orgName>
								<orgName type="institution" key="instit1">Mines ParisTech</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Robotics</orgName>
								<orgName type="institution" key="instit1">Mines ParisTech</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran√ßois</forename><surname>Goulette</surname></persName>
							<email>francois.goulette@mines-paristech.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Robotics</orgName>
								<orgName type="institution" key="instit1">Mines ParisTech</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Urban Point Cloud</term>
					<term>Dataset</term>
					<term>Classification</term>
					<term>Segmentation</term>
					<term>Mobile Laser Scanning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces a new Urban Point Cloud Dataset for Automatic Segmentation and Classification acquired by Mobile Laser Scanning (MLS). We describe how the dataset is obtained from acquisition to post-processing and labeling. This dataset can be used to train pointwise classification algorithms, however, given that a great attention has been paid to the split between the different objects, this dataset can also be used to train the detection and segmentation of objects. The dataset consists of around 2km of MLS point cloud acquired in two cities. The number of points and range of classes make us consider that it can be used to train Deep-Learning methods. Besides we show some results of automatic segmentation and classification. The dataset is available at: http://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the development of segmentation and classification methods of 3D point clouds by machine-learning, more and more data are needed in quantity and quality (number of points, number of classes, quality of segmentation).</p><p>There are always more datasets of classification and segmentation of images, visual and LiDAR odometry or SLAM, detection of vehicles and pedestrians on videos, stereo-vision, optical flow, etc. But it is still difficult to find datasets of segmented and classified urban 3D point clouds. The only comparable datasets are the ones described in section Available Datasets. Each of them has its advantages and disadvantages, but we believe that none of them combines all the advantages of the dataset we publish.</p><p>In section Our Dataset: Paris-Lille-3D, we present a new urban dataset that we have created, where the objects are sufficiently segmented that the task of segmentation can be learned very precisely. Our dataset can be found at the following address: http://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/.</p><p>In section Results of automatic segmentation and classification, we give some results of automatic segmentation and classification on our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Available Datasets</head><p>Numerous datasets are used for training and benchmarking machine learning algorithms. The provided data allows for the training of methods that perform a given task, and the evaluation of performance on a test set allows evaluation of the quality of the results obtained and comparison of different methods according to various metrics.</p><p>Many different tasks can be learned, the most common being classification (for example, for an image, it is giving the class of the principal object visible). Another task may be to segment the data into its relevant parts (for the images it is grouping all the pixels that belong to the same object). There are multiple other tasks that can be learned, from image analysis to translation in natural language processing, for a survey see <ref type="bibr">[FMH + 15]</ref>.</p><p>There is a bunch of existing datasets in many fields. Each dataset has different types of data, in type (image, sound, text, point clouds, graphs), quantity (from hundreds to billion of samples), quality, number of classes (from tens to thousands), and tasks to learn. Amongst the most famous are:   <ref type="bibr" target="#b12">[NRS14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lidar type Length</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of points</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of classes</head><p>The data we are interested in are urban 3D point clouds. There are mainly two methods that allow to acquire these data in quality sufficient for us:</p><p>Mobile Laser Scanning (MLS), with a LiDAR mounted on a ground vehicle or a drone. To register the clouds, an accurate 6D-pose of the vehicle must be known. Terrestrial Laser Scanning (TLS) by static LiDAR, the LiDAR must be moved between each acquisition and clouds must be registered. The ALS does not allow to obtain a sufficient density of points because of the distance and the angle of acquisition.</p><p>There are already some segmented and classified urban 3D point cloud datasets. However these datasets are very heterogeneous and each has features that can be seen as defects. In the 4 next sub-sections, we make a comparison of the existing datasets and identify their strengths and weaknesses for automatic classification and segmentation. <ref type="table">Table 1</ref> presents a quantitative comparison of these datasets with ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Oakland 3-D Point Cloud Dataset [MBVH09]</head><p>This dataset was acquiered by a MLS system mounted with a side looking Sick monofiber LiDAR. Since it is a mono-fiber LiDAR, it has the disadvantage of hitting the objects from a single point of view, so there are many occlusions. In addition it is much less dense than other datasets because of the low acquisition rate of the LiDAR (see <ref type="figure" target="#fig_1">figure 2</ref>). In addition, this dataset contains 44 classes of which a large part (24) have less than 1000 labeled points. This is very low to be able to distinguish objects, especially when these points are distributed over several samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic3D [HWS16]</head><p>This dataset was acquired by static laser scanners. It is therefore much more precise and dense than a dataset acquired by MLS, but it has disadvantages inherent to static LiDARs (see <ref type="figure" target="#fig_2">figure 3</ref>):</p><p>The density of points varies considerably depending on the distance to the sensor. There are occlusions due to the fact that sensors do not turn around the objects. Even by registering several clouds acquired from different viewpoints, there are still a lot of occlusions. The acquisition time is much more important than by MLS, which prevents to obtain very miscellaneous scenes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Paris-rue-Madame Database [SMGD14]</head><p>This dataset was acquired by an earlier version of our MLS system [GNA + 06]. This dataset was segmented and annotated semi-automatically, first by a mathematical morphology method on elevation images <ref type="bibr" target="#b20">[SMGD14]</ref> and then refined by hand. Some segmentation inaccuracies at the edges of objects remain (see <ref type="figure" target="#fig_3">figure 4</ref>), in particular the bottom of the objects is annotated as belonging to the ground. Moreover, the system as well as the point cloud processing pipeline have been greatly improved. We can now generate clouds much less noisy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">IQmulus &amp; TerraMobilita Contest [VBS + 15]</head><p>This dataset was acquired by a MLS system mounted with a monofiber Riegl LMS-Q120i LiDAR. This LiDAR has the advantage of being more accurate than a multi-fiber LiDAR such as the Velodyne HDL-32E, but it is also more expensive. Moreover, since it is mono-fiber, it has the disadvantage of hitting the objects from a single point of view, so there are many occlusions.</p><p>For the annotation, the scan lines of the LiDAR were concatenated one above the other to form 2D images. The values of the pixels are the intensity of laser return.</p><p>This method has the advantage of being easy to put into production, which allowed the IGN to annotate a large dataset. However, inaccuracies in countouring annotation of 2D images generate badly classified points, the points around the occlusions are classified in the class of the object that creates the occlusion. (see <ref type="figure" target="#fig_4">figure 5</ref>). 3 Our Dataset: Paris-Lille-3D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Acquisition</head><p>All point clouds used in our dataset were acquired with the MLS prototype of the center for robotics of Mines ParisTech: L3D2 [GNA + 06] (as seen in <ref type="figure" target="#fig_5">figure 6</ref>). It is a Citro√´n Jumper equipped with a GPS (Novatel FlexPak 6), an IMU (Ixsea PHINS in LANDINS mode) and a Velodyne HDL-32E LiDAR mounted at the rear of the truck with an angle of 30 degrees between the axis of rotation and the horizontal. For localization, we use a dual-phase L1/L2 RTK-GPS at 1Hz with a fixed base provided by the IGN RGP 1 (Permanent GNSS Network). RGP bases are: SMNE for Paris dataset and LMCU for Lille dataset. The IMU sends data at 100Hz. Data from the LiDAR and IMU are synchronised thanks to PPS signal from the GPS.</p><p>In post-process, we retrieve data from RGP fixed base, and we generate the trajectory with the Inertial Explorer 2 software. The method used is Tightly Coupled GPS-RTK/INS Kalman Smoothing EKF. We obtain a trajectory in WGS84 system at 100Hz, that we convert to Lambert RGF93.</p><p>Then, as each point has its own timestamp, we linearly interpolate the trajectory. Moreover we only keep points measured at a distance less than 20m in order to keep only areas of sufficiently high density. Finally we build clouds for which each point is characterized by a vector (x, y, z, x origin , y origin , z origin ,t, i), where i is the intensity of the LiDAR return.</p><p>We do not apply any method of SLAM, cloud registration or loop closure. All trajectories are built with Inertial Explorer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Description of point clouds</head><p>The dataset consists of three parts, two parts in the agglomeration of Lille and one in Paris (see <ref type="figure" target="#fig_6">figure 7</ref>). For the sake of precision, an offset has been substracted in the plane (x,y) to all the points so that they hold in float (32 bits). Data are distributed as explained in  The clouds have high density with between 1000 and 2000 points per square meter on the ground, but there are some anisotropic patterns due to the multi-beam LiDAR sensor as seen in figure 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Description of segmented and classified data</head><p>The clouds obtained were segmented and classified by hand using CloudCompare 3 software. Some illustrations of the segmented and classified data are shown in figure 1.</p><p>We chose to re-use the class tree of iQmulus/Terramobilita benchmark, in which we only change a few classes and add classes relevant to our dataset. It can be found at url: http://data.ign.fr/benchmarks/UrbanAnalysis/download/ classes.xml For a distribution of number of points by classes, see table 3. Classes added:</p><p>bicycle rack (id = 302021200) statue (id = 302021300) distribution box (id = 302040600) lighting console (id = 302040700) windmill (id = 302040800) We also change the way vehicles are seen. More precisely, for each class of vehicle, we distinguish sub-classes depending on whether they are parked, stopped (on the road) or moving. And Velib terminal is changed to bicycle terminal (id = 302021100) which is more generic.</p><p>Except the few classes mentioned above, this class tree appears to be sufficiently complete for classes encountered in our dataset. The XML file describing this tree is named classes.xml and is provided with the dataset. We also provide three ASCII-files (.txt) containing annotations for particular samples. Each line of these files contains:</p><p>sample_id, class_id, class_name, annotation1, annotation2, ...</p><p>The most common annotations are: "several", for example when trees are interlaced and can not be delimited precisely by hand, "overturned", for trash cans laid on their side.    </p><formula xml:id="formula_0">object Ô£± Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£≤ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£≥ surface Ô£± Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£≤ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£≥ static Ô£± Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£≤ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£≥ dynamic Ô£± Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£≤ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£≥ natural Ô£± Ô£¥ Ô£¥ Ô£≤ Ô£¥ Ô£¥ Ô£≥</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Description of files</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results of automatic segmentation and classification</head><p>In this section, we evaluate an automatic segmentation and classification method on our dataset. There are many approaches to achieve this task, most of them look like one of the following pipelines: classify each point for example by computing local features (hand-made <ref type="bibr" target="#b23">[WJHM15]</ref> or by Deep-Learning methods [?]), then group them into objects for example by CRF methods (see <ref type="bibr">[?]</ref>). segment the cloud into segments, for example by mathematical morphology (see <ref type="bibr" target="#b19">[SM14]</ref>) or supervoxel (see <ref type="bibr" target="#b0">[ACT13]</ref>), then classify each segment (by hand-made global descriptors <ref type="bibr" target="#b8">[JH99,</ref><ref type="bibr" target="#b22">VSS12]</ref> or by deep-learning <ref type="bibr" target="#b11">[MS15,</ref><ref type="bibr" target="#b15">QSMG16]</ref>). The method used here <ref type="bibr" target="#b17">[RDG16]</ref> belongs to the first category. The detailed processing pipeline is: extraction of the ground by region growing on an elevation map, segmentation of objects by connectivity of the remaining point cloud, computation of descriptors on each object (some simple geometric descriptors inspired by <ref type="bibr" target="#b19">[SM14]</ref> and some 3D descriptor of the literature as CVFH <ref type="bibr" target="#b16">[RBTH10]</ref>, GRSD [MPB + 10] and ESF <ref type="bibr" target="#b13">[OFCD01]</ref>), classification of the objects with a Random Forest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Improvements of [RDG16]</head><p>Two improvements are proposed to increase the robustness of this method: first on the segmentation by new extraction of the ground (using better seed for the region growing), then on the classification with new descriptors (to take the context of objects into account).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Ground Extraction</head><p>In <ref type="bibr" target="#b17">[RDG16]</ref>, the seed for region growing is found by computing a histogram in z on the whole cloud, which is not robust in case the road is sloping. As we know the exact position of the LiDAR sensor with respect to the ground (2.71m above ground), we can extract the points that are just below the sensor in a cylinder parameterized by:</p><formula xml:id="formula_1">(x ‚àí x origin ) 2 + (y ‚àí y origin ) 2 ‚â§ 1 (1) |z origin ‚àí z ‚àí 2.71| ‚â§ 0.3<label>(2)</label></formula><p>Points lying in this cylinder are then taken as seeds for the region growing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Features for Classification</head><p>It was observed that some objects (such as cars) were detected way above the ground. We propose to solve this problem by adding a contextual descriptor which gives the altitude of the object with respect to the ground detected in the previous step.</p><p>In a first step we calculate an image of elevation of the ground, for example with a resolution 10cm √ó 10cm. Then empty pixels are filled with elevation of the closest non-empty pixel. And the image is smoothed to avoid segmentation artefacts (for example where the ground meets the foot of the buildings).</p><p>Then for each object, the barycenter is projected onto this elevation image of the ground, which gives us the elevation of the ground under this object: z ground . If z min is the minimum elevation of the object, the descriptor added is: z min ‚àí z ground . Our segmentation method is very basic, indeed it makes very strong a priori on the way to distinguish objects from each other. Two objects are different if they are in different connected component of the point cloud from which the ground has been removed. This explains some problems (see <ref type="figure" target="#fig_0">figure 10</ref>) like two cars too close one from another segmented as a single object, or buildings just linked by a cable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation: Segmentation</head><p>Our segmentation method is very basic, indeed it makes very strong a priori on the way to distinguish objects from each other. Two objects are different if they are in different connected components of the point cloud from which the ground has been removed. This explains some problems (see <ref type="figure" target="#fig_0">figure 10</ref>) like two cars too close one from another segmented as a single object, or buildings just linked by a cable.</p><p>To evaluate detection of objects, we use the same metric as used in iQmulus/TerraMobilita contest [VBS + 15].</p><p>For an object of the ground truth (represented by the subset S GT ) and an object resulting from our segmentation method (S SR ), we estimate that they match if the following conditions are respected:</p><formula xml:id="formula_2">|S GT | |S GT ‚à™ S SR | &gt; m and |S SR | |S GT ‚à™ S SR | &gt; m</formula><p>Then detection precision and recall are computed by the following formulas: We evaluate our results with m = 0.5 which is the minimal value that ensures that a Ground Truth object matches at most one object segmented by our method (see <ref type="table" target="#tab_8">table 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Precision</head><p>Recall F1  It is believed that methods that learn segmentation will yield much better results.  In this section we only evaluate the classification method assuming good segmentation. To do this, we take the set of objects of the dataset that are randomly divided into a training set (80%) and a test set (20%). We use only a few coarser classes than described in table 3 to evaluate our classification algorithm, see table 6 for a distribution of samples per class. In addition, we add a coarse_classes.xml file to the dataset that adds a coarse field to each class.  Even with these coarse classes, there are a few samples in some of them. Then precision and recall numbers in table 7 should be taken with caution. Metrics used to evaluate performance are the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation: Classification</head><formula xml:id="formula_3">P = T P T P + FP R = T P T P + FN (3) F1 = 2T P 2T P + FP + FN MCC = T P ¬∑ T N ‚àí FP ¬∑ FN (T P + FP)(T P + FN)(T N + FP)(T N + FN)</formula><p>Where P, R, F1 and MCC represent respectively Precision, Recall, F1-score and Matthews correlation coefficient. And T P, T N, FP and FN are respectively the number of True-Positives, True-Negatives, False-Positives and False-Negatives.</p><p>Moreover, it can be noted that the best results are obtained with the combination of descriptors: Geometric and GRSD, which are the descriptors composed of the least number of variables. This can be explained by the few samples of the dataset and therefore adding a large number of features does not provide more relevant information. Then this dataset is more appropriate for the evaluation of per-point classification methods.  <ref type="table">Table 7</ref>: Classification performance for each combination of descriptors, these metrics are averaged over all classes (the OOB score is given by Random-Forest during training).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descriptors</head><p>It can be concluded that it is not necessary to calculate all the descriptors to obtain the best classification results. It is possible to gain in computation time by calculating only the geometric descriptors and GRSD (see table 8 for precise gains).</p><p>And for applications where time is critical, we can even calculate only the geometric descriptors (which also avoids having to calculate the normals).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descriptors Proportion Mean Time per object (ms)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a dataset of urban 3D point cloud for automatic segmentation and classification. This dataset contains 140 million points on 2km in two different cities. The objects were segmented by hand and a class was associated with each one among 50 classes. We hope that this dataset will help to train and evaluate methods as deep-learning, which are very demanding in terms of quantity of points.</p><p>In addition, we have tested a first method of segmentation and automatic classification from <ref type="bibr" target="#b17">[RDG16]</ref> to which we have made some improvements for robustness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Part of our dataset. Top: reflectance from blue(0) to red(255), middle: object label (different color for each object), bottom: object class (different color for each class).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example of cloud in Oakland dataset. Low density, few classes, big shadows behind trees (due to monofiber LiDAR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Example of cloud in Semantic3D dataset (3 clouds registered). Occlusions, density depends on the distance to the LiDAR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Example of cloud in Rue-Madame dataset. Ground truth mistakes: can be very noisy (top), parts of cars are seen as road (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Example of cloud in iQmulus/TerraMobilita dataset. As a monofiber LiDAR is used, there are shadows behind objects. Moreover points of the wall behind cars are classified as car.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>MLS prototype: L3D2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1Figure 7 :</head><label>7</label><figDesc>http://rgp.ign.fr/ 2 https://www.novatel.com/products/software/inertial-explorer/ Trajectories of the experimental vehicle during acquisition. Top: the 2 trajectories in agglomeration of Lille are in green and blue. Bottom: the trajectory in Paris is in red. (Pictures from Google Maps)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Anisotropic pattern on the ground (color of points is the reflectance)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Exemple of cloud segmented by our method (each object has a different color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>precision(m) = number of detected objects matched number of detected objects recall(m) = number of detected objects matched number of ground truth objects F1(m) = 2 precision(m) ¬∑ recall(m) precision(m) + recall(m)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Comparison between clouds segmented automatically by our method (bottom) and by hand (top). Each object has a different color. Two cars too close one from another are segmented as a single object. The bottom part of each object is segmented as part of the ground. A trash can placed against the facade is seen as a part of the facade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Example of cloud classified by our method (each class has a different color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Number</cell><cell></cell></row><row><cell cols="2">Section Length</cell><cell>of points</cell><cell>RGF93 Offset</cell></row><row><cell cols="2">Lille1 1150m</cell><cell>71.3M</cell><cell>(711164.0m, 7064875.0m)</cell></row><row><cell>Lille2</cell><cell>340m</cell><cell>26.8M</cell><cell>(711164.0m, 7064875.0m)</cell></row><row><cell>Paris</cell><cell>450m</cell><cell>45.7M</cell><cell>(650976.0m, 6861466.0m)</cell></row><row><cell>Total</cell><cell cols="2">1940m 143.1M</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Description of the three parts of the dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Number of samples/points for each class (k for thousand and M for million). Trash cans appear twice, the first time is for only fixed trash can.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>x_origin, y_origin, z_origin (float) : the position of the LiDAR, GPS_time (double) : the moment when the point was acquired, reflectance (uint8) : the intensity of laser return, label (uint32) : the label of the object to which the point belongs, class (uint32) : the class of the object to which the point belongs.</figDesc><table><row><cell></cell><cell></cell><cell>Number</cell><cell>Number</cell><cell>Number</cell></row><row><cell cols="2">Section Length</cell><cell>of points</cell><cell>of objects</cell><cell>of classes</cell></row><row><cell cols="2">Lille1 1150m</cell><cell>71.3M</cell><cell>1349</cell><cell>39</cell></row><row><cell>Lille2</cell><cell>340m</cell><cell>26.8M</cell><cell>501</cell><cell>29</cell></row><row><cell>Paris</cell><cell>450m</cell><cell>45.7M</cell><cell>629</cell><cell>41</cell></row><row><cell>Total</cell><cell>1940m</cell><cell>143.1M</cell><cell>2479</cell><cell>50</cell></row></table><note>Each part of the dataset is in a separate PLY-file, a summary of each file can be found in table 4. Each point of PLY-files has 10 attributes: x, y, z (float) : the position of the point,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Overview of our dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Precision and Recall of object detection for m = 0.5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Number of samples/points for each coarse class used for classification evaluation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Mean computational time for calculating descriptors on segmented objects. Time to compute normal vectors is added for comparison.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.danielgm.net/cc/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segmentation based classification of 3d urban point clouds: A super-voxel based approach with evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Kamal Aijazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Checchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Trassoudaine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
	</analytic>
	<monogr>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<editor>AEHKL + 16] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1624" to="1650" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">University of michigan north campus long-term vision and lidar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlevaris-Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">M</forename><surname>Ushani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1023" to="1035" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>COR + 16</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
	<note>DDS + 09</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A survey of current datasets for vision and language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06833</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An integrated on-board laser range sensing system for on-the-way city and road modelling. The International Archives of the Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nashashibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abuhadrous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ammoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurgeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Part A</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast semantic segmentation of 3d point clouds with strongly varying density. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="177" to="184" />
			<pubPlace>Prague, Czech Republic</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Tsung-Yi</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextual classification with functional max-margin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delfina</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert ; Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Pangercic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Kleinehellefort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3700" to="3705" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Contextual classification of lidar data and building object detection in urban areas. ISPRS journal of photogrammetry and remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Soergel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="152" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matching 3d models with shape distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Osada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Chazelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dobkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SMI 2001 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="154" to="166" />
		</imprint>
	</monogr>
	<note>Shape Modeling and Applications</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ford campus vision and lidar data set. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">M</forename><surname>James R Mcbride</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1543" to="1552" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast 3d recognition and pose using the viewpoint feature histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2155" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and robust segmentation and classification for change detection in urban point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Roynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Goulette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS 2016-XXIII ISPRS Congress</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>SHK + 14</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschm√ºller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nera</forename><surname>Ne≈°iƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Porter</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detection, segmentation and classification of 3d urban objects using mathematical morphology and supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr√©s</forename><surname>Serna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="243" to="255" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Paris-rue-madame database: a 3d mobile laser scanner dataset for benchmarking urban detection, segmentation and classification methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr√©s</forename><surname>Serna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran√ßois</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Pattern Recognition, Applications and Methods ICPRAM 2014</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Terramobilita/iqmulus urban point cloud analysis benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Vallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Br√©dif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr√©s</forename><surname>Serna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Paparoditis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="126" to="133" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Implicit shape models for object detection in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Velizhev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society of Photogrammetry and Remote Sensing Congress</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Weinmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Jutzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl√©ment</forename><surname>Mallet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="286" to="304" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
