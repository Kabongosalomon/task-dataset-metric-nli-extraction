<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRDN:Grouped Residual Dense Network for Real Image Denoising and GAN-based Real-world Noise Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Wook</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Multimedia Engineering</orgName>
								<orgName type="institution">Dongguk University</orgName>
								<address>
									<postCode>04620</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Ryun</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Multimedia Engineering</orgName>
								<orgName type="institution">Dongguk University</orgName>
								<address>
									<postCode>04620</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Jung</surname></persName>
							<email>swjung83@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Multimedia Engineering</orgName>
								<orgName type="institution">Dongguk University</orgName>
								<address>
									<postCode>04620</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GRDN:Grouped Residual Dense Network for Real Image Denoising and GAN-based Real-world Noise Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent research on image denoising has progressed with the development of deep learning architectures, especially convolutional neural networks. However, real-world image denoising is still very challenging because it is not possible to obtain ideal pairs of ground-truth images and real-world noisy images. Owing to the recent release of benchmark datasets, the interest of the image denoising community is now moving toward the real-world denoising problem. In this paper, we propose a grouped residual dense network (GRDN), which is an extended and generalized architecture of the state-of-the-art residual dense network (RDN). The core part of RDN is defined as grouped residual dense block (GRDB) and used as a building module of GRDN. We experimentally show that the image denoising performance can be significantly improved by cascading GRDBs. In addition to the network architecture design, we also develop a new generative adversarial network-based real-world noise modeling method. We demonstrate the superiority of the proposed methods by achieving the highest score in terms of both the peak signal-to-noise ratio and the structural similarity in the NTIRE2019 Real Image Denoising Challenge -Track 2:sRGB.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the field of image denoising, recent studies show that learning-based methods are more efficient than previous handcrafted methods such as block matching 3D (BM3D) <ref type="bibr" target="#b5">[6]</ref> and its variants. It is essential for learningbased methods to have a sufficient amount of dataset with high quality. Because a pair of noisy and noise-free images can be easily constructed by adding synthetic noise to noise-free images, a majority of previous learning-based methods focus on the classic Gaussian denoising task and pay the most attention to the architecture design of net- * These authors contributed equally. Corresponding author: S.-W. Jung works, especially convolutional neural networks (CNNs). However, due to the gap between synthetically generated noisy images and real-world noisy images, it was found that CNNs trained using synthetic images do not perform well on real-world noisy images and sometimes even inferior to BM3D <ref type="bibr" target="#b21">[22]</ref>.</p><p>Toward real-world image denoising, there have been two main approaches. The first approach is to find a better statistical model of real-world noise rather than the additive white Gaussian noise <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>. In particular, a combination of Gaussian and Poisson distributions was shown to closely model both signal-dependent and signalindependent noise. The networks trained using these new synthetic noisy images demonstrated the superiority in denoising real-world noisy images. One clear advantage of this approach is that we can have infinitely many training image pairs by simply adding the synthetic noise to noise-free ground-truth images. However, it is still arguable whether the real-world noise can be modeled by statistic models. The second approach is thus in an opposite direction. From real-world noisy images, nearly noise-free ground-truth images can be obtained by inverting an image acquisition procedure <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2]</ref>. To our knowledge, smartphone image denoising dataset (SIDD) <ref type="bibr" target="#b0">[1]</ref> is one of the largest high quality image datasets on the second approach. However, the amount of provided images may not be enough for training a large network and without a sufficient knowhow it is difficult to generate ground-truth images from real-world noisy images. We thus adopt the second approach but applied our own generative adversarial network (GAN)-based data augmentation technique to obtain a larger dataset.</p><p>The network architecture is of course the utmost important. In CNN-based image restoration, dense residual blocks (RDBs) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref> have received great attention. In this paper, we propose a new architecture called grouped residual dense network (GRDN). In particular, the proposed architecture adopts the recent residual dense network (RDN) as a component with a minor modification and defines it as grouped residual dense block (GRDB). By cascading the GRDBs with attention modules, we could obtain the state-of-the-art performance in real-world image denoising task <ref type="bibr" target="#b27">[28]</ref>. We achieved the best performance in terms of the peak signal-to-noise ratio (PSNR) of 39.93 dB and the structural similarity (SSIM) of 0.9736 in the NTIRE2019 Real Image Denoising Challenge -Track 2:sRGB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image Restoration</head><p>Image denoising is one of the most extensively studied topics in image processing. Owing to significant advances in deep learning, CNN-based methods are now dominating in image denoising. However, most previous learning-based image denoising methods have focused on the classic Gaussian denoising task. Toward real-world image denoising, the first approach was to capture a pair of noisy and noisefree images by using different camera settings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. It was shown in <ref type="bibr" target="#b21">[22]</ref> that earlier learning-based methods were comparable or sometimes even inferior to classic methods such as BM3D. We consider this is mainly because of insufficient quality and quantity of training dataset. Consequently, more abundant and elaborate datasets such as Darmstadt noise dataset (DND) and SIDD <ref type="bibr" target="#b0">[1]</ref> were developed, and recent learning-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref> showed their superiority over classic methods for real-world image denoising.</p><p>In addition to the efforts in generating high quality datasets, a significant amount of research has been made to find better network architectures for image denoising. From the viewpoint of CNNs, network architectures developed for different image restoration tasks such as image denoising, image deblurring, super-resolution, and compress artifact reduction share similarities. It has been repeatedly demonstrated that one architecture developed for a certain image restoration task also performs well in other restoration tasks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b22">23]</ref>. We thus examined many of the architectures developed for different image restoration tasks, especially super-resolution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. Among them, RDN <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref> and residual channel attention network (RCAN) <ref type="bibr" target="#b30">[31]</ref> are most closely related to our network architecture.</p><p>In particular, we attempt to take advantage of novel ideas in RDN and RCAN. RCAN introduced residual in residual (RIR) architecture, and the ablation study showed that the performance gain by RIR was the most significant. Thus, we use this RIR principle in our architecture design. In addition, RDN itself is an image restoration network but we use it with modifications as a component of our network and construct a cascaded structure of RDNs as our image denoising network. Recent studies also showed the effectiveness of attention modules. Among many attention modules, convolutional block attention module (CBAM) <ref type="bibr" target="#b27">[28]</ref>, an easily implantable module that sequentially estimates channel attention and spatial attention, showed efficacy in general object detection and image classification, and thus we include CBAM into our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">GAN</head><p>The amount of training images in publicly available realworld image denoising datasets such as SIDD and DND may not be enough to train a deep and wide neural network. One feasible way of augmenting these datasets is to exploit the capability of GAN <ref type="bibr" target="#b8">[9]</ref>. The first GAN-based real-world noise modeling method <ref type="bibr" target="#b4">[5]</ref> uses only real-world noisy images for training the noise generator, where the discriminator is trained to distinguish between real and simulated noise signals. The noise generator is then used to add synthetic but realistic noise to noise-free ground-truth images, and the denoising network is finally trained using the generated pairs of ground-truth and noisy images. The real-world image denoising performance was significantly improved by using the dataset generated by GAN.</p><p>We improve the previous GAN-based real-world noise simulation technique <ref type="bibr" target="#b4">[5]</ref> by including conditioning signals such as the noise-free image patch, ISO, and shutter speed as additional inputs to the generator. The conditioning on the noise-free image patch can help generating more realistic signal-dependent noise and the other camera parameters can increase controllability and variety of simulated noise signals. We also change the discriminator of the previous architecture <ref type="bibr" target="#b4">[5]</ref> by using a recent relativistic GAN <ref type="bibr" target="#b11">[12]</ref>. Unlike conventional GANs, the discriminator of the relativistic GAN learns to determine which is more realistic between real data and fake data. Our method is different from the conventional relativistic GAN in that both the real and fake data are used as an input to make the discriminator more explicitly compare the two data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Denoising Network</head><p>Our image denoising network architecture called GRDN is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Our designing principle is to distribute burdens of each layer such that a deeper and wider network can be well trained. To this end, residual connections are applied in four different levels. Down-sampling and upsampling layers are included to enable a deeper and wider architecture and CBAM <ref type="bibr" target="#b27">[28]</ref> is also applied.</p><p>Inspired by RDN <ref type="bibr" target="#b32">[33]</ref>, we use RDB as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>(a) as a building module. In RDN, the features from cascaded RDBs are concatenated together and followed by the 1×1 convolutional layer. We define this feature concatenation part of RDN, as shown in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, as GRDB and use it as a building module of our GRDN. Note that the original RDN <ref type="bibr" target="#b32">[33]</ref> applies convolutional layers before and after GRDB and uses global residual learning for image denoising. However, we consider that RDN imposes a heavy burden to the very last 1×1 convolutional layer of GRDB. Therefore, we instead cascade GRDBs such that the features from RDBs can be fused in multiple stages. Motivated by many recent image restoration networks including RDN <ref type="bibr" target="#b32">[33]</ref>, we also include the global residual connection such that the network can focus on learning the difference between the noisy and ground-truth images. Last, we exploit CBAM as a building module to further improve the denoising performance. The position of the CBAM block was empirically chosen as in-between the upconvolutional layer and the last convolutional layer. Although GRDN is structurally deeper than RDN <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref>, we used the same number of RDBs. Specifically, <ref type="bibr" target="#b15">16</ref> RDBs were used in the original RDN for image denoising. We use 4 stack of GRDBs and each GRDB consists of 4 RDBs, resulting 16 RDBs in GRDN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GAN-based Real-world Noise Modeling</head><p>Motivated by the recent technique <ref type="bibr" target="#b4">[5]</ref>, we develop our own generator and discriminator for real-world noise modeling. Likewise with the previous technique <ref type="bibr" target="#b20">[21]</ref>, we use residual blocks (ResBlocks) as a building module of the generator. However, we made several modifications to improve the performance of real-world noise modeling. <ref type="figure" target="#fig_2">Fig. 3</ref> show the generator architecture. First, we include conditioning signals: the noise-free image patch, ISO, shutter speed, and smartphone model as an additional input to the generator. The conditioning on the noise-free image patch can help generating more realistic signal-dependent noise and the other camera-related parameters can increase controllability and variety of simulated noise signals. To train the generator with these conditioning signals, we used the metadata of SIDD <ref type="bibr" target="#b0">[1]</ref>. Second, spectral normalization (SN) <ref type="bibr" target="#b19">[20]</ref> is applied before batch normalization in the basic convolutional units like the one used in <ref type="bibr" target="#b28">[29]</ref>. Third, our ResBlock includes the residual scaling <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>. SN and residual scaling were empirically found to be useful in training our generator.</p><p>Our discriminator architecture as shown in <ref type="figure" target="#fig_3">Fig. 4</ref> is also different from the previous GAN-based noise simulation technique <ref type="bibr" target="#b4">[5]</ref>. Enhanced super-resolution GAN (ES-RGAN) <ref type="bibr" target="#b26">[27]</ref> showed that relativistic GAN <ref type="bibr" target="#b11">[12]</ref> is effective in generating realistic image textures. Unlike original GAN <ref type="bibr" target="#b8">[9]</ref>, the discriminator of relativistic GAN learns to determine which is more realistic between real data and fake data. Let C(x) denote the non-transformed discriminator output for input image x. The standard discriminator can then be expressed as D(x) = σ(C(x)), σ is the sigmoid function. The discriminator of relativistic average GAN (RaGAN) adopted in ESGAN is defined as:</p><formula xml:id="formula_0">D RaGAN (x r , x f ) = σ(C(x r ) − E[C(x f )]),<label>(1)</label></formula><formula xml:id="formula_1">D RaGAN (x f , x r ) = σ(C(x f ) − E[C(x r )]),<label>(2)</label></formula><p>where x r and x f denote real data and fake data, respectively, and E[·] represents the expectation operator, which is applied to all of the data in the mini-batch <ref type="bibr" target="#b26">[27]</ref>. The discriminator of the proposed network, defined as conditioned explicit relativistic GAN (cERGAN), is given as</p><formula xml:id="formula_2">D cERGAN (x c , x r , x f ) = σ(C(x c , x r , x f )),<label>(3)</label></formula><formula xml:id="formula_3">D cERGAN (x c , x f , x r ) = σ(C(x c , x f , x r )),<label>(4)</label></formula><p>where x c denote the conditioning signal. Specifically, we make each conditioning data have the same size as the training patch by replicating values, and thus our x c consists of 4 patches: 3 constant patches from smartphone code (e.g. Google Pixel = 0, iPhone 7 = 1, etc), ISO level, and shutter speed, and one noise-free image patch. In addition to x c , we also use both x r and x f as an input of the discriminator. Note that ESGAN uses either x r or x f as an input of the discriminator.</p><p>The loss functions of the generator and discriminator, denoted as L cERGAN G and L cERGAN D , respectively, are finally defined as follows:</p><formula xml:id="formula_4">L cERGAN G = 1 2 E[(D cERGAN (x c , x r , x f )) 2 ]+ 1 2 E[(D cERGAN (x c , x f , x r ) − 1) 2 ],<label>(5)</label></formula><formula xml:id="formula_5">L cERGAN D = 1 2 E[(D cERGAN (x c , x r , x f ) − 1) 2 ]+ 1 2 E[(D cERGAN (x c , x f , x r )) 2 ].<label>(6)</label></formula><p>In other words, if the second input is x r and the third input is x f , the discriminator is trained to predict a value close to 1, i.e., x r is more realistic than x f . If the two inputs are switched, the discriminator is trained to predict a value close to 0, i.e., x f is less realistic than x r . The generator is trained to fool the discriminator. By requiring the network to explicitly compare between real data and fake data, we could simulate more realistic real-world noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We implemented all of our models using PyTorch library with Intel i7-8700 @3.20GHz, 32GB of RAM, and NVIDIA Titan XP. <ref type="table">1st   2nd  3rd  4th  5th  6th  7th  Model  RDN GRDN GRDN GRDN GRDN GRDN GRDN  CBAM  ---Patch size  48  48  48  96  96  96  96  # of RDBs  16  16  16  16  16  20</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We used the training and validation images of NTIRE 2019 Real Image Denoising Challenge, which is a subset of SIDD dataset <ref type="bibr" target="#b0">[1]</ref>. Let ChDB denote the dataset we used for our experiment. Specifically, 320 high-resolution images and 1280 cropped image blocks with the size 256×256 were used for training and validation, respectively. The provided images were taken by five smartphone cameras -Apple iPhone 7, Google Pixel, Samsung Galaxy S6 Edge, Motorola Nexus 6, and LG G4. Because the ground-truth images of the test dataset are not publicly available, we report the performance of image denoising models using the validation dataset in this Section. Since we noticed nonmarginal degradations around image borders in groundtruth images, we excluded the first and last 8 rows/columns when generating training patches. General data augmentation techniques such as scaling, flipping, and rotation were not applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Denoising</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Implementation details</head><p>We augmented the provided training dataset by two ways. First, we used the author-provided source code of <ref type="bibr" target="#b9">[10]</ref> for adding synthetic noise to the ground-truth images. We also applied our own GAN-based noise simulator described in Sec.3.2 to generate additional synthetic noisy images.</p><p>In each training batch, we randomly extracted 16 pairs of ground-truth and noisy image patches. We trained using Adam <ref type="bibr" target="#b14">[15]</ref> with β 1 = 0.9, β 2 = 0.999. The initial learning rate was set to 10 −4 and then decreased to half at every 2 × 10 5 iteration. We trained the network using L 1 loss. We trained our model for approximately 5 days.</p><p>We used 4×4 filters for up/down-convolutional layer and 1×1 filters for fusing the features concatenated from RDBs. Otherwise, we used 3×3 filters. Zero-padding was used and dilation was not used for all convolutional layers. Each RDB has 8 pairs of convolutional layers and ReLU activation layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparison to RDN</head><p>First, we compared our GRDN model with RDN <ref type="bibr" target="#b31">[32]</ref>. The experimental result is shown in <ref type="table">Table 1</ref>. We re-trained RDN using ChDB. The 1st and 2nd columns in <ref type="table">Table 1</ref> correspond to RDN and proposed GRDN. It can be seen that the PSNR of our model is 0.04 dB higher than that of RDN. Note that RDN and GRDN have the same number of RDBs, and thus the number of parameters is similar. Specifically, our basic GRDN model has 22M parameters while RDN has 21.9M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Experiments on patch size</head><p>Since the original image resolution is very high (more than 12M pixels), the largest possible patch size needs to be used to include sufficient image contents. We thus increased the patch size to 96 × 96, which was the largest possible size in our experimental environment. By comparing the 2nd and 5th columns of <ref type="table">Table 1</ref>, we can see that the significant performance gain of 0.22dB was obtained by increasing the patch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Experiments on CBAM module</head><p>CBAM <ref type="bibr" target="#b27">[28]</ref> is a simple but effective module for CNNs. Because it is a lightweight and general module, it can be easily implanted to any CNN architectures without largely increasing the number of parameters. In particular, CBAM can be placed at bottlenecks of the network. Since we have down-sampling and up-sampling layers, we examined different positions and combinations of CBAMs. We concluded that for our model the best position of CBAM is after the up-sampling layer. We believe this indicates that CBAM enhances important features from the up-sampled data. It also helps to construct a final denoised image for the last convolution layer which comes after. The effectiveness of CBAM was found to be dependent on the complexity of the network. Comparing the 2nd and 3rd columns of <ref type="table">Table 1</ref>, CBAM increased the PSNR by 0.05 dB. However, after increasing the patch size, the gain by CBAM became diluted. Comparing the 5th and 6th columns of <ref type="table">Table 1</ref>, CBAM even decreased the PSNR by 0.01 dB. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Hyper parameter adjustment</head><p>We compared networks with different numbers of filters and GRDBs. Comparing the 6th and 7th columns of <ref type="table">Table 1</ref>, a less deeper but more wider network performed 0.02 dB better. Therefore, the model on the 7th column is the best performing model under our hardware constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Real-world Noise Modeling</head><p>For training the generator and discriminator of cER-GAN, we cropped image patches with the size 48×48 from real-world noisy images and their ground-truth images from ChDB. We used the batch size of 32 and Adam optimizer with β 1 = 0 and β 2 = 0.9. The generator and discriminator were trained for 340k iterations. The initial learning rate was set as 0.0002 for both discriminator and generator, and we linearly decayed the learning rate after 320k iterations such that the learning rate became 0 after the last iteration. <ref type="figure" target="#fig_4">Fig. 5</ref> illustrates some of noise image patches generated by the proposed cERGAN. As can be seen in Figs. 5(c) and (d), the proposed cERGAN can generate noise patches close to real-world noise.</p><p>The effectiveness of simulated noisy images was evaluated by comparing the proposed image denoising network trained with/without the simulated data. Here, the tested network corresponds to the 4th column of <ref type="table">Table 1</ref>. We first attempted to train our image denoising network using only the synthesized real-world noisy images obtained by cER-GAN. The average PSNR was obtained as 38.63 dB in ChDB validation set, which is inferior to the one we obtained using only the provided ChDB dataset (39.62 dB in <ref type="table">Table 1</ref>).</p><p>Second, we used the author-provided source code of <ref type="bibr" target="#b9">[10]</ref> for adding statistically modeled real-world noise to ground-truth images of ChDB. Our image denoising network trained using these dataset only resulted in 36.17 dB, which demonstrates that the proposed GAN-based noise modeling at least performs better than the statistic noise modeling method <ref type="bibr" target="#b9">[10]</ref>.</p><p>Last, we combined the original ChDB dataset with the synthetic datasets generated by the proposed cERGAN and conventional method <ref type="bibr" target="#b9">[10]</ref>. Here, we could test only one configuration: 90% from ChDB, 5% from simulated ChDB using <ref type="bibr" target="#b9">[10]</ref>, and 5% from simulated ChDB using cERGAN. <ref type="figure" target="#fig_5">Fig. 6</ref> shows that the PSNR obtained using the augmented dataset increases more stably. The resultant PSNR was obtained as 39.64 dB, which is slightly higher than the PSNR obtained using the original dataset (39.62 dB). we used the augmented ChDB using the technique mentioned in Sec. 4.3. Our model ranked 1st place for real image denoising both in terms of PSNR and SSIM. As shown in <ref type="table">Table 2</ref>, our model outperformed the 2nd rank method by 0.05 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">NTIRE2019 Image Denoising Challenge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed an improved network architecture for real-world image denoising. By using resid-ual connections extensively and hierarchically, our model achieved the state-of-the-art performance. Furthermore, we developed an improved GAN-based real-world noise modeling method.</p><p>Although we could evaluate the proposed network only to real-world image denoising, we believe that the proposed network is generally applicable. We thus plan to apply the proposed image denoising network to other image restoration tasks. We also could not fully and quantitatively justify the effectiveness of the proposed real-world noise modeling method. A more elaborate design is clearly necessary for better real-world noise modeling. We believe that our realworld noise modeling method can be extended to other realworld degradations such as blur, aliasing, and haze, which will be demonstrated in our future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed network architecture: GRDN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Components of GRDN: (a) RDB and (b) GRDB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>cERGAN generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>cERGAN discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Experimental results on real-world noise modeling: (a) The real-world noisy image patches in ChDB, (b) the ground-truth image patches, (c) difference between noisy and ground-truth image patches, and (d) noise patches generated by cERGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Convergence analysis of image denoising network with different dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>This work was supported by Institute for Information and communications Technology Promotion(IITP) grant funded by the Korea government(MSIP) 2017-0-00072, Development of Audio/Video Coding and Light Field Media Fundamental Technologies for Ultra Realistic Tera-media.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This work is proposed for participating in the NTIRE2019 Real Image Denoising Challenge -Track 2:sRGB. The challenge aims to develop an image denoising system with the highest PSNR and SSIM. The submitted image denoising network corresponds to 7th column of Table 1. One minor change in the submitted model is that we included skip connections for every 2 GRDBs. For training, standard deviation of 50, (d) denoised images using RDN, and (e) denoised images using our proposed network. Below each noisy and denoised image, its quality is provided as (PSNR (dB) / SSIM). The results are best viewed in the electronic version.</figDesc><table><row><cell></cell><cell cols="2">(26.32 / 0.7576)</cell><cell>(35.49 / 0.9812)</cell><cell>(39.11 / 0.9899)</cell><cell>(39.59 / 0.9902)</cell></row><row><cell></cell><cell cols="2">(19.05 / 0.3623)</cell><cell>(29.86 / 0.9314)</cell><cell>(37.05 / 0.9749)</cell><cell>(37.13 / 0.9748)</cell></row><row><cell></cell><cell cols="2">(17.56 / 0.2444)</cell><cell>(26.27 / 0.8255)</cell><cell>(33.50 / 0.9305)</cell><cell>(33.76 / 0.9347)</cell></row><row><cell></cell><cell cols="2">(18.73 / 0.2757)</cell><cell>(28.12 / 0.8385)</cell><cell>(29.44 / 0.8688)</cell><cell>(31.38 / 0.8846)</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell></cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell></row><row><cell cols="6">Figure 7: Comparison of image denoising methods: (a) Ground-truth images, (b) noisy images, (c) denoised images using</cell></row><row><cell>BM3D with the Method</cell><cell>PSNR</cell><cell>SSIM</cell><cell></cell><cell></cell></row><row><cell cols="3">GRDN (Ours) 39.931743 0.973589</cell><cell></cell><cell></cell></row><row><cell>2nd method</cell><cell cols="2">39.883139 0.973113</cell><cell></cell><cell></cell></row><row><cell>3rd method</cell><cell cols="2">39.818198 0.972963</cell><cell></cell><cell></cell></row><row><cell>4th method</cell><cell cols="2">39.675235 0.972554</cell><cell></cell><cell></cell></row><row><cell>5th method</cell><cell cols="2">39.610533 0.972637</cell><cell></cell><cell></cell></row><row><cell cols="4">Table 2: Performance comparison on the test dataset</cell><cell></cell></row><row><cell cols="4">of NTIRE 2019 Real Image Denoising Challenge -</cell><cell></cell></row><row><cell>Track2:sRGB.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1692" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">RENOIR -A benchmark dataset for real noise reduction evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<idno>abs/1409.8230</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<idno>abs/1811.11127</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3291" to="3300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image blind denoising with generative adversarial network based noise modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3155" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical Poissonian-Gaussian noise modeling and fitting for single-image raw-data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trimeche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1737" to="1754" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04686</idno>
		<title level="m">Toward convolutional blind denoising of real photographs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The relativistic discriminator: A key element missing from standard GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia J.-M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00734</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Practical signal dependent noise parameter estimation from a single noisy image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masayuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4361" to="4371" />
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05637</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">cGANs with projection discriminator. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1586" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<title level="m">Neural nearest neighbors networks. CoRR, abs/1810.12575</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DeepISP: Toward learning an end-to-end image processing pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="912" to="923" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MemNet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4549" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ESGAN: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops</title>
		<meeting>the European Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CBAM: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>In</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Selfattention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Residual dense network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno>abs/1812.10477</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
