<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Video Representation Learning Using Inter-intra Contrastive Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Tao</surname></persName>
							<email>taoli@hal.t.u-tokyo.ac.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Wang</surname></persName>
							<email>xt_wang@hal.t.u-tokyo.ac.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
							<email>yamasaki@hal.t.u-tokyo.ac.jp</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Video Representation Learning Using Inter-intra Contrastive Framework</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">20</biblScope>
							<date type="published">October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413694</idno>
					<note>ACM Reference Format: Li Tao, Xueting Wang, and Toshihiko Yamasaki. 2020. Self-supervised Video Representation Learning Using Inter-intra Contrastive Framework. In Pro-ceedings of the 28th ACM International Conference on Multimedia (MM &apos;20), October 12-16, 2020, Seattle, WA, USA. ACM, New York, NY, USA, 9 pages. * Corresponding Author ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Image representations</term>
					<term>Ac- tivity recognition and understanding</term>
					<term>Activity recognition and understanding</term>
					<term>• Information systems → Video search KEYWORDS Self-supevised learning, video representation, video recognition, video retrieval, spatio-temporal convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a self-supervised method to learn feature representations from videos. A standard approach in traditional self-supervised methods uses positive-negative data pairs to train with contrastive learning strategy. In such a case, different modalities of the same video are treated as positives and video clips from a different video are treated as negatives. Because the spatio-temporal information is important for video representation, we extend the negative samples by introducing intra-negative samples, which are transformed from the same anchor video by breaking temporal relations in video clips. With the proposed Inter-Intra Contrastive (IIC) framework, we can train spatio-temporal convolutional networks to learn video representations. There are many flexible options in our IIC framework and we conduct experiments by using several different configurations. Evaluations are conducted on video retrieval and video recognition tasks using the learned video representation. Our proposed IIC outperforms current state-of-the-art results by a large margin, such as 16.7% and 9.5% points improvements in top-1 accuracy on UCF101 and HMDB51 datasets for video retrieval, respectively. For video recognition, improvements can also be obtained on these two benchmark datasets. Code is available at https: //github.com/BestJuly/Inter-intra-video-contrastive-learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>There are many video understanding tasks, such as video captioning and video segmentation. These tasks rely on effective motion representation extractors, which are usually trained on the basis of video recognition. For video recognition, the works presented in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">42]</ref> have explored different network architectures. In <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref>, an additional optical flow stream was used to form a two-stream model. With optical flow, better results were achieved <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">42]</ref>. Hara et al. <ref type="bibr" target="#b11">[12]</ref> argued that they can imitate image recognition procedures, which means that the performance can be significantly improved with large datasets.</p><p>Though larger datasets are helpful for video understanding tasks and numerous unlabeled videos are available on the Internet, annotating new video datasets requires a wealth of resources. Ensuring the performance of training action classification networks usually requires properly trimmed action video clips, which makes the situation more serious. Therefore, it is valuable if the unlabeled videos can be leveraged to facilitate learning. From this point of view, self-supervised learning is drawing a lot of attention these days beacuse it does not require any labels.</p><p>Many self-supervised learning techniques are proposed for image data. There are several designed tasks such as solving jigsaw puzzles <ref type="bibr" target="#b26">[27]</ref>, image inpainting <ref type="bibr" target="#b28">[29]</ref>, and image color channel prediction <ref type="bibr" target="#b44">[44]</ref>. For video data, existing works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b43">43]</ref> have focused on changing the temporal information and making models sensitive to the differences. The aforementioned methods can be classified into a single category, which we call as intra-sample learning because all operations are carried out in the sample itself. For example, if a video contains several frames, shuffling frames to change their order is performed within this sample.</p><p>In addition to intra-sample learning, inter-sample learning is also a kind of self-supervised learning technique. For image data, when we have an anchor image crop, crops from the same image are treated as positives while crops from different images are treated as negatives. If a model can distinguish whether samples come from the same sample set or not, it is certain that this model can extract discriminative features, which may be good feature representations. The procedure is almost the same for video data.</p><p>For intra-sample learning methods, tasks should be carefully designed, whereas inter-sample learning methods are simpler. However, for inter-sample learning, whether good temporal information can be extracted relies on the model itself. Further, if spatial information is sufficient enough as compared to its temporal information, the model will not be helpful for other video related tasks. Therefore, our goal is to learn better representations that can capture rich temporal information. To do so, we break the temporal relationship of the anchor sample to generate intra-negative samples. Then the models can learn spatial differences as well as temporal differences between samples. In particular, we adapt the recently proposed method of Contrastive Multiview Coding (CMC) <ref type="bibr" target="#b35">[36]</ref>, extend it with more generalized video recognition models, and improve it by introducing intra-negative sample learning. The general idea of our proposed method is illustrated in <ref type="figure">Fig. 1</ref> In this paper, we propose Inter-Intra Contrastive (IIC) learning framework in videos, which are built on the basis of many existing techniques such as inter-intra learning, contrastive learning, and deep representation learning. To the best of our knowledge, we are the first to focus and apply these techniques together to videos. Recent self-supervised learning in videos has mainly used intra-sample learning methodologies, and video retrieval and video recognition tasks were considered as evaluation tasks. Our main contribution is to combine the advantages of inter-and intra-sample learning and establish a general framework for self-supervised learning for video representation. We have explored several options towards best practices in our framework. In addition, for both video retrieval and video recognition tasks, we outperform the existing state-of-the-art results by a notably large margin.</p><p>Our contributions are summarized as follows:</p><p>• We generate intra-negative samples by breaking temporal relations, which encourage the model to learn rich temporal information as well as spatial information, and it is helpful for motion feature representation. • We extend the contrastive multiview coding framework to have an inter-intra style for video representation, where many useful options in the framework are also provided. • Our experiments show that by using the proposed IIC framework, significant improvements over the state-of-the-art methods are achieved with the same network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>In this section, we divide the existing self-supervised learning methods into two categories according to their learning style, namely inter-sample learning and intra-sample learning. In addition, because we focus on video representation, we add another subsection to briefly introduce the techniques in video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Intra-sample learning</head><p>For intra-sample learning methods, the constraints are in the sample itself. By using different transformation functions, some relations are broken down even though statistical or semantic information remains. Different target tasks are carefully designed to help train the model. Self-supervised learning methods are close to unsupervised representation learning, and include methods such as autoencoders <ref type="bibr" target="#b15">[16]</ref> and variational autoencoders <ref type="bibr" target="#b19">[20]</ref>. Noroozi et al. <ref type="bibr" target="#b26">[27]</ref> proposed to learn features by solving Jigsaw puzzles. Pathak et al. <ref type="bibr" target="#b28">[29]</ref> trained context inpainting models to learn feature representation. Gidaris et al. <ref type="bibr" target="#b7">[8]</ref> rotated images and trained models by predicting the rotated angles.</p><p>Because videos have an additional temporal axis compared to images, for video representation learning, how to efficiently extract temporal information is important. There are many existing works focusing on temporal orders <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b43">43]</ref>. Misra et al. <ref type="bibr" target="#b25">[26]</ref> treated several video frames as a sequence, and trained a network to distinguish whether these video frames were in the right order. Odd-one-out network (O3N) <ref type="bibr" target="#b5">[6]</ref> was proposed to identify unrelated or odd video clips. Order prediction network (OPN) <ref type="bibr" target="#b22">[23]</ref> shuffled frames and trained networks to predict the correct order of input frames. Similar to OPN, Xu et al. <ref type="bibr" target="#b43">[43]</ref> set video clips as inputs and used 3D convolutional networks to predict the order. In addition to focusing on the temporal order, Wang et al. <ref type="bibr" target="#b38">[39]</ref> proposed regressing motion and appearance statistics to learn video representations. Kim et al. <ref type="bibr" target="#b18">[19]</ref> proposed training models by completing space-time cubic puzzles. Luo et al. <ref type="bibr" target="#b23">[24]</ref> applied one transformation from several options, including spatial rotation and temporal shuffling, to video clips and trained models to recognize which action has been applied. The performance of these methods depends highly on the special designed tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inter-sample learning</head><p>For inter-sample learning methods, features from the same sample should be close to each other while the distance between different samples should be far from each other.</p><p>In <ref type="bibr" target="#b30">[31]</ref>, frames from the same video were treated as positives while frames from different videos were negatives. And triplet loss <ref type="bibr" target="#b14">[15]</ref> was used to train the network. After contrastive losses <ref type="bibr" target="#b10">[11]</ref> were proposed, contrastive learning has become the core of selfsupervised learning, especially on image data. Contrastive Predictive Coding (CPC) <ref type="bibr" target="#b27">[28]</ref> used sequential data to learn the future from the past. Deep InfoMax <ref type="bibr" target="#b16">[17]</ref> and Instance Discrimination <ref type="bibr" target="#b41">[41]</ref> learned to maximize information probability from the same sample. Contrastive Multiview Coding (CMC) <ref type="bibr" target="#b35">[36]</ref> used different views for the same sample and minimized the distance between different views while maximizing the distance between different samples. MoCo <ref type="bibr" target="#b12">[13]</ref> used a momentum encoder with a momentum-updated encoder to conduct contrastive learning. In SimCLR <ref type="bibr" target="#b2">[3]</ref>, different combinations of data augmentation methods were experimented for paired samples. Note that none of these inter-sample learning methods require specially designed tasks.</p><p>Our proposed method is closely related to CMC <ref type="bibr" target="#b35">[36]</ref>, where multi-view coding is used. Most methods treat data from different samples as negatives. In our research, we deal with video data and extend negative samples by breaking the temporal relationships in video clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Video representation</head><p>Previous self-supervised learning methods have mainly been applied on images. Some video representation learning methods still use image frames as inputs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>, which do not enjoy the benefits from new techniques related to video understanding.</p><p>For video representation, many supervised methods have been proposed. Temporal Segment Networks (TSN) <ref type="bibr" target="#b39">[40]</ref> split one video into several segments and sampled one frame from each segment as the input data of a 2D CNN. In addition to a single 2D CNN for RGB data, two-stream ConvNets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref> have been used with an additional optical flow stream. Recently, spatio-temporal convolution (3D-CNN) was applied to video recognition task. In C3D <ref type="bibr" target="#b36">[37]</ref>, Tran et al. used 3D convolutional layers to form their network and achieved good performance. 3D convolutional versions of ResNet <ref type="bibr" target="#b13">[14]</ref> and Inception net <ref type="bibr" target="#b33">[34]</ref>, R3D <ref type="bibr" target="#b11">[12]</ref> and I3D <ref type="bibr" target="#b1">[2]</ref>, were proposed and showed promising performance on benchmark datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>. By separating one 3D convolutional kernel into two steps, a spatial part and a temporal part, R(2+1)D <ref type="bibr" target="#b37">[38]</ref> and S3D <ref type="bibr" target="#b42">[42]</ref> were proposed. Those trained models were proved to be effective feature extractors when applied to other video related tasks.</p><p>The aforementioned models can also be used in self-supervised learning to handle video data. By replacing a 2D CNN with a 3D CNN, <ref type="bibr" target="#b43">[43]</ref> reported better performance than <ref type="bibr" target="#b22">[23]</ref> as their target tasks were the same, predicting the temporal orders of inputs. C3D, R3D, and R(2+1)D were used in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">43]</ref> and proved to be effective for self-supervised learning with video data. Similar to these methods, 3D convolutional networks are used in our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>Our goal is to learn discriminative feature representations from videos, not only for distinguishing one action from another, but also for capturing rich temporal information. The entire IIC framework is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. In this section, we start from the novel input part, and then elaborate on contrastive learning with these inputs.</p><p>Because we simplify the model by using only one network to cope with three kinds of input data, an unique joint retrieval method will also be introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-view and intra-negative inputs</head><p>We denote video data from two different views as X 1 and X 2 , and data with the same video id i from these two views as x 1</p><p>i and x 2 i , respectively. The definition of view here is broad, including data in different color space, depth information, segmentation information, etc. Without loss of generality, only two views are used in this work, which can be extended with more views of the same sample. We use a 3D convolutional network as our backbone. Therefore, the referred data x 1 i and x 2 i are in shape T HW C, where T continuous frames with height H and width W are stacked together. C is the channel number of frames. Temporal information relies on the connections among T stacked frames.</p><p>For multi-view contrastive learning, feature v 1 i and feature v 2 i should be close to each other because those features are extracted from the same video i. In addition, feature v 1 i should be far from features v 1 j (for j i). This is effective enough for images. On the other hand, video data have one more dimension. When the same person behaves in opposite ways, e.g. standing up and sitting down, the appearance information of each frame is similar, however, traditional contrastive learning methods will easily be fooled.</p><p>Here, we introduce intra-negative samples in multi-view contrastive learning for videos by breaking the temporal relationship. For one video clip, the data x 1 i is a set of frames. To simplify, we use { f rame 1 , ..., f rame T } to represent a set of temporally-ordered frames. Two kinds of methods, frame repeating and temporal shuffling, are proposed to break the temporal relationship of a video clip ( <ref type="figure" target="#fig_1">Fig. 3</ref>). Frame repeating. One frame that is randomly selected from one video clip is repeated T times to generate intra-negative samples (eq. 1). Then all frames in this video clip are the same and no movements exist anymore, despite the scene of this intra-negative sample is almost the same as the source.</p><formula xml:id="formula_0">x r epeat = { f rame k , ..., f rame k }, k = random(1,T ).</formula><p>(1)</p><p>Temporal shuffling. In the original video clip, frames are in the correct sequence. By randomly shuffling the frames (eq. 2), the actions will be strange and the corresponding action information should be different. After this transformation, the global statistical information of the intra-negative sample remains the same as its source.</p><formula xml:id="formula_1">x shuf f l e = shu f f le(x), where x shuf f l e x .<label>(2)</label></formula><p>Note that intra-negative samples can be generated from data for both view 1 and view 2, and both generating functions can be used simultaneously. To simplify, in this paper, we only generated intra-negative samples from view 1 (original RGB video clips) and only one generating function was used for each experiment. Then x neд is used to represent intra-negative samples from x 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive learning</head><p>Similar to learning with triplet loss, which aims to learn an embedding that separates the negative samples from the positive and the anchor, contrastive learning aims to separate samples from two different distributions. In traditional multi-view coding <ref type="bibr" target="#b35">[36]</ref>, the sample pairs  contrastive learning methods train this function to correctly select a positive sample out of a set S = {v 2 1 , ..., v 2 i , ..., v 2 k +1 }, which contains one positive sample v 2 i and k negative samples. In our proposed method, another set S neд = {v neд 1 , ..., v neд k +1 } is also used which only contains negative samples. The loss function is similar to recent works for contrastive learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref>:</p><formula xml:id="formula_2">{x 1 i , x 2 i } are positives while {x 1 i , x 2 j }(i j) are</formula><formula xml:id="formula_3">L v 1 i cont r ast = − log h θ ({v 1 i , v 2 i }) k+1 j=1 h θ ({v 1 i , v 2 j }) + k+1 j=1 h θ ({v 1 i , v neд j }) .</formula><p>(3) Here, k is the number of negative samples, which can be equal to N −1, where N is the total number of training samples. To accelerate training, we randomly select k samples from N where k ≪ N .</p><p>The critic h θ (·) is implemented by feature representations using the non-parametric softmax technique <ref type="bibr" target="#b41">[41]</ref>. Then we can compute this function as the following:</p><formula xml:id="formula_4">h θ ({v 1 i , v 2 j }) = exp v 1 i · v 2 j ∥v 1 i ∥ · ∥v 2 j ∥ · 1 τ ,<label>(4)</label></formula><p>where τ is a hyper-parameter that controls the range of the results. In practice, three memory banks are used to store the extracted features from previous iteration, and these features function as weights in the non-parametric softmax learning <ref type="bibr" target="#b41">[41]</ref>. Eq. 3 only treats view 1 as an anchor. When treating view 2 as an anchor, symmetrically, another loss can be calculated and they are added to form the final loss function:</p><formula xml:id="formula_5">L = L v 1 cont r ast + L v 2 cont r ast .<label>(5)</label></formula><p>To summarize this section, we write the process flow of our proposal in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint representation</head><p>For learning video representations with supervision, different modality data require different models because the input channels are usually different. Because stacked frame differences, which have the same shape as the original RGB video clips, have succeeded in supervised learning <ref type="bibr" target="#b34">[35]</ref>, it is possible to use one network to handle video data in different views. In practice, we constrain data from different views in the same shape and use only one model to process data from different views. The options for different views are original RGB clips, optical flow (u or v) frame clips and stacked frame differences, and we used RGB clips as the anchor view. In the following part, for convenience, we use residual clip to represent stacked frame differences.</p><p>After training is complete, the model can be used to extract features from different views of videos. These features can be concatenated to form two-view features and can be applied to video retrieval <ref type="figure" target="#fig_2">(Fig. 4)</ref>. We address this because for two-stream methods, features extracted from different models can be also concatenated to represent one video. With our multi-view contrastive learning approach, different views of the same video are set as the input of only one model. According to our experiments, it is sufficient to use one model to handle different view data. This strategy can help to extract more comprehensive representations of videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We conducted extensive experiments to evaluate our proposed IIC framework and its transferability when applied to other tasks or datasets. Because there are several options in our framework, we first elaborate on some option configurations on video retrieval tasks because this can be evaluated directly when the training has been performed. The trained models are also treated as one kind of weight initialization strategy and by fine-tuning trained models on video recognition datasets, we can further evaluate the performance of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>There are several existing labeled benchmark datasets in video recognition: UCF101 <ref type="bibr" target="#b32">[33]</ref>, HMDB51 <ref type="bibr" target="#b21">[22]</ref>, something-something <ref type="bibr" target="#b8">[9]</ref>, and Kinetics400 <ref type="bibr" target="#b17">[18]</ref>. For fair comparison, we follow <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">43]</ref> and use the UCF101 and HMDB51 datasets. The UCF101 dataset consists of 13,320 videos in 101 action categories. HMDB51 is comprised of 7,000 videos with a total of 51 action classes. If not specially declared, experiments were on split 1 for both UCF101 and HMDB51 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation tasks</head><p>Our goal is to learn effective and discriminative video representations using IIC learning framework. After training has been performed, the direct way to evaluate is to use the trained model to extract video features, then video retrieval can be tested easily. UCF101 and HMDB51 are two different datasets. We trained our model only on UCF101 split 1 and performed video retrieval on both UCF101 and HMDB51 datasets. When applied to HMDB51, the model generalization ability was mainly tested. To evaluate whether good feature representations are learned with our proposed method, we also conducted experiments by fine-tuning trained models on both UCF101 and HMDB51 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Options in our IIC framework</head><p>Multiple Views. For video representation learning, traditional RGB input and the corresponding optical flow were set as two common views <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36]</ref>. Optical flow data require additional calculation and storage. In addition to optical flow, in video tasks, frame difference has also been used in existing works <ref type="bibr" target="#b39">[40]</ref> with 2D ConvNets. Residual frames with 3D ConvNets have been proved to be more effective compared to original RGB video clips <ref type="bibr" target="#b34">[35]</ref>, which can also be set as one view of video data. In our experiments, we chose RGB video clips and another view from residual frames or optical flow. Then the modality for the second view was from res, u, and v.</p><p>Backbone networks. 3D convolutional kernels have been proved effective in many recent works. Recent self-supervised learning methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">43]</ref> used C3D <ref type="bibr" target="#b36">[37]</ref>, R3D <ref type="bibr" target="#b11">[12]</ref>, and R(2+1)D <ref type="bibr" target="#b37">[38]</ref> as their network backbones. We mainly used R3D in our experiments, where each residual block consists of two 3D convolution layers. Intra-negative generation. As we discussed in section 3.1, we have two ways to generate intra-negative samples, frame repeating and temporal shuffling. In our experiments, both were tested and only one generation method was used in each experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation details</head><p>The input preparation part follows <ref type="bibr" target="#b36">[37]</ref>. Sixteen successive frames are sampled with size 128 × 171 to form a video clip for both view 1 and view 2. Random spatial cropping was conducted to generate an input data of size 16 × 112 × 112, where the channel number 3 was ignored. For the data from the second view, when residual frames were used, we shifted the RGB video clip along the temporal axis and calculated the difference between the original clip and the shifted clip. When optical flow was used, because traditional tv-l1 optical flow calculates motion features in two directions-u and v, we picked one direction and duplicate it to generate optical flow clips with channel dimension 3. Then we can use one 3D ConvNet to handle data from different views.</p><p>When performing temporal shuffling, similar to <ref type="bibr" target="#b23">[24]</ref>, one clip was divided into four sub-clips, and we shuffled the sub-clips to conduct temporal shuffling.</p><p>When training unsupervised procedure, the batch size is set to 16 and training lasted for 240 epochs. The initial learning rate was set to 0.01 and it was updated by multiplying a fixed rate of 0.1 after 45, 90, 125 and 160 epochs. In non-parametric learning part, 2k negative samples were sampled from memory banks, with k set to 1,024 for all our experiments. Video retrieval was conducted by K nearest neighbor search. When evaluating task transferability, we used our trained models as an initialization strategy and the learning rate was set to 0.001 for fine-tuning. The best performance on the validation dataset was used for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND DISCUSSION</head><p>In this section, we first report our ablation studies with several option configurations that were mentioned above. Then, we outline the four option configurations that were selected to compete with the state-of-the-art methods in self-supervised spatio-temporal learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation study</head><p>Joint retrieval. Because the models were evaluated on video retrieval task after self-supervised training process was done, we first conducted ablation studies on the effectiveness of proposed joint retrieval strategy. Results are shown in <ref type="table" target="#tab_0">Table 1</ref>. Here, the performance was tested by treating residual frames as the second view. Experiments were conducted on UCF101 split 1.</p><p>As we can see from the table, by using only one model to process data from two different views, when features were concatenated to represent videos, nearly 2% points improvements could be achieved at top-1 retrieval accuracy. This indicates that these  features were more robust for video representation. We set the proposed joint retrieval strategy as the default setting for the following experiments.</p><p>Option configurations. We conducted experiments in three parts: 1. whether to use intra-negative samples or not; 2. which intranegative sample generation method to use; 3. which modality was to be chosen for the second view. As we can see from <ref type="table" target="#tab_1">Table 2</ref>, no matter which modality was used as the second view, all retrieval accuracies were better when the proposed intra-negative samples were used in multi-view coding.</p><p>For residual frames, which can be treated as one solution without optical flow, the best performances were achieved by using frame repeating strategy. If temporal shuffling was used, improvements could also be obtained.</p><p>When optical flow data was used, the results outperformed those using residual frames as the second view. This makes sense because optical flow data requires additional calculations and is designed to represent motion features, which is suitable for video representation. The best performance on the top-1 and top-5 accuracies was achieved by using temporal shuffling to generate intra-negative samples with v data while for the top-10, top-20, and top-50 accuracies, using frame shuffling with u data was better. This is reasonable because u and v are two dimensions of optical flow data, which record movements in two directions. The performance relies on the main movement directions in videos. This also highlighted one limitation of the current settings, and indicated that it may be better to use both u and v data to form the optical flow stream for selfsupervised learning, which requires two different models. In the present study, we used only one model for data from both views. We intend to explore the use of multiple models for different views in our future work.</p><p>All the aforementioned results indicate that by introducing intranegative samples as additional negatives in contrastive learning, the model could focus more on learning discriminative temporal information, which is helpful for feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visualization: feature embedding</head><p>Before applying our proposed method to other evaluation tasks, we used trained models to extract video features and qualitatively evaluated these features by visualization in order to verify whether good feature representations have been learned. We selected videos from UCF101 which belong to the first 10 categories (arranged by action names in alphabetical order). Features were projected to 2-dimensional space using t-SNE <ref type="bibr" target="#b24">[25]</ref>. <ref type="figure" target="#fig_3">Fig. 5</ref> visualizes the embedding of the features extracted by the baseline <ref type="bibr" target="#b35">[36]</ref> and our proposed method. It is obvious that with intra-negative samples, the trained models showed better clustering ability for video data. We quantitatively observe that better video representations were able to be learned by our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison: video retrieval</head><p>For a fair comparison, we trained our models on UCF101 and tested them on both UCF101 and HMDB51 datasets. Specially, the most related work CMC <ref type="bibr" target="#b35">[36]</ref> mainly focused on Image-related tasks and for video representation part, two CaffeNets <ref type="bibr" target="#b20">[21]</ref> with two views-RGB and optical flow-were used. We reimplemented their work using a more common model, ResNet-18 <ref type="bibr" target="#b13">[14]</ref>. The model is a 2D convolutional style and for both RGB and optical flow data, the spatial size is 224 × 224. In addition, we experimented with a oneview model to prove the effectiveness of multiple views for videos. To simplify, we set the modality of the second view to be RGB which is the same as the first view.</p><p>The results on the UCF101 dataset are shown in <ref type="table" target="#tab_2">Table 3</ref>. The compared models mostly design a special task for the model to learn, which belong to the intra-sample learning category. IIC treats every different samples as negative. In addition, intra-negative samples are generated to enable the model to learn more temporal clues. We picked four option configurations, two of which do not use optical  flow data. As shown in this table, when only RGB video clips were used with contrastive learning, the top-1 accuracy was already higher than that of previous works. Our implemented CMC <ref type="bibr" target="#b35">[36]</ref> used an optical flow stream and two models for different views, obtaining 26.4% points at top-1 accuracy. By using our proposed method with residual frames as the second view, the performance was improved to 36.5%, which is 10.8% points higher than the current state-of-the-art results. With optical flow data as the second view, this record could even reach 42.4%. VCP <ref type="bibr" target="#b23">[24]</ref> and VCOP <ref type="bibr" target="#b43">[43]</ref> could reach a better performance than the numbers we picked in the table using R(2+1)D network, the best of which is 19.9%. We did not include this here because we only use R3D as our network backbone. However, the performance of our worst model, which was 34.6%, was still much better. In <ref type="figure" target="#fig_4">Fig. 6</ref>, qualitative results also show superiority of IIC compared with the baseline. The transferability of the trained model was also tested on the HMDB51 dataset. The results are shown in <ref type="table" target="#tab_3">Table 4</ref>. A similar conclusion can be drawn. If only RGB video clips were used in the contrastive learning framework, the top-1 accuracy was 10.8%. This performance was even 0.6% higher than our implementation of the CMC method. This may have been caused by the effectiveness of 3D ConvNets. Without optical flow data, the performance of our proposed method is 13.4% and 13.2% respectively for the two different intra-negative sample generation strategies, revealing that inter-sample learning with intra-negatives is a good approach for self-supervised spatio-temporal feature learning, and is also good for unseen datasets. The best performance was obtained when optical flow data were treated as the second view, where 19.7% on top-1 accuracy was achieved, outperforming the current state-of-the-art results by VCP <ref type="bibr" target="#b23">[24]</ref> over 12% points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison: video recognition</head><p>Video feature representation is usually learned by supervised learning for video recognition task. Here we used our proposed method as an initialization strategy and the models were fine-tuned on two benchmark datasets, UCF101 <ref type="bibr" target="#b32">[33]</ref> and HMDB51 <ref type="bibr" target="#b21">[22]</ref>. The comparisons are among self-supervised methods for fair comparison because supervised methods usually used a much larger dataset, Kinetics <ref type="bibr" target="#b17">[18]</ref>, together with labels to pre-train their models. When training models using our proposed IIC, data from different views were set as inputs to the same network. The distributions of data from different views were different. We argue that even though the network could handle different kinds of input, it would become a bottleneck when fine-tuned with labeled data from different views. Therefore, we had two choices when fine-tuning our trained models on the action recognition datasets: 1. use RGB video clips; 2. use the same data modality as view 2.</p><p>Because we had several settings for self-supervised training, four option configurations were selected and these models were fine-tuned with data from the two different views separately. This small test was conducted on UCF101 split 1 only, and the results are shown in <ref type="table" target="#tab_4">Table 5</ref>. As we can see from the table, fine-tuning with RGB inputs yielded a worse performance than using the modality of the second view. Then we compare our model with other selfsupervised learning methods by fine-tuning our models with the modality of the second view.</p><p>From <ref type="table" target="#tab_5">Table 6</ref>, we can see that the model with ImageNet pretrained weights can had a better performance than most methods. Recent methods have achieved better performances than a random initialization strategy, which means temporal information has been embedded by self-supervised learning to some extent. By fine-tuning models with data from the second modality, IIC could outperform VCP <ref type="bibr" target="#b23">[24]</ref> by at maximum 8.4% points when using the same network architecture. Note that the best performance on UCF101 dataset, 74.4% at the top-1 accuracy, was achieved without using optical flow data, and better than O3N <ref type="bibr" target="#b5">[6]</ref> which also used residual inputs. Though the best performance for video retrieval was achieved with optical flow data, it was not as good as residual frames when fine-tuning. This could have been caused by a bias when training during self-supervised learning. If we only used one model to handle all input data, RGB video clips, its intra-negatives and optical flow data, the model may have concatenated more on distinguishing inputs with similar distributions as RGB video clips, resulting in bad initialization when fine-tuning only using optical flow data. Retrieval with our joint strategy can eliminate this drawback. This phenomenon could be improved if we use different models to handle different modality of data, which is also an option in our proposed framework. We leave this as our further work.</p><p>The transferability was again tested on the HMDB51 dataset, which is more complicated because this is not only transferable for different tasks, but also on different datasets. <ref type="table" target="#tab_5">Table 6</ref> shows that IIC could handle this situation. With residual frames, our model yielded 38.3% on HMDB51, which is the best among those methods without optical flow data. The size of ImageNet is much larger than our pre-trained dataset, UCF101 split 1, while better performance was achieved. This indicates that our proposed method can be set as a good initialization strategy for other video related tasks. With optical flow data, especially u data, good results can also be obtained, outperforming VCP [24] by 5.3% points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we proposed IIC, a self-supervised method for video representation learning, to learn rich temporal features from videos. We utilized the advantages of intra-and inter-sample learning and trained a spatio-temporal convolution neural network (3D-CNN) with intra-negative samples in contrastive multiview coding. Two intra-negative sample generation functions were proposed which break the temporal relations in input video clips. Different view selection options were also experimented. The trained models had learnt video representation and were applied to two video tasks, video retrieval and video recognition. With only one model handling different inputs, we could apply a joint retrieval strategy and our results showed that our models could outperform other methods by a large margin on video retrieval task. Experiments on video recognition also indicated that our proposed method could help the model learn better video representation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>negatives. Because we generate intra-negative samples, the negatives are extended by adding pairs {x 1 i , x neд j }, where j can be equal to i. A discriminative function h θ (·) is used to ensure that positive pairs have high values while the value for negative pairs should be low. The function is trained by selecting a single positive sample from a set of data. After feature v 1 i has been extracted, traditional (v, v, M์, Mํ, M neg ) The main framework of IIC. Intra-negative samples are generated from the first view by breaking its temporal relationship. Video clips from two different views as well as the intra-negative clip are used in one iteration. Features are processed with three corresponding memory banks and non-parametric weights are obtained. Contrastive loss is used for the optimization of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Generating intra-negative samples from original video clips.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Joint retrieval using two different kinds of input data and only one network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Feature visualization by t-SNE. Features extracted by our proposed method are more semantically separable compared to the baseline, which does not use intra-negative samples during training. Each video is visualized as a point, with videos belonging to the same action category having the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of video retrieval results with baseline method. Red fonts indicate correct retrieval results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effectiveness of joint retrieval.</figDesc><table><row><cell cols="3">intra-neg retrieval mode top1 top5 top10 top20 top50</cell></row><row><cell>×</cell><cell>view1: rgb</cell><cell>32.5 47.6 57.2 68.3 81.0</cell></row><row><cell>×</cell><cell>view2: res</cell><cell>32.4 50.6 60.4 69.8 80.6</cell></row><row><cell>×</cell><cell>joint</cell><cell>34.6 52.1 61.8 71.4 82.4</cell></row><row><cell>✓</cell><cell>view1: rgb</cell><cell>34.8 51.6 60.8 69.7 80.6</cell></row><row><cell>✓</cell><cell>view2: res</cell><cell>33.4 53.2 62.5 72.0 83.6</cell></row><row><cell>✓</cell><cell>joint</cell><cell>36.5 54.1 62.9 72.4 83.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on different option configurations. The best performances are in bold for each modality. Note that RGB video clips are fixed for view 1 and the modalities here represent only for view 2.</figDesc><table><row><cell cols="8">Intra-neg type view2 top1 (%) top5 (%) top10 (%) top20 (%) top50 (%)</cell></row><row><cell>×</cell><cell>-</cell><cell>res</cell><cell>34.6</cell><cell>52.1</cell><cell>61.8</cell><cell>71.4</cell><cell>82.4</cell></row><row><cell>×</cell><cell>-</cell><cell>u</cell><cell>37.5</cell><cell>54.8</cell><cell>64.1</cell><cell>72.6</cell><cell>83.5</cell></row><row><cell>×</cell><cell>-</cell><cell>v</cell><cell>34.7</cell><cell>53.4</cell><cell>63.5</cell><cell>72.0</cell><cell>82.9</cell></row><row><cell>✓</cell><cell cols="2">repeat res</cell><cell>36.5</cell><cell>54.1</cell><cell>62.9</cell><cell>72.4</cell><cell>83.4</cell></row><row><cell>✓</cell><cell cols="2">repeat u</cell><cell>41.8</cell><cell>60.4</cell><cell>69.5</cell><cell>78.4</cell><cell>87.7</cell></row><row><cell>✓</cell><cell cols="2">repeat v</cell><cell>34.3</cell><cell>55.9</cell><cell>65.3</cell><cell>73.2</cell><cell>83.3</cell></row><row><cell>✓</cell><cell cols="2">shuffle res</cell><cell>34.6</cell><cell>53.0</cell><cell>62.3</cell><cell>71.7</cell><cell>82.4</cell></row><row><cell>✓</cell><cell cols="2">shuffle u</cell><cell>39.2</cell><cell>57.7</cell><cell>66.6</cell><cell>75.1</cell><cell>84.7</cell></row><row><cell>✓</cell><cell cols="2">shuffle v</cell><cell>42.4</cell><cell>60.9</cell><cell>69.2</cell><cell>77.1</cell><cell>86.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Video retrieval performance on UCF101.</figDesc><table><row><cell>Methods</cell><cell cols="5">top1 (%) top5 (%) top10 (%) top20 (%) top50 (%)</cell></row><row><cell>Jigsaw [27]</cell><cell>19.7</cell><cell>28.5</cell><cell>33.5</cell><cell>40.0</cell><cell>49.4</cell></row><row><cell>OPN [23]</cell><cell>19.9</cell><cell>28.7</cell><cell>34.0</cell><cell>40.6</cell><cell>51.6</cell></row><row><cell>Büchler [1]</cell><cell>25.7</cell><cell>36.2</cell><cell>42.2</cell><cell>49.2</cell><cell>59.5</cell></row><row><cell>R3D (random)</cell><cell>9.9</cell><cell>18.9</cell><cell>26.0</cell><cell>35.5</cell><cell>51.9</cell></row><row><cell>VCOP [43]</cell><cell>14.1</cell><cell>30.3</cell><cell>40.4</cell><cell>51.1</cell><cell>66.5</cell></row><row><cell>VCP [24]</cell><cell>18.6</cell><cell>33.6</cell><cell>42.5</cell><cell>53.5</cell><cell>68.1</cell></row><row><cell>One-view</cell><cell>26.2</cell><cell>39.3</cell><cell>46.8</cell><cell>55.6</cell><cell>66.8</cell></row><row><cell>CMC [36]</cell><cell>26.4</cell><cell>37.7</cell><cell>45.1</cell><cell>53.2</cell><cell>66.3</cell></row><row><cell cols="2">IIC (repeat + res) 36.5</cell><cell>54.1</cell><cell>62.9</cell><cell>72.4</cell><cell>83.4</cell></row><row><cell>IIC (repeat + u)</cell><cell>41.8</cell><cell>60.4</cell><cell>69.5</cell><cell>78.4</cell><cell>87.7</cell></row><row><cell cols="2">IIC (shuffle + res) 34.6</cell><cell>53.0</cell><cell>62.3</cell><cell>71.7</cell><cell>82.4</cell></row><row><cell>IIC (shuffle + v)</cell><cell>42.4</cell><cell>60.9</cell><cell>69.2</cell><cell>77.1</cell><cell>86.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Video retrieval performance on HMDB51.</figDesc><table><row><cell>Methods</cell><cell cols="5">top1(%) top5(%) top10(%) top20(%) top50(%)</cell></row><row><cell>R3D (random)</cell><cell>6.7</cell><cell>18.3</cell><cell>28.3</cell><cell>43.1</cell><cell>67.9</cell></row><row><cell>VCOP [43]</cell><cell>7.6</cell><cell>22.9</cell><cell>34.4</cell><cell>48.8</cell><cell>68.9</cell></row><row><cell>VCP [24]</cell><cell>7.6</cell><cell>24.4</cell><cell>36.3</cell><cell>53.6</cell><cell>76.4</cell></row><row><cell>One-view</cell><cell>10.8</cell><cell>26.2</cell><cell>40.1</cell><cell>54.3</cell><cell>74.9</cell></row><row><cell>CMC [36]</cell><cell>10.2</cell><cell>25.3</cell><cell>36.6</cell><cell>51.6</cell><cell>74.3</cell></row><row><cell cols="2">IIC (repeat + res) 13.4</cell><cell>32.7</cell><cell>46.7</cell><cell>61.5</cell><cell>83.8</cell></row><row><cell>IIC (repeat + u)</cell><cell>17.1</cell><cell>41.9</cell><cell>55.1</cell><cell>70.4</cell><cell>84.9</cell></row><row><cell cols="2">IIC (shuffle + res) 13.2</cell><cell>32.9</cell><cell>47.3</cell><cell>62.8</cell><cell>81.9</cell></row><row><cell>IIC (shuffle + v)</cell><cell>19.7</cell><cell>42.9</cell><cell>57.1</cell><cell>70.6</cell><cell>85.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results for different fine-tuning modes.</figDesc><table><row><cell cols="2">Type view1 view2</cell><cell cols="4">fine-tuning mode 1 fine-tuning mode 2 modality accuracy modality accuracy</cell></row><row><cell>repeat RGB</cell><cell>res</cell><cell>RGB</cell><cell>61.6</cell><cell>res</cell><cell>71.8</cell></row><row><cell>repeat RGB</cell><cell>u</cell><cell>RGB</cell><cell>59.8</cell><cell>u</cell><cell>73.5</cell></row><row><cell>shuffle RGB</cell><cell>res</cell><cell>RGB</cell><cell>61.2</cell><cell>res</cell><cell>74.9</cell></row><row><cell>shuffle RGB</cell><cell>v</cell><cell>RGB</cell><cell>61.6</cell><cell>v</cell><cell>63.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of action recognition accuracy on UCF101 and HMDB51 datasets. Results are averaged over three splits. * indicates results using the same network backbone, R3D. † indicates methods using optical flow.</figDesc><table><row><cell>Method</cell><cell cols="2">UCF101(%) HMDB51(%)</cell></row><row><cell>Jigsaw [27]</cell><cell>51.5</cell><cell>22.5</cell></row><row><cell>O3N (res) [6]</cell><cell>60.3</cell><cell>32.5</cell></row><row><cell>OPN [23]</cell><cell>56.3</cell><cell>22.1</cell></row><row><cell>Büchler [1]</cell><cell>58.6</cell><cell>25.0</cell></row><row><cell>Mas [39]</cell><cell>58.8</cell><cell>32.6</cell></row><row><cell>Geometry [7]</cell><cell>54.1</cell><cell>22.6</cell></row><row><cell>CrossLearn [30]  †</cell><cell>58.7</cell><cell>27.2</cell></row><row><cell>CMC (3 views) [36]  †</cell><cell>59.1</cell><cell>26.7</cell></row><row><cell>R3D (random) [43]  *</cell><cell>54.5</cell><cell>23.4</cell></row><row><cell>ImageNet-inflated [19]  *</cell><cell>60.3</cell><cell>30.7</cell></row><row><cell>3D ST-puzzle [19]  *</cell><cell>65.8</cell><cell>33.7</cell></row><row><cell>VCOP [43]  *</cell><cell>64.9</cell><cell>29.5</cell></row><row><cell>VCP [24]  *</cell><cell>66.0</cell><cell>31.5</cell></row><row><cell>IIC (repeat + res)  *</cell><cell>72.8</cell><cell>35.3</cell></row><row><cell>IIC (repeat + u)  *  †</cell><cell>72.7</cell><cell>36.8</cell></row><row><cell>IIC (shuffle + res)  *</cell><cell>74.4</cell><cell>38.3</cell></row><row><cell>IIC (shuffle + v)  *  †</cell><cell>67.0</cell><cell>34.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially financially supported by the Grants-in-Aid for Scientific Research Numbers JP19K20289 and JP18H03339 from JSPS.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A ALGORITHM <ref type="bibr">Algorithm 1</ref> Training with inter-intra contrastive learning framework model: net, video view:</p><p>Form non-parametric weights W 1 = concat(W 1 ,W neд ),</p><p>Update net with loss 9:</p><p>Update memory banks M 1 ,</p><p>10: end for Line 3,6 are the main differences from the baseline method CMC <ref type="bibr" target="#b35">[36]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving spatiotemporal self-supervision by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="770" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5589" to="5597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The&quot; Something Something&quot; Video Database for Learning and Evaluating Visual Common Sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8545" to="8552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00294</idno>
		<title level="m">Video Cloze Procedure for Self-Supervised Spatio-Temporal Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross and learn: Crossmodal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nawid</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="228" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Selfsupervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05661</idno>
		<title level="m">Rethinking Motion Representation: Residual Frames with 3D ConvNets for Better Action Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4006" to="4015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Selfsupervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10334" to="10343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
