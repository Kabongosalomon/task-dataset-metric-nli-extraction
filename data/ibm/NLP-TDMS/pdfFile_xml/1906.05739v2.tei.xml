<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating and Exploiting Probabilistic Monocular Depth Estimates</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Xia</surname></persName>
							<email>zhihao.xia@wustl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Washington University in St. Louis</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Sullivan</surname></persName>
							<email>patrick.l.sullivan2@boeing.com</email>
							<affiliation key="aff1">
								<orgName type="department">The Boeing Company</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Washington University in St. Louis</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generating and Exploiting Probabilistic Monocular Depth Estimates</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Beyond depth estimation from a single image, the monocular cue is useful in a broader range of depth inference applications and settings-such as when one can leverage other available depth cues for improved accuracy. Currently, different applications, with different inference tasks and combinations of depth cues, are solved via different specialized networks-trained separately for each application. Instead, we propose a versatile task-agnostic monocular model that outputs a probability distribution over scene depth given an input color image, as a sample approximation of outputs from a patch-wise conditional VAE. We show that this distributional output can be used to enable a variety of inference tasks in different settings, without needing to retrain for each application. Across a diverse set of applications (depth completion, user guided estimation, etc.), our common model yields results with high accuracycomparable to or surpassing that of state-of-the-art methods dependent on application-specific networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Monocular depth estimation methods-that predict scene depth from only a single color image-have achieved surprising success through the use of deep neural networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b43">44]</ref>. This success confirms that even a single view contains considerable information about scene geometry. Purely monocular depth map estimates, however, are far from being precisely accurate given the ill-posed nature of the task. Fortunately, many practical systems are able to rely on other (yet also imperfect) sources of depth information-limited measurements from depth sensors, interactive user guidance, consistency across frames or views, etc. And so, it is desirable to combine these other sources with the monocular cue to extract depth estimates that are more accurate than possible from one source alone.</p><p>Although the monocular cue is useful for augmenting other depth cues, the same isn't true for monocular estimators that simply output a depth map, a form which can not be directly combined with additional depth cues. Instead, re-searchers have treated depth estimation using different combinations of cues as different applications in their own right (e.g., depth up-sampling <ref type="bibr" target="#b3">[4]</ref>, estimation from sparse <ref type="bibr" target="#b32">[33]</ref> and line <ref type="bibr" target="#b26">[27]</ref> measurements, etc.), and solved each by learning separate estimators that take their corresponding set of cues, in addition to the color image, as input. This requires, for each application, determining the types of inputs that will be available, constructing a corresponding training set, choosing an appropriate network architecture, and then training that application-specific network-a process that is redundant and often onerous.</p><p>In this paper, we introduce a universal and versatile network to leverage the monocular depth cue in multiple application settings without re-training. Our network is trained in an application-agnostic way on image-depth pairs, but can be utilized for inference in different applications and combined with different external depth cues as illustrated in <ref type="figure">Fig. 1</ref>. Rather than producing a depth map estimate, our monocular network outputs a probability distribution over scene depth given an input color image. This distribution faithfully encodes both the information and ambiguity of depth values and their spatial dependencies based on the monocular input, and is produced in a form that can be combined with other depth cues during inference.</p><p>Our contributions are as follows:</p><p>• We propose a novel approach to output a probability density function that can express arbitrary beliefs and spatial dependencies for depth, conditioned on the image input. We train a conditional VAE <ref type="bibr" target="#b16">[17]</ref> to output multiple plausible depth samples independently for individual overlapping patches, and form the density as a sample approximation from all samples and patches. • We describe an efficient optimization method for inference that combines this image-conditional density function with other sources of depth information (e.g., from sensors or user input). • We show that our probabilistic outputs are useful for general inference tasks beyond depth map estimatione.g., predicting pairwise ordinal depth relationships. • We carry out extensive experiments on the NYUv2 dataset <ref type="bibr" target="#b41">[42]</ref> to demonstrate the efficacy of our approach <ref type="figure">Figure 1</ref>. Overview of our approach. Given an input color image, we use a common task-agnostic network to output a joint probability distribution p(Z|I) over the depth map-formed as a sample approximation using outputs of a conditional VAE that generates plausible estimates for depth in overlapping patches. The mean of this distribution represents a standard monocular depth estimate, but the distribution itself can be used to solve a variety of inference tasks in different application settings-including leveraging additional depth cues to yield improved estimates. All these applications are enabled by a common model, that is trained only once.</p><p>on a diverse variety of applications. All applications are enabled by the same network that is trained only once, but delivers accuracy comparable to or surpassing stateof-the-art methods dependent on task-specific models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Monocular Depth Estimation. First attempted by Saxena et al. <ref type="bibr" target="#b37">[38]</ref>, early work in estimating scene depth from a single color image relied on hand-crafted features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, use of graphical models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref>, and databases of exemplars <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. More recently, Eigen et al. <ref type="bibr" target="#b6">[7]</ref> showed that, given a large enough database of image-depth pairs <ref type="bibr" target="#b41">[42]</ref>, convolutional neural networks could be trained to achieve significantly more reliable depth estimates. Since then, there have been steady gains in accuracy through the development of improved neural network-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref>, as well as strategies for unsupervised an semi-supervised learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>. Beyond estimating absolute depth, some works have also looked at pairwise ordinal depth relations between pair of points in the scene from a input color image <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">51]</ref>. Probabilistic Outputs. Monocular depth estimators commonly output a single estimate of the depth value at each pixel, hindering their use in different estimation settings. Some existing methods do produce distributional outputs, but as per-pixel variance maps <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> or per-pixel probability distributions <ref type="bibr" target="#b27">[28]</ref>. Note that depth values at different locations are not statistically independent, i.e., different values at different locations may be plausible independently, but not in combination. Thus, per-pixel distributions provide only a limited characterization that, while useful in some applications, can not be used more generally, e.g., to spatially propagate information from sparse measurements. Beyond per-pixel distributions, Chakrabarti et al. <ref type="bibr" target="#b1">[2]</ref> train a network to produce independent distributions for dif-ferent local depth derivatives. They describe a method to use these derivative distributions to generate a better estimate of global depth, but do not provide a way to solve other tasks. Also, since their network output is restricted to uni-variate distributions for hand-chosen derivatives, it can not express the general spatial dependencies in a joint distribution over depth that we seek to encode for inference. Depth from Partial Measurement. Since making dense depth measurements is slow and expensive, it is useful to be able to recover a high-quality dense depth map from a small number of direct measurements by exploiting the monocular cues in a color image. A popular way of combining color information with partial measurements is by requiring color and depth edges to co-occur: this approach is often successful for "depth inpainting", i.e., filling in gaps of missing measurements in a depth map (common in measurements from structured light sensors). A notable and commonlyused example is the colorization method of Levin et al. <ref type="bibr" target="#b23">[24]</ref>. Other methods along this line include <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>, while Zhang and Funkhouser <ref type="bibr" target="#b47">[48]</ref> used a neural network to predict normals and occlusion boundaries to aid inpainting.</p><p>However, when working with a very small number of measurements, the task is significantly more challenging (see discussion in <ref type="bibr" target="#b3">[4]</ref>) and requires relying more heavily on the monocular cue. In this regime, the solution has been to train a network that takes the color image and the provided sparse samples as input. Various works have adopted this approach for measurements along a single horizontal line from a line sensor <ref type="bibr" target="#b26">[27]</ref>, random sparse measurements <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>, and sub-sampled measurements on a regular grid <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref>. Note that several of these methods also train separate networks even for different settings of the same application, such as for different sparsity levels <ref type="bibr" target="#b32">[33]</ref> and different resolution grids <ref type="bibr" target="#b3">[4]</ref>.</p><p>An exception here is the depth completion method of Wang et al. <ref type="bibr" target="#b44">[45]</ref> who use a pre-trained monocular depth network, and provide a way to improve its monocular predictions when given sparse depth measurements. They iteratively back-propagate errors between measurements and the network output to update activations of an intermediate layer (but not the network weights), leading to an improved depth map output. Thus, their method uses the monocular network's output as an initialization, and its internal representation as a structured way to spatially propagate measurement information. In contrast, our method outputs an explicit probabilistic representation which can be used for depth completion as well as for other inference tasks, and as our experiments show, yields more accurate results. Networks for Generating Samples. In this work, we form a conditional joint distribution of depth values by training our network to generate samples of multiple plausible depth values. In particular, we follow the approach of <ref type="bibr" target="#b16">[17]</ref> to train a conditional VAE and use its outputs to form a sample approximation to the joint distribution. Note that instead of generating samples of a global map (like in <ref type="bibr" target="#b16">[17]</ref>), we train the VAE to produce samples for individual overlapping patches independently. We also conduct ablation experiments using a conditional GAN <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref> to produce these samples, and while the VAE formulation performs better, our results with the GAN are also reasonable. This suggests our approach is able to exploit any neural network-based method for generating conditional samples, and can benefit from future advances in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Given the RGB image I of a scene, our goal is to reason about its corresponding depth map Z ∈ R N , represented as a vector containing depth values for all N pixels in the image. Rather than predict a single estimate for Z, we seek to output a distribution p(Z|I), to more generally characterize depth information and ambiguity present in the image. In this section, we describe our approach for generating this distributional output, and equally importantly, for exploiting it for inference in various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Probabilistic Monocular Depth</head><p>We form the distribution p(Z|I) as a product of functions defined on individual overlapping patches as</p><formula xml:id="formula_0">p(Z|I) ∝ i ψ i (P i Z|I),<label>(1)</label></formula><p>where ψ i (·) is a potential function for the i th patch, and P i a sparse matrix that crops out that patch from Z (for patches of size K × K, each P i is a K 2 × N matrix). Note that this is a Markov Random Field with K × K patches as the maximal cliques, and since these patches overlap, depth values at all pixels-including those that do not lie in the same patch-are statistically inter-dependent. <ref type="figure">Figure 2</ref>. Generating samples with a conditional VAE. Our network generates samples for depth independently in each overlapping patch, and we run it multiple times to generate multiple plausible samples per-patch. The input to the VAE comes from pretrained feature extraction layers from a state-of-the-art monocular model <ref type="bibr" target="#b7">[8]</ref>. Samples generated for different patches (including those that overlap) are kept statistically independent-after conditioning on the image-by using separate per-patch latent vectors.</p><p>Generating Samples. To form the per-patch potentials ψ i (·), we train a network that produces samples of depth given the image input, and run it multiple times during inference to generate multiple plausible samples. A crucial aspect of this network is that, instead of sampling the global depth map, it generates separate samples independently for the depth P i Z of every patch i. This ensures that depth values within each sample represent a plausible estimate for the corresponding patch, but that samples of different patches are conditionally independent given the image. Limiting the dimensionality of each sample allows us to approximate the per-patch potential ψ i (·) with a reasonable number of samples, while enforcing independence between samples of different patches ensures that the overall distribution p(Z|I) in (1) sufficiently captures the global ambiguity in depth. We adopt the conditional VAE framework proposed in <ref type="bibr" target="#b16">[17]</ref> for generating samples-that features a "prior-net" to predict distribution over values of a latent vector from the image, with an encoder-decoder network that predicts depth values from the image and a sample from this latent distribution. To reduce complexity, we bootstrap our network by taking a pre-trained state-of-the-art monocular depth estimation network (DORN <ref type="bibr" target="#b7">[8]</ref>), removing the last two convolution layers, and treating the remaining layers as a "feature extractor". These features, rather than the image itself, are provided as input to the conditional VAE.</p><p>We achieve patch independent sampling by having a separate latent vector for each patch. We set up the architecture of the decoder in the encoder-decoder network to produces an estimate of the depth of each overlapping patch using only its own latent vector, and not those of overlapping patches. The prior-net is also setup to predict separate distributions for the latent vector of each patch (as is the posterior-net during training). At test time, we draw mul-tiple samples independently from the latent space for each patch, which the encoder-decoder network uses to generate correspondingly independent per-patch depth samples. A more detailed description of the VAE architecture and training approach is included in the supplementary. Sample Approximation. Next, given a set S i of samples {x s i } for each patch i, we define its potential ψ i (·) as</p><formula xml:id="formula_1">ψ i (P i Z|I) = 1 |S i | xi∈Si exp − P i Z − x i 2 2h 2 .<label>(2)</label></formula><p>This can be interpreted as forming a kernel density estimate from the depth samples in S i using a Gaussian kernel, were the Gaussian bandwidth h is a scalar hyper-parameter 1 .</p><p>Unlike independent per-pixel <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28]</ref> or perderivative <ref type="bibr" target="#b1">[2]</ref> distributions, the samples {S i } enable the patch potentials ψ i (·) to express complex spatial dependencies between depth values in local regions. Moreover, our joint distribution p(Z|I) is defined in terms of overlapping patches, and thus models dependencies across the entire depth map. During inference, this enables information propagation across the entire scene, and reasoning about the global plausibility of scene depth estimates.</p><p>Note that the distribution p(Z|I) can be used to recover a monocular depth map estimate as the mean over p(Z|I) by computing the average estimate of depth at each pixel from all samples from all patches that include that pixel. But the real utility of our distributional output comes from enabling a variety of inference tasks, as we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Depth Estimation with Additional Information</head><p>In several applications, a system has access to additional sources beyond the monocular image that provide some partial information about depth. Our distributional output allows us to combine the monocular cue with these sources, and derive a more accurate scene depth estimate than possible from either source alone. Specifically, we assume the additional depth information is provided in the form of a cost C(Z), and combine it with our distribution p(Z|I) to derive a depth estimateẐ as:</p><formula xml:id="formula_2">Z = arg min Z − log p(Z|I) + C(Z), log p(Z|I) = i log xi∈Si exp − P i Z − x i 2 2h 2 .<label>(3)</label></formula><p>With some abuse of terminology, this can be thought of as computing the maximum a posteriori (MAP) estimate of Z, where p(Z|I) is the image-conditional "prior", and C(Z) can be interpreted as a "likelihood" from the additional depth information source.</p><p>The log-likelihood of our distribution in (3) can be simplified with a standard approximation of replacing the summation over exponentials with a maximum (since P i Z is high-dimensional, the largest term typically dominates):</p><formula xml:id="formula_3">Z ≈ arg min Z − i log max xi∈Si exp − P i Z − x i 2 2h 2 + C(Z) = arg min Z min {xi∈Si} i P i Z − x i 2 + 2h 2 C(Z). (4)</formula><p>Note that this expression now involves a minimization over both Z and selections of samples x i ∈ S i for every patch. We will use two forms of the external cost C(Z) to encode available information in various applications. The first is simply a generic global cost that we denote by C G (Z), and the other is one that can be expressed as a summation over the depth values of individual patches i C i (P i Z). Including both these possible forms in <ref type="formula">(4)</ref>, we arrive at the following optimization task:</p><formula xml:id="formula_4">min Z min {xi∈Si} i P i Z−x i 2 + i C i (x i ) + C G (Z) Possible forms of C(Z) ,<label>(5)</label></formula><p>where the factor 2h 2 is absorbed in the definitions of the costs, and the per-patch costs C i (P i Z) are approximated as C i (x i ) to act on samples instead of crops of Z (we assume this will roughly be equivalent at convergence). We use a simple iterative algorithm to carry out this optimization. The global depth Z is initialized to the mean per-pixel depth from p(Z|I), and the following updates are applied alternatingly to {x i } and Z till convergence:</p><formula xml:id="formula_5">x i ← arg min xi∈Si P i Z − x i 2 + C i (x i ), ∀i. (6) Z ← arg min Z P i Z − x i 2 + C G (Z).<label>(7)</label></formula><p>The updates to patch estimates x i can be done independently, and in parallel, for different patches. The cost in <ref type="formula">(6)</ref> is the sum of the squared distance from corresponding crop P i Z of the current global estimate, and the per-patch cost C i (·) when available. We can compute these costs for all samples in S i , and select the one with the lowest cost. Note that the cost C i (·) on all samples need only be computed once at the start of optimization.</p><p>The update to the global map Z in (7) depends on the form of the global cost C G (·). If no such cost is present, Z is given by simply the overlap-average of the currently selected samples x i for each patch. For applications that do feature a global cost, we find it sufficient to solve <ref type="bibr" target="#b6">(7)</ref> by first initializing Z to the overlap-average, and then carrying out a small number of gradient descent steps as</p><formula xml:id="formula_6">Z ← Z − γ∇ Z C G (Z),<label>(8)</label></formula><p>where the scalar step-size γ is a hyper-parameter. We now discuss concrete examples of our inference approach by considering specific applications, and describe associated choices of the costs C G (·) and C i (·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Depth Completion</head><p>Dense Depth from Sparse Measurements. We consider the task of estimating the depth map Z when an input sparse set F of depth measurements at isolated points in the scene is available, along with a color image. We use the measurements F to define a global cost C G (·) in <ref type="formula" target="#formula_4">(5)</ref> as</p><formula xml:id="formula_7">C G (Z) = λ Z ↓ −F 2 ,<label>(9)</label></formula><p>where ↓ represents sampling Z at the measured locations. Based on this, we define the gradients to be applied in <ref type="bibr" target="#b7">(8)</ref> for computing the global depth updates as</p><formula xml:id="formula_8">∇ Z C G (Z) = λ(Z ↓ −F) ↑,<label>(10)</label></formula><p>where ↑ represents the transpose of the sampling operation.</p><p>Since both the weight λ and the step-size γ in <ref type="formula" target="#formula_6">(8)</ref> are hyperparameters, we simply set λ = 1, and set the step-size γ (as well as number of gradient steps) based on a validation set. We consider two kinds of sparse inputs. The first are at arbitrary random locations like in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, where we use nearest neighbor interpolation for the transpose sampling operation ↑ in <ref type="bibr" target="#b9">(10)</ref>. The other case is depth up-sampling, where measurements are on a regular lowerresolution grid. Given their regularity, we are able to use bi-linear interpolation for the transpose operation ↑. Depth Un-cropping. We next consider applications where the available measurements are dense in a contiguous (but small) portion of the image-such as from a sensor with a smaller field-of-view (FOV), or alone a single line <ref type="bibr" target="#b26">[27]</ref>. In this case, we define F and W are set to measured values and one at measured locations, and zero elsewhere. We use these to define a per-patch cost C i (·) for use in <ref type="formula" target="#formula_4">(5)</ref> as</p><formula xml:id="formula_9">C i (x i ) = λ P i W • (P i Z − P i F) 2 ,<label>(11)</label></formula><p>where the weight λ is determined on a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Incorporating User Guidance</head><p>Depth estimates are often useful in interactive image editing and graphics applications. We consider a couple of settings where our estimation method can be used to include feedback from a user in the loop for improved depth accuracy. Diverse Estimates for User Selection. We use Batra et al.'s approach <ref type="bibr" target="#b0">[1]</ref> to derive multiple diverse global estimates {Z 1 , . . . Z M } of the depth map Z from our distribution p(Z|I), and propose presenting these as alternatives to the user. We set the first estimate Z 1 to our mean estimate, generate every subsequent estimate Z m+1 by finding a mode using (5) with per-patch costs C i (·) defined as</p><formula xml:id="formula_10">C i (x i ) = −λ/m m m =1 P i Z m − x i 2 .<label>(12)</label></formula><p>This introduces a preference for samples that are different from corresponding patches in previous estimates, weighted by a scalar hyper-paramter λ (set on a validation set).</p><p>Using Annotations of Erroneous Regions. As a simple extension, we consider also getting annotations of regions with high error from the user, in each estimate Z m . Note that we only get the locations of these regions, not their correct depth values. Given this annotation, we define a mask W M that is one within the region and zero elsewhere, and now recover each Z m+1 , with a modified cost C i (·):</p><formula xml:id="formula_11">C i (x i ) = −λ/m m m =1 (P i W m )•(P i Z m −x i ) 2 ,<label>(13)</label></formula><p>where • denotes element-wise multiplication, and the masks focuses the cost on regions marked as erroneous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Other Inference Tasks</head><p>Our distributional output is versatile and can be used to perform general inference tasks, not just estimate per-pixel depth. We describe two such applications below. Confidence-guided Sampling. We can use p(Z|I) to compute a per-pixel variance map, as the variance of each pixel's depth value across patches and samples in {S i } (which differs from the actual variance under p(Z|I) by a constant h 2 ). This gives us spatial map of the relative monocular ambiguity in depth at different locations. When seeking to estimate depth from arbitrary sparse measurements, we can use this map to select where to make measurements (assuming the depth sensor provides such control). Specifically, given a budget on the total number of measurements, we propose choosing an optimal set of measurement points as local maxima of the variance map. Pair-wise Depth. A useful monocular depth inference task, introduced in <ref type="bibr" target="#b50">[51]</ref>, is to predict the ordinal relative depth of pairs of nearby points in the scene: whether the points are at similar depths (within some threshold), and if not, which point is nearer. We use our distributional output to solve this task, by looking at the relative depth in all samples in all patches that contain a pair of queried points, outputting the ordinal relation that is most frequent. We find this leads to more accurate ordinal estimates, in comparison to simply using the ordering of the individual depth value pairs in a monocular depth map estimate (as done in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">51]</ref> Ours 0.364 0.315 0.090 91.9 98.7 99.7 <ref type="table">Table 1</ref>. Results for various applications on the NYUv2 test set. We use distributional outputs from our common model to generate depth estimates in a diverse variety of application settings: from standard monocular estimation to several applications when different forms of additional depth cues are available. We compare to other methods for these applications, including those (shaded background) dependent on task-specific networks trained separately for each setting. Our network, in contrast, is task-agnostic and trained only once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We now evaluate our approach on the NYUv2 dataset <ref type="bibr" target="#b41">[42]</ref> by training a common task-agnostic distributional monocular model and applying it to solve a diverse range of inference tasks in various application settings. Preliminaries. We use raw frames from scenes in the official train split for NYUv2 <ref type="bibr" target="#b41">[42]</ref> to construct train and val sets, and report performance on the official test set. We use feature extraction layers from a pre-trained DORN model <ref type="bibr" target="#b7">[8]</ref>, and since it operates on inputs and outputs rescaled to a lower resolution (to 257×353 from 640×480), we do the same for our VAE. However, our outputs are rescaled back to the orginal full resolution to compute errors. Input depth measurements, if any, are also provided at full resolution (see supplementary). We use overlapping patches of size 33 × 33 with stride four, and generate 100 samples per-patch to construct {S i }. Generating samples takes 5.7s on a 1080Ti GPU for each image, while inference from these samples is faster (see supplementary). Our code and trained model will be made available on publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance on Various Inference Tasks</head><p>We evaluate depth estimation using our common model for several applications, and report performance in terms of standard error metrics on the official NYUv2 test set (see <ref type="bibr" target="#b5">[6]</ref>) 2 in <ref type="table">Table 1</ref>. We report performances on standard monocular estimation, as well for the different depth completion and user guided applications described in Sec. 3.2. We simulate user-guidance using ground-truth depthselection of a global depth map is done automatically based on lowest error, and annotation by choosing 50 × 50 windows with the highest error against the ground truth and no more than 50% overlap with previously marked regions.</p><p>Not only does our method perform well in the monocular setting-outperforming the DORN [8] whose features it uses-it is able to improve upon this monocular estimate with different available depth cues in the various applications. We find sparse measurements are most complementary to the monocular cue, and that user annotation is more <ref type="figure">Figure 3</ref>. Example depth estimates for different applications. We show outputs from our method for both the pure monocular setting, as well as the improved estimates we obtain combining our distributional output with additional depth information-such as different kinds of partial measurements, and user guidance with annotation and selection. useful than selection alone. <ref type="figure">Figure 3</ref> shows example depth reconstructions by our method for several applications. <ref type="table">Table 1</ref> provides comparisons to a number of other depth completion methods. Two of these do not require taskspecific training-Levin et al.'s colorization method <ref type="bibr" target="#b23">[24]</ref>, and Wang et al.'s <ref type="bibr" target="#b44">[45]</ref> approach to back-propagating errors from measurements. As Wang et al.'s own results were with older monocular networks, for a fairer comparison, we derive improved results by applying their method on the same DORN <ref type="bibr" target="#b7">[8]</ref> model as used by our network (finding optimal settings on a val set). As seen in <ref type="table">Table 1</ref>, our approach is more accurate than both these methods.</p><p>We also compare to application-specific approaches that train specialized networks separately for each application (and each setting). For depth completion from sparse measurements, we compare to the work of Chen et al. <ref type="bibr" target="#b3">[4]</ref> for measurements on a regular grid, and of Ma et al. <ref type="bibr" target="#b32">[33]</ref> 3 for those at random locations. For estimation from horizontal line measurements, we show comparisons to the method by Liao et al. <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b3">4</ref> . We find that our results-from a common task-agnostic network model-are comparable, and indeed often better, than these application-specific methods.</p><p>Next, we evaluate the efficacy of our approach to enabling applications beyond those that estimate depth maps.  <ref type="table">Table 3</ref>. Error rates for pairwise ordinal depth ordering from our common model, compared to other methods that used accurate ordering as an objective during training. We also report baseline errors from predictions just based on our mean depth estimate.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, we report results for making sparse depth measurements guided by the color image using our approach for different budgets on the number of measurements. Our guided measurements lead to better dense depth estimates than those at random locations (given measurements, we use our depth estimation algorithm in both cases). Finally, we evaluate using our distribution to predict pairwise depth ordering in <ref type="table">Table 3</ref>, comparing it to three methods that specifically target this task: <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51]</ref>. Results are reported in terms of the WKDR error metrics, on a standard set of point pairs on the NYUv2 test set (see <ref type="bibr" target="#b50">[51]</ref>). We find that using our method leads to better predictions than from these methods, and that using our distributional output is crucial-since the accuracy of simply using the orderings from our monocular mean estimate is much lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis and Ablation</head><p>We visualize the diversity of depth hypotheses in our distribution in <ref type="figure">Fig. 4</ref>. We choose one sample for each patchbased on its rank among samples for that patch in terms of accuracy relative to ground-truth. We vary this rank from best to worse, form a global depth map for each rank by overlap-average, and plot the resulting accuracies. Given the ambiguity of the monocular cue, these span a diverse range-from a very accurate estimate when an oracle allows ideal selection, to higher errors when adversarially choosing the worst samples in every patch. <ref type="figure">Figure 4</ref> also overlays the performance of several our inference tasks from <ref type="table">Table 1</ref>. As expected, the accuracy of pure monocular estimation is roughly at the center of the distirbution range. But when additional depth cues are available, we see that our results begin to shift to have higher accuracy-by different amounts for different applications. This shows that our inference method is successful in incorporating the information present in these depth cues.</p><p>We also study different variations to our approach for generating samples for our distribution p(Z|I) in <ref type="table">Table 4</ref>measuring performance, on a validation set, in terms of accuracy for a ground truth-based oracle as described above, and more realistically, accuracy at monocular estimation and depth completion (from 100 measurements).</p><p>First, we evaluate using a conditional GAN <ref type="bibr" target="#b34">[35]</ref> instead of a VAE (see supplementary for architecture details). While the VAE performs better, results with the GAN are also reasonable-suggesting that our approach is compatible with different network-based sampling approaches.</p><p>Then, we consider varying the size of our patches (and proportionally, the stride). We find smaller patches actually helps oracle performance, since with the same number of samples, it is easier to generate a sample close to the ground-truth in a lower-dimensional space. However, smaller patches do not accurately capture the spatial dependencies within a patch, leading to poorer performance for actual inference. Conversely, while a higher patch size could allow encoding longer range spatial dependencies, doing so is harder via approximation from a reasonable number of samples-leading to lower accuracy both with the oracle and during inference.</p><p>For our chosen patch-size, we also evaluate higher strides, and thus lower overlap. This leads to lower performance (on depth completion), highlighting the utility of patch-overlap in the global distribution p(Z|I), and in propagating information during inference. <ref type="figure">Figure 4</ref>. Analysis of distributional output and inference method on the test set. Our distribution allows for many possible global depth explanations, visualized here by choosing one of the generated samples in each patch based on the rank of its accuracy going from best (oracle) to worst (adversary), and computing global depth by overlap-average. These solutions span a large range in accuracy, and without any additional information, the mean monocular estimate lies in the middle of this range. But when additional cues are available, they can be effectively exploited by our MAP estimation method to extract better solutions from our distribution.  <ref type="table">Table 4</ref>. Ablation study on validation set. We evaluate different ways of generating samples: using a GAN instead of a VAE, and using different patch-sizes p (with proportional strides s). For each case, we compare achievable accuracy of individual samples via the "oracle" estimate (see <ref type="figure">Fig. 4</ref>), vs. their utility for actual inference-in the pure monocular case and with random sparse measurements (#100). We also evaluate the importance of patch overlap by considering larger strides for our chosen model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>With distributional monocular outputs, our approach enables a variety of applications without the need for repeated training. While we considered tasks directly focused on scene geometry in this paper, we are interested in exploring how our distributional outputs can be used to manage ambiguity in downstream processing-such as for re-rendering or path planning-in future work. We also believe probabilistic predictions can be useful for other low-and midlevel scene properties, like motion and reflectance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results</head><p>We include additional example results of depth reconstruction for various applications in <ref type="figure" target="#fig_1">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. DORN Usage and Resolution</head><p>Our conditional VAE leverages a pre-trained feature extractor derived from a state-of-the-art monocular depth network. Specifically, we take the pre-trained DORN model <ref type="bibr" target="#b7">[8]</ref>, remove its last two convolutional layers, and use it as our feature extractor. The DORN network works at a lower resolution of 257×353, compared to the original NYUv2 resolution of 640×480, for both its RGB input and depth output. Therefore, our feature extractor takes an RGB image as input after resizing to the lower DORN resolution of 257×353. The output of the feature extractor layers is a 2560-dimensional feature map with a spatial size of 33×45. Our VAE takes this feature map as input, and reasons about an output depth map at the 257×353 DORN resolution. We consider overlapping 33×33 patches at stride 4 also at the lower DORN resolution of 257×353, giving us a total of 57×81 patches, each of size 33×33.</p><p>Thus, our distributional output corresponds to the lower DORN <ref type="bibr" target="#b7">[8]</ref> resolution of 257×353 for the depth map. However, all error metrics in the paper are computed (inside the valid crop) at full resolution. To do so, we resize our method's outputs i to 640×480 (by simple bilinear interpolation). Moreover, in all applications with additional inputs, these are also provided at the original higher resolution. For user annotations, erroneous regions are marked as 50×50 windows at the full resolution, and we map the locations of these windows to the lower resolution to construct our masks W M . Similarly, for depth from sparse measurements, F corresponds to sparse measurements of depth at the full-resolution, and our global cost C G (·) is defined in terms of a full-resolution depth map (we scale our depth map to the full resolution, apply gradients, and scale the updated depth map back). For depth un-cropping, we again provide depth measurements at the full resolution, and scale these to the DORN resolution to construct our measurement and mask vectors F and W. Thus, all inputs and all evaluation metrics are based on the standard benchmark resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Conditional VAE Architecture</head><p>Our conditional VAE treats the output of the DORN feature extractor-with a spatial resolution of 33×45 and 2560 feature channels-as an encoding of the input image. Closely following the formulation of <ref type="bibr" target="#b16">[17]</ref>, this VAE has the following three sub-networks:</p><p>1. Prior-net: Given the input image feature encoding, this network produces the mean and variance vectors for each of the 57×81 overlapping patches. These vectors represent the parameters of diagonal Gaussian distributions over the latent space of the corresponding patches. The latent space, and the per-patch mean and variance vectors, are 128-dimensional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Encoder-decoder:</head><p>This network takes as input both the image feature encoding, and per-patch latent vectors sampled as-per the distributions produced by the prior-net. The encoder produces a 256-dimensional feature vector for each patch (i.e., at a spatial resolution of 57×81), which is then concatenated with the patch's corresponding sampled latent vector. This concatenated vector is then decoded to output 33×33 depth value estimates for each patch-i.e., the output of the decoder is 57×81×[33×33]. Note that the decoder path is independent for each patch to ensure independent sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Posterior-net:</head><p>This network is used only during training, and takes the image feature encoding and ground-truth patch depths as input. It uses two streams to first encode each of these to 256-dimensional per-patch feature vectors, concatenates them, and uses a series of 1×1 convolution layers to predict mean and variance vectors-the "posterior" equivalents of the prior-net's outputs.</p><p>The detailed architectures of these three networks are included in <ref type="table">Table 5</ref>-with convolution and reshape operations allowing us to run the network efficiently in a fully-convolutional way, while still producing independent samples for overlapping patches. We train these three networks in a similar way as <ref type="bibr" target="#b16">[17]</ref>, using a weighted combination of two losses: (1) an L 1 loss between ground-truth patch depth and the output of the encoder-decoder network; and (2) a KL-divergence loss between the distributions produced by the prior-net and posterior-net; with a weight of 1e − 4 for the latter. After training, we discard the posterior-net. Given an image, we run the prior-net and the encoder-half of the encoder-decoder network, and then run the decoder-half multiple (100) times with different samples from the latent distributions to produce multiple samples of depth estimates for each patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Inference Hyperparameter Selection</head><p>For applications with a per-patch cost C i (·)-i.e., user-guidance and depth un-cropping-the value of λ is chosen based on a small validation set, as λ = 10 for user-guidance, and 150 for un-cropping. Moreover, for user guidance, we find that slowly increasing the value of λ from 5 to its final value of 10 during optimization leads to convergence to better solutions. For depth completion from sparse (both random and regularly spaced) measurements, we set the value of the parameters for gradient-based updates for the global cost-step-size γ (in range [0.1, 1.0]) and number of steps (in range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>)-based on a validation set as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Running Time</head><p>Our method works by first generating multiple (100) samples for each overlapping patch, and then carrying out inference using these samples for different applications. In particular, for "MAP" estimation to compute depth estimates with additional information, this involves running our iterative optimization method. We report these running time (on a 1080Ti GPU) for this optimization for different applications in <ref type="table" target="#tab_3">Table 6</ref>-these times vary both because of variance in time taken per-iteration, and number of iterations needed for convergence.</p><p>ii No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>Output <ref type="table" target="#tab_1">Shape  0  features from feature extractor  1×33×45×2560  1  bilinear upsample  1×65×89×2560  2  conv 1×1  1×65×89×1024  3  conv 1×1  1×65×89×512  4  conv 3×3 dilation=2  1×61×85×512  5</ref> conv <ref type="formula" target="#formula_2">3×3</ref>  Posterior-net <ref type="table">Table 5</ref>. Conditional VAE architecture. We show architecture details of the three different sub-networks in our VAE, with the posterior-net used only during training. Valid padding is used everywhere. Every convolutional layer is followed by a ReLU, unless otherwise specified. The output of the encoder-decoder network has a tanh activation, followed by scaling to map to the depth range of the NYUv2 dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Ablation: Conditional GAN architecture</head><p>Our conditional GAN architecture features generator and discriminator networks, with similar architecture design choices to the VAE-the generator has a similar architecture the encoder-decoder network in the VAE, and the discriminator to the posterior-net. The generator uses dropout as the noise-source to enable sampling in different runs of the generator-and ensure that per-patch estimates are independent by ensuring that different patches are based on different instantiations of dropout noise values. The architecture is detailed in <ref type="table">Table 7.</ref> B.6. Half-resolution Comparison to Ma et al. <ref type="bibr" target="#b32">[33]</ref>.</p><p>Note that <ref type="bibr" target="#b32">[33]</ref> evaluate their methods by reporting errors on a centered crop of half-resolution depth-maps, and also derive their input sparse measurements at this half-resolution. In contrast, our results in <ref type="table">Table 1</ref> in the paper represent the official benchmark metrics (in the valid crop at full resolution) for consistency to other evaluations-in our paper and elsewhere. For a more direct comparison to <ref type="bibr" target="#b32">[33]</ref>, we also evaluated our method by replicating their setting. Specifically, to provide input sparse measurements, we first down-sample the ground-truth depth map and randomly sample depth values from this downsampled map. We then provide these as inputs to our method (which resizes these back to the full resolution to compute the global cost C G (·)). Then, we take the full-resolution depth map estimates produced by our method, down-sample them to iii Discriminator <ref type="table">Table 7</ref>. Conditional GAN architecture. We show architectures for the generator and discriminator for the GAN used in our ablation study, which follows a similar overall design as our VAE. For all dropout layers, we use probability 0.5. Every convolutional layer is followed by a ReLU, unless otherwise specified, and valid padding is used everywhere.  <ref type="table">Table 8</ref>. Performance on depth estimation from arbitrary sparse measurements, using the same evaluation setting as <ref type="bibr" target="#b32">[33]</ref> (half-resolution, evaluated on a center-crop).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head><p>half-resolution, and compute error metrics on the same centered crop as <ref type="bibr" target="#b32">[33]</ref>. We report these results in <ref type="table">Table 8</ref>, and find they are similar to standard evaluation in <ref type="table">Table 1</ref> in the paper.</p><p>iv</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>p=33,s=4 0.384 0.597 0.428 C-VAE p=17,s=2 0.263 0.518 0.413 C-VAE p=33,s=4 0.323 0.516 0.377 C-VAE p=65,s=8 0.474 0.522 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Additional examples of depth reconstructions using our common model for various applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). .470 0.131 83.7 97.1 99.4 DORN [8] 0.545 0.462 0.114 85.8 96.2 98.7 Ours 0.512 0.433 0.116 86.1 96.9 99.1</figDesc><table><row><cell cols="2">Setting Method</cell><cell cols="3">lower is better</cell><cell cols="3">higher is better</cell><cell>Setting Method</cell><cell>lower is better</cell><cell>higher is better</cell></row><row><cell></cell><cell></cell><cell cols="3">rms m-rms rel</cell><cell>δ1</cell><cell>δ2</cell><cell>δ3</cell><cell>rms m-rms rel</cell><cell>δ1</cell><cell>δ2</cell><cell>δ3</cell></row><row><cell cols="3">Monocular Depth Estimation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Arbitrary Sparse Measurements (Setting = #measurements)</cell></row><row><cell cols="6">Lee [23] 0.538 0Depth Un-cropping (Setting = measurement FOV)</cell><cell></cell><cell></cell><cell>20</cell><cell>Ma [33] Levin [24] 0.703 0.602 0.175 75.5 93.0 97.9 -0.351 0.078 92.8 98.4 99.6 Wang [45] 0.399 0.322 0.065 94.2 98.4 99.5 Ours 0.359 0.298 0.068 94.1 98.8 99.7</cell></row><row><cell cols="8">Liao [27] 0.442 Horiz. Levin [24] 1.003 0.852 0.281 63.8 83.2 92.3 -0.104 87.8 96.4 98.9 Line Wang [45] 0.482 0.394 0.089 90.7 97.3 99.1 Ours 0.431 0.356 0.088 91.1 98.1 99.5  *  120 Levin [24] 1.104 0.953 0.348 57.5 79.2 90.0 x Wang [45] 0.493 0.409 0.097 89.1 96.9 98.9 160 Ours 0.447 0.374 0.097 89.5 97.7 99.3  *  240 Levin [24] 0.664 0.578 0.196 74.2 91.8 96.7 x Wang [45] 0.416 0.342 0.081 91.5 97.7 99.2 320 Ours 0.363 0.298 0.076 92.5 98.3 99.5  Depth Up-sampling (Setting = ↑ factor)</cell><cell>Ma [33] Levin [24] 0.507 0.436 0.117 86.4 97.1 99.3 -0.281 0.059 95.5 99.0 99.7 Wang [45] 0.364 0.291 0.056 95.5 98.8 99.6 Ours 0.320 0.262 0.056 95.6 99.1 99.8 Levin [24] 0.396 0.340 0.085 92.2 98.5 99.6 Wang [45] 0.336 0.271 0.052 96.2 99.0 99.7 Ours 0.279 0.231 0.046 96.6 99.4 99.9 Ma [33] -0.230 0.044 97.1 99.4 99.8 Levin [24] 0.305 0.264 0.061 95.7 99.2 99.8 Wang [45] 0.316 0.254 0.048 96.6 99.2 99.6 Ours 0.246 0.203 0.039 97.4 99.5 99.9 User Selection (Setting = #choices) 50 100 200</cell></row><row><cell></cell><cell>Chen [4]</cell><cell>0.318</cell><cell>-</cell><cell cols="4">0.061 94.2 98.9 99.8</cell><cell>5</cell><cell>Ours</cell><cell>0.471 0.406 0.113 87.1 97.4 99.3</cell></row><row><cell>96x</cell><cell cols="7">Levin [24] 0.512 0.443 0.120 85.9 97.1 99.4 Wang [45] 0.367 0.296 0.057 95.4 98.7 99.6</cell><cell>10 15</cell><cell>Ours Ours</cell><cell>0.457 0.394 0.109 87.9 97.6 99.4 0.447 0.385 0.108 88.3 97.8 99.4</cell></row><row><cell></cell><cell>Ours</cell><cell cols="6">0.313 0.259 0.056 95.7 99.2 99.8</cell></row><row><cell></cell><cell>Chen [4]</cell><cell>0.193</cell><cell>-</cell><cell cols="4">0.032 98.3 99.7 99.9</cell><cell>User Selection with Annotation (Setting = #choices)</cell></row><row><cell>48x</cell><cell cols="7">Levin [24] 0.319 0.275 0.065 95.4 99.1 99.8 Wang [45] 0.318 0.256 0.048 96.7 99.2 99.8</cell><cell>5 10</cell><cell>Ours Ours</cell><cell>0.398 0.342 0.098 90.4 98.2 99.6 0.372 0.322 0.093 91.5 98.5 99.7</cell></row><row><cell></cell><cell>Ours</cell><cell cols="6">0.235 0.195 0.035 97.7 99.6 99.9</cell><cell>15</cell></row></table><note>* Metrics computed only on filled-in regions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>RMS error for depth estimation from different numbers of sparse measurements, when making measurements at random locations vs. with guidance from our distribution. Given the measurements, we use our depth completion approach in both cases.</figDesc><table><row><cell>Measurements</cell><cell>20</cell><cell>50</cell><cell></cell><cell>100</cell><cell>200</cell></row><row><cell>Random</cell><cell>0.359</cell><cell cols="2">0.320</cell><cell>0.279</cell><cell>0.246</cell></row><row><cell>Guided</cell><cell>0.331</cell><cell cols="2">0.286</cell><cell>0.253</cell><cell>0.227</cell></row><row><cell>Method</cell><cell>WKDR</cell><cell cols="4">WKDR = WKDR =</cell></row><row><cell>Zoran [51]</cell><cell>43.5%</cell><cell></cell><cell cols="2">44.2%</cell><cell>41.4%</cell></row><row><cell>Chen [3]</cell><cell>28.3%</cell><cell></cell><cell cols="2">30.6%</cell><cell>28.6%</cell></row><row><cell>Xian [47]</cell><cell>29.1%</cell><cell></cell><cell cols="2">29.5%</cell><cell>29.7%</cell></row><row><cell>Ours: mean</cell><cell>30.2%</cell><cell></cell><cell cols="2">29.9%</cell><cell>30.5%</cell></row><row><cell cols="3">Ours (distribution) 27.1%</cell><cell cols="2">26.0%</cell><cell>27.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Optimization running time for different applications (does not include sample generation time). Note that for user-guidance, the reported time is for each generated mode Z m .</figDesc><table><row><cell cols="3">Application Un-cropping Up-sampling</cell><cell cols="2">Sparse User Sel. Meaus.</cell><cell>User Sel. w/ Annot.</cell></row><row><cell>Time</cell><cell>1.0 s</cell><cell>0.4 s</cell><cell>0.7 s</cell><cell>0.8 s</cell><cell>2.2 s</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While h can be estimated based on the variance between x i and true patch depths, as we will see, its actual value is often not needed as it is factored into other manually-set, task-specific parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Some papers interpret RMSE as mean of per-image RMSE values. We report the standard definition as rms, and this per-image version as m-rms.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3"><ref type="bibr" target="#b32">[33]</ref> uses a non-standard resolution and crop to evaluate their method and report errors. We report our performance with official settings here be consistent with the benchmark and the other applications. Our performance under<ref type="bibr" target="#b32">[33]</ref>'s settings is similar, and reported in the supplementary.<ref type="bibr" target="#b3">4</ref> <ref type="bibr" target="#b26">[27]</ref> uses measurements along a line simulated to be horizontal in 3D, leading to different y image co-ordinates for each x. Lacking exact details for replicating their setting, we use the same number of measurements but from a line that is horizontal simply in the image plane.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported by the NSF under award no. IIS-1820693.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>Output <ref type="table">Shape  0  features from feature extractor  1×33×45×2560  1  resize  1×65×89×2560  2  conv 1×1  1×65×89×1024  3  conv 1×1  1×65×89×512  4  conv 3×3 dilation=2  1×61×85×512  5  conv 3×3 dilation=2  1×57×81×256  6</ref> reshape </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diverse m-best solutions in markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payman</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abner</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Depth from a single image by harmonizing overcomplete local network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Singleimage depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating depth from rgb and sparse sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Filling large holes in lidar data by inpainting depth gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard J Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning dynamic guidance for depth image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using whole strip masking and reliability-based refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyeok</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Rae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Ul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth map inpainting under a second-order smoothness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fawzi</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conference on 3D Vision (3DV)</title>
		<meeting>Intl. Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A probabilistic u-net for segmentation of ambiguous images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning-based, automatic 2d-to-3d image and video conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janusz</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakash</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debargha</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semisupervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Vasileios Belagiannis, Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single-image depth estimation based on fourier domain analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Han</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyeok</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Rae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using relative depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Han</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep joint image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parse geometry from a line: Monocular depth estimation with partial laser observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICRA</title>
		<meeting>ICRA</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural rgb-&gt;d sensing: Depth and uncertainty from a video camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasa</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02571</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Guided depth enhancement via anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Rim Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Guided inpainting and filtering for kinect depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICRA</title>
		<meeting>ICRA</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Depth image enhancement using local tangent plane approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoshi</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimitsu</forename><surname>Aoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dense monocular depth estimation in complex dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Break ames room illusion: depth from general single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dfusenet: Deep fusion of rgb and sparse depth information for image guided dense depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shreyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ty</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camillo J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00761</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05356</idno>
		<title level="m">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Plug-and-play: Improve depth prediction via sparse data propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Ting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICRA</title>
		<meeting>ICRA</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Monocular relative depth perception with web stereo data supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Indoor scene structure analysis for single image depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning ordinal relationships for mid-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
