<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-domain Correspondence Learning for Exemplar-based Image Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Cloud+AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-domain Correspondence Learning for Exemplar-based Image Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a general framework for exemplar-based image translation, which synthesizes a photo-realistic image from the input in a distinct domain (e.g., semantic segmentation mask, or edge map, or pose keypoints), given an exemplar image. The output has the style (e.g., color, texture) in consistency with the semantically corresponding objects in the exemplar. We propose to jointly learn the crossdomain correspondence and the image translation, where both tasks facilitate each other and thus can be learned with weak supervision. The images from distinct domains are first aligned to an intermediate domain where dense correspondence is established. Then, the network synthesizes images based on the appearance of semantically corresponding patches in the exemplar. We demonstrate the effectiveness of our approach in several image translation tasks. Our method is superior to state-of-the-art methods in terms of image quality significantly, with the image style faithful to the exemplar with semantic consistency. Moreover, we show the utility of our method for several applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Conditional image synthesis aims to generate photorealistic images based on certain input data <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b5">6]</ref>. We are interested in a specific form of conditional image synthesis, which converts a semantic segmentation mask, an edge map, and pose keypoints to a photo-realistic image, given an exemplar image, as shown in <ref type="figure">Figure 1</ref>. We refer to this form as exemplar-based image translation. It allows more flexible control for multi-modal generation according to a user-given exemplar.</p><p>Recent methods directly learn the mapping from a semantic segmentation mask to an exemplar image using neural networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>. Most of these methods encode the style of the exemplar into a latent style vector, from which the network synthesizes images with the desired style similar to the examplar. However, the style code only characterizes the global style of the exemplar, regardless of spa- * Author did this work during the internship at Microsoft Research Asia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Exemplar Input Exemplar Input Exemplar <ref type="figure">Figure 1</ref>: Exemplar-based image synthesis. Given the exemplar images (1st row), our network translates the inputs, in the form of segmentation mask, edge and pose, to photorealistic images (2nd row). Please refer to supplementary material for more results. tial relevant information. Thus, it causes some local style "wash away" in the ultimate image.</p><p>To address this issue, the cross-domain correspondence between the input and the exemplar has to be established before image translation. As an extension of Image Analogies <ref type="bibr" target="#b13">[14]</ref>, Deep Analogy <ref type="bibr" target="#b26">[27]</ref> attempts to find a dense semantically-meaningful correspondence between the image pair. It leverages deep features of VGG pretrained on real image classification tasks for matching. We argue such representation may fail to handle a more challenging mapping from mask (or edge, keypoints) to photo since the pretrained network does not recognize such images. In order to consider the mask (or edge) in the training, some methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b4">5]</ref> explicitly separate the exemplar image into semantic regions and learns to synthesize different parts individually. In this way, it successfully generates highquality results. However, these approaches are task specific, and are unsuitable for general translation.</p><p>How to find a more general solution for exemplar-based image translation is non-trivial. We aim to learn the dense semantic correspondence for cross-domain images (e.g., mask-to-image, edge-to-image, keypoints-to-image, etc.), and then use it to guide the image translation. It is weakly supervise learning, since we have neither the correspondence annotations nor the synthesis ground truth given a random exemplar.</p><p>In this paper, we propose a CrOss-domain COrreSpondence network (CoCosNet) that learns cross-domain correspondence and image translation simultaneously. The network architecture comprises two sub-networks: 1) Crossdomain correspondence Network transforms the inputs from distinct domains to an intermediate feature domain where reliable dense correspondence can be established; 2) Translation network, employs a set of spatially-variant de-normalization blocks <ref type="bibr" target="#b37">[38]</ref> to progressively synthesizes the output, using the style details from a warped exemplar which is semantically aligned to the mask (or edge, keypoints map) according to the estimated correspondence. Two sub-networks facilitate each other and are learned endto-end with novel loss functions. Our method outperforms previous methods in terms of image quality by a large margin, with instance-level appearance being faithful to the exemplar. Moreover, the cross-domain correspondence implicitly learned enables some intriguing applications, such as image editing and makeup transfer. Our contribution can be summarized as follows:</p><p>• We address the problem of learning dense cross-domain correspondence with weak supervision-joint learning with image translation.</p><p>• With the cross-domain correspondence, we present a general solution to exemplar-based image translation, that for the first time, outputs images resembling the fine structures of the exemplar at instance level.</p><p>• Our method outperforms state-of-the-art methods in terms of image quality by a large margin in various application tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image-to-image translation The goal of image translation is to learn the mapping between different image domains.</p><p>Most prominent contemporary approaches solve this problem through conditional generative adversarial network <ref type="bibr" target="#b35">[36]</ref> that leverages either paired data <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b37">38]</ref> or unpaired data <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref>. Since the mapping from one image domain to another is inherently multi-modal, following works promote the synthesis diversity by performing stochastic sampling from the latent space <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>. However, none of these methods allow delicate control of the output since the latent representation is rather complex and does not have an explicit correspondence to image style. In contrast, our method supports customization of the result according to a user-given exemplar, which allows more flexible control for multi-modal generation. Exemplar-based image synthesis Very recently, a few works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b1">2]</ref> propose to synthesize photorealistic images from semantic layout under the guidance of exemplars. Non-parametric or semi-parametric approaches <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b1">2]</ref> synthesize images by compositing the image fragments retrieved from a large database. Mainstream works, however, formulate the problem as image-to-image translation. Huang et al. <ref type="bibr" target="#b16">[17]</ref> and Ma et al. <ref type="bibr" target="#b33">[34]</ref> propose to employ Adaptive Instance Normalization (AdaIN) <ref type="bibr" target="#b15">[16]</ref> to transfer the style code from the exemplar to the source image. Park et al. <ref type="bibr" target="#b37">[38]</ref> learn an encoder to map the exemplar image into a vector from which the images are further synthesized. The style consistency discriminator is proposed in <ref type="bibr" target="#b43">[44]</ref> to examine whether the image pairs exhibit a similar style. However, this method requires to constitute style consistency image pairs from video clips, which makes it unsuitable for general image translation. Unlike all of the above methods that only transfer the global style, our method transfers the fine style from a semantically corresponding region of the exemplar. Our work is inspired by the recent exemplar-based image colorization <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13]</ref>, but we solve a more general problem: translating images between distinct domains. Semantic correspondence Early studies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">43]</ref> on semantic correspondence focus on matching hand-crafted features. With the advent of the convolutional neural network, deep features are proven powerful to represent the high-level semantics. Long et al. <ref type="bibr" target="#b31">[32]</ref> first propose to establish semantic correspondence by matching deep features extracted from a pretrained classification model. Following works further improve the correspondence quality by incorporating additional annotations <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>, adopting coarse-to-fine strategy <ref type="bibr" target="#b26">[27]</ref> or retaining reliable sparse matchings <ref type="bibr" target="#b0">[1]</ref>. However, all these methods can only handle the correspondence between natural images instead of cross-domain images, e.g., edge and photorealistic images. We explore this new scenario and implicitly learns the task with weak supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cross-domain correspondence network</head><p>Usually the semantic correspondence is found by matching patches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25]</ref> in the feature domain with a pre-trained classification model. However, pre-trained models are typically trained on a specific type of images, e.g., natural images, so the extracted features cannot generalize to depict the semantics for another domain. Hence, prior works cannot establish the correspondence between heterogeneous images, e.g., edge and photo-realistic images. To tackle this, we propose a novel cross-domain correspondence network, mapping the input domains to a shared domain S in which the representation is capable to represent the semantics for both input domains. As a result, reliable semantic correspondence can be found within domain S. <ref type="figure">Figure 2</ref>, we first adapt the input image and the exemplar to a shared domain S. To be specific, x A and y B are fed into the feature pyramid network that extracts multi-scale deep features by leveraging both local and global image context <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b27">28]</ref>. The extracted feature maps are further transformed to the representations in S, denoted by x S ∈ R HW ×C and y S ∈ R HW ×C respectively (H,W are feature spatial size; C is the channel-wise dimension). Let F A→S and F B→S be the domain transformation from the two input domains respectively, so the adapted representation can be formulated as,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain alignment As shown in</head><formula xml:id="formula_0">x S = F A→S (x A ; θ F ,A→S ), (1) y S = F B→S (y B ; θ F ,B→S ).<label>(2)</label></formula><p>where θ denotes the learnable parameter. The representation x S and y S comprise discriminative features that characterize the semantics of inputs. Domain alignment is, in practice, essential for correspondence in that only when x S and y S reside in the same domain can they be further matched with some similarity measure.</p><p>Correspondence within shared domain We propose to match the features of x S and y S with the correspondence layer proposed in <ref type="bibr" target="#b48">[49]</ref>. Concretely, we compute a correlation matrix M ∈ R HW ×HW of which each element is a pairwise feature correlation,</p><formula xml:id="formula_1">M(u, v) =x S (u) Tŷ S (v) x S (u) ŷ S (v) ,<label>(3)</label></formula><p>wherex S (u) andŷ S (v) ∈ R C represent the channel-wise centralized feature of x S and y S in position u and v, i.e.,</p><formula xml:id="formula_2">x S (u) = x S (u) − mean(x S (u)) andŷ S (v) = y S (v) − mean(y S (v)). M(u, v)</formula><p>indicates a higher semantic similarity between x S (u) and y S (v). Now the challenge is how to learn the correspondence without direct supervision. Our idea is to jointly train with image translation. The translation network may find it easier to generate high-quality outputs only by referring to the correct corresponding regions in the exemplar, which implicitly pushes the network to learn the accurate correspondence. In light of this, we warp y B according to M and obtain the warped exemplar r y→x ∈ R HW . Specifically, we obtain r y→x by selecting the most correlated pixels in y B and calculating their weighted average,</p><formula xml:id="formula_3">r y→x (u) = v softmax v (αM(u, v)) · y B (v).<label>(4)</label></formula><p>Here, α is the coefficient that controls the sharpness of the softmax and we set its default value as 100. In the following, images will be synthesized conditioned on r y→x and the correspondence network, in this way, learns its assignment with indirect supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Translation network</head><p>Under the guidance of r y→x , the translation network G transforms the constant code z to the desired output x B ∈ B. In order to preserve the structural information of r y→x , we employ the spatially-adaptive denormalization (SPADE) block <ref type="bibr" target="#b37">[38]</ref> to project the spatially variant exemplar style to different activation locations. As shown in <ref type="figure">Figure 2</ref>, the translation network has L layers with the exemplar style progressively injected. As opposed to <ref type="bibr" target="#b37">[38]</ref> which computes layer-wise statistics for batch normalization (BN), we empirically find the normalization that computes the statistics at each spatial position, the positional normalization (PN) <ref type="bibr" target="#b25">[26]</ref>, better preserves the structure information synthesized in prior layers. Hence, we propose to marry positional normalization and spatially-variant denormalization for high-fidelity texture transfer from the exemplar.</p><p>Formally, given the activation F i ∈ R Ci×Hi×Wi before the i th normalization layer, we inject the exemplar style through,</p><formula xml:id="formula_4">α i h,w (r y→x ) × F i c,h,w − µ i h,w σ i h,w + β i h,w (r y→x ),<label>(5)</label></formula><p>where the statistic value µ i h,w and σ i h,w are calculated exclusively across channel direction compared to BN. The denormalization parameter α i and β i characterize the style of the exemplar, which is mapped from r y→x with the projection T parameterized by θ T , i.e.,</p><formula xml:id="formula_5">α i , β i = T i (r y→x ; θ T ).<label>(6)</label></formula><p>We use two plain convolutional layers to implement T so α and β have the same spatial size as r y→x . With the style modulation for each normalization layer, the overall image translation can be formulated aŝ</p><formula xml:id="formula_6">x B = G(z, T i (r y→x ; θ T ); θ G ),<label>(7)</label></formula><p>where θ G denotes the learnable parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Losses for exemplar-based translation</head><p>We jointly train the cross-domain correspondence along with image synthesis with following loss functions, hoping the two tasks benefit each other.</p><p>Losses for pseudo exemplar pairs We construct exemplar training pairs by utilizing paired data {x A , x B } that are semantically aligned but differ in domains. Specifically, we apply random geometric distortion to x B and get the dis-</p><formula xml:id="formula_7">torted image x B = h(x B ),</formula><p>where h denotes the augmentation operation like image warping or random flip. When x B is regarded as the exemplar, the translation of x A is expected to be its counterpart x B . In this way, we obtain pseudo exemplar pairs. We propose to penalize the difference between the translation output and the ground truth x B by minimizing the feature matching loss <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6]</ref> </p><formula xml:id="formula_8">L f eat = l λ l φ l (G(x A , x B )) − φ l (x B ) 1 , (8)</formula><p>where φ l represents the activation of layer l in the pretrained VGG-19 model and λ l balance the terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain alignment loss</head><p>We need to make sure the transformed embedding x S and y S lie in the same domain. To achieve this, we once again make use of the image pair {x A , x B }, whose feature embedding should be aligned exactly after domain transformation:</p><formula xml:id="formula_9">L 1 domain = F A→S (x A ) − F B→S (x B ) 1 .<label>(9)</label></formula><p>Note that we perform channel-wise normalization as the last layer of F A→S and F B→S so minimizing this domain discrepancy will not lead to a trivial solution (i.e., small magnitude of activations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exemplar translation losses</head><p>The learning with pair or pseudo exemplar pair is hard to generalize to general cases where the semantic layout of exemplar differs significantly from the source image. To tackle this, we propose the following losses.</p><p>First, the ultimate output should be consistent with the semantics of the input x A , or its counterpart x B . We thereby penalize the perceptual loss to minimize the semantic discrepancy:</p><formula xml:id="formula_10">L perc = φ l (x B ) − φ l (x B ) 1 .<label>(10)</label></formula><p>Here we choose φ l to be the activation after relu4 2 layer in the VGG-19 network since this layer mainly contains highlevel semantics.</p><p>On the other hand, we need a loss function that encouragesx B to adopt the appearance from the semantically corresponding patches from y B . To this end, we employ the contextual loss proposed in <ref type="bibr" target="#b34">[35]</ref> to match the statistics be-tweenx B and y B , which is</p><formula xml:id="formula_11">L context = l ω l −log 1 n l i max j A l (φ l i (x B ),φ l j (y B )) ,<label>(11)</label></formula><p>where i and j index the feature map of layer φ l that contains n l features, and ω l controls the relative importance of different layers. Still, we rely on pretrained VGG features. As opposed to L perc which mainly utilizes high-level features, the contextual loss uses relu2 2 up to relu5 2 layers since low-level features capture richer style information (e.g., color or textures) useful for transferring the exemplar appearance.</p><p>Correspondence regularization Besides, the learned correspondence should be cycle consistent, i.e., the image should match itself after forward-backward warping, which is</p><formula xml:id="formula_12">L reg = r y→x→y − y B 1 ,<label>(12)</label></formula><p>where r y→x→y (v) = u softmax u (αM(u, v)) · r y→x (u) is the forward-backward warping image. Indeed, this objective function is crucial because the rest loss functions, imposed at the end of the network, are weak supervision and cannot guarantee that the network learns a meaningful correspondence. <ref type="figure" target="#fig_5">Figure 9</ref> shows that without L reg the network fails to learn the cross-domain correspondence correctly although it is still capable to generate plausible translation result. The regularization L reg enforces the warped image r y→x remain in domain B by constraining its backward warping, implicitly encouraging the correspondence to be meaningful as desired.</p><p>Adversarial loss We train a discriminator <ref type="bibr" target="#b8">[9]</ref> that discriminates the translation outputs and the real samples of domain B. Both the discriminator D and the translation network G are trained alternatively until synthesized images look indistinguishable to real ones. The adversarial objectives of D and G are respectively defined as:</p><formula xml:id="formula_13">L D adv = −E[h(D(y B ))] − E[h(−D(G(x A , y B )))] L G adv = −E[D(G(x A , y B ))],<label>(13)</label></formula><p>where h(t) = min(0, −1 + t) is a hinge function used to regularize the discriminator <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Total loss In all, we optimize the following objective,</p><formula xml:id="formula_14">L θ = min F ,T ,G max D ψ 1 L f eat + ψ 2 L perc + ψ 3 L context + ψ 4 L G adv + ψ 5 L 1 domain + ψ 6 L reg ,<label>(14)</label></formula><p>where weights ψ are used to balance the objectives.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Implementation We use Adam <ref type="bibr" target="#b22">[23]</ref> solver with β 1 = 0, β 2 = 0.999. Following the TTUR <ref type="bibr" target="#b14">[15]</ref>, we set imbalanced learning rates, 1e−4 and 4e−4 respectively, for the generator and discriminator. Spectral normalization <ref type="bibr" target="#b36">[37]</ref> is applied to all the layers for both networks to stabilize the adversarial training. Readers can refer to the supplementary material for detailed network architecture. We conduct experiments using 8 32GB Tesla V100 GPUs, and it takes roughly 4 days to train 100 epochs on the ADE20k dataset <ref type="bibr" target="#b50">[51]</ref>. Datasets We conduct experiments on multiple datasets with different sorts of image representation. All the images are resized to 256×256 during training.</p><p>• ADE20k <ref type="bibr" target="#b50">[51]</ref> consists of ∼20k training images, each image associated with a 150-class segmentation mask. This is a challenging dataset for most existing methods due to its large diversity.</p><p>• ADE20k-outdoor contains the outdoor images extracted from ADE20k, as the same protocol in SIMS <ref type="bibr" target="#b38">[39]</ref>.</p><p>• CelebA-HQ <ref type="bibr" target="#b29">[30]</ref> contains high quality face images. We connect the face landmarks for face region, and use Canny edge detector to detect edges in the background. We perform an edge-to-face translation on this dataset.</p><p>• Deepfashion <ref type="bibr" target="#b30">[31]</ref> consists of 52,712 person images in fashion clothes. We extract the pose keypoints using the OpenPose <ref type="bibr" target="#b3">[4]</ref>, and learn the translation to human body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground truth Pix2pixHD <ref type="bibr" target="#b44">[45]</ref> MUINT <ref type="bibr" target="#b16">[17]</ref> EGSC-IT <ref type="bibr" target="#b33">[34]</ref> SPADE <ref type="bibr" target="#b37">[38]</ref> Ours Exemplar <ref type="figure">Figure 3</ref>: Qualitative comparison of different methods.   Baselines We compare our method with state-of-the-art image translation methods: 1) Pix2pixHD <ref type="bibr" target="#b44">[45]</ref>, a leading supervised approach; 2) SPADE <ref type="bibr" target="#b37">[38]</ref>, a recently proposed supervised translation method which also supports the style injection from an exemplar image; 3) MUNIT <ref type="bibr" target="#b16">[17]</ref>, an unsupervised method that produces multi-modal results; 4) SIMS <ref type="bibr" target="#b38">[39]</ref>, which synthesizes images by compositing image segments from a memory bank; 5) EGSC-IT <ref type="bibr" target="#b33">[34]</ref>, an exemplar-based method that also considers the semantic consistency but can only mimic the global style. These methods except Pix2pixHD can generate exemplar-based results, and we use their released codes in this mode to train on several datasets. Since it is computationally prohibitive to prepare a database using SIMS, we directly use their reported figures. As we aim to propose a general translation framework, we do not include other task-specific methods.</p><p>To provide the exemplar for our method, we first train a plain translation network to generate natural images and use them to retrieve the exemplars from the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative evaluation</head><p>We evaluate different methods from three aspects.</p><p>• We use two metrics to measure image quality. First, we use the Fréchet Inception Score (FID) <ref type="bibr" target="#b14">[15]</ref> to measure the distance between the distributions of synthesized images and real images. While FID measures the semantic realism, we also adopt sliced Wasserstein distance (SWD) <ref type="bibr" target="#b19">[20]</ref> to measure their statistical distance of lowlevel patch distributions. Measured by these two metrics, <ref type="table" target="#tab_0">Table 1</ref> shows that our method significantly outperforms prior methods in almost all the comparisons. Our method improves the FID score by 7.5 compared to previous leading methods on the challenging ADE20k dataset.    • The ultimate output should not alter the input semantics.</p><p>To evaluate the semantic consistency, we adopt an Ima-geNet pretrained VGG model <ref type="bibr" target="#b2">[3]</ref>, and use its high-level features maps, relu3 2, relu4 2 and relu5 2, to represent high-level semantics. We calculate the cosine similarity for these layers and take the average to yield the final score. <ref type="table" target="#tab_1">Table 2</ref> shows that our method best maintains the semantics during translation.</p><p>• Style relevance. We use low level features relu1 2 and relu2 2 respectively to measure the color and texture distance between the semantically corresponding patches in the output and the exemplar. We do not include Pix2pixHD as it does not produce an exemplar-based translation. Still, our method achieves considerably better instance-level style relevance as shown in <ref type="table" target="#tab_3">Table 3</ref>. <ref type="figure">Figure 3</ref> provides a qualitative comparison of different methods. It shows that our Co-cosNet demonstrates the most visually appealing quality  with much fewer artifacts. Meanwhile, compared to prior exemplar-based methods, our method demonstrates the best style fidelity, with the fine structures matching the semantically corresponding regions of the exemplar. This also correlates with the quantitative results, showing the obvious advantage of our approach. We show diverse results by changing the exemplar image in <ref type="figure" target="#fig_0">Figure 4</ref>-6. Please refer to the supplementary material for more results. Subjective evaluation We also conduct user study to compare the subjective quality. We randomly select 10 images for each task, yielding 30 images in total for comparison. We design two tasks, and let users sort all the methods in terms of the image quality and the style relevance. <ref type="figure" target="#fig_3">Figure 7</ref> shows the results, where our method demonstrates a clear advantage. Our method ranks the first in 84.2% cases in evaluating the image quality, with 93.8% chance to be the best in the style relevance comparison. Cross-domain correspondence <ref type="figure" target="#fig_4">Figure 8</ref> shows the crossdomain correspondence. For better visualization, we just annotate the sparse points. As the first approach in doing this, our CoCosNet successfully establishes meaningful semantic correspondence which is even difficult for manual labeling. The network is still capable to find the correspondence for sparse representation such as edge map, which captures little explicit semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative comparison</head><p>Ablation study In order to validate the effectiveness of each component, we conduct comprehensive ablation studies. Here we want to emphasize two key elements <ref type="figure" target="#fig_5">(Figure 9)</ref>. First, the domain alignment loss L 1 domain with data pairs x A and x B is crucial. Without it, the correspondence will fail in unaligned domains, leading to oversmooth dense warping. We also ablate the correspondence regularization loss L reg , which leads to incorrect dense correspondence, e.g., face to hair in <ref type="figure" target="#fig_5">Figure 9</ref>, though the network still yields plausible final output. With L reg , the correspondence becomes meaningful, which facilitates the image synthesis as well. We also quantitatively measures the role of different losses in <ref type="table" target="#tab_5">Table 4</ref> where the full model demonstrates the best performance in terms of all the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Applications</head><p>Our method can enable a few intriguing applications.</p><p>Here we give two examples. Image editing Given a natural image, we can manipulate its content by modifying the segmentation layout and synthesizing the image by using the original image as the <ref type="figure">Figure 10</ref>: Image editing. Given the input image and its mask (1st column), we can semantically edit the image content through the manipulation on the mask (column 2-4). <ref type="figure">Figure 11</ref>: Makeup transfer. Given a portrait and makeup strokes (1st column), we can transfer these makeup edits to other portraits by matching the semantic correspondence. We show more examples in the supplementary material. self-exemplar. Since this is similar to the pseudo exemplar pairs we constitute for training, our CocosNet could perfectly handle it and produce the output with high quality. <ref type="figure">Figure 10</ref> illustrates the image editing, where one can move, add and delete instances. Makeup transfer Artists usually manually adds digital makeup on portraits. Because of the dense semantic correspondence we find, we can transfer the artistic stokes to other portraits. In this way, one can manually add makeup edits on one portrait, and use our network to process a large batch of portraits automatically based on the semantic correspondence, which illustrated in <ref type="figure">Figure 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we present the CocosNet, which translates the image by relying on the cross-domain correspondence. Our method achieves preferable performance than leading approaches both quantitatively and qualitatively. Besides, our method learns the dense correspondence for cross-domain images, paving a way for several intriguing applications. Our method is computationally intensive and we leave high-resolution synthesis to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Additional Qualitative Results</head><p>Mask-to-image We perform mask-to-image synthesis on three datasets -ADE20k, CelebA-HQ and Flickr dataset, and we show their results in <ref type="figure">Figure 12</ref> Positional Normalization In the translation sub-network, we empirically find the normalization that computes the statistics at each spatial position better preserves the structure information synthesized in prior layers. Such positional normalization significantly improves the lower bound of our approach. We show the worst case result of ADE20k dataset in <ref type="figure" target="#fig_4">Figure 18</ref>, where the normalization helps produce vibrant image even when the correspondence is hard to be established in the complex scene. Feature normalization for correspondence Note that we normalize the features before computing the correlation matrix. Likewise, we propose to calculate the statistics along the channel dimension while keeping the spatial size as the feature maps. This helps transfer the fine structures in the exemplar. As shown in <ref type="figure" target="#fig_5">Figure 19</ref>, the channel-wise normalization betters maintains the window structures in the ultimate output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Exemplar Feature-wise normalization Channel-wise normalization <ref type="figure" target="#fig_5">Figure 19</ref>: Channel-wise normalization during correspondence. The channel-wise normalization helps transfer the window structures from the exemplar image. Makeup transfer Thanks to the dense semantic correspondence, we can transfer the makeup brushes to a batch of portraits. <ref type="figure">Figure 21</ref> gives more supplementary results. <ref type="figure">Figure 21</ref>: Makeup transfer. Given a portrait along with makeup edits (1st column), we can transfer the makeup to other portraits by matching the semantic correspondence. <ref type="figure" target="#fig_9">Figure 22</ref> shows the detailed results of user study. In ADE20k, there are 67.3% and 91.9% users respectively that prefer the image quality and style relevance for our method. Regarding edge-to-face translation on CelebA-HQ, 91.3% users prefer our image quality while 90.6% users believes our method most resembles the exemplar. For pose synthesis on DeepFashion dataset, 90.6% and 98.8% users prefer our results according to the image quality and the style resemblance respectively.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Our results of segmentation mask to image synthesis (ADE20k dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Our results of edge to face synthesis (CelebA-HQ dataset). First row: exemplars. Second row: our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Our results of pose to body synthesis (Deep-Fashion).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>User study results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Sparse correspondence of different domains. Given the manual annotation points in domain A (first row), our method finds their corresponding points in domain B (second row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Ablation study of loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 :Figure 13 :Figure 14 :Figure 15 :Figure 16 :</head><label>1213141516</label><figDesc>Our results of mask-to-image synthesis (ADE20k dataset). In each group, the first row shows exemplars, and the second row shows the segmentation masks along with our results. Our results of mask-to-image synthesis (CelebAHQ dataset). In each group, the first row shows exemplars, and the second row shows the segmentation masks along with our results. Our results of mask-to-image synthesis (Flickr dataset). In each group, the first row shows exemplars, and the second row shows the segmentation masks along with our results.Edge-to-faceFigure 15shows additional results of edge-to-face synthesis on CelebA-HQ dataset. Our results of edge-to-face synthesis (CelebA-HQ dataset). In each group, the first row shows exemplars, and the second row shows the edge maps along with our results.Pose-to-bodyFigure 16shows more pose synthesis results on DeepFashion dataset. Our results of pose to image synthesis (DeepFashion dataset). In each group, the first row shows exemplars, and the second row shows the pose images along with our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 18 :</head><label>18</label><figDesc>Positional Normalization vs. Batch Normalization. The positional normalization significantly improves the lower bound of the translation image quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 20 :</head><label>20</label><figDesc>Image editing. Giving the original input image along with segmentation mask (1st column), we manipulate the image by changing its semantic layout (2nd-5th columns).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 22 :</head><label>22</label><figDesc>Detailed user study results for ADE20k, CelebA-HQ and DeepFashion dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Image quality comparison. Lower FID or SWD score indicates better image quality. The best scores are highlighted.</figDesc><table><row><cell></cell><cell cols="3">ADE20k ADE20k-outdoor CelebA-HQ DeepFashion</cell></row><row><cell></cell><cell>FID SWD FID</cell><cell>SWD</cell><cell>FID SWD FID SWD</cell></row><row><cell cols="2">Pix2pixHD 81.8 35.7 97.8</cell><cell>34.5</cell><cell>62.7 43.3 25.2 16.4</cell></row><row><cell>SPADE</cell><cell>33.9 19.7 63.3</cell><cell>21.9</cell><cell>31.5 26.9 36.2 27.8</cell></row><row><cell>MUNIT</cell><cell>129.3 97.8 168.2</cell><cell>126.3</cell><cell>56.8 40.8 74.0 46.2</cell></row><row><cell>SIMS</cell><cell>N/A N/A 67.7</cell><cell>27.2</cell><cell>N/A N/A N/A N/A</cell></row><row><cell cols="2">EGSC-IT 168.3 94.4 210.0</cell><cell>104.9</cell><cell>29.5 23.8 29.0 39.1</cell></row><row><cell>Ours</cell><cell>26.4 10.5 42.4</cell><cell>11.5</cell><cell>14.3 15.2 14.4 17.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of semantic consistency. The best scores are highlighted.</figDesc><table><row><cell></cell><cell cols="4">ADE20k ADE20k-outdoor CelebA-HQ DeepFashion</cell></row><row><cell>Pix2pixHD</cell><cell>0.833</cell><cell>0.848</cell><cell>0.914</cell><cell>0.943</cell></row><row><cell>SPADE</cell><cell>0.856</cell><cell>0.867</cell><cell>0.922</cell><cell>0.936</cell></row><row><cell>MUNIT</cell><cell>0.723</cell><cell>0.704</cell><cell>0.848</cell><cell>0.910</cell></row><row><cell>SIMS</cell><cell>N/A</cell><cell>0.822</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>EGSC-IT</cell><cell>0.734</cell><cell>0.723</cell><cell>0.915</cell><cell>0.942</cell></row><row><cell>Ours</cell><cell>0.862</cell><cell>0.873</cell><cell>0.949</cell><cell>0.968</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of style relevance. A higher score indicates a higher appearance similarity relative to the exemplar. The best scores are highlighted.</figDesc><table><row><cell></cell><cell cols="2">ADE20k</cell><cell cols="2">CelebA-HQ</cell><cell cols="2">DeepFashion</cell></row><row><cell></cell><cell>Color</cell><cell>Texture</cell><cell>Color</cell><cell>Texture</cell><cell>Color</cell><cell>Texture</cell></row><row><cell>SPADE</cell><cell>0.874</cell><cell>0.892</cell><cell>0.955</cell><cell>0.927</cell><cell>0.943</cell><cell>0.904</cell></row><row><cell>MUNIT</cell><cell>0.745</cell><cell>0.782</cell><cell>0.939</cell><cell>0.884</cell><cell>0.893</cell><cell>0.861</cell></row><row><cell cols="2">EGSC-IT 0.781</cell><cell>0.839</cell><cell>0.965</cell><cell>0.942</cell><cell>0.945</cell><cell>0.916</cell></row><row><cell>Ours</cell><cell>0.962</cell><cell>0.941</cell><cell>0.977</cell><cell>0.958</cell><cell>0.982</cell><cell>0.958</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>First row: exemplars. Second row: our results.</figDesc><table><row><cell></cell><cell cols="2">Image quality</cell><cell></cell><cell></cell><cell cols="2">Style relevance</cell></row><row><cell>pix2pixHD</cell><cell></cell><cell></cell><cell></cell><cell>pix2pixHD</cell><cell></cell></row><row><cell>MUNIT</cell><cell></cell><cell></cell><cell></cell><cell>MUNIT</cell><cell></cell></row><row><cell>EGSC-IT</cell><cell></cell><cell></cell><cell></cell><cell>EGSC-IT</cell><cell></cell></row><row><cell>SPADE</cell><cell></cell><cell></cell><cell></cell><cell>SPADE</cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell></row><row><cell>0%</cell><cell cols="3">20% 40% 60% 80% 100%</cell><cell>0%</cell><cell cols="2">20% 40% 60% 80% 100%</cell></row><row><cell cols="2">Top1 Top2</cell><cell>Top3 Top4</cell><cell>Top5</cell><cell cols="2">Top1 Top2</cell><cell>Top3 Top4</cell><cell>Top5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study.</figDesc><table><row><cell></cell><cell cols="3">FID ↓ Semantic consistency ↑ Style (color/texture) ↑</cell></row><row><cell>w/o L f eat</cell><cell>14.4</cell><cell>0.948</cell><cell>0.975 / 0.955</cell></row><row><cell>w/o L domain</cell><cell>21.1</cell><cell>0.933</cell><cell>0.983 / 0.957</cell></row><row><cell>w/o Lperc</cell><cell>59.3</cell><cell>0.852</cell><cell>0.971 / 0.852</cell></row><row><cell>w/o Lcontext</cell><cell>28.4</cell><cell>0.931</cell><cell>0.954 / 0.948</cell></row><row><cell>w/o Lreg</cell><cell>19.3</cell><cell>0.929</cell><cell>0.981 / 0.951</cell></row><row><cell>Full</cell><cell>14.3</cell><cell>0.949</cell><cell>0.977 / 0.958</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Additional Results of Dense Correspondence</head><p>The proposed CoCosNet is able to establish the dense correspondence between different domains. <ref type="figure">Figure 17</ref> shows the dense warping results from domain B to domain A according to the correspondence (r y→x in <ref type="bibr">Equation 4</ref>). <ref type="bibr">yB</ref> xA ry→x yB xA ry→x <ref type="figure">Figure 17</ref>: Warping according to the dense correspondence. The warped image r y→x is semantically aligned as the image in domain A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Additional Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Additional Application Results</head><p>Image editing We show another example of the semantic image editing in <ref type="figure">Figure 20</ref>, where we manipulate the instances in the image by modifying their segmentation masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Implementation Details</head><p>The detailed architecture of CoCosNet is shown in <ref type="table">Table 5</ref>, with the naming convention as the CycleGAN.</p><p>Cross-domain correspondence network Two domain adaptors without weight sharing are used to adapt the input image and the exemplar to a shared domain S. The domain adaptors comprise several Conv-InstanceNorm-LeakReLU blocks and the spatial size of features in S is 64×64. Once the intermediate domain S is found, a shared adaptive feature block further transforms the features from two branches to the representation suitable for correspondence. The correlation layer computes pairwise affinity values between 4096×1 normalized features vectors. We downscale the exemplar image to 64×64 to fit the size of correlation matrix, and thus obtain the warped image on this scale. We use synchronous batch normalization within this sub-network.</p><p>Translation network The translation network generates the final output based on the style of the warped exemplar. We encode the exemplar style through two convolutional layers, which outputs α i and β i to modulate the normalization layer in the generator network. We have seven such style encoder, each responsible for modulating an individual normalization layer. The generator consists of seven normalization layer, which progressively utilizes the style code to synthesize the final output.</p><p>The generator also employs a nonlocal block so that a larger receptive field can be utilized to enhance the global structural consistency. We use positional normalization within this sub-network. Warm-up strategy For the most challenging ADE20k dataset, a mask warm strategy is used. At the beginning of the training, we explicitly provide the segmentation mask for the domain adaptors, and employ cross-entropy loss to encourage that the masks are correctly aligned after dense warping. Such warm-up helps speed up the convergence of the correspondence network and improve the correspondence accuracy. After training 80 epochs, we replace the segmentation masks with Gaussian noise. We just use the segmentation mask for warm up and there is no need to provide the masks during inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix G. Multimodal results for Flickr dataset</head><p>Similar to the practice in <ref type="bibr" target="#b37">[38]</ref>, we collect 56,568 landscape images from Flickr. The semantic segmentation masks are computed using a pre-trained UPerNet101 <ref type="bibr" target="#b45">[46]</ref> network. By feeding different exemplar, our method supports multimodal landscape synthesis. <ref type="figure">Figure 23</ref> shows highly realistic landscape results using the images in Flicker dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix H. Limitation</head><p>As an exemplar-based approach, our method may not produce satisfactory results due to one-to-many and many-to-one mappings as shown in <ref type="figure">Figure 24</ref>. We leave further research tackling these issues as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Exemplar Synthesis <ref type="figure">Figure 24</ref>: Limitation. Our method may produce mixed color artifact due the one-to-many mapping (1st row). Besides, the multiple instances (pillows in the figure) may use the same style in the cases of many-to-one mapping (2nd row).</p><p>Another limitation is that the computation of the correlation matrix takes tremendous GPU memory, which makes our method hardly scale for high resolution images. We leave the solve of this issue in future work.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural best-buddies: Sparse cross-domain correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shapes and context: In-the-wild image synthesis &amp; manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2317" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pairedcyclegan: Asymmetric style transfer for applying and removing makeup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask-guided portrait editing with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3436" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Proposal flow: Semantic correspondences from object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1711" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scnet: Learning semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep exemplar-based colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in realtime with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fcss: Fully convolutional self-similarity for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6560" to="6569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2278" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04312</idno>
		<title level="m">Positional Normalization</title>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Visual attribute transfer through deep image analogy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01088</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised image-toimage translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1601" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Exemplar guided unsupervised image-to-image translation with semantic consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The contextual loss for image transformation with non-aligned data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Talmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="768" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-parametric image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8808" to="8816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Inspirational adversarial image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rapin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11661</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Xgan: Unsupervised imageto-image translation for many-to-many mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bertsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05139</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Daisy: An efficient dense descriptor applied to wide-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="815" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Example-guided style consistent image synthesis from semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01314</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Apdrawinggan: Generating artistic portrait drawings from face photos with hierarchical gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2849" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep exemplar-based video colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bermak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8052" to="8061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Selfattention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning dense correspondence via 3d-guided cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international</title>
		<meeting>the IEEE international</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Toward multimodal image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<title level="m">Conv2d / k3s1</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
