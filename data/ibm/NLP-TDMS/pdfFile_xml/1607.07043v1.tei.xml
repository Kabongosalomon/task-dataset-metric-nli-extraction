<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">University of Sydney</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
							<email>wanggang@ntu.edu.sgdong.xu@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D action recognition</term>
					<term>recurrent neural networks</term>
					<term>long short- term memory</term>
					<term>trust gate</term>
					<term>spatio-temporal analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D action recognition -analysis of human actions based on 3D skeleton data -becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains concurrently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long-term context information stored in the memory cell. Our method achieves state-of-the-art performance on 4 challenging benchmark datasets for 3D human action analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, action recognition based on the locations of major joints of the body in 3D space has attracted a lot of attention. Different feature extraction and classifier learning approaches are studied for 3D action recognition <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. For example, Yang and Tian <ref type="bibr" target="#b3">[4]</ref> represented the static postures and the dynamics of the motion patterns via eigenjoints and utilized a Naïve-Bayes-Nearest-Neighbor classifier learning. A HMM was applied by <ref type="bibr" target="#b4">[5]</ref> for modeling the temporal dynamics of the actions over a histogram-based representation of 3D joint locations. Evangelidis et al. <ref type="bibr" target="#b5">[6]</ref> learned a GMM over the Fisher kernel representation of a succinct skeletal feature, called skeletal quads. Vemulapalli et al. <ref type="bibr" target="#b6">[7]</ref> represented the skeleton configurations and actions as points and curves in a Lie group respectively, and utilized a SVM classifier to classify the actions. A skeleton-based dictionary learning utilizing group sparsity and geometry constraint was also Corresponding author. arXiv:1607.07043v1 [cs.CV] 24 Jul 2016 proposed by <ref type="bibr" target="#b7">[8]</ref>. An angular skeletal representation over the tree-structured set of joints was introduced in <ref type="bibr" target="#b8">[9]</ref>, which calculated the similarity of these features over temporal dimension to build the global representation of the action samples and fed them to SVM for final classification.</p><p>Recurrent neural networks (RNNs) which are a variant of neural nets for handling sequential data with variable length, have been successfully applied to language modeling <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, image captioning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, video analysis <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>, human re-identification <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, and RGB-based action recognition <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. They also have achieved promising performance in 3D action recognition <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>.</p><p>Existing RNN-based 3D action recognition methods mainly model the longterm contextual information in the temporal domain to represent motion-based dynamics. However, there is also strong dependency between joints in the spatial domain. And the spatial configuration of joints in video frames can be highly discriminative for 3D action recognition task.</p><p>In this paper, we propose a spatio-temporal long short-term memory (ST-LSTM) network which extends the traditional LSTM-based learning to two concurrent domains (temporal and spatial domains). Each joint receives contextual information from neighboring joints and also from previous frames to encode the spatio-temporal context. Human body joints are not naturally arranged in a chain, therefore feeding a simple chain of joints to a sequence learner cannot perform well. Instead, a tree-like graph can better represent the adjacency properties between the joints in the skeletal data. Hence, we also propose a tree structure based skeleton traversal method to explore the kinematic relationship between the joints for better spatial dependency modeling.</p><p>In addition, since the acquisition of depth sensors is not always accurate, we further improve the design of the ST-LSTM by adding a new gating function, so called "trust gate", to analyze the reliability of the input data at each spatiotemporal step and give better insight to the network about when to update, forget, or remember the contents of the internal memory cell as the representation of long-term context information.</p><p>The contributions of this paper are: (1) spatio-temporal design of LSTM networks for 3D action recognition, (2) a skeleton-based tree traversal technique to feed the structure of the skeleton data into a sequential LSTM, (3) improving the design of the ST-LSTM by adding the trust gate, and (4) achieving stateof-the-art performance on all the evaluated datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Human action recognition using 3D skeleton information is explored in different aspects during recent years <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>. In this section, we limit our review to more recent RNN-based and LSTM-based approaches.</p><p>HBRNN <ref type="bibr" target="#b29">[30]</ref> applied bidirectional RNNs in a novel hierarchical fashion. They divided the entire skeleton to five major groups of joints and each group was fed into a separated bidirectional RNN. The output of these RNNs were concatenated to represent upper-body and lower-body, then each was fed into another set of RNNs. The global body representation was obtained by concatenating the output of these two RNNs and it was fed to the next layer of RNN. The hidden representation of the final RNN was fed to a softmax classifier layer for action classification.</p><p>Zhu et al. <ref type="bibr" target="#b50">[51]</ref> added a mixed-norm regularization term to a deep LSTM network's cost function in order to push the network towards learning co-occurrence of discriminative joints for action classification. They further introduced an internal dropout <ref type="bibr" target="#b51">[52]</ref> technique within the LSTM unit, which was applied on all the gate activations.</p><p>Differential LSTM <ref type="bibr" target="#b30">[31]</ref> added a new gating inside LSTM to keep track of the derivatives of the memory states in order to discover patterns within salient motion patterns. All the input features for each frame were concatenated and fed to the differential LSTM.</p><p>Part-aware LSTM <ref type="bibr" target="#b31">[32]</ref> separated the memory cell to part-based sub-cells and pushed the network towards learning the long-term context representations individually for each part. The output of the network was learned over the concatenated part-based memory cells followed by the common output gate.</p><p>Unlike the above mentioned works, the framework proposed in this paper does not concatenate the joint-based input features, instead it explicitly models the dependencies between the joints and applies recurrent analysis over spatial and temporal domains concurrently. Besides, a novel trust gate is developed to make LSTM robust to noisy input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spatio-Temporal Recurrent Networks</head><p>Human actions can be characterized by the motion of body parts over time. In 3D human action recognition, we have three dimensional locations of the major body joints in each frame. Recently, recurrent neural networks have been successfully employed for skeleton-based 3D action recognition <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Long Short-Term Memory (LSTM) networks <ref type="bibr" target="#b52">[53]</ref> are very successful extensions of the recurrent neural networks (RNNs). They utilize the gating mechanism over an internal memory cell to learn and represent a better and more complex representation of the long-term dependencies among the input sequential data, thus they are suitable for feature learning over a sequence of temporal data.</p><p>In this section, first we will briefly review the standard LSTM networks, then describe the proposed spatio-temporal LSTM model and the skeleton-based tree traversal. Next we will introduce an effective gating scheme for LSTM to deal with the measurement noise in the input data (body joint locations) for the task of 3D human action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Temporal Modeling with LSTM</head><p>A typical LSTM unit contains an input gate i t , a forget gate f t , an output gate o t , and an output state h t , together with an internal memory cell state c t . The LSTM transition equations are formulated as:</p><formula xml:id="formula_0">    i t f t o t u t     =     σ σ σ tanh     M x t h t−1 (1) c t = i t u t + f t c t−1 (2) h t = o t tanh(c t )<label>(3)</label></formula><p>where indicates element-wise product, x t denotes the input to the network at time step t, and u t denotes the modulated input. σ is the sigmoid activation function. M : D+d → 4d is an affine transformation consisting of model parameters, where D is the dimensionality of input x t and d is the number of LSTM cell state units.</p><p>Intuitively, the input gate i t determines the extent to which the modulated input information (u t ) is supposed to update the memory cell at time t. The forget gate f t determines the effectiveness of the previous state of the memory cell (c t−1 ) on its current state (c t ). Finally, the output gate o t governs the amount of information output from the memory cell. Readers are referred to <ref type="bibr" target="#b53">[54]</ref> for more details about the mechanism of LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatio-Temporal LSTM</head><p>Very recent attempts on applying RNNs for 3D human action recognition <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b50">51]</ref> show outstanding performance and prove the strengths of RNNs in modeling the complex dynamics of the human actions in temporal space.</p><p>The main focus of these existing methods was on utilizing RNNs over temporal domain for discovering the discriminative dynamics and body motion patterns for 3D action recognition. However, there is also discriminative information in static postures encoded within the joints' 3D locations in each individual frame and the sequential nature of skeleton data makes it possible to adopt RNN-based learning in spatial domain as well. Unlike other existing methods, which concatenated the joints information, we extend the recurrent analysis towards spatial domain to discover the spatial dependency patterns between different joints at each frame.</p><p>In this fashion, we propose a spatio-temporal LSTM (ST-LSTM) model which simultaneously models the spatial dependencies of the joints and the temporal dependencies among the frames. As shown in <ref type="figure">Fig. 1</ref>, every ST-LSTM unit corresponds to one of the skeletal joints. Each of the units receives the hidden representation of the previous joint and also the hidden representation of its own joint from the previous frame. In this section we assume joints are arranged in a chain-like sequence with the order shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. In Section 3.3, we will show a more advanced method to take advantage of the adjacency information of the body joints as a tree structure.</p><p>We use j ∈ {1, ..., J} and t ∈ {1, ..., T } to denote the indices of joints and frames respectively. Each ST-LSTM unit is fed with its input (x j,t , location of  <ref type="figure">Fig. 1</ref>. The illustration of the proposed spatio-temporal LSTM network. In the spatial direction, body joints in a frame are fed in a sequence. In the temporal direction, the locations of the corresponding joints are fed over time. Each unit receives the hidden representation of previous joints and previous frames of the same joint as contextual information.</p><formula xml:id="formula_1">h j-1,t h j,t-1 h j,t h j,t</formula><p>the corresponding joint at current frame), its own hidden representation at the previous time step (h j,t−1 ), and the hidden representation of the previous joint at current frame (h j−1,t ). Each unit is also equipped with two different forget gates corresponding to the two incoming channels of context information: f S j,t for the spatial domain, and f T j,t for the temporal domain. The proposed ST-LSTM is formulated as:</p><formula xml:id="formula_2">      i j,t f S j,t f T j,t o j,t u j,t       =       σ σ σ σ tanh         M   x j,t h j−1,t h j,t−1     (4) c j,t = i j,t u j,t + f S j,t c j−1,t + f T j,t c j,t−1 (5) h j,t = o j,t tanh(c j,t )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tree-Structure based Traversal</head><p>Arranging joints in a simple chain ignores the kinematic dependency relations between the joints and adds false connections between body joints which are not strongly related. In human parsing, skeletal joints are popularly modeled as a tree-based pictorial structure <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>, as illustrated in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. In our ST-LSTM framework, it is also beneficial to model the spatial dependency of the joints based on their adjacency tree structure. For example, hidden representation of the neck joint (number 2 in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>) is expected to be more informative for the right hand joints <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9)</ref> than the joint number 6. However, trees cannot be directly fed into the ST-LSTM framework. To mitigate this issue, we propose a bidirectional tree traversal method to visit joints in a sequence which maintains the adjacency information of the skeletal tree structure. As illustrated in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>, at the first spatial step, the root node (central spine joint) is fed to the network, then the network follows a depth-first traversal in the spatial domain. When it reaches a leaf node, it goes back. In this fashion, each connection of the tree structure will be passed twice and the context information is fed along both directions. Upon the end of the traversal, it gets back to the root node.</p><p>This traversal strategy guarantees the transmission of the data in both directions (top-down and bottom-up) inside the adjacency tree structure. Therefore each node will have the contextual information from both its descendants and ancestors. Compared to the simple chain model described in section 3.2, this tree traversal technique can discover stronger long-term spatial dependency patterns based on the joints' adjacency structure.</p><p>In addition, the input to the ST-LSTM network at each step is limited to a single joint in a specific frame, which is much smaller in size compared to the concatenated input features of other existing methods. As a result, we have much fewer model parameters and this can be considered as a weight sharing regularization inside our learning framework, which leads to better generalization in the scenarios with limited training samples. This is an advantage in 3D action recognition, because most of the current datasets have a small number of training samples.</p><p>Similar to other LSTM implementations <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>, the representation capacity of our network can be improved by stacking multiple layers of the tree structured ST-LSTMs and constructing a deep yet completely tractable network, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Spatio-Temporal LSTM with Trust Gates</head><p>The inputs of the proposed tree-structured ST-LSTM are the 3D positions of skeletal joints collected by sensors like Microsoft Kinect, which are not always reliable due to noise and occlusion. This limits the performance of the network.</p><p>To address this issue, we propose to add a new gate to the LSTM unit which analyzes the reliability of the input at each spatio-temporal step, based on the estimation of the input from the available contextual information. Our novel gating method is inspired by the works in natural language processing <ref type="bibr" target="#b57">[58]</ref> which predict next word based on LSTM representation of previous words. This idea worked well because of the high dependency among the words in a sentence. Similarly, since the skeletal joints often move together and this articulated motion follows common yet complex patterns at each spatio-temporal step, the input data x j,t is supposed to be predictable from the contextual representations h j,t−1 and h j−1,t .</p><p>This predictability inspired us to add new mechanism to ST-LSTM to predict the input and compare it with the actual incoming input. The amount of the estimation error is used as input to a new "trust gate". The derived trust value provides information to the long-term memory mechanism to learn better decisions about when and how to remember and forget the contents of the memory cell. For example, when the trust gate finds out the current joint has wrong 3D measurements, it can block the input gate and prevent the memory cell from updating based on current unreliable input.</p><p>Mathematically, for an input at step (j, t), we develop a function to generate its prediction, based on the available contextual information:</p><formula xml:id="formula_3">p j,t = tanh M p h j−1,t h j,t−1<label>(7)</label></formula><p>where the affine transformation M p maps the data from 2d to d , so the dimensionality of p j,t is d. It is worth noting that the contextual information at each step is not limited to the hidden states of the previous spatial step but it also includes the previous temporal step, i.e., the long-term memory information of the same joint in previous frames and the contextual information of other visited <ref type="figure">Fig. 4</ref>. Schema of the proposed ST-LSTM with trust gate. joints in the same frame are seamlessly incorporated. Therefore, we can expect this function to be able to produce good predictions. The activation of the proposed trust gate τ is a vector in d , which is similar to the activation of the input gate and the forget gate, and it will be calculated as:</p><formula xml:id="formula_4">p j,t uj,t ij,t c j,t-1 o j,t c - j,t f j,t S f j,t T + cj-1,t c j,t c j,t h j-1,t h j,t-1 xj,t h j-1,t h j,t-1 xj,t h j-1,t h j,t-1 h j-1,t h j,t-1 xj,t h j-1,t h j,t-1 xj,t h j-1,t h j,t-1 xj,t h j,t x j,t xj,t '</formula><formula xml:id="formula_5">x j,t = tanh M x x j,t (8) τ j,t = G(x j,t − p j,t )<label>(9)</label></formula><p>where M x : D → d is an affine transformation, and the new activation function G(·) is an element-wise operation formulated as:</p><formula xml:id="formula_6">G(z) = exp(−λz 2 )<label>(10)</label></formula><p>In this equation, λ &gt; 0 is a parameter to control the spread of the Gaussian function. G(z) produces a large response if z is close to origin, and small response when z has a large absolute value. Utilizing the proposed trust gate, the cell state of the ST-LSTM neuron can be updated as:</p><formula xml:id="formula_7">c j,t = τ j,t i j,t u j,t + (1 − τ j,t ) f S j,t c j−1,t + (1 − τ j,t ) f T j,t c j,t−1 (11)</formula><p>If the new input x j,t cannot be trusted (because of noise or occlusion), then we need to take advantage of more history information and try to block the new input. In contrast, if the input is reliable, we can let the learning algorithm update the memory cell by importing input information. <ref type="figure">Fig. 4</ref> depicts the scheme of the new ST-LSTM unit empowered with the trust gate. This can be learned similar to other gates by back-propagation. The proposed trust gate technique is theoretically general and can be applied to other applications to deal with unreliable input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Learning the Classifier</head><p>Since the action labels are always given at the video level, we feed them as the training outputs of the ST-LSTM at each spatio-temporal step. The network learns to predict the action classŷ among a discrete set of classes Y using a softmax layer. The overall prediction of a video is computed by averaging the predictions of all the steps. Empirically, this method provides better performance compared to the minimization of the loss at the last step only.</p><p>The objective function of our model is formulated as:</p><formula xml:id="formula_8">L = J j=1 T t=1 l(ŷ j,t , y)<label>(12)</label></formula><p>where l(ŷ j,t , y) is the negative log-likelihood loss <ref type="bibr" target="#b53">[54]</ref> measuring the difference between the true label y and the predicted resultŷ j,t at step (j, t). The objective function can be minimized using back-propagation through time (BPTT) algorithm <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The proposed model is evaluated on four datasets: NTU RGB+D dataset, SBU Interaction dataset, UT-Kinect dataset, and Berkeley MHAD dataset. We conduct extensive experiments with different configurations as follows:</p><p>(1) "ST-LSTM (Joint Chain)": In this configuration, the joints are visited one by one in a simple chain order (see <ref type="figure" target="#fig_1">Fig. 2(a)</ref>).</p><p>(2) "ST-LSTM (Tree Traversal)": The proposed tree traversal strategy <ref type="figure" target="#fig_1">(Fig.  2(c)</ref>) is adopted in this configuration to fully exploit the tree-based spatial structure of human joints.</p><p>(3) "ST-LSTM (Tree Traversal) + Trust Gate": This configuration involves the trust gate to deal with noisy input. <ref type="bibr" target="#b31">[32]</ref>. To the best of our knowledge, this dataset is currently the largest depth-based action recognition dataset. It is collected by Kincet v2 and contains more than 56 thousand sequences and 4 million frames. A total of 60 different action classes including daily actions, pair actions, and medical conditions are performed by 40 subjects aged between 10 and 35. The 3D coordinates of 25 joints are provided in this dataset. The large intra-class and view point variations make this dataset very challenging. Due to the large amount of samples, this dataset is highly suitable for deep learning based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation datasets NTU RGB+D Dataset</head><p>SBU Interaction Dataset <ref type="bibr" target="#b58">[59]</ref>. This dataset is captured with Kinect and contains 8 classes of two-person interactions. It includes 282 skeleton sequences in 6822 frames. Each skeleton has 15 joints. The challenges of this dataset include: <ref type="bibr" target="#b0">(1)</ref> in most interactions, one person is acting and the other one is reacting; and (2) the joint coordinates in many sequences are of low accuracy.</p><p>UT-Kinect Dataset <ref type="bibr" target="#b4">[5]</ref>. This dataset contains 10 action classes performed by 10 subjects, captured with a stationary Kinect. Each action was performed twice by every subject. The locations of 20 joints are provided in this dataset. The high intra-class variation and viewpoint diversity makes it challenging.</p><p>Berkeley MHAD <ref type="bibr" target="#b59">[60]</ref>. The MHAD dataset is captured by a motion capture system. It consists of 659 sequences and about 82 minutes of recording. Eleven different action classes were performed by 7 male and 5 female subjects. The 3D locations of 35 joints are provided in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>In our experiments, each video sequence is divided to T sub-sequences with the same length, and one frame was randomly selected from each sub-sequence. Such a method adds randomness into the process of data generation and improves the generalization capability. We observe this strategy achieves better performance in contrast to uniformly sampled frames. We cross-validated the performance based on leave-one-subject-out protocol on NTU RGB+D dataset, and found T = 20 as the optimum value.</p><p>We use Torch toolbox as the deep learning platform and an NVIDIA Tesla K40 GPU to run our experiments. We train the network using stochastic gradient descent, and set learning rate, momentum and decay rate as 2×10 −3 , 0.9 and 0.95, respectively. For our network, we set the neuron size d to 128, and the parameter λ used in G(·) to 0.5. We use two ST-LSTM layers in the stacked network, and the applied probability of dropout is 0.5. Though there are variations in terms of sequence length, joint number, and data acquisition equipment for different datasets, we use the same parameter settings mentioned above. This indicates the insensitiveness of our method to the parameter settings, as it achieves promising results on all the datasets with the same configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental results</head><p>NTU RGB+D Dataset. This dataset has two standard evaluation protocols <ref type="bibr" target="#b31">[32]</ref>. One is cross-subject evaluation, for which half of the subjects are used for training and the remaining are for testing. The second is cross-view evaluation, for which two viewpoints are used for training and one is left out for testing.  The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. Deep RNN and deep LSTM models concatenate the joints features at each frame and then feed them to the network to model the temporal dynamics and ignore the spatial dynamics. As can be seen, both "ST-LSTM (Joint Chain)" and "ST-LSTM (Tree Traversal)" models outperform these methods by a notable margin.</p><p>It can also be observed that the trust gate brings significant performance improvement, because the data acquired by Kinect is noisy and some joints are frequently occluded in this dataset.</p><p>A notable portion of samples of this dataset are captured from side view, and based on the design of Kinect's body tracking mechanism, side view skeletal data is less accurate than the front view. To further show the effectiveness of trust gate, we analyze the performance using only the samples in side views. When using "ST-LSTM (Tree Traversal)", the accuracy is 76.5%, while "ST-LSTM (Tree Traversal) + Trust Gate" achieves 81.6%. This indicates the proposed trust gate can effectively handle severely noisy data.</p><p>To verify the effectiveness of layer stacking, we decrease the network size by using only one ST-LSTM layer, and the accuracies drop to 65.5% (crosssubject) and 77.0% (cross-view). It indicates our two-layer stacked model has better representation strengths than a single-layer model.</p><p>The sensitivity of the proposed model to neural unit sizes and λ values are also evaluated and the results are depicted in <ref type="figure" target="#fig_4">Fig. 6</ref>. When trust gate is used, our model achieves better performance for all the λ values tested compared to the model without trust gate.   <ref type="bibr" target="#b58">[59]</ref> 80.3% Ji et al., <ref type="bibr" target="#b62">[63]</ref> 86.9% CHARM <ref type="bibr" target="#b63">[64]</ref> 83.9% HBRNN <ref type="bibr" target="#b29">[30]</ref> (reported by <ref type="bibr" target="#b50">[51]</ref>) 80.4% Co-occurrence LSTM <ref type="bibr" target="#b50">[51]</ref> 90.4% Deep LSTM (reported by <ref type="bibr" target="#b50">[51]</ref>) 86.0% ST-LSTM (Joint Chain) 84.7% ST-LSTM (Tree) 88.6% ST-LSTM (Tree) + Trust Gate 93.3% <ref type="table">Table 3</ref>. Results on UT-Kinect Dataset (leave-one-out-cross-validation protocol <ref type="bibr" target="#b4">[5]</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy Histogram of 3D Joints <ref type="bibr" target="#b4">[5]</ref> 90.9% Grassmann Manifold <ref type="bibr" target="#b64">[65]</ref> 88.5% Riemannian Manifold <ref type="bibr" target="#b65">[66]</ref> 91.5% ST-LSTM (Joint Chain) 91.0% ST-LSTM (Tree) 92.4% ST-LSTM (Tree) + Trust Gate 97.0%</p><p>Finally, we evaluate the classification performance on early stopping conditions by feeding the first p (0 &lt; p &lt; 1) portion of the testing video to the trained network on cross-subject protocol. When setting p as 0.1, 0.2, ..., 1.0, the corresponding accuracies are 13.4%, 21.6%, 33.9%, 46.6%, 55.5%, 61.1%, 64.6%, 66.7%, 68.2%, 69.2%, respectively. We can find that the results improve when a larger portion of video is fed.</p><p>SBU Interaction Dataset. We follow the standard experimental protocol of <ref type="bibr" target="#b58">[59]</ref> and perform 5-fold cross validation on SBU Interaction Dataset. In this dataset, two human skeletons are provided in each frame, so our traversal visits the joints throughout the two skeletons over the spatial steps. We summarize the results in terms of average classification accuracy in <ref type="table" target="#tab_1">Table 2</ref>. In the table, <ref type="bibr" target="#b50">[51]</ref> and <ref type="bibr" target="#b29">[30]</ref> are both LSTM-based methods, which are more relevant to our model.</p><p>As can be seen, the proposed "ST-LSTM (Tree Traversal) + Trust Gate" model outperforms all other skeleton-based methods. "ST-LSTM (Tree Traversal)" yields higher accuracy than "ST-LSTM (Joint Chain)", as the latter adds some unreasonable links between the less related joints.</p><p>It is worth noting that deep LSTM <ref type="bibr" target="#b50">[51]</ref>, Co-occurrence LSTM <ref type="bibr" target="#b50">[51]</ref>, and HBRNN <ref type="bibr" target="#b29">[30]</ref> all use the Svaitzky-Golay filter in temporal domain to smooth the skeleton joint positions to reduce the influence of the noise in the data captured by Kinect. However, even without trust gate (which aims at handling noisy input), the "ST-LSTM (Tree Traversal)" model outperforms HBRNN and deep LSTM, and achieves comparable result (88.6%) to Co-occurrence LSTM. Once the trust gate is utilized, the accuracy jumps to 93.3%. We do not adopt any skeleton normalization operation, such as translation or rotation of the skeleton <ref type="bibr" target="#b6">[7]</ref>, and achieve state-of-the-art performance. We notice that <ref type="bibr" target="#b61">[62]</ref> obtained very similar result (93.4%) on SBU dataset. However, their method utilized both RGB and depth images, while our method just uses the skeleton data.</p><p>UT-Kinect Dataset. There are two popular protocols on UT-Kinect dataset. First is the leave-one-out-cross-validation protocol <ref type="bibr" target="#b4">[5]</ref>. Second is proposed in <ref type="bibr" target="#b66">[67]</ref>, for which half of the subjects are used for training and the remaining are used for testing. We use both protocols to evaluate the proposed method more extensively.</p><p>On the first protocol, our model achieves superior performance over other skeleton-based methods by a large margin, as shown in <ref type="table">Table 3</ref>. On the second evaluation protocol <ref type="table" target="#tab_2">(Table 4)</ref>, our model achieves competitive result (95.0%) to Elastic functional coding <ref type="bibr" target="#b67">[68]</ref> (94.9%), which is an extension of the Lie Group model <ref type="bibr" target="#b6">[7]</ref>.</p><p>Berkeley MHAD. We follow the protocol in <ref type="bibr" target="#b29">[30]</ref> on MHAD dataset, in which 384 sequences corresponding to the first 7 subjects are used for training and the 275 sequences of the remaining 5 subjects are used for testing. The results are shown in <ref type="table">Table 5</ref>. Our method achieves the accuracy of 100% without preliminary smoothing operations, which are adopted in <ref type="bibr" target="#b29">[30]</ref>.</p><p>Besides, we have tested our model on MSR Action3D dataset <ref type="bibr" target="#b68">[69]</ref> following the protocol in <ref type="bibr" target="#b29">[30]</ref>, and achieved an accuracy of 94.8%, which is slightly superior to 94.5% achieved by HBRNN <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effectiveness of Trust Gate</head><p>To better study the effectiveness of the trust gate in the proposed network model, we specifically evaluate noisy samples from MSR Action3D dataset. We manually rectify some noisy joints of these samples by referring to the corresponding depth maps, and compared the activations of the trust gates on noisy and rectified inputs. As shown in <ref type="figure">Fig. 7(a)</ref>, the activation of the trust gate is smaller when a noisy joint is fed, compared to the corresponding rectified joint. This shows how the network reduces the impact of the noisy input data.</p><p>To comprehensively evaluate the trust gate, we also manually add noise to one joint for all testing samples on MHAD dataset. Note that MHAD dataset was captured with motion capture system, thus the skeleton joints are much more accurate than those collected by Kinect. We add noise to the right foot joint by moving the joint away from the original position. The direction of the translation vector is randomly chosen and the norm is also a random value around 30cm (this is a significant noise in the scale of human bodies). For each video, we add noise to the same joint at the same time step, and then analyze the effect in average.  <ref type="figure">Fig. 7</ref>. Behavior of trust gate when inputting noisy data. (a) j 3 is a noisy joint location, and j3 is the corresponding rectified joint position. In the histogram, the blue bar is the magnitude of the trust gate when inputting the noisy joint j 3 . The red bar is the magnitude of the corresponding trust gate when j 3 is rectified to j3. (b) The difference between the trust gate calculated when inputting the original data and that calculated when the noise is imposed at the jN -th spatial step and tN -th time step. Method Accuracy Skeleton Joint Features <ref type="bibr" target="#b66">[67]</ref> 87.9% Lie Group <ref type="bibr" target="#b6">[7]</ref> (reported by <ref type="bibr" target="#b67">[68]</ref>) 93.6% Elastic functional coding <ref type="bibr" target="#b67">[68]</ref> 94.9% ST-LSTM (Tree) + Trust Gate 95.0% <ref type="table">Table 5</ref>. Experimental results on MHAD Dataset Method Accuracy Vantigodi et al. <ref type="bibr" target="#b69">[70]</ref> 96.1% Ofli et al. <ref type="bibr" target="#b70">[71]</ref> 95.4% Vantigodi et al. <ref type="bibr" target="#b71">[72]</ref> 97.6% Kapsouras et al. <ref type="bibr" target="#b72">[73]</ref> 98.2% HBRNN <ref type="bibr" target="#b29">[30]</ref> 100% ST-LSTM (Tree) + Trust Gate 100%</p><p>We measure the difference in the magnitude of the trust gate activations between the original data and the noisy ones. For all the testing samples, we perform the same procedure, then calculate the average difference. The result is depicted in <ref type="figure">Fig. 7(b)</ref>. We can see when the noisy data is fed to the network, the magnitude of the trust gate is reduced. This shows how the network ignores the noisy input, and tries to prevent it from affecting the network. In this experiment, we observe the overall accuracy does not drop after adding the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we propose to extend the RNN-based 3D action recognition to spatio-temporal domain. A new ST-LSTM network is introduced which analyses the 3D location of each individual joint in each video frame, at each processing step. For better representation of the structured input to the network, a skeleton tree traversal algorithm is proposed which takes the adjacency graph of body joints into account and improves the performance of the network by arranging the most related joints together in the input sequence. Due to the unreliability of the 3D input data, a new gating mechanism is also proposed to improve the robustness of the network against noise and occlusion. The provided experimental results validate the proposed contributions and prove the effectiveness of our method by achieving superior performance over the existing state-of-the-art methods on four evaluated datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Skeletal joints of a human body. In the simple joint chain model, the joint visiting order is 1-2-3-...-16. (b) Skeleton is transformed to a tree structure. (c) Tree traversal over the spatial steps. The tree can be unfolded to a chain with the traversal, and the joint visiting order is 1-2-3-2-4-5-6-5-4-2-7-8-9-8-7-2-1-10-11-12-13-12-11-10-14-15-16-15-14-10-1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>A graphical model of the deep tree-structured ST-LSTM network. For clarity, some arrows are omitted in the stacked network (better viewed in color). In this figure, the output of the first ST-LSTM layer is fed to the second ST-LSTM layer as its input. The second ST-LSTM layer's output is fed to softmax layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Example images with noisy skeletons from NTU RGB+D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>(a) Comparison of the performance for different neuron size (d) values on NTU RGB+D dataset (cross-subject). (b) Comparison of different λ values on NTU RGB+D dataset (cross-subject). The blue line indicates the results when different λ values are used for trust gate, and the red dashed line indicates the performance when trust gate is not added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Experimental results (accuracies) on NTU RGB+D Dataset</figDesc><table><row><cell>Method</cell><cell cols="2">Cross subject Cross view</cell></row><row><cell>Lie Group [7]</cell><cell>50.1%</cell><cell>52.8%</cell></row><row><cell>Skeletal Quads [6]</cell><cell>38.6%</cell><cell>41.4%</cell></row><row><cell>Dynamic Skeletons [61]</cell><cell>60.2%</cell><cell>65.2%</cell></row><row><cell>HBRNN [30]</cell><cell>59.1%</cell><cell>64.0%</cell></row><row><cell>Part-aware LSTM [32]</cell><cell>62.9%</cell><cell>70.3%</cell></row><row><cell>Deep RNN [32]</cell><cell>56.3%</cell><cell>64.1%</cell></row><row><cell>Deep LSTM [32]</cell><cell>60.7%</cell><cell>67.3%</cell></row><row><cell>ST-LSTM (Joint Chain)</cell><cell>61.7%</cell><cell>75.5%</cell></row><row><cell>ST-LSTM (Tree Traversal)</cell><cell>65.2%</cell><cell>76.1%</cell></row><row><cell>ST-LSTM (Tree Traversal) + Trust Gate</cell><cell>69.2%</cell><cell>77.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Experimental results on SBU Interaction Dataset</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Yun et al.,</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Experimental results on UT-Kinect Dataset (half-vs-half protocol [67])</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. The research is supported by Singapore Ministry of Education (MOE) Tier 2 ARC28/14, and Singapore A*STAR Science and Engineering Research Council PSF1321202099. This research was carried out at the Rapid-Rich Object Search (ROSE) Lab at Nanyang Technological University. The ROSE Lab is supported by the National Research Foundation, Singapore, under its Interactive Digital Media (IDM) Strategic Research Programme. We also would like to thank NVIDIA for the GPU donation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">3d skeleton-based human action classification: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Presti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>La Cascia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Space-time representation of people based on 3d skeletal data: a review. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From handcrafted to learned representations for human action recognition: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective 3d action recognition using eigenjoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCIR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<editor>ICPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Group sparsity and geometry constrained dictionary learning for action recognition from depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Joint angles similarities and hog 2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<editor>INTERSPEECH.</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>IN-TERSPEECH.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICML.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A multi-stream bidirectional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A hierarchical deep temporal model for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Progressively parsing interactional objects for fine grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Online human action detection using joint classification-regression recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Action recognition by learning deep multi-granular spatio-temporal video representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<editor>ICMR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Human-object interaction recognition by learning the distances between the object and the skeleton joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boonaert</surname></persName>
		</author>
		<editor>FG.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multimodal multipart learning for action recognition in depth videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning maximum margin temporal warping for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Real time action recognition using histograms of depth gradients and random decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-modal feature fusion for action recognition in rgb-d sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCCSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mining 3d key-pose-motifs for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning a non-linear knowledge transfer model for crossview action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A hierarchical pose-based approach to complex action understanding using dictionaries of actionlets and motion poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Real-time rgb-d activity prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fusion of depth, skeleton, and inertial data for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d-based deep convolutional neural network for action recognition with depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Effective active skeleton representation for low latency human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spatiotemporal representation of 3d skeleton jointsbased action recognition using modified spherical harmonics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Alwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Chahir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR Letters</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Moving poselets: A discriminative and interpretable skeletal motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep multimodal feature analysis for action recognition in rgb+d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Representation learning of temporal dynamics for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<title level="m">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Automatic reconstruction of 3d human motion pose from uncalibrated monocular video sequences based on markerless human motion tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">M</forename><surname>Providence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-ofparts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Two-person interaction detection using body-pose features and multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Honorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Berkeley mhad: A comprehensive multimodal human action database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">A deep structured model with radius-margin bound for 3d human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Interactive body part contrast mining for human interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICMEW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Category-blind human action recognition: a practical recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choo Chuah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Accurate 3d action recognition using learning on the grassmann manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">3-d human action recognition by shape analysis of motion trajectories on riemannian manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Fusing spatiotemporal features and joints for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Elastic functional coding of human actions: from vector-fields to latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anirudh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Action recognition based on a bag of 3d points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Real-time human action recognition from motion capture data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vantigodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<editor>NCVPRIPG.</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Sequence of the most informative joints (smij): A new representation for human skeletal action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JVCIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Action recognition from motion capture data using meta-cognitive rbf network classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vantigodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISSNIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Action recognition on motion capture data using a dynemes and forward differences representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kapsouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nikolaidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JVCIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
