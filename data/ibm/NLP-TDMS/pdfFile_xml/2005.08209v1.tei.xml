<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Prajwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIT</orgName>
								<orgName type="institution" key="instit2">Hyderabad Vinay P. Namboodiri IIT</orgName>
								<orgName type="institution" key="instit3">Kanpur C V Jawahar IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Hyderabad</roleName><forename type="first">Rudrabha</forename><surname>Iiit</surname></persName>
							<email>radrabha.m@research.iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIT</orgName>
								<orgName type="institution" key="instit2">Hyderabad Vinay P. Namboodiri IIT</orgName>
								<orgName type="institution" key="instit3">Kanpur C V Jawahar IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mukhopadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIT</orgName>
								<orgName type="institution" key="instit2">Hyderabad Vinay P. Namboodiri IIT</orgName>
								<orgName type="institution" key="instit3">Kanpur C V Jawahar IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Humans involuntarily tend to infer parts of the conversation from lip movements when the speech is absent or corrupted by external noise. In this work, we explore the task of lip to speech synthesis, i.e., learning to generate natural speech given only the lip movements of a speaker. Acknowledging the importance of contextual and speakerspecific cues for accurate lip-reading, we take a different path from existing works. We focus on learning accurate lip sequences to speech mappings for individual speakers in unconstrained, large vocabulary settings. To this end, we collect and release a large-scale benchmark dataset, the first of its kind, specifically to train and evaluate the singlespeaker lip to speech task in natural settings. We propose a novel approach with key design choices to achieve accurate, natural lip to speech synthesis in such unconstrained scenarios for the first time. Extensive evaluation using quantitative, qualitative metrics and human evaluation shows that our method is four times more intelligible than previous works in this space. Please check out our demo video for a quick overview of the paper, method, and qualitative results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Babies actively observe the lip movements of people when they start learning to speak <ref type="bibr" target="#b24">[24]</ref>. As adults, we pay high attention to lip movements to "visually hear" the speech in highly noisy environments. Facial actions, specifically the lip movements, thus reveal a useful amount of speech information. This fact is also exploited by individuals hard of hearing, who often learn to lip read their close acquaintances over time <ref type="bibr" target="#b14">[15]</ref> to engage in more fluid conversations. Naturally, the question arises as to whether a model can learn to voice the lip movements of a speaker by "observing" him/her speak for an extended period. Learning such a model would only require videos of people talk- * Both authors have contributed equally to this work. <ref type="figure">Figure 1</ref>. We propose "Lip2Wav": a sequence-to-sequence architecture for accurate speech generation from silent lip videos in unconstrained settings for the first time. The text in the bubble is manually transcribed and is shown for presentation purposes.</p><p>ing with no further manual annotation. It also has a variety of practical applications such as (i) video conferencing in silent environments, (ii) high-quality speech recovery from background noise <ref type="bibr" target="#b0">[1]</ref>, (iii) long-range listening for surveillance and (iv) generating a voice for people who cannot produce voiced sounds (aphonia). Another interesting application would be "voice inpainting" <ref type="bibr" target="#b41">[41]</ref>, where the speech generated from lip movements can be used in place of a corrupted speech segment.</p><p>Inferring the speech solely from the lip movements, however, is a notoriously difficult task. A major challenge <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref> is the presence of homophenes: multiple sounds (phonemes) that are auditorily distinct being perceptually very similar with almost identical lip shapes (viseme). For instance, the lip shape when uttering the phoneme /p/ (park) can be easily confused with /b/ (bark), and /m/ (mark). As a matter of fact, only 25% to 30% of the English language is discernible through lip-reading alone <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>. This implies that professional lip readers do not only lip-read but also piece together multiple streams of information: the familiarity with their subjects, the topic being spoken about, the facial expressions and head gestures of the subject and also their linguistic knowledge. In contrast to this fact, contemporary works in lip to speech and lip to text take a drastically different approach.</p><p>Recent attempts in lip to text <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref> learn from unconstrained, large vocabulary datasets with thousands of speakers. However, these datasets only contain about 2 minutes of data per speaker which is insufficient for models to learn concrete speaker-specific contextual cues essential for lipreading. The efforts in lip to speech also suffer from a similar issue, but for a different reason. These works are constrained to small datasets <ref type="bibr" target="#b6">[7]</ref> with narrow vocabulary speech in artificially constrained environments.</p><p>In this work, we explore the problem of lip to speech synthesis from a unique perspective. We take inspiration from the fact that deaf individuals or professional lip readers find it easier to lip read people who they frequently interact with. Thus, rather than attempting lip to speech on random speakers in the wild, we focus on learning speech patterns of a specific speaker by simply observing the person's speech for an extended period. We explore the following question from a data-driven learning perspective: "How accurately can we infer an individuals speech style and content from his/her lip movements?".</p><p>To this end, we collect and publicly release a 120-hour video dataset of 5 speakers uttering natural speech in unconstrained settings. Our Lip2Wav dataset contains 800× more data per speaker than the current multi-speaker datasets <ref type="bibr" target="#b1">[2]</ref> to facilitate accurate modeling of speaker-specific audiovisual cues. The natural speech is spread across a diverse vocabulary 1 that is about 100× larger than the current single speaker lip to speech datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>. To the best of our knowledge, our dataset is the only publicly available large-scale benchmark to evaluate single-speaker lip to speech synthesis in unconstrained settings. With the help of this dataset, we develop Lip2Wav, a sequence-to-sequence model to generate accurate, natural speech that matches the lip movements of a given speaker. We support our results through extensive quantitative and qualitative evaluations and ablation studies. Our key contributions are as follows:</p><p>• We investigate the problem of silent lip videos to speech generation in large vocabulary, unconstrained settings for the first time.</p><p>• We release a novel 120-hour person-specific Lip2Wav dataset specifically for learning accurate lip to speech models of individual speakers. Each speaker contains 80× more data with 100× larger vocabulary than its counterparts. The speech is uttered in natural settings with no restriction to head pose or sentence lengths.</p><p>• Our sequence-to-sequence modeling approach produces speech that is almost 4× more intelligible in unconstrained environments compared to the previous works. Human evaluation studies also show that our generated speech is more natural with rich prosody. <ref type="bibr" target="#b0">1</ref> only words with frequency &gt; 4 are considered We release the data 2 , code 3 , trained models publicly for future research along with a demonstration video here <ref type="bibr" target="#b3">4</ref> . The rest of the paper is organized as follows: In Section 2, we survey the recent developments in this space. Following this, we describe our novel Lip2Wav dataset in Section 3. Our approach and training details are explained in Sections 4 and 5. We evaluate and compare our model with previous works in Section 6. We perform various ablation studies in Section 7 and conclude our work in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Lip to Speech Generation</head><p>While initial approaches <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b23">23]</ref> to this problem extracted the visual features from sensors or active appearance models, the recent works employ end-to-end approaches. Vid2Speech <ref type="bibr" target="#b9">[10]</ref> and Lipper <ref type="bibr" target="#b22">[22]</ref> generate low-dimensional LPC (Linear Predictive Coding) features given a short sequence of K frames (K &lt; 15). The facial frames are concatenated channel-wise and a 2D-CNN is used to generate the LPC features. We show that this architecture is very inadequate to model real-world talking face videos that contain significant head motion, silences and large vocabularies. Further, the low dimensional LPC features used in these works do not contain a significant amount of speech information leading to robotic, artificial sounding speech.</p><p>A follow-up work <ref type="bibr" target="#b8">[9]</ref> of Vid2Speech does away with LPC features and uses high-dimensional melspectrograms along with optical flows to force the network to explicitly condition on lip motion. While this can be effective for the GRID corpus that has no head movements, optical flow could be a detrimental feature in unconstrained settings due to large head pose changes. Another work <ref type="bibr" target="#b36">[36]</ref> strives for improved speech quality by generating raw waveforms using GANs. However, both these works do not make use of the well-studied sequence-to-sequence paradigm <ref type="bibr" target="#b31">[31]</ref> that is used for text-to-speech generation <ref type="bibr" target="#b30">[30]</ref>; thus leaving a large room for improvement in speech quality and correctness.</p><p>Finally, all the above works show results primarily on the GRID corpus <ref type="bibr" target="#b6">[7]</ref> which has a very narrow vocabulary of 56 tokens and very minimal head motion. We are the first to study this problem in a large vocabulary setting with thousands of words and sentences. Our datasets are collected from YouTube video clips and hence contain a significant amount of natural speech variations and head movements. This makes our results directly relevant to several real-world applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Lip to Text Generation</head><p>In this space as well, several works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38]</ref> are limited to narrow vocabularies and small datasets, however, unlike lip to speech, there have been multiple works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref> that specifically tackle open vocabulary lip to text in-thewild. They employ transformer sequence-to-sequence <ref type="bibr" target="#b35">[35]</ref> models to generate sentences given a silent lip movement sequence. These works also highlight multiple issues in the space of lip-reading, particularly the inherent ambiguity and hence the importance of using a language model. Our task at hand is arguably harder, as we not only have to infer the linguistic content, but also generate with rich prosody in the voice of the target speaker. Thus, we focus on extensively analyzing the problem in a single-speaker unconstrained setting, and learning precise individual speaking styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Text to Speech Generation</head><p>Over the recent years, neural text-to-speech models <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b30">30]</ref> have paved the way to generate high-quality natural speech conditioned on any given text. Using sequenceto-sequence learning <ref type="bibr" target="#b31">[31]</ref> with attention mechanisms, they generate melspectrograms in an auto-regressive manner. In our work, we propose Lip2Wav, a modified version of Tacotron 2 <ref type="bibr" target="#b30">[30]</ref> that conditions on face sequences instead of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Speaker-specific Lip2Wav Dataset</head><p>The current datasets for lip to speech (or) text are at the two opposite ends of the spectrum: (i) small, constrained narrow vocabulary like GRID <ref type="bibr" target="#b6">[7]</ref>, TCD-TIMIT <ref type="bibr" target="#b12">[13]</ref> or (ii) unconstrained, open vocabulary multi-speaker like LRS2 <ref type="bibr" target="#b1">[2]</ref>, LRW <ref type="bibr" target="#b5">[6]</ref> and LRS3 <ref type="bibr" target="#b2">[3]</ref>. The latter class of datasets contains only about 2 -5 minutes of data per speaker, making it significantly harder for models to learn speaker-specific visual cues that are essential for inferring accurate speech from lip movements. Further, the results would also be directly affected by the existing challenges of multi-speaker speech synthesis <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">19]</ref>. In the other extreme, the single-speaker lip to speech datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>, do not emulate the natural settings as they are constrained to narrow vocabularies and artificial environments. Thus, both of these extreme cases do not test the limits of unconstrained single-speaker lip to speech synthesis.</p><p>We introduce a new benchmark dataset for unconstrained lip to speech synthesis that is tailored towards exploring the following line of thought: How accurately can we infer an individuals speech style and content from his/her lip movements? To create the Lip2Wav dataset, we collect a total of about 120 hours of talking face videos across 5 speakers. The speakers are from various online lecture series and chess analysis videos. We choose English as the sole language of the dataset. With about 20 hours of natural speech per speaker and vocabulary sizes over 5000 words 5 for each of them, our dataset is significantly more unconstrained and realistic than GRID <ref type="bibr" target="#b6">[7]</ref> or TIMIT <ref type="bibr" target="#b12">[13]</ref> datasets. It is thus ideal for learning and evaluating accurate person-specific models for the lip to speech task. <ref type="table" target="#tab_0">Table 1</ref> compares the features of our Lip2Wav dataset with other standard singlespeaker lip-reading datasets. Note that a word is included in the vocabulary calculation for <ref type="table" target="#tab_0">Table 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem Formulation</head><p>Prior works in lip to speech regard their speech representation as a 2D-image <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">36]</ref> in the case of melspectrograms or as a single feature vector <ref type="bibr" target="#b9">[10]</ref> in the case of LPC features. In both these cases, they use a 2D-CNN to decode these speech representations. By doing so, they violate the ordering in which they model the sequential speech data, i.e. the future time steps influence the prediction of the current time step. In contrast, we formulate this problem in the standard sequence-to-sequence learning paradigm <ref type="bibr" target="#b31">[31]</ref>. Concretely, each output speech time-step S k is modelled as a conditional distribution of the previous speech time-steps S &lt;k and the input face image sequence I = (I 1 , I 2 , . . . , I T ). The probability distribution of each output speech time-step is given by:</p><formula xml:id="formula_0">P (S|I) = Π k (S k |S &lt;k , I)<label>(1)</label></formula><p>Lip2Wav, as shown in <ref type="figure">Figure 3</ref> consists of two modules: (i) Spatio-temporal face encoder (ii) Attention-based speech decoder. The modules are trained jointly in an end-to-end fashion. The sequence-to-sequence approach enables the model to learn an implicit speech-level language model that helps it to disambiguate homophenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Speech Representation</head><p>There are multiple output representations from which we can recover intelligible speech, but each of them have their trade-offs. The LPC features are low-dimensional and easier to generate, however, they result in robotic, artificial sounding speech. At the other extreme <ref type="bibr" target="#b36">[36]</ref>, one can generate raw waveforms but the high dimensionality of the output (16000 samples per second) makes the network training process computationally inefficient. We take inspiration from previous text-to-speech works <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b30">30]</ref> and generate melspectrograms conditioned on lip movements. We sample the raw audio at 16kHz. The window-size, hop-size and mel dimension are 800, 200, and 80 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Spatio-temporal Face Encoder</head><p>Our visual input is a short video sequence of face images. The model must learn to extract and process the finegrained sequence of lip movements. 3D convolutional neural networks have been shown to be effective <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36]</ref> in multiple tasks involving spatio-temporal video data. In this work, we try to encode the spatio-temporal information of the lip movements using a stack of 3D convolutions <ref type="figure">(Figure 3)</ref>. The input to our network is a sequence of facial images of the dimension T × H × W × 3, where T is the number of time-steps (frames) in the input video sequence, H, W correspond to the spatial dimensions of the face image. We gradually down-sample the spatial extent of the feature maps and preserve the temporal dimension T . We <ref type="figure">Figure 3</ref>. Lip2Wav model for lip to speech synthesis. The spatiotemporal encoder is a stack of 3D convolutions to extract the sequence of lip movements. This is followed by a decoder adapted from <ref type="bibr" target="#b30">[30]</ref> for high-quality speech generation. The decoder is conditioned on the face image features from the encoder and generates the melspectrogram in an auto-regressive fashion. also employ residual skip connections <ref type="bibr" target="#b13">[14]</ref> and batch normalization <ref type="bibr" target="#b15">[16]</ref> throughout the network. The encoder outputs a single D-dimensional vector for each of the T input facial images to get a set of spatio-temporal features T × D to be passed to the speech decoder. Each time-step of the embedding generated from the encoder also contains information about the future lip movements and hence helps in the subsequent generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Attention-based Speech Decoder</head><p>To achieve high-quality speech generation, we exploit the recent breakthroughs <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b30">30]</ref> in text-to-speech generation. We adapt the Tacotron 2 <ref type="bibr" target="#b30">[30]</ref> decoder which has been used to generate melspectrograms conditioned on text inputs. For our work, we condition the decoder on the encoded face embeddings from the previous module. We refer the reader to the Tacotron 2 <ref type="bibr" target="#b30">[30]</ref> paper for more details about the decoder. The encoder and decoder are trained end-to-end by minimizing the L1 reconstruction loss between the generated and the ground-truth melspectrogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Gradual Teacher Forcing Decay</head><p>In the initial stages of training, up to ≈ 30K iterations, we employ teacher forcing similar to the text-to-speech counterpart. We hypothesize that this enables the decoder to learn an implicit speech-level language model to help in disambiguating homophenes. Similar observations are made in lip to text works <ref type="bibr" target="#b1">[2]</ref> which employ a transformer-based sequence-to-sequence model. Over the course of the training, we gradually decay the teacher forcing to enforce the model to attend to the lip region and to prevent the implicit language model from over-fitting to the train set vocabulary. We examine the effect of this decay in sub-section 7.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Context Window Size</head><p>The size of the visual context window for inferring the current speech time-step helps the model to disambiguate homophenes <ref type="bibr" target="#b9">[10]</ref>. We employ about 6× larger context size than prior works and show in sub-section 7.1 that this design choice results in significantly more accurate speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Benchmark Datasets and Training Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>The primary focus of our work is on single-speaker lip to speech synthesis in unconstrained, large vocabulary settings. For the sake of comparison with previous works, we also train the Lip2Wav model on the GRID corpus <ref type="bibr" target="#b6">[7]</ref> and the TCD-TIMIT lip speaker corpus <ref type="bibr" target="#b12">[13]</ref>. Next, we train on all five speakers of our newly collected speaker-specific Lip2Wav dataset. Unless specified, all the datasets are divided into 90-5-5% train, validation and unseen test splits. In the Lip2Wav dataset, we create these splits using different videos ensuring that no part of the same video is used for both training and testing. The train and test splits are also released for fair comparison in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training Methodology and Hyper-parameters</head><p>We prepare a training input example by randomly sampling a contiguous sequence of 3 seconds which corresponds to T = 75 or T = 90 depending on the frame rate (FPS) of the video. The effect of various context window sizes is studied in Section 7.1. We detect and crop the face from the video frames using the S 3 F D face detector <ref type="bibr" target="#b40">[40]</ref>. The face crops are resized to 48×48. The melspectrogram representation of the audio corresponding to the chosen short video segment is used as the desired groundtruth for training. For training on small datasets like GRID and TIMIT, we halve the hidden dimension to prevent overfitting. We set the training batch size to 32 and train until the mel reconstruction loss plateaus for at least 30K iterations. In our experiments for unconstrained single-speaker, convergence was achieved in about 200K iterations. The optimizer used is Adam <ref type="bibr" target="#b21">[21]</ref> with an initial learning rate of 10 −3 . The model with the best performance on the validation set is chosen for testing and evaluation. More details, specifically a few minor speaker-specific hyper-parameter changes can be found in the publicly released code 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Speech Generation at Test Time</head><p>During inference, we provide only the sequence of lip movements and generate the speech in an auto-regressive fashion. Note that we can generate speech for any length of lip sequences. We simply take consecutive T second windows and generate the speech for each of them independently and concatenate them together. We also maintain a small overlap across the sliding windows to adjust for boundary effects. We obtain the waveform from the generated melspectrogram using the standard Griffin-Lim algorithm <ref type="bibr" target="#b11">[12]</ref>. We observed that neural vocoders <ref type="bibr" target="#b34">[34]</ref> perform poorly in our case as our generated melspectrograms are significantly less accurate than state-of-the-art TTS systems. Finally, the ability to generate speech for lip sequences of any length is worth highlighting as the performance of the current lip-to-text works trained at sentencelevel deteriorates sharply for long sentences that barely last over just 4 -5 seconds [2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments and Results</head><p>We obtain results from our Lip2Wav model on all the test splits as described above. For comparing related work, we use the open-source implementations provided by the authors if available or re-implement a version ourselves. We compare our models with the previous lip to speech works using three standard speech quality metrics: Short-Time Objective Intelligibility (STOI) <ref type="bibr" target="#b32">[32]</ref> and Extended Short-Time Objective Intelligibility (ESTOI) <ref type="bibr" target="#b16">[17]</ref> for estimating the intelligibility and Perceptual Evaluation of Speech Quality (PESQ) <ref type="bibr" target="#b29">[29]</ref> to measure the quality. Using an outof-the-box ASR system 6 , we obtain textual transcripts for our generated speech and evaluate our speech results using word error rates (WER) for the GRID <ref type="bibr" target="#b6">[7]</ref> and TCD-TIMIT lip speaker corpus <ref type="bibr" target="#b12">[13]</ref>. We, however do not compute WER for the proposed Lip2Wav corpus due to the lack of text transcripts. We also perform human evaluation and report the mean opinion scores (MOS) for the proposed Lip2Wav model and the competing methods. Next, we also perform extensive ablation studies for our approach and report our observations. Finally, as we achieve superior results compared to previous works in single-speaker settings, we end the experimental section by also reporting baseline results for word-level multi-speaker lip to speech generation using the LRW <ref type="bibr" target="#b5">[6]</ref> dataset and highlight its challenges as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Lip to Speech in Constrained Settings</head><p>We start by evaluating our approach against previous lip to speech works in constrained datasets, namely the GRID <ref type="bibr" target="#b6">[7]</ref> corpus and TCD-TIMIT lip speaker corpus <ref type="bibr" target="#b12">[13]</ref>. For the GRID dataset, we report the mean test scores for 4 speakers which are also reported in the previous works. <ref type="table" target="#tab_1">Tables 2 and 3</ref>   <ref type="table">Table 3</ref>. Objective speech quality, intelligibility and WER scores for the TCD-TIMIT dataset unseen test split.</p><p>As we can see, our approach outperforms competing methods across all objective metrics by a significant margin. The difference is particularly visible in the TIMIT <ref type="bibr" target="#b12">[13]</ref> dataset, where the test set contains a lot of novel words unseen during training. This shows that our model learns to capture correlations across short phoneme sequences and pronounces new words better than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Lip to Speech in Unconstrained Settings</head><p>We now move on to evaluating our approach in unconstrained datasets that contain a lot of head movements and much broader vocabularies. They also contain a significant amount of silences or pauses between words and sentences. It is here that we see a more vivid difference in our approach compared to previous approaches. We train our model independently on all 5 speakers of our newly collected Lip2Wav dataset. The training details are mentioned in the sub-section 5.2. For comparison with previous works, we choose the best performing models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">36]</ref> on the TIMIT dataset based on STOI scores and report their performance after training on our Lip2Wav dataset. We compute the same metrics for speech intelligibility and quality that are used in <ref type="table">Table 3</ref>. The scores for all five speakers for our method and the two competing methods across all three metrics are reported in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Speaker STOI ESTOI PESQ GAN-based <ref type="bibr" target="#b36">[36]</ref> Chemistry Lectures 0.192 0.132 1.057 Ephrat et al. <ref type="bibr" target="#b8">[9]</ref> 0.165 0.087 1.056 Lip2Wav (ours) 0.416 0.284 1.300 GAN-based <ref type="bibr" target="#b36">[36]</ref> Chess Analysis 0.195 0.104 1.165 Ephrat et al. <ref type="bibr" target="#b8">[9]</ref> 0.184 0.098 1.139 Lip2Wav (ours) 0.418 0.290 1.400 GAN-based <ref type="bibr" target="#b36">[36]</ref> Deep Learning 0.144 0.070 1.121 Ephrat et al. <ref type="bibr" target="#b8">[9]</ref> 0.112 0.043 1.095 Lip2Wav (ours) 0.282 0.183 1.671 GAN-based <ref type="bibr" target="#b36">[36]</ref> Hardware Security 0.251 0.110 1.035 Ephrat et al. <ref type="bibr" target="#b8">[9]</ref> 0.192 0.064 1.043 Lip2Wav (ours) 0.446 0.311 1.290 GAN-based <ref type="bibr" target="#b36">[36]</ref> Ethical hacking 0.171 0.089 1.079 Ephrat et al. <ref type="bibr" target="#b8">[9]</ref> 0.143 0.064 1.065 Lip2Wav (ours) 0.369 0.220 1.367 <ref type="table">Table 4</ref>. In unconstrained single-speaker settings, our Lip2Wav model achieves almost 4× more intelligible speech than the previous methods.</p><p>Our approach produces much more intelligible and natural speech across different speakers and vocabulary sizes. Notably, our model has more accurate pronunciation, as can be seen in the increased STOI and ESTOI scores compared to the previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Human Evaluation</head><p>In addition to speech quality and intelligibility metrics, it is important to manually evaluate the speech as these metrics are not perfect <ref type="bibr" target="#b8">[9]</ref> measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Objective Human Evaluation</head><p>In this study, we ask the human participants to manually identify and report (A) the percentage of mispronunciations, (B) the percentage of word skips and (C) the percentage of mispronunciations that are homophenes. Word skips denotes the number of words that are either completely unintelligible due to noise or slurry speech. We choose 10 predictions from the unseen test split of each speaker in our Lip2Wav dataset to get a total of 50 files. We report the mean numbers of (A), (B), and (C) in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>(A) (B) (C) GAN-based <ref type="bibr" target="#b36">[36]</ref> 36.6% 24.3% 63.8% Ephrat et al <ref type="bibr" target="#b8">[9]</ref> 43.3% 27.5% 60.7% Lip2Wav (ours) 21.5% 8.6% 49.8% <ref type="table">Table 5</ref>. Objective Human evaluation results. The participants manually identified the percentage of (A) Mispronunciations, (B) Word skips and (C) Homophene-based errors in the test samples.</p><p>Our approach makes far fewer mispronunciations than the current state-of-the-art method. It also skips words 3× lesser, however, the key point to note is that the issue of homophenes is still a dominant cause for errors in all cases indicating there is still scope for improvement in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Subjective Human Evaluation</head><p>We ask 15 participants to rate the different approaches for unconstrained lip to speech synthesis on a scale of 1 − 5 for each of the following criteria: (i) Intelligibility and (ii) Naturalness of the generated speech. Using 10 samples of generated speech for each of the 5 speakers from our Lip2Wav dataset, we compare the following approaches: (i) Our Lip2Wav model (ii) Current state-of-the-art lip to speech models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">36]</ref> (iii) Manually transcribed text followed by a multi-speaker TTS <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b30">30]</ref> to show that even with the most accurate text, lip to speech is not a concatenation of lip-to-text and text-to-speech. And finally, (iv) Human speech is also added for reference. In all the cases, we overlay the speech on the face video before showing it to the rater. The mean scores are reported in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Intelligibility Naturalness GAN-based <ref type="bibr" target="#b36">[36]</ref> 1.56 1.71 Ephrat et al. <ref type="bibr" target="#b8">[9]</ref> 1.34 1.67 Lip2Wav (ours) 3.04 3.63 MTT + TTS <ref type="bibr" target="#b30">[30]</ref> 3.86 3.15 Actual Human Speech 4.82 4.95 <ref type="table">Table 6</ref>. Mean human evaluation scores based on speech quality and intelligibility for various approaches for lip to speech. MTT denotes "manually-transcribed text". The penultimate row simulates the best possible case of automatic lip to text followed by a state of the art text-to-speech system. The drop in naturalness score in this case illustrates the loss in speech style and prosody.</p><p>In line with the previous evaluations, we can see that our approach produces significantly higher quality and legible speech compared to the previous state-of-the-art <ref type="bibr" target="#b36">[36]</ref>. It is also evident that generating the speech from the text that is read from lip movements (lip to text), cannot achieve the desired prosody and naturalness even if the text is fully accurate. Further, this method will also cause the lips and audio to be out of sync. Thus, our approach is currently the best method to produce natural speech from lip movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Multi-speaker Word-level Lip to Speech</head><p>Given the superior performance of our Lip2Wav approach on single-speaker lip to speech, we also obtain baseline results on the highly challenging problem of multispeaker lip to speech synthesis for random identities. Note that the focus of the work is still primarily on single-speaker lip to speech. We adapt the approach presented in <ref type="bibr" target="#b19">[19]</ref> and feed a speaker embedding as input to our model. We report our baseline results on the LRW <ref type="bibr" target="#b5">[6]</ref> dataset intended for word-level lip-reading, i.e. it is used to measure the performance of recognizing a single word in a given short phrase of speech. We do not demonstrate on the LRS2 dataset <ref type="bibr" target="#b4">[5]</ref> as its clean train set contains just 29 hours of data, which is quite small for multi-speaker speech generation. For instance, multi-speaker text-to-speech generation datasets <ref type="bibr" target="#b39">[39]</ref> containing a similar number of speakers contain several hundreds of hours of speech data.</p><p>In <ref type="table">Table 7</ref>, we report the speech quality and intelligibility metrics achieved by our multi-speaker Lip2Wav model on the LRW test split. As none of the previous works in lip to speech tackle the multi-speaker case, we do not make any comparisons with them. We also report the WER by getting the text using the Google ASR API. For comparison, we also report the WER of the baseline lip to text work on LRW <ref type="bibr" target="#b5">[6]</ref>. Note that the speech metric scores shown in <ref type="table">Table  7</ref> for word-level lip to speech cannot be directly compared with the single-speaker case which contains word sequences of various lengths along with pauses and silences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>STOI ESTOI PESQ WER Lip2Wav (Ours) 0.543 0.344 1.197 34.2% Chung et al. <ref type="bibr" target="#b5">[6]</ref> NA NA NA 38.8% <ref type="table">Table 7</ref>. Objective speech quality and intelligibility scores on the LRW dataset. WER is also calculated after using an ASR on the generated speech. Our model outperforms the baseline method proposed in <ref type="bibr" target="#b5">[6]</ref>, without any text-level supervision. The speech metrics are not applicable for <ref type="bibr" target="#b5">[6]</ref> as it is a lip to text work.</p><p>We end our experimental section here. Apart from showing significant increases in performance from previous lip to speech works, we also achieve word-level multi-speaker lip to speech synthesis. In the next section, we conduct ablation studies on our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Ablation Studies</head><p>In this section, we probe different aspects of our Lip2Wav approach. All results in this section are calculated using the unseen test predictions on the "Hardware Security" speaker of our Lip2Wav dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Larger context window helps in disambiguation</head><p>As stated before, the lip to speech task is highly ambiguous to be inferred solely from lip movements. One of the ways to combat this, is to provide reasonably large context information to the model to disambiguate a given viseme. Previous works, however, use only about 0.3 − 0.5 seconds of context. In this work, we use close to 6× this number and provide a context of 3 seconds. This helps the model to disambiguate by learning co-occurrences of phonemes and words and the resulting improvement is evident in  <ref type="table" target="#tab_2">Table 8</ref>. Larger context information consistently results in more accurate speech generation. We limit the window size to 3 seconds due to memory constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Model is Highly Attentive to the Mouth</head><p>We plot the activations of the penultimate layer of the spatio-temporal face encoder in <ref type="figure">Figure 4</ref> to show that our encoder is highly attentive towards the mouth region of the speaker. The attention alignment curve in <ref type="figure">Figure 5</ref> shows that the decoder conditions on the appropriate video frame's lips while generating the corresponding speech. <ref type="figure">Figure 4</ref>. We plot the activations of the penultimate layer of the face encoder and the attention alignment from the decoder. We see that the face encoder is highly attentive towards the mouth region. <ref type="figure">Figure 5</ref>. The decoder alignment curve illustrates that the model is generating speech by strongly conditioning on the corresponding lip movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Teacher Forcing vs Non-Teacher Forcing</head><p>To accelerate the training of a sequence-to-sequence architecture, typically, the previous time step's ground-truth (instead of the generated output) is given as input to the current time-step. While this is highly beneficial in the initial stages of training, we observed that gradually decaying the teacher forcing from ≈ 30K iterations significantly improves results and prevents over-fitting to the train vocabulary. A similar improvement is also observed in lip to text works <ref type="bibr" target="#b1">[2]</ref>. In <ref type="table">Table 9</ref>, we show the significant improvement in test scores by gradually decaying teacher forcing.</p><p>Teacher-forcing STOI ESTOI PESQ Always forced 0.221 0.162 1.141 Gradual decay 0.446 0.311 1.290 <ref type="table">Table 9</ref>. Gradually decaying the teacher forcing enables the model to generalize to unseen vocabulary by forcing it to look at the visual input and not just predict from the previously uttered speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Effect of Different Visual Encoders</head><p>While using a 3D-CNN worked best in our experiments to capture both the spatial and temporal information in unconstrained settings, we also report in <ref type="table" target="#tab_0">Table 10</ref> the effect of using different kinds of encoders. We replace the encoder module while keeping the speech decoder module intact. We see that the best performance is obtained with a 3D-CNN encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>STOI ESTOI PESQ 2D-CNN 0.291 0.211 1.112 2D-CNN + 1D-CNN 0.298 0.223 1.170 3D-CNN (ours) 0.446 0.311 1.290 <ref type="table" target="#tab_0">Table 10</ref>. Our Lip2Wav model employs a 3D-CNN encoder to capture the spatio-temporal visual information and is the superior choice over the other alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this work, we investigated the problem of synthesizing speech based on lip movements. We specifically solved the problem by focusing on individual speakers. We did this in a data-driven learning approach by creating a largescale benchmark dataset for unconstrained, large vocabulary single-speaker lip to speech synthesis. We formulate the task at hand as a sequence-to-sequence problem, and show that by doing so, we achieve significantly more accurate and natural speech than previous methods. We evaluate our model with extensive quantitative metrics and human studies. All the code and data for our work has been made publicly available 2 . Our work opens up several new directions. One of them would be to examine related works in this space such as lip to text generation from a speakerspecific perspective. Similarly, explicitly addressing the dominant issue of homophenes can yield more accurate speech. Generalizing to vocabulary outside the typical domain of the speaker can be another fruitful venture. We believe that exploring some of the above problems in a datadriven fashion could lead to further useful insights in this space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Our Lip2Wav dataset contains talking face videos of 5 speakers from chess analysis and lecture videos. Each speaker has about 20 hours of YouTube video content spanning a rich vocabulary of 5000+ words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>only if its frequency in the dataset is at least five.</figDesc><table><row><cell></cell><cell>Num.</cell><cell>Total</cell><cell>#hours</cell><cell>Vocab</cell><cell>Natural</cell></row><row><cell>Dataset</cell><cell>speak-</cell><cell>#hours</cell><cell>per</cell><cell>per</cell><cell>set-</cell></row><row><cell></cell><cell>ers</cell><cell></cell><cell>speaker</cell><cell>speaker</cell><cell>ting?</cell></row><row><cell>GRID [7]</cell><cell>34</cell><cell>28</cell><cell>0.8</cell><cell>56</cell><cell>×</cell></row><row><cell>TIMIT [13]</cell><cell>3</cell><cell>1.5</cell><cell>0.5</cell><cell>82</cell><cell>×</cell></row><row><cell>Lip2Wav (Ours)</cell><cell>5</cell><cell>120</cell><cell>≈ 20</cell><cell>≈ 5K</cell><cell></cell></row></table><note>. The Lip2Wav dataset is the first large-scale dataset tai- lored towards acting as a reliable benchmark for single-speaker lip to speech synthesis.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>summarize the results for GRID and TIMIT datasets respectively. Objective speech quality, intelligibility and WER scores for the GRID dataset unseen test split.</figDesc><table><row><cell>Method</cell><cell>STOI ESTOI PESQ</cell><cell>WER</cell></row><row><cell cols="3">Vid2Speech [10] 0.491 0.335 1.734 44.92%</cell></row><row><cell cols="3">Lip2AudSpec [4] 0.513 0.352 1.673 32.51%</cell></row><row><cell cols="3">GAN-based [36] 0.564 0.361 1.684 26.64%</cell></row><row><cell>Ephrat et al. [9]</cell><cell cols="2">0.659 0.376 1.825 27.83%</cell></row><row><cell cols="3">Lip2Wav (ours) 0.731 0.535 1.772 14.08%</cell></row><row><cell>Method</cell><cell>STOI ESTOI PESQ</cell><cell>WER</cell></row><row><cell cols="3">Vid2Speech [10] 0.451 0.298 1.136 75.52%</cell></row><row><cell cols="3">Lip2AudSpec [4] 0.450 0.316 1.254 61.86%</cell></row><row><cell cols="3">GAN-based [36] 0.511 0.321 1.218 49.13%</cell></row><row><cell>Ephrat et al. [9]</cell><cell cols="2">0.487 0.310 1.231 53.52%</cell></row><row><cell cols="3">Lip2Wav (ours) 0.558 0.365 1.350 31.26%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 8 .</head><label>8</label><figDesc></figDesc><table><row><cell>Context Window size</cell><cell cols="3">STOI ESTOI PESQ</cell></row><row><cell>0.5 seconds</cell><cell>0.264</cell><cell>0.193</cell><cell>1.062</cell></row><row><cell>1.5 seconds</cell><cell>0.321</cell><cell>0.226</cell><cell>1.080</cell></row><row><cell>3 seconds</cell><cell>0.446</cell><cell>0.311</cell><cell>1.290</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://cvit.iiit.ac.in/research/projects/ cvit-projects/speaking-by-observing-lip-movements 3 https://github.com/Rudrabha/Lip2Wav 4 https://www.youtube.com/watch?v=HziA-jmlk_4</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. Lip to Speech Synthesis in the Wild Given a sequence of face images I = (I 1 , I 2 , . . . , I T ) with lip motion, our goal is to generate the corresponding speech segment S = (S 1 , S 2 , . . . , S T ). To obtain natural speech in unconstrained settings, we make numerous key design choices in our Lip2Wav architecture. Below, we highlight and discuss how they are different from previous methods for lip to speech synthesis.<ref type="bibr" target="#b4">5</ref> approximate; texts obtained using Google ASR API</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Google Speech-to-Text API</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joon Son Chung, and Andrew Zisserman. The conversation: Deep audio-visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04121</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep lip reading: a comparison of models and an online application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In INTERSPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Joon Son Chung, and Andrew Zisserman</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00496</idno>
		<title level="m">Joon Son Chung, and Andrew Zisserman. Lrs3-ted: a large-scale dataset for visual speech recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lip2audspec: Speech reconstruction from silent lip movements video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himani</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2516" to="2520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, and Andrew Zisserman. Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Communication with deaf patients: knowledge, beliefs, and practices of physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heckerling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="229" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved speech reconstruction from silent video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tavi</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="455" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vid2speech: speech reconstruction from silent video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5095" to="5099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep voice 2: Multi-speaker neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sercan</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2962" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">S</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tcd-timit: An audio-visual corpus of continuous speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Harte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eoin</forename><surname>Gillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="615" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Communicating about health care: observations from persons who are deaf or hard of hearing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lisa I Iezzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bonnie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>O&amp;apos;day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heather</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="356" to="362" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An algorithm for predicting the intelligibility of speech masked by modulated noise maskers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><surname>Taal</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer learning from speaker verification to multispeaker text-to-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4480" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A neural network model of the articulatory-acoustic forward mapping trained on recordings of articulatory parameters. The Journal of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plaut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Acoustical Society of America</publisher>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="2354" to="2364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lipper: Synthesizing thy speech using multi-view lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaman</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khwaja</forename><forename type="middle">Mohd</forename><surname>Salik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv Ratn</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2588" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating intelligible audio speech from visual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Cornu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1751" to="1761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Infants deploy selective attention to the mouth of a talking face when learning speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">M</forename><surname>Lewkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansen-Tift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1431" to="1436" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Communication strategies for nurses interacting with patients who are deaf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Chong-Hee Lieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><forename type="middle">Robins</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">T</forename><surname>Fullerton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulette</forename><forename type="middle">Deyo</forename><surname>Stohlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dermatology Nursing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">541</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Developing a standardized comprehensive health survey for use with deaf adults</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Margellos-Anast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teri</forename><surname>Hedding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perlman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Kivland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorothea</forename><surname>Degutis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Giloth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Whitman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Annals of the Deaf</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="388" to="396" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep voice 3: Scaling text-to-speech with convolutional sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07654</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lipsound: Neural mel-spectrogram reconstruction for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyuan</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelius</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2768" to="2772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Perceptual evaluation of speech quality (pesq): A new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antony</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andries</forename><surname>Hekstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-02" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rj</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A short-time objective intelligibility measure for time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-04" />
			<biblScope unit="page" from="4214" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Video-driven speech reconstruction using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06301</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lipreading with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6115" to="6119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lcanet: End-to-end lipreading with cascaded attention-ctc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Cassimatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="548" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Libritts: A corpus derived from librispeech for text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02882</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vision-infused deep audio inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
