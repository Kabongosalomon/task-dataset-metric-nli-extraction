<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MHP-VOS: Multiple Hypotheses Propagation for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjie</forename><surname>Xu</surname></persName>
							<email>shuangjiexu@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daizong</forename><surname>Liu</surname></persName>
							<email>dzliu@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
							<email>linchaobao@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
							<email>panzhou@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MHP-VOS: Multiple Hypotheses Propagation for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of semi-supervised video object segmentation (VOS), where the masks of objects of interests are given in the first frame of an input video. To deal with challenging cases where objects are occluded or missing, previous work relies on greedy data association strategies that make decisions for each frame individually. In this paper, we propose a novel approach to defer the decision making for a target object in each frame, until a global view can be established with the entire video being taken into consideration. Our approach is in the same spirit as Multiple Hypotheses Tracking (MHT) methods, making several critical adaptations for the VOS problem. We employ the bounding box (bbox) hypothesis for tracking tree formation, and the multiple hypotheses are spawned by propagating the preceding bbox into the detected bbox proposals within a gated region starting from the initial object mask in the first frame. The gated region is determined by a gating scheme which takes into account a more comprehensive motion model rather than the simple Kalman filtering model in traditional MHT. To further design more customized algorithms tailored for VOS, we develop a novel mask propagation score instead of the appearance similarity score that could be brittle due to large deformations. The mask propagation score, together with the motion score, determines the affinity between the hypotheses during tree pruning. Finally, a novel mask merging strategy is employed to handle mask conflicts between objects. Extensive experiments on challenging datasets demonstrate the effectiveness of the proposed method, especially in the case of object missing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semi-supervised Video Object Segmentation (VOS) is the task to automatically segment the objects of interests in a video given the annotations in the first frame, which is a fundamental task with wide applications in video editing, video summarization, action recognition, etc. Although tremendous progress has been made with semantic segmen- ‡ Part of the work was done during an internship at Tencent AI Lab. * Equal contributions. † Corresponding authors. <ref type="figure">Figure 1</ref>. Challenging examples handled by previous approaches.</p><p>In the first example, the front object instance is segmented as two different objects and the farther instance is missing in the result.</p><p>In the second example, the occluded instance and the re-appearing instance are missing. In the last example, the smaller object near the larger object is incorrectly segmented to be the larger one.</p><p>tation CNNs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28]</ref> recently, VOS is still challenging in objects missing and association problems due to occlusions, large deformations, complex object interactions, rapid motions, etc., as shown in <ref type="figure">Fig. 1</ref>.</p><p>To tackle these challenges, many recent works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b24">25]</ref> resort to object proposal schemes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref> to restore missing objects or re-establish objects associations. In these works, proposals of target objects are either generated individually in each frame <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref> by semantic detectors, or further merged with a few neighboring frames <ref type="bibr" target="#b24">[25]</ref>. However, these approaches rely on a greedy selection of the best object proposal at each time step, for a given object, which becomes a complication with utter dependence on a reliable Re-ID network <ref type="bibr" target="#b24">[25]</ref> that can provide accurate similarity scores. In this paper, we instead deal with this problem by employing a multiple hypotheses propagation approach, which builds up a tracking tree for different hypotheses in time steps, enabling us to defer the selection of the best object proposal for each target till a whole proposal tree along temporal domain is established. This delayed decision making provides us a global view to determine data associations in each frame by considering objects information over the entire video, provably more reliable than greedy methods.</p><p>The idea of tracking using multiple hypotheses is not new. In the seminal work by Cox and Hingorani <ref type="bibr" target="#b10">[11]</ref>, Multiple Hypotheses Tracking (MHT) was first introduced to the vision community and applied in the context of visual tracking. Unfortunately, the performance of MHT was limited by unreliable target detectors at that time and later abandoned for decades. More recently, it is again demonstrated to achieve state-of-the-art performances for multiple objects tracking when implemented with modern techniques <ref type="bibr" target="#b19">[20]</ref>. The basic idea of MHT is to build up a tracking tree with proposals from each frame, and then prune the tree using the tracking scores until the best track left. The key ingredients for the success of MHT in <ref type="bibr" target="#b19">[20]</ref> are the gating scheme and scoring function during the construction and pruning of the tracking trees. In the gating scheme, Kalman filtering is employed to restrict proposal children to be spawned within a certain gating area near their parent, such that the tree does not expand too quickly. The scoring function is to determine the similarity between two hypotheses using motion and appearance cues. However, the algorithm is not that reliable when it comes to VOS, especially when there are large object deformations or sudden changes of object movements (see carousel in <ref type="figure">Fig. 1</ref> as an example). In this case, the simple motion model of Kalman filtering would break and the appearance score would be very brittle.</p><p>In this paper, we adapt MHT to VOS and propose a novel method called Multiple Hypotheses Propagation for Video Object Segmentation (MHP-VOS). Starting from the initial bounding box (bbox) of object mask in the first frame, multiple hypotheses are spawned by proposals from the classagnostic detector within a novel motion gated region instead of Kalman filtering. We also design a novel mask propagation score instead of the appearance similarity score that could be brittle due to large deformations in challenging cases. The mask propagation score, together with motion score, determines the affinity between hypotheses during the tree pruning. After pruning the proposal tree, the final instance segmentation can be generated and propagated with a mask refinement CNN for each object of interests. And the conflicts between objects are further handled with a novel mask merging strategy. Comparing to state-of-the-art approaches, our method is much more robust and achieves the best performances on the DAVIS datasets.</p><p>Our main contributions are summarized as follows:</p><p>• We adapt a multiple hypotheses tracking method to the VOS task to build up a bbox proposal tracking tree for different objects with a new gating and pruning method, which can be regarded as a delayed decision for global consideration. • We apply a motion model to proposal gating instead of using the Kalman filtering, and design a novel hybrid pruning score of motion and mask propagation, which are tailored for VOS tasks. We also design a novel mask merging strategy for multi-objects tasks. • We conduct extensive experiments to show the effectiveness of our method in distinguishing similar objects, handling occluded and re-appearing objects, modeling long-term object deformations, etc., which are very difficult to deal with for previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly summarize recent researches related to our work, including semi-supervised video object segmentation and multiple hypotheses tracking.</p><p>Matching-based Video Object Segmentation. This type of approaches generally utilize the given mask in the first frame to extract appearance information for objects of interests, which is then used to find similar objects in succeeding frames. Yoon et al. <ref type="bibr" target="#b41">[42]</ref> proposed a siamese network to match the object between frames in a deep feature space. In <ref type="bibr" target="#b4">[5]</ref>, Caelles trained a parent network on still images and then finetuned the pre-trained work with one-shot online learning. To further improve the finetuning performance in <ref type="bibr" target="#b4">[5]</ref>, Khoreva et al. <ref type="bibr" target="#b18">[19]</ref> synthesized more training data to enrich the appearances on the basis of the first frame. In addition, Chen et al. <ref type="bibr" target="#b8">[9]</ref> and Hu et al. <ref type="bibr" target="#b15">[16]</ref> used pixel-wise embeddings learned from supervision in the first frame to classify each pixel in succeeding frames. Cheng et al. <ref type="bibr" target="#b9">[10]</ref> proposed to track different parts of the target object to deal with challenges like deformations and occlusions.</p><p>Propagation-based Video Object Segmentation. Different from the appearance matching methods, mask propagation methods utilize temporal information to refine segmentation masks propagated from preceding frames. Mask-Tracker <ref type="bibr" target="#b28">[29]</ref> is a typical method following this line, which is trained from segmentation masks of static images with mask augmentation techniques. Hu et al. <ref type="bibr" target="#b14">[15]</ref> extended MaskTracker <ref type="bibr" target="#b28">[29]</ref> by applying active contour on optical flow to find motion cues. To overcome the problem of target missing when fast motion or occlusion occurs, methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b37">38]</ref> combined temporal information from nearby frame to track the target. The CNN-in-MRF method <ref type="bibr" target="#b0">[1]</ref> embeds the mask propagation step into the inference of a spatiotemporal MRF model to further improve temporal coherency. Oh et al. <ref type="bibr" target="#b38">[39]</ref> applied instance detection to mask propagation using a siamese network without online finetuning for a given video. Another method <ref type="bibr" target="#b40">[41]</ref> that does not need online learning uses Conditional Batch Normalization (CBN) to gather spatiotemporal features.</p><p>Detection-based Video Object Segmentation. Object detection has been widely used to crop out the target from a frame before sending it to a segmentation model. Li et al. <ref type="bibr" target="#b21">[22]</ref> proposed VS-ReID algorithm to detect missing objects in video object segmentation. Sharir et al. <ref type="bibr" target="#b34">[35]</ref> produced object proposals using Faster R-CNN <ref type="bibr" target="#b33">[34]</ref> to gather proper bounding boxes. Luiten et al. <ref type="bibr" target="#b24">[25]</ref> used Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> to detect supervised targets among the frames and crop them as the inputs of Deeplabv3+ <ref type="bibr" target="#b7">[8]</ref>. Most works based on detections select one proposal at each time step greedily. In contrast, we keep multiple proposals at each time step and make decisions globally for the segmentation.</p><p>Multiple Hypotheses Tracking. MHT method is widely used in the field of target tracking <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Hypotheses tracking <ref type="bibr" target="#b10">[11]</ref> algorithm originally evaluates its usefulness in the context of visual tracking and motion correspondence, and the MHT in <ref type="bibr" target="#b19">[20]</ref> proposed a scoring function to prune the hypothesis space efficiently and accurately which is suited to current visual tracking context. Also, Vazquez et al. <ref type="bibr" target="#b35">[36]</ref> first adopted MHT in the semantic video segmentation task without pruning. In our method, we adapt the approach to the class-agnostic video object segmentation scenario, where propagation scoring is class-agnostic with the motion rules and the propagation correspondences instead of the unreliable appearance scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The overall architecture of our proposed MHP-VOS is illustrated in <ref type="figure">Fig. 2</ref>. We first generate bbox object proposals P t = {p t n , n = 1, . . . , N roi } of image I t from frame t with a class-agnostic detection approach in Sec. 3.1, and then apply multiple hypotheses propagation recurrently during building the hypotheses propagation tree (Sec. 3.2) with our novel gating and scoring strategies and filter out disturbing hypotheses by N -scan pruning (Sec. 3.3) to introduce long-term knowledge for hypotheses decision. To take advantage of spatial information between different objects in a sequence, the propagation trees for each object are built at the same time. After acquiring each corresponding bounding box proposal b t associated with the best hypotheses for each object, we obtain current mask M i for object i using a segmentation model with b t in Sec. 3.4. At last, we merge instance masks M i to multi-objects mask M with consideration of intra-objects conflicts in Sec. 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proposal Generation</head><p>There are many approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b12">13]</ref> used to detect the target object in each video frame. In this paper, we take Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> network fine-tuned on each sequence as the base-model to generate coarse object proposals, which are the bbox around the objects. Specially, we change the category number of Mask R-CNN from N coco classes to only one class to make it class-agnostic for detecting foreground objects. Note that segmentation results from the Mask branch are not used for VOS, as this branch shares the classification confidence which is not suitable for the segmentation task. With the input of each frame image, we just extract coarse object bounding box proposals with the detection confidence greater than th p , and non-maximum suppression threshold of th n to retain all possible proposals for the further mask proposal propagation in the next step. Here, we denote the output proposal of frame t as p t n , where n is the n-th proposal of all N t proposals in detection step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hypotheses Tree Construction</head><p>After generating coarse object proposals, we construct the hypotheses propagation tree, whose data structures are designed as follows: each hypothesis node in the tree consists of a bounding box proposal p t k and its corresponding mask hypothesis M p t k . For each target object, the tree starts from the ground-truth mask in the first frame, and will be extended by appending children proposals in the next frame. In this children spawning step, only proposals within a gated region are considered. And the mask hypothesis M p t k for each child proposal p t k is obtained using the method detailed in Sec. 3.4. This process is repeated until the final hypotheses tree is constructed completely. In addition, each proposal outside the gated region is treated as the starting node in a new tree to catch missing objects. During the tree construction, a novel mask propagation score of each node can be recorded and would be used for tree pruning later, which is more robust than the appearance score.</p><p>Gating. To build the hypotheses tree, we need to gate most closely proposals in next frame to be the child nodes, shown in <ref type="figure">Fig. 3 (a)</ref>. In general, the bounding box of objects in frame t depends on two main variables: size s t , (w t , h t ) and center point coordinate p t , (x t , y t ). Thus, the historical movements in n frame from t − n to t − 1 are adopted as prior knowledge to predict the probability bbox in frame t. For the position prediction, the velocity v t is estimated by</p><formula xml:id="formula_0">v t = 1 n t−n m=t−1 (p m − p m−1 ).<label>(1)</label></formula><p>Then the predicted center point is obtained by p t = p t−1 + v t . And the corresponding average size is taken as the predicted object size s t = 1 n t−n m=t−1 s m , since the change in size is tiny and smooth. With the estimation of p t and s t , it gives the bbox candidate c t for comparison in gating.</p><p>In order to filter out disturbing proposals, we gate the candidate proposals by computing the IOU score with the bounding box c t in the last frame as follows:</p><formula xml:id="formula_1">1 t n = 1, iou(c t , p t n ) &gt; th g 0, iou(c t , p t n ) ≤ th g ,<label>(2)</label></formula><p>where th g is the threshold of gating, and 1 t n denotes whether the candidate box p t n gates in or out. With proposals chosen from gating, we can build up the propagation tree to simulate multiple hypotheses proposal propagation.</p><p>Scoring. In the propagation tree, each hypotheses is associated with a class-agnostic score for further pruning. It is a recurrent process in each tree node, which is formalized as:</p><formula xml:id="formula_2">S t, p t k = w m S m t, p t k + w p S p t, p t k ,<label>(3)</label></formula><p>where S m (t, p t k ) and S p (t, p t k ) denote the motion score and mask propagation score, respectively. t = 0, 1, ..., T means ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Frame</head><p>Proposal Generation Tree Construction Propagation Tree</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pruning Proposal Segmentation</head><p>Best Branch Corresponding Mask <ref type="figure">Figure 2</ref>. The pipeline of our MHP-VOS algorithm. We first obtain bounding box proposals from Mask RCNN <ref type="bibr" target="#b12">[13]</ref>, and then construct the proposal propagation tree for each object with gating and scoring strategies. To avoid calculation explosion, an N-scan pruning strategy is applied to remove branches that are far from the best hypothesis. Through this recurrent process between tree building and branches pruning, we can obtain the best propagation track, and then obtain the segmentation mask for each object by mask propagation and merging.</p><p>the current video frame number, p t k denotes the proposal of the k-th hypotheses track. w m and w p control the ratio between motion score and propagation score. There is no Re-ID score involved since it may cause ambiguity when objects of similar appearances exist.</p><p>For each bounding box proposal p t k of the node in the propagation tree, we define the motion score as:</p><formula xml:id="formula_3">S t m t, p t k = w f p t k ∩ p t−1 k p t k ∪ p t−1 k +w n max i =k p t k ∩ p t−1 i p t k ∪ p t−1 i .<label>(4)</label></formula><p>The motion score is composed of two parts: a) iou score between proposals of same hypotheses in continuous frames, which is positive to the decision; b) iou score between frame t proposal of k-th track and the (t − 1)-th proposal node in other hypotheses track, and it is expected to be small. Motion score gives a qualitative mark when the continuity of propagation track is smooth. However, the motion score will be out of order when severe occlusion occurs. In order to handle such case, the mask propagation score is proposed utilizing the quality of segmentation propagated in target proposal, which can be formalized as:</p><formula xml:id="formula_4">S t p t, p t k = M p t k ∩ Q t • M p t−1 k M p t k ∪ Q t • M p t−1 k ,<label>(5)</label></formula><p>where • denotes the warp operation that warps mask from last frame to current frame with optic flow Q. And M p t k denotes the single object mask segmentation obtained by method in Sec. 3.4 with the proposal p t k . M p t k composes the mask hypothesis with bounding box proposals: it starts from ground-truth in frame t = 0, and forwards propagation with the construction of proposal tree (warp to next frame as priori mask for mask generation in p t+1 k progressively). As for the new start tree for the missing object, the mask of tree root is obtained with blank mask as the priori mask.</p><p>At last, the final score of the long-term hypotheses can be computed recursively as:</p><formula xml:id="formula_5">S t, p t k = S t − 1, p t−1 k + S t t, p t k ,<label>(6)</label></formula><formula xml:id="formula_6">S t t, p t k = ln (1 − P D ) , t = 0 w m S t m + w p S t p , t = 0 ,<label>(7)</label></formula><p>where P D denotes the probabilities of detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hypotheses Tree Pruning</head><p>During the construction of the hypotheses tree, the number of hypotheses tracks increases exponentially during propagation, which leads to the explosion of memory and computation. Thus, we have to take a pruning step to limit the size of the tree. In other words, we need to determine the most likely context propagation tracks in long term, of which the optimization can be formulated as:</p><formula xml:id="formula_7">max H T t=0 S t, p t k ,<label>(8)</label></formula><p>where H k = p i k |t = 0, 1, . . . , t means a proposal propagation hypothesis (track path from root to leaf node in the propagation tree) and H = {H k |k = 0, 1, . . . , N h } means hypothesis space for tracks of an object. N h means the Hypotheses space size for the target object.</p><p>To find the best track among the kinds of propagation tracks, this task can be formulated as a Maximum Weighted Independent Set (MWIS) problem as described in <ref type="bibr" target="#b26">[27]</ref>. For the track tree in frame t, we build an undigraph G = (V, E) with each propagation hypothesis H k taken as a node in V . The edge (l, j) in E connects the hypothesis pair (H l , H j ) which has the same proposal at the same frame, which means the two hypotheses are conflicting and cannot coexist for the final independent set B = b i |i = 0, . . . , t . With the track score described in Eq. (8) as the weight w of each track branch, we optimize the problem to find the maximum weight independent set B as follows:</p><formula xml:id="formula_8">max i w i , i ∈ {l, j}, ∀(i, j) ∈ E.<label>(9)</label></formula><p>We utilize the existing phased local search (PLS) algorithm <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2]</ref> to solve the MWIS optimization problem. Also, we take the N -scan pruning method to prune the disturbing branches gradually instead of pruning the whole tree. First, we apply the Eq. (9) to choose the maximum independent set as the best hypothesis from hypothesis space H, and then track the nodes in frame k back to the node in frame k − N as sub-trees. Finally, we prune the sub-trees except the independent tracks. A larger N makes a longer decision delay, which will bring an improvement in precision but take time efficiency as price. In addition, we also limit the number of branches to avoid proposal tree growing too large. If the number of branches is more than th b at any node in any frame, we retain the top th b branches with the propagation scores and prune the other branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Single Object Segmentation</head><p>We employ Deeplabv3+ <ref type="bibr" target="#b7">[8]</ref> network with a ResNet101 <ref type="bibr" target="#b13">[14]</ref> backbone as our segmentation module, to generate segmentation results from bounding box proposals. Similar to MaskTracker <ref type="bibr" target="#b28">[29]</ref>, the segmentation network takes an additional rough mask as input, which is warped from the mask of the previous frame to the current frame using optical flow estimated by FlowNet2 <ref type="bibr" target="#b16">[17]</ref>. This module is used to generate mask hypothesis from proposal during the tree construction, and can produce the final segmentation result once the best proposal for an object is obtained after the tree pruning. Taking the final segmentation as an example, we crop the bounding box of a single object and its previous mask by b t i with margin ratio r, and then concatenate the RGB image with the warped mask Q t i as a fourth channel input. After obtaining the segmentation probability map Z t i from Deeplabv3+, we obtain the instance-specific mask M t i with </p><formula xml:id="formula_9">from M t−1 Ids[1] ; if sum G b t Ids[0] * Q t Ids[0] [a] &gt;sum G b t Ids[1] * Q t Ids[1] [a] then Y t [a] ⇐ Ids[0]; else Y t [a] ⇐ Ids[1]</formula><p>; return Y t for the multi-instance segmentation; threshold th m as following:</p><formula xml:id="formula_10">M t i = (Z t i &gt; th m ), i = 1, 2, ...., C,<label>(10)</label></formula><p>where C denotes the total object number in one sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Conflicts Handling for Multiple Objects</head><p>To merge the instance-specific masks M t i into the final multi-instance segmentation Y t , we propose a merging strategy as shown in Algorithm 1. In general, there are two   kinds of cases when we decide each pixel id in the final segmentation. For the pixel belonging to one object, we set the object id to be the same as the the corresponding pixel among the single instance masks. However, the pixel may belong to different objects at the same time when the overlap conflicts happen between multi-instance masks. To determine the object id for the overlapped region, we first take the top two possible object ids sorted by the corresponding values in the probability map from DeeplabV3+ as id candidates. We then accept the object id with higher probability only when there is a large margin between the two probability values (we use a marginal ratio λ = 0.8). Otherwise, we take temporal coherency of the warped mask in consideration when it is ambiguous to use spatial information only. Besides, a two-dimensional gaussian map G b t is generated from the proposal b t with parameters of σ t x = w / 2 and σ t y = h / 2 as prior knowledge to obtain the weighted mask without noise out of the region of interests, where w and h are the width and height of proposal b t , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we investigate the performance of our method on standard benchmark datasets: DAVIS2016 <ref type="bibr" target="#b29">[30]</ref> and DAVIS2017 <ref type="bibr" target="#b5">[6]</ref>. We compare our model with state-ofthe-art methods and perform ablation study to demonstrate the advantage of each component in MHP-VOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>To adapt the Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> network to DAVIS <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6]</ref> task, we first train the network on COCO <ref type="bibr" target="#b22">[23]</ref> dataset with the pre-trained ImageNet <ref type="bibr" target="#b11">[12]</ref> weights, and then finetune it on DAVIS dataset. Before testing, we finetune the parent model weights on each sequence respectively with the corresponding N l = 200 synthetic indomain image-pairs of Lucid Dreaming <ref type="bibr" target="#b4">[5]</ref>. Then, coarse proposals are selected with the th p = 0.05 and th n = 0.6.</p><p>During the training of the Deeplabv3+ <ref type="bibr" target="#b7">[8]</ref> network with a ResNet101 <ref type="bibr" target="#b13">[14]</ref> backbone, we crop the bbox of the four channel input by using the spatial information of the annotation with margin ratio r = 0.15. Then, we resize the cropped data into 512 × 512, jitter the image color, and then train them for 100 epochs both on COCO <ref type="bibr" target="#b22">[23]</ref> and DAVIS <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6]</ref> datasets. We use BCEWithLogits loss function, and set Adam <ref type="bibr" target="#b20">[21]</ref> optimizer with lr = 1e − 5 which reduces by power of 0.9 for every 10 epochs. In the fine-tuning, we only train the parent model on synthetic image-pairs for 50 epochs, and the lr starts from 5e − 6 and also reduces by power of 0.9 for every 10 epochs. We set th m = 0.3 to get the valid mask with the corresponding probability map. At last, the instance masks are merged with λ = 0.8. In N-scan pruning phase, we set N = 3 and th b = 50. All experiments are implemented on a single NVIDIA 1080 GPU. The code is available at https://github.com/shuangjiexu/MHP-VOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Evaluation</head><p>DAVIS2016. DAVIS2016 <ref type="bibr" target="#b29">[30]</ref> dataset is proposed recently to evaluate VOS methods and contains 50 video sequences divided into train and test parts. Each video sequence consists of a single object, and it provides each object with the corresponding mask among the sequences.</p><p>DAVIS2017. DAVIS2017 <ref type="bibr" target="#b30">[31]</ref> dataset is extended from</p><formula xml:id="formula_11">0% 25% 50% 75% 100%</formula><p>DAVIS2017 test-dev DAVIS2016 val <ref type="figure">Figure 5</ref>. Qualitative results from the DAVIS2017 test-dev and DAVIS2016 validation sets, where the images are sampled at the average intervals for each video. From top to bottom, the sequences are "carousel", "monkeys-trees", and "salsa" on the DAVIS2017 test-dev, "bmx-trees" and "libby" on the DAVIS2016 validation. Different objects are highlighted as different colors.</p><p>DAVIS2016, and it is more challenging in multiple objects which correspond to different targets. It provides extra testdev data with 30 challenging videos, which contains some similar objects in the same videos and object occlusion or missing in the continues frames. Background noise is also a challenge which has similar appearance with target objects. Evaluation. We adopt the protocols in <ref type="bibr" target="#b29">[30]</ref> which contains two evaluation metrics, region similarity J and contour accuracy F. In addition, both two evaluation metrics consist of three statistics measurement: mean M, recall R and decay D. The global metric G is the mean of J and F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">DAVIS2017</head><p>Comparison to the State-of-the-arts. <ref type="table">Table.</ref> 1 shows the quantitative comparison on DAVIS-2017 valid and testdev sets, where we find that MHP-VOS performs the stateof-the-art in most evaluation matrices. Especially on the validation set, MHP-VOS beats all the latest methods and achieves higher Mean value. As illustrated in <ref type="table">Table.</ref> 1 on the more challenge test-dev set, our model also gets great results. In terms of M J , M F and M G , our method outperforms the state-of-the-art CINM [1] by 2.1%, 2.2% and 2.0% respectively, with neither CRF or MRF applied.</p><p>Improvement. Many previous works are troubled by occlusion, similar objects or fast motion. However, as shown in <ref type="figure">Fig. 5</ref>, our method handles these challenges well. In the case of similar objects like "carousel", which will be mistakenly switched identities by OSVOS <ref type="bibr" target="#b4">[5]</ref>, our propaga- tion proposals can track different instances well and identify each object. Also, we investigate that our method is robustly enough to the issues of fast motion and small instances, especially in "monkeys-tree" sequence. For the occlusion problem, we find that the segmentation on "salsa" performs identifiable which demonstrates the strong representation power of our model. The performances on these challenge sequences can also be illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>, where we achieve the state-of-the-art on almost all the videos. Ablation Study. <ref type="table">Table.</ref> 2 shows how much each presented component builds up to the final result. We start by the baseline model only with the motion score for pruning (w m = 1.0,w p = 0.0), and there is no no multiple hypotheses (N=1), no merge strategy (× in Merge, which means choose area with larger probability when conflict) and no traditional gating strategy <ref type="bibr" target="#b19">[20]</ref> (× in Gating) in addition. Results show that the hybrid scoring of motion and propagation achieves 4.8 higher than the original motion score. Multiple hypotheses and the conflicts handling strategy both make the maximum improvement of performance with 7.6, respectively. At last, our gating strategy brings another improvement of 2.2 instead of using Kalman Filter <ref type="bibr" target="#b17">[18]</ref>.</p><p>In the scoring phase, four hyper-parameters (w m , w p , w f and w n ) are introduced to balance the weights between the scores of motion and propagation, where w p = 1 − w m and w f = 1. We apply grid search on parameters w m ∈ [0, 1] and w n ∈ [−1, 0] with the step set as 0.1. Part of the grid search result is shown in <ref type="figure">Fig. 6</ref>. Experimental results show that MHP-VOS achieves the best result when w m = 0.3 and w n = −0.4. As the phase of proposal tree formation, we apply N-scan pruning with parameter N to control the delay time of proposal decision. In practice, N is an interesting parameter that makes a trades off between performance and speed. Shown as <ref type="table">Table.</ref> 3, lager decision delay time (N ) receives a performance boost, but gets the punishment in speed. We set N = 3 to achieve a balanced performance.  Weakness. Here we report typical examples of mistaken cases on DAVIS2017 test-dev. In the first video sequence, the segmentation of deer in the left (green) is partly missing, which is due to the similar appearance in the context pixels. The instance detector may regard the body of the deer to part of the tree and only generates the proposal of the head with the contrast background. Next in the middle sequence, we find that the racket is segmented well in previ- <ref type="figure">Figure 7</ref>. Mistaken cases on DAVIS2017 test-dev. Sequences correspond to "deer", "tennis-vest" and "people-sunset" respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Mean ous frames but missed in the later. This is because the proposed merging strategy that classifies the identity of overlap region wrongly in the ambiguous case. In the last video, the person in yellow is gradually switched to blue which means the proposal of this person is propagated wrongly during the tree building with two overlap bounding boxes of these disturbing objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">DAVIS2016</head><p>As illustrated in <ref type="table">Table.</ref> 4, our method achieves great progress with the M J , M F and M G of 85.7%, 88.1% and 86.9%, which outperforms the state-of-the-art OSVOS-S <ref type="bibr" target="#b25">[26]</ref> by 0.1%, 0.6% and 0.3% respectively. Compared to the traditional method MSK <ref type="bibr" target="#b28">[29]</ref>, our MHP-VOS improves a lot by 9.3% on the Global Mean M G . Also, we investigate that our performance is better than many latest models, like FAVOS <ref type="bibr" target="#b9">[10]</ref> and MoNet <ref type="bibr" target="#b39">[40]</ref>. Although our method performs well on DAVIS2016 validation set, there are not huge improvement between ours and the state-of-the-art models, for the reason that the proposal propagation is not essential for single object tracking, and the CNN-based segmentation module is capable enough to locate the foreground instance. As shown in <ref type="figure">Fig. 5</ref>, each target object has corresponding accurate segmentation even in motion blur or occlusion cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we presented a novel detection based Multiple Hypotheses Propagation (MHP-VOS) method for semisupervised video object segmentation. The key to MHP-VOS is that the decision for proposal in one frame is delayed to eliminate ambiguity with long-term information. Therefore, a hypothesis propagation tree was introduced to catch more potential proposals in each frame for tracking, with a novel class-agnostic gating and scoring strategy adapted to the VOS scenario. In addition, a novel conflicts handling method for multiple objects was proposed to transfer MHP-VOS to the multiple objects setting. Our experiments investigate performances of the pipeline and each component module, which are demonstrated to achieve significant performance gains compared against the state-of-the-arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 3 .</head><label>23</label><figDesc>The illustration of MHP at time k. (a) A gating example for propagation track of two objects from frame k − 1 to k. Bbox IOU scores between proposal from the current frame and the predicted bbox from the last frame are utilized as a gate with thresholds d th . (b) The corresponding propagation trees. Each tree node is associated with a proposal observation. (c) The undigraph for the example of (b), in which each node represents a propagation path in the tree and each edge connects two tracks that are conflicted. The black nodes in graph form the Maximum Weighted Independent Set (MWIS). (d) An N-scan pruning example when N = 2. The dark branches denote the global hypothesis at frame k, and the oblique lines represent the pruning of this branch which is far from the global hypothesis in k − N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Per-sequence results of metric G on the DAVIS2017 test-dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 Figure 6 .</head><label>36</label><figDesc>Segmentation qualities on DAVIS17 according to the two hyper-parameters: wn, wm. (a) Score versus wm when wn = −0.4. (b) Score versus wn when wm = 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Metric</cell><cell cols="7">OSMN [41] FAVOS [10] OSVOS [5] OnAVOS [37] OSVOS-S [26] CINM [1] Ours</cell></row><row><cell></cell><cell>J</cell><cell>Mean M ↑</cell><cell>52.5</cell><cell>54.6</cell><cell>56.6</cell><cell>61.6</cell><cell>64.7</cell><cell>67.2</cell><cell>71.8</cell></row><row><cell>validation</cell><cell cols="2">F Mean M ↑</cell><cell>57.1</cell><cell>61.8</cell><cell>63.9</cell><cell>69.1</cell><cell>71.3</cell><cell>74.0</cell><cell>78.8</cell></row><row><cell></cell><cell cols="2">G Mean M ↑</cell><cell>54.8</cell><cell>58.2</cell><cell>60.3</cell><cell>65.4</cell><cell>68.0</cell><cell>70.6</cell><cell>75.3</cell></row><row><cell></cell><cell></cell><cell>Mean M ↑</cell><cell>37.7</cell><cell>42.9</cell><cell>47.0</cell><cell>49.9</cell><cell>52.9</cell><cell>64.5</cell><cell>66.4</cell></row><row><cell></cell><cell>J</cell><cell>Recall R ↑</cell><cell>38.9</cell><cell>48.1</cell><cell>52.1</cell><cell>54.3</cell><cell>60.2</cell><cell>73.8</cell><cell>76.0</cell></row><row><cell></cell><cell></cell><cell>Decay D ↓</cell><cell>19.0</cell><cell>18.1</cell><cell>19.2</cell><cell>23.0</cell><cell>24.1</cell><cell>20.0</cell><cell>18.0</cell></row><row><cell>test-dev</cell><cell></cell><cell>Mean M ↑</cell><cell>44.9</cell><cell>44.2</cell><cell>54.8</cell><cell>55.7</cell><cell>62.1</cell><cell>70.5</cell><cell>72.7</cell></row><row><cell></cell><cell>F</cell><cell>Recall R ↑</cell><cell>47.4</cell><cell>51.1</cell><cell>59.7</cell><cell>60.3</cell><cell>70.5</cell><cell>79.6</cell><cell>82.2</cell></row><row><cell></cell><cell></cell><cell>Decay D ↓</cell><cell>17.4</cell><cell>19.8</cell><cell>19.8</cell><cell>23.4</cell><cell>21.9</cell><cell>20.0</cell><cell>19.0</cell></row><row><cell></cell><cell cols="2">G Mean M ↑</cell><cell>41.3</cell><cell>43.6</cell><cell>50.9</cell><cell>52.8</cell><cell>57.5</cell><cell>67.5</cell><cell>69.5</cell></row></table><note>Quantitative comparison of state-of-the-art methods on the DAVIS2017 validation and test-dev sets. The up-arrow ↑ means that larger is better while the down-arrow ↓ means that smaller is better. Our algorithm achieves the best performances on both sets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the DAVIS2017 test-dev set.</figDesc><table><row><cell cols="4">Settings wm wp N Merge Gating</cell><cell cols="2">Mean M Boost</cell></row><row><cell>1.0 0.0</cell><cell>1</cell><cell>×</cell><cell>×</cell><cell>47.3</cell><cell>-</cell></row><row><cell>0.3 0.7</cell><cell>1</cell><cell>×</cell><cell>×</cell><cell>52.1</cell><cell>4.8</cell></row><row><cell>0.3 0.7</cell><cell>3</cell><cell>×</cell><cell>×</cell><cell>59.7</cell><cell>7.6</cell></row><row><cell>0.3 0.7</cell><cell>3</cell><cell></cell><cell>×</cell><cell>67.3</cell><cell>7.6</cell></row><row><cell>0.3 0.7</cell><cell>3</cell><cell></cell><cell></cell><cell>69.5</cell><cell>2.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Trade-off effect of N-scan pruning on DAVIS2017.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison results on the DAVIS2016 validation set.</figDesc><table><row><cell></cell><cell></cell><cell>M</cell><cell cols="2">Recall R</cell></row><row><cell></cell><cell>MJ</cell><cell>MF MG</cell><cell>RJ</cell><cell>RF</cell></row><row><cell>OSMN [41]</cell><cell>74.0</cell><cell>72.9 73.5</cell><cell cols="2">87.6 84.0</cell></row><row><cell>PML [9]</cell><cell>75.5</cell><cell>79.3 77.4</cell><cell cols="2">89.6 93.4</cell></row><row><cell>MSK [29]</cell><cell>79.7</cell><cell>75.4 77.6</cell><cell cols="2">93.1 87.1</cell></row><row><cell>FAVOS [10]</cell><cell>82.4</cell><cell>79.5 81.0</cell><cell cols="2">96.5 89.4</cell></row><row><cell>RGMP [39]</cell><cell>81.5</cell><cell>82.0 81.8</cell><cell cols="2">91.7 90.8</cell></row><row><cell>CINM [1]</cell><cell>83.4</cell><cell>85.0 84.2</cell><cell cols="2">94.9 92.1</cell></row><row><cell>MoNet [40]</cell><cell>84.7</cell><cell>84.8 84.7</cell><cell cols="2">96.8 94.7</cell></row><row><cell>MGCRN [15]</cell><cell>84.4</cell><cell>85.7 85.1</cell><cell cols="2">97.1 95.2</cell></row><row><cell>OnAVOS [37]</cell><cell>86.1</cell><cell>84.9 85.5</cell><cell cols="2">96.1 89.7</cell></row><row><cell cols="2">OSVOS-S [26] 85.6</cell><cell>87.5 86.6</cell><cell cols="2">96.8 95.5</cell></row><row><cell>Ours</cell><cell>85.7</cell><cell>88.1 86.9</cell><cell cols="2">96.6 94.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cnn in mrf: Video object segmentation via inference in a cnn-based higher-order spatio-temporal mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5977" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal map labeling: A new unified framework with experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Niedermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nöllenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Strash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Design and analysis of modern tracking systems (artech house radar library). Artech house</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blackman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Popoli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multiple hypothesis tracking for multiple target tracking. IEEE Aerospace and Electronic Systems Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Blackman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="5" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<title level="m">The 2018 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02323</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An efficient implementation of reid&apos;s multiple hypothesis tracking algorithm and its evaluation for the purpose of visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Hingorani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motionguided cascaded refinement network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ASME-Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09554</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00197</idno>
		<title level="m">Video object segmentation with re-identification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Premvos: Proposalgeneration, refinement and merging for the davis challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The maximum weight independent set problem for data association in multiple hypothesis tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Salpukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization and Cooperative Control Strategies</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="235" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large kernel mattersimprove semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1743" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Phased local search for the maximum clique problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pullan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Optimization</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="323" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimisation of unweighted/weighted maximum independent sets and minimum vertex covers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pullan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Optimization</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="219" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Video object segmentation using tracked object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Smolyansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06545</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiple hypothesis video segmentation from superpixel flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vazquez-Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09364</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully motion-aware network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="542" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1140" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Efficient video object segmentation via network modulation. algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pixel-level matching for video object segmentation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2186" to="2195" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
