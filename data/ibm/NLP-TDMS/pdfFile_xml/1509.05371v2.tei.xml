<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeXpression: Deep Convolutional Neural Network for Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Burkert</surname></persName>
							<email>burkert11@cs.uni-kl.de</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Kaiserslautern</orgName>
								<address>
									<addrLine>Gottlieb-Daimler-Str</addrLine>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Trier</surname></persName>
							<email>ftrier10@cs.uni-kl.de</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Kaiserslautern</orgName>
								<address>
									<addrLine>Gottlieb-Daimler-Str</addrLine>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Zeshan Afzal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Kaiserslautern</orgName>
								<address>
									<addrLine>Gottlieb-Daimler-Str</addrLine>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
							<email>andreas.dengel@dfki.de</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Kaiserslautern</orgName>
								<address>
									<addrLine>Gottlieb-Daimler-Str</addrLine>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
							<email>liwicki@dfki.uni-kl.de!</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Kaiserslautern</orgName>
								<address>
									<addrLine>Gottlieb-Daimler-Str</addrLine>
									<postCode>67663</postCode>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">â€ </forename><surname>German</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Artificial Intelligence (DFKI)</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeXpression: Deep Convolutional Neural Network for Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a convolutional neural network (CNN) architecture for facial expression recognition. The proposed architecture is independent of any hand-crafted feature extraction and performs better than the earlier proposed convolutional neural network based approaches. We visualize the automatically extracted features which have been learned by the network in order to provide a better understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP) and MMI Facial Expression Databse are used for the quantitative evaluation. On the CKP set the current state of the art approach, using CNNs, achieves an accuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotion recognition is 93.33%. The proposed architecture achieves 99.6% for CKP and 98.63% for MMI, therefore performing better than the state of the art using CNNs. Automatic facial expression recognition has a broad spectrum of applications such as human-computer interaction and safety systems. This is due to the fact that non-verbal cues are important forms of communication and play a pivotal role in interpersonal communication. The performance of the proposed architecture endorses the efficacy and reliable usage of the proposed work for real world applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Humans use different forms of communications such as speech, hand gestures and emotions. Being able to understand one's emotions and the encoded feelings is an important factor for an appropriate and correct understanding.</p><p>With the ongoing research in the field of robotics, especially in the field of humanoid robots, it becomes interesting to integrate these capabilities into machines allowing for a more diverse and natural way of communication. One example is the Software called EmotiChat <ref type="bibr" target="#b0">[1]</ref>. This is a chat application with emotion recognition. The user is monitored and whenever an emotion is detected (smile, etc.), an emoticon is inserted into the chat window. Besides Human Computer Interaction other fields like surveillance or driver safety could also profit from it. Being able to detect the mood of the driver could help to detect the level of attention, so that automatic systems can adapt. Many methods rely on extraction of the facial region. This can be realized through manual inference <ref type="bibr" target="#b1">[2]</ref> or an automatic detection approach <ref type="bibr" target="#b0">[1]</ref>. Methods often involve the Facial Action Coding System (FACS) which describes the facial expression using Action Units (AU). An Action Unit is a facial action like "raising the Inner Brow". Multiple activations of AUs describe the facial expression <ref type="bibr" target="#b2">[3]</ref>. Being able to correctly detect AUs is a helpful step, since it allows making a statement about the activation level of the corresponding emotion. Handcrafted facial landmarks can be used such as done by Kotsia et al. <ref type="bibr" target="#b1">[2]</ref>. Detecting such landmarks can be hard, as the distance between them differs depending on the person <ref type="bibr" target="#b3">[4]</ref>. Not only AUs can be used to detect emotions, but also texture. When a face shows an emotion the structure changes and different filters can be applied to detect this <ref type="bibr" target="#b3">[4]</ref>.</p><p>The presented approach uses Artificial Neural Networks (ANN). ANNs differ, as they are trained on the data with less need for manual interference. Convolutional Neural Networks are a special kind of *F. Trier and P. Burkert contributed equally to this work. arXiv:1509.05371v2 [cs.CV] 17 Aug 2016 ANN and have been shown to work well as feature extractor when using images as input <ref type="bibr" target="#b4">[5]</ref> and are real-time capable. This allows for the usage of the raw input images without any pre-or postprocessing. GoogleNet <ref type="bibr" target="#b5">[6]</ref> is a deep neural network architecture that relies on CNNs. It has been introduced during the Image Net Large Scale Visual Recognition Challenge(ILSVRC) 2014. This challenge analyses the quality of different image classification approaches submitted by different groups. The images are separated into 1000 different classes organized by the WordNet hierarchy. In the challenge "object detection with additional training data" GoogleNet has achieved about 44% precision <ref type="bibr" target="#b7">[7]</ref>. These results have demonstrated the potential which lies in this kind of architecture. Therefore it has been used as inspiration for the proposed architecture. The proposed network has been evaluated on the Extended Cohn-Kanade Dataset (Section 4.2) and on the MMI Dataset (Section 4.1). Typical pictures of persons showing emotions can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref>. The emotion Contempt of the CKP set is not shown as no subject with consent for publication and an annotated emotion is part of the dataset. Results of experiments on these datasets demonstrate the success of using a deep layered neural network structure. With a 10-fold cross-validation a recognition accuracy of 99.6% has been achieved.</p><p>The paper is arranged as follows: After this introduction, Related Work (Section 2) is presented which focuses on Emotion/Expression recognition and the various approaches scientists have taken. Next is Section 3, Background, which focuses on the main components of the architecture proposed in this article. Section 4 contains a summary of the used Datasets. In Section 5 the architecture is presented. This is followed by the experiments and its results (Section 6) . Finally, Section 8 summarizes the article and concludes the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>A detailed overview for expression recognition was given by CÈƒleanu <ref type="bibr" target="#b9">[8]</ref> and Bettadapura <ref type="bibr" target="#b10">[9]</ref>. In this Section mainly work which similar to the proposed method is presented as well as few selected articles which give a broad overview over the different methodologies.</p><p>Recently Szegedy et al. <ref type="bibr" target="#b5">[6]</ref> have proposed an architecture called GoogLeNet. This is a 27 layer deep network, mostly composed of CNNs. The network is trained using stochastic gradient descent. In the ILSVRC 2014 Classification Challenge this network achieved a top-5 error rate of 6.67% winning the first place. Using the the Extended Cohn-Kanade Dataset (Section 4.2), Happy and Routray <ref type="bibr" target="#b3">[4]</ref> classify between six basic emotions. Given an input image, their solution localizes the face region. From this region, facial patches e.g. the eyes or lips are detected and points of interest are marked. From the patches which have the most variance between two images, features are extracted. The dimensionality of the features is reduced and then given to a Support Vector Machine (SVM). To evaluate the method, a 10-fold cross-validation is applied. The average accuracy is 94.09%. Video based emotion recognition has been proposed by Byeon and Kwak <ref type="bibr" target="#b11">[10]</ref>. They have developed a three dimensional CNN which uses groups of 5 consecutive frames as input. A database containing 10 persons has been used to achieve an accuracy of 95%. Song et al. <ref type="bibr" target="#b12">[11]</ref> have used a deep convolutional neural network for learning facial expressions. The created network consists of five layers with a total of 65k neurons. Convolutional, pooling, local filter layers and one fully connected layer are used to achieve an accuracy of 99.2% on the CKP set. To avoid overfitting the dropout method was used. Luecy et al. <ref type="bibr" target="#b13">[12]</ref> have created the Extended Cohn-Kanade dataset. This dataset contains emotion annotations as well as Action Unit annotations. In regards to classification, they also have evaluated the datasets using Active Appearance Models (AAMs) in combination with SVMs. To find the position and track the face over different images, they have employed AAM which generates a Mesh out of the face. From this mesh they have extracted two feature vectors. First, the normalized vertices with respect to rotation, translation, and scale. Second a gray-scale image from the mesh data, and the input images has been extracted. They have chosen a cross-validation strategy, where one subject is left out in the training process, achieving an accuracy of over 80%. Anderson et al. <ref type="bibr" target="#b0">[1]</ref> have developed a face expression system, which is capable of recognizing the six basic emotions. Their system is built upon three components. The first one is a face tracker (derivative of ratio template) to detect the location of the face. The second component is an optical flow algorithm to track the motion within the face. The last component is the recognition engine itself. It is based upon Support Vector Machines and Multilayer Perceptrons. This approach has been implemented in EmotiChat. They achieve a recognition accuracy of 81.82%. Kotsia and Pitas <ref type="bibr" target="#b1">[2]</ref> detect emotions by mapping a Candide grid, a face mask with a low number of polygons, onto a person's face. The grid is initially placed randomly on the image, then it has to be manually placed on the persons face. Throughout the emotion, the grid is tracked using a KanadeLucasTomasi tracker. The geometric displacement information provided by the grid is used as feature vector for multiclass SVMs. The emotions are anger, disgust, fear, happiness, sadness, and surprise. They evaluate the model on the Cohn-Kanade dataset and an accuracy of 99.7% has been achieved. Shan et al. <ref type="bibr" target="#b15">[13]</ref> have created an emotion recognition system based on Local Binary Patterns (LBP). The LBPs are calculated over the facial region. From the extracted LBPs a feature vector is derived. The features depend on the position and size of the sub-regions over witch the LBP is calculated. AdaBoost is used to find the sub-regions of the images which contain the most discriminative information. Different classification algorithms have been evaluated of which an SVM with Boosted-LBP features performs the best with a recognition accuracy of 95.1% on the CKP set. In 2013 Zafar et al. <ref type="bibr" target="#b16">[14]</ref> proposed an emotion recognition system using Robust Normalized Cross Correlation (NCC). The used NCC is the "Correlation as a Rescaled Variance of the Difference between Standardized Scores". Outlier pixels which influence the template matching too strong or too weak are excluded and not considered. This approach has been evaluated on different databases including AR FaceDB (85% Recognition Accuracy) and the Extended Cohn Kanade Database (100% Recognition Accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONVOLUTIONAL NEURAL NETWORKS</head><p>Convolutional Layer: Convolutional Layers perform a convolution over the input. Let f k be the filter with a kernel size n Ã— m applied to the input x. n Ã— m is the number of input connections each CNN neuron has. The resulting output of the layer calculates as follows:</p><formula xml:id="formula_0">C(x u,v ) = n 2 i=âˆ’ n 2 m 2 j=âˆ’ m 2 f k (i, j)x uâˆ’i,vâˆ’j (1)</formula><p>To calculate a more rich and diverse representation of the input, multiple filters f k with k âˆˆ N can be applied on the input. The filters f k are realized by sharing weights of neighboring neurons. This has the positive effect that lesser weights have to be trained in contrast to standard Multilayer Perceptrons, since multiple weights are bound together.</p><p>Max Pooling: Max Pooling reduces the input by applying the maximum function over the input x i . Let m be the size of the filter, then the output calculates as follows:</p><formula xml:id="formula_1">M (x i ) = max{x i+k,i+l | |k| â‰¤ m 2 , |l| â‰¤ m 2 k, l âˆˆ N} (2)</formula><p>This layer features translational invariance with respect to the filter size.</p><p>Rectified Linear Unit: A Rectified Linear Unit (ReLU) is a cell of a neural network which uses the following activation function to calculate its output given x:</p><formula xml:id="formula_2">R(x) = max(0, x) (3)</formula><p>Using these cells is more efficient than sigmoid and still forwards more information compared to binary units. When initializing the weights uniformly, half of the weights are negative. This helps creating a sparse feature representation. Another positive aspect is the relatively cheap computation. No exponential function has to be calculated. This function also prevents the vanishing gradient error, since the gradients are linear functions or zero but in no case non-linear functions <ref type="bibr" target="#b17">[15]</ref>.</p><p>Fully Connected Layer: The fully connected layer also known as Multilayer Perceptron connects all neurons of the prior layer to every neuron of its own layer. Let the input be x with size k and l be the number of neurons in the fully connected layer. This results in a Matrix W lÃ—k .</p><formula xml:id="formula_3">F (x) = Ïƒ(W * x) (4)</formula><p>Ïƒ is the so called activation function. In our network Ïƒ is the identity function.</p><p>Output Layer: The output layer is a one hot vector representing the class of the given input image. It therefore has the dimensionality of the number of classes. The resulting class for the output vector x is:</p><formula xml:id="formula_4">C(x) = {i | âˆƒiâˆ€j = i : x j â‰¤ x i } (5)</formula><p>Softmax Layer: The error is propagated back over a Softmax layer. Let N be the dimension of the input vector, then Softmax calculates a mapping such that:</p><formula xml:id="formula_5">S(x) : R N â†’ [0, 1] N</formula><p>For each component 1 â‰¤ j â‰¤ N , the output is calculated as follows:</p><formula xml:id="formula_6">S(x) j = e xj N i=1 e xi<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATASETS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MMI Dataset</head><p>The MMI dataset has been introduced by Pantic et al. <ref type="bibr" target="#b18">[16]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CKP Dataset</head><p>This dataset has been introduced by Lucey et al. <ref type="bibr" target="#b13">[12]</ref>. are Afro-Americans. The images are of size 640Ã—490 px as well 640Ã—480 px. They are both grayscale and colored. In total this set has 593 emotion-labeled sequences. The emotions consist of Anger, Disgust, Fear, Happiness, Sadness, Surprise, and Contempt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison</head><p>In the MMI Dataset <ref type="figure" target="#fig_2">(Fig. 2)</ref> the emotion Anger is displayed in different ways, as can be seen by the eyebrows, forehead and mouth. The mouth in the lower image is tightly closed while in the upper image the mouth is open. For Disgust the differences are also visible, as the woman in the upper picture has a much stronger reaction. The man depicting Fear has contracted eyebrows which slightly cover the eyes. On the other hand the eyes of the woman are wide open. As for Happy both persons are smiling strongly. In the lower image the woman depicting Sadness has a stronger lip and chin reaction. The last emotion Surprise also has differences like the openness of the mouth.</p><p>Such differences also appear in the CKP set ( <ref type="figure">Fig. 3)</ref>. For Anger the eyebrows and cheeks differ. For Disgust larger differences can be seen. In the upper picture not only the curvature of the mouth is stronger, but the nose is also more involved. While both women displaying Fear show the same reaction around the eyes the mouth differs. In the lower image the mouth is nearly closed while teeth are visible in the upper one. Happiness is displayed similar. For the emotion Sadness the curvature of the mouth is visible in both images, but it is stronger in the upper one. The regions around the eyes differ as the eyebrows of the woman are straight. The last emotion Surprise has strong similarities like the open mouth an wide open eyes. Teeth are only displayed by the woman in the upper image. Thus for a better evaluation it is helpful to investigate multiple datasets. This aims at investigating whether the proposed approach works on different ways emotions are shown and whether it works on different emotions. For example Contempt which is only included in the CKP set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PROPOSED ARCHITECTURE</head><p>The proposed deep Convolutional Neural Network architecture (depicted in <ref type="figure">Figure 4</ref>) consists of four parts. The first part automatically preprocesses the data. This begins with Convolution 1, which applies 64 different filters. The next layer is Pooling 1, which down-samples the images and then they are normalized by LRN 1. The next steps are the two FeatEx (Parallel Feature Extraction Block) blocks, highlighted in <ref type="figure">Figure 4</ref>. They are the core of the proposed architecture and described later in this section. The features extracted by theses blocks are forwarded to a fully connected layer, which uses them to classify the input into the different emotions. The described architecture is compact, which makes it not only fast to train, but also suitable for real-time applications. This is also important as the network was built with resource usage in mind. FeatEx:</p><p>The key structure in our architecture is the Parallel Feature Extraction Block (FeatEx). It is inspired by the success of GoogleNet. The block consists of Convolutional, Pooling, and ReLU Layers. The first Convolutional layer in FeatEx reduces the dimension since it convolves with a filter of size 1 Ã— 1. It is enhanced by a ReLU layer, which creates the desired sparseness. The output is then convolved with a filter of size 3 Ã— 3. In the parallel path a Max Pooling layer is used to reduce information before applying a CNN of size 1 Ã— 1. This application of differently sized filters reflects the various scales at which faces can appear. The paths are concatenated for a more diverse representation of the input. Using this block twice yields good results.</p><p>Visualization: The different layers of the architecture produce feature vectors as can be seen in <ref type="figure" target="#fig_4">Fig 5.</ref> The first part until LRN 1 preprocesses the data and creates multiple modified instances of the input. These show mostly edges with a low level of abstraction. The first FeatEx block creates two parallel paths of features with different scales, which are combined in Concat 2. The second FeatEx block refines the representation of the features. It also decreases the dimensionality. This visualization shows that the concatenation of FeatEx blocks is a valid approach to create an abstract feature representation. The output dimensionality of each layer can be seen in <ref type="table" target="#tab_0">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS AND RESULTS</head><p>As implementation Caffe <ref type="bibr" target="#b19">[17]</ref> was used. This is a deep learning framework, maintained by the Berkeley Vision and Learning Center (BVLC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CKP:</head><p>The CKP database has been analyzed often and many different approaches have been evaluated in order to "solve" this set. To determine whether the architecture is competitive, it has been evaluated on the CKP dataset. For the experiments all 5870 annotated images have been used to do a 10-fold cross-validation. The proposed architecture has proven to be very effective on this dataset with an average accuracy of 99.6%. In <ref type="table">Table 2</ref> different results from state of the art approaches are listed as comparison. The 100% accuracy reported by Zafar <ref type="bibr" target="#b16">[14]</ref> is based on hand picked images. The results are not validated using cross-validation. The confusion matrix in <ref type="figure">Fig. 6a</ref> depicts the results and shows that some emotions are perfectly recognized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMI:</head><p>The MMI Database contains videos of people showing emotions. From each video the 20 frames, which represent the content of the video the most, have been extracted fully automatically. The first two of these frames have been discarded since they provide neutral expressions.</p><p>To determine the frames, the difference between grayscale consecutive frames was calculated. To compensate noise the images have been smoothed using a Gaussian filter before calculation. To find the 20 most representative images, changes which occur in a small timeframe, should only be represented by a single image. This was achieved by iterating over the differences using a maximum filter with decreasing filter size until 20 frames have been found. In total 3740 images have been extracted. The original images were then used for training and testing. A 10-fold cross-validation has been applied. The average accuracy is 98.63%. This is better than the accuracies achieved by Wang and Yin <ref type="bibr" target="#b21">[19]</ref>  <ref type="table" target="#tab_1">(Table 3</ref>). To our knowledge they have been the only ones to evaluate the MMI database on Emotions instead of Action Units. The results of the proposed approach are depicted in the Confusion Matrix in <ref type="figure">Fig. 6b</ref>. In the figure it is shown that the accuracy for Fear is the lowest with 93.75% while Happiness is almost perfectly recognized with 98.21%. Fear and Surprise are the emotions confused the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>The accuracy on the CKP set shows that the chosen approach is robust, misclassification usually occurs on pictures which are the first few instances of an emotion sequence. Often a neutral facial expression is depicted in those frames. Thus those misclassifications are not necessarily an error in the approach, but in the data selection. Other than that no major problem could be TABLE 2: The CKP database has been very well analyzed and the best possible recognition accuracy has been achieved by Aliya Zafar. It is noteworthy that the samples he used for training are not randomly selected and no cross-validation has been applied. Evaluating this database provides information whether the proposed approach can compete with those results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author</head><p>Method Accuracy Aliya Zafar <ref type="bibr" target="#b16">[14]</ref> NCC 100% Happy et al. <ref type="bibr" target="#b3">[4]</ref> Facial Patches + SVM 94.09% Lucey et al. <ref type="bibr" target="#b13">[12]</ref> AAM + SVM â‰¥ 80% Song et al. <ref type="bibr" target="#b20">[18]</ref> ANN (CNN) 99.2% DeXpression(Proposed) 99.6% Similar effects are experienced when dealing with the MMI Dataset. Since the first two frames are discarded most pictures with neutral positions are excluded. In few images a neutral position can still be found which gives rise to errors. For the same reason as the CKP set images will not be displayed. Due to the approach to extract images of the videos, a unique identifier for the misclassified image cannot be provided. The top confusions are observed for Fear and Surprise with a rate of 0.0159% where Fear is wrongly misclassified as Surprise. Session 1937 shows a woman displaying Fear but it is classified as Surprise. Both share common features like similar eye and mouth movement. In both emotions, participants move the head slightly backwards. This can be identified by wrinkled skin. The second most confusion rate, Surprise being mistaken as Sadness, is mostly based on neutral position images. Although the first two images are not used, some selected frames still do not contain an emotion. In Session 1985 Surprise is being mistaken as Sadness. The image depicts a man with his mouth being slightly curved, making him look sad.</p><p>DeXpression extracts features and uses them to classify images, but in very few cases the emotions are confused. This happens, as discussed, usually in pictures depicting no emotion. DeXpression performs very well on both tested sets, if an emotion is present.</p><p>(a) The confusion matrix of the averaged 10-fold crossvalidation on the CKP Dataset. The lowest accuracy is achieved by the emotion Surprise with 98.79% while Contempt/Sadness are both recognized with 100%.</p><p>(b) The confusion matrix of the averaged 10-fold crossvalidation on the MMI Dataset. The lowest accuracy is achieved by the emotion Fear with 93.75%. Happiness is recognized with 98.21%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>In this article DeXpression is presented which works fully automatically. It is a neural network which has little computational effort compared to current state of the art CNN architectures. In order to create it the new composed structure FeatEx has been introduced. It consists of several Convolutional layers of different sizes, as well as Max Pooling and ReLU layers. FeatEx creates a rich feature representation of the input. The results of the 10-fold cross-validation yield, in average, a recognition accuracy of 99.6% on the CKP dataset and 98.36% on the MMI dataset. This shows that the proposed architecture is capable of competing with current state of the art approaches in the field of emotion recognition. In Section 7 the analysis has shown, that DeXpression works without major mistakes. Most misclassifications have occurred during the first few images of an emotion sequence. Often in these images emotions are not yet displayed.</p><p>Future Work: An application built on DeXpression which is used in a real environment could benefit from distinguishing between more emotions such as Nervousness and Panic. Such a scenario could be large events where an early detection of Panic could help to prevent mass panics. Other approaches to enhance emotion recognition could be to allow for composed emotions. For example frustration can be accompanied by anger, therefore not only showing one emotion, but also the reason. Thus complex emotions could be more valuable than basic ones. Besides distinguishing between different emotions, also the strength of an emotion could be considered. Being able to distinguish between different levels could improve applications, like evaluating reactions to new products. In this example it could predict the amount of orders that will be made, therefore enabling producing the right amount of products.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Example images from the MMI (top) and CKP (bottom). The emotions from left to right are: Anger, Sadness, Disgust, Happiness, Fear, Surprise. The emotion Contempt of the CKP set is not displayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>contains over 2900 videos and images of 75 persons. The annotations contain action units and emotions. The database contains a web-interface with an integrated search to scan the database. The videos/images are colored. The people are of mixed age, different gender and have different ethnical background. The emotions investigated are the six basic emotions: Anger, Disgust, Fear, Happiness, Sadness, Surprise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>210 persons, aged 18 to 50, have been recorded depicting emotions. This dataset presented by contains recordings of emotions of 210 persons at the ages of 18 to 50 years. Both female and male persons are present and from different background. 81% are Euro-Americans and 13% This Figure shows the differences within the MMI dataset. The six used emotions are listed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>This Figure showsthe differences within the Cohn-Kanade Plus (CKP) dataset. The emotion Contempt is not shown since there is no annotated image with the emotion being depicted, which is allowed to be displayed. This is the proposed architecture. The main component of this architecture is the FeatEx block. In the Convolutional layer S depicts the Stride and P the Padding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>This Figure shows example visualizations of the different layers. The data is taken from the MMI set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>This Table liststhe different output sizes produced by each layer.</figDesc><table><row><cell>Layer</cell><cell>Output Size</cell></row><row><cell>Data</cell><cell>224 Ã— 224</cell></row><row><cell>Convolution 1</cell><cell>64 Ã— 112 Ã— 112</cell></row><row><cell>Pooling 1</cell><cell>64 Ã— 56 Ã— 56</cell></row><row><cell>LRN 1</cell><cell>64 Ã— 56 Ã— 56</cell></row><row><cell>Convolution 2a</cell><cell>96 Ã— 56 Ã— 56</cell></row><row><cell cols="2">Convolution 2b 208 Ã— 56 Ã— 56</cell></row><row><cell>Pooling 2a</cell><cell>64 Ã— 56 Ã— 56</cell></row><row><cell>Convolution 2c</cell><cell>64 Ã— 56 Ã— 56</cell></row><row><cell>Concat 2</cell><cell>272 Ã— 56 Ã— 56</cell></row><row><cell>Pooling 2b</cell><cell>272 Ã— 28 Ã— 28</cell></row><row><cell>Convolution 3a</cell><cell>96 Ã— 28 Ã— 28</cell></row><row><cell cols="2">Convolution 3b 208 Ã— 28 Ã— 28</cell></row><row><cell>Pooling 3a</cell><cell>272 Ã— 28 Ã— 28</cell></row><row><cell>Convolution 3c</cell><cell>64 Ã— 28 Ã— 28</cell></row><row><cell>Concat 3</cell><cell>272 Ã— 28 Ã— 28</cell></row><row><cell>Pooling 3b</cell><cell>282 Ã— 14 Ã— 14</cell></row><row><cell>Classifier</cell><cell>11 Ã— 1 Ã— 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 3 :</head><label>3</label><figDesc>This Table summarizes the current state of the art in emotion recognition on the MMI database (Section 4.1). Fear while the annotated emotion corresponds to Surprise. The image depicts a person with a wide open mouth and open eyes. Pictures representing Surprise are often very similar, since the persons also have wide open mouths and eyes. In image S032 004 00000014 the targeted label Fear is confused with Anger. While the mouth region in pictures with Anger differ, the eye regions are alike, since in both situations the eyes and eyebrows are contracted.</figDesc><table><row><cell>Author</cell><cell cols="2">Method Accuracy</cell></row><row><cell>Wang and Yin [19]</cell><cell>LDA</cell><cell>93.33%</cell></row><row><cell>Wang and Yin [19]</cell><cell>QDC</cell><cell>92.78%</cell></row><row><cell>Wang and Yin [19]</cell><cell>NBC</cell><cell>85.56%</cell></row><row><cell>DeXpression (Proposed)</cell><cell></cell><cell>98.63%</cell></row><row><cell cols="3">detected. The emotion Surprise is often confused with</cell></row><row><cell cols="3">Disgust with a rate of 0.045% which is the highest. Of</cell></row><row><cell cols="3">those images, where an emotion is present, only few</cell></row><row><cell>are wrongly classified.</cell><cell></cell><cell></cell></row><row><cell cols="3">As there is no consent for the misclassified images,</cell></row><row><cell cols="3">they cannot be depicted here. However some unique</cell></row><row><cell>names are provided.</cell><cell></cell><cell></cell></row><row><cell cols="3">Image S119 001 00000010 is classified as</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the Affect Analysis Group of the University of Pittsburgh for providing the Extended CohnKanade database, and Prof. Pantic and Dr. Valstar for the MMI data-base.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A real-time automated system for recognition of human facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="page" from="96" to="105" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>IEEE Trans. Syst.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Facial expression recognition in image sequences using geometric deformation features and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="187" />
			<date type="published" when="2007-01" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face expression recognition and analysis: the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Course Paper, Visual Interfaces to Computer</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition using features of salient facial patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Happy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Routray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
	<note>Affective Computing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<ptr target="http://arxiv.org/abs/1409.4842" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Results of the lsvrc challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsvrc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<ptr target="http://www.image-net.org/challenges/LSVRC/2014/results" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face expression recognition: A brief overview of the last decade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-D</forename><surname>Caleanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied Computational Intelligence and Informatics (SACI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="157" to="161" />
		</imprint>
	</monogr>
	<note>IEEE 8th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Face expression recognition and analysis: the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bettadapura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1203.6722</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Facial expression recognition using 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Kwak</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning for realtime robust facial expression recognition on a smartphone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer Electronics (ICCE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="564" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<biblScope unit="page">2010</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Conference on</title>
		<imprint>
			<biblScope unit="page" from="94" to="101" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on local binary patterns: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face recognition with expression variation via robust ncc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iqbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 9th International Conference on</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Emerging Technologies (ICET)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>G. J. Gordon and D. B. Dunson</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
	<note>Journal of Machine Learning Research -Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Webbased database for facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Int&apos;l Conf. Multimedia and Expo (ICME&apos;05)</title>
		<meeting>IEEE Int&apos;l Conf. Multimedia and Expo (ICME&apos;05)<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07" />
			<biblScope unit="page" from="317" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning for real-time robust facial expression recognition on a smartphone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer Electronics (ICCE), 2014 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2014-01" />
			<biblScope unit="page" from="564" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Static topographic modeling for facial expression recognition and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="19" to="34" />
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
