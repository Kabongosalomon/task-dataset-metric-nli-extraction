<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-Shot Neural Architecture Search via Self-Evaluated Template Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
							<email>xuanyi.dxy@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baidu</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">One-Shot Neural Architecture Search via Self-Evaluated Template Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural architecture search (NAS) aims to automate the search procedure of architecture instead of manual design. Even if recent NAS approaches finish the search within days, lengthy training is still required for a specific architecture candidate to get the parameters for its accurate evaluation. Recently one-shot NAS methods are proposed to largely squeeze the tedious training process by sharing parameters across candidates. In this way, the parameters for each candidate can be directly extracted from the shared parameters instead of training them from scratch. However, they have no sense of which candidate will perform better until evaluation so that the candidates to evaluate are randomly sampled and the top-1 candidate is considered the best. In this paper, we propose a Self-Evaluated Template Network (SETN) to improve the quality of the architecture candidates for evaluation so that it is more likely to cover competitive candidates. SETN consists of two components:</p><p>(1) an evaluator, which learns to indicate the probability of each individual architecture being likely to have a lower validation loss. The candidates for evaluation can thus be selectively sampled according to this evaluator. (2) a template network, which shares parameters among all candidates to amortize the training cost of generated candidates. In experiments, the architecture found by SETN achieves the state-of-the-art performance on CIFAR and ImageNet benchmarks within comparable computation costs. Code is publicly available on GitHub: https://github.com/ D-X-Y/AutoDL-Projects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Representation learning <ref type="bibr" target="#b4">[5]</ref> is a fundamental research problem in computer vision, because it is beneficial to a variety of computer vision applications, such as detection and segmentation. Due to the success of deep learning, it has undergone a transition from "feature engineering" <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10]</ref>  Architectures for evaluation are generated by sampling candidates in the template network using the evaluator. The template network shares the parameters for different candidates as indicated by connections of arrow lines in different colors. The evaluator learns the distribution of the architectures being likely to have a lower validation loss.</p><p>to "architecture engineering" <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12]</ref>. However, a large amount of expert knowledge and ample computational resources are still required to secure an architecture for good feature representations <ref type="bibr" target="#b42">[43]</ref>. Fortunately, neural architecture search (NAS) brings hope to deep learning researchers and alleviate their labours <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>The goal of NAS is to discover an optimal network in the search space, which can maximize the validation accuracy after training. Typical algorithms apply reinforcement learning (RL) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b8">9]</ref> or evolutionary strategy (EA) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31]</ref> to solve this problem, and most are computationally expensive, e.g., 500 GPUs over four days <ref type="bibr" target="#b43">[44]</ref>. Such huge computational costs motivate the researchers to focus on efficient architecture search algorithms. Recently, several works reduced the computational cost through weight sharing <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b6">7]</ref>, weight generation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4]</ref>, accuracy prediction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>, progressive strategy <ref type="bibr" target="#b22">[23]</ref>, etc.</p><p>One-shot NAS approaches stand out among these efficient NAS approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>, because they can significantly squeeze the tedious training process by sharing pa-rameters across architecture candidates. A typical one-shot NAS paradigm is to <ref type="bibr" target="#b0">(1)</ref> randomly sample hundreds of architecture candidates from the parameter-shared network; <ref type="bibr" target="#b1">(2)</ref> evaluate these candidates; and (3) find the candidate with the highest validation accuracy. We observed that most of these randomly sampled candidates are useless, since most of them have a poor performance. Besides, these sampled candidates are only a small portion of the whole search space; therefore the probability for inclusion of the best architecture is extremely low.</p><p>To solve the above problem, we propose Self-Evaluated Template Network (SETN) for one-shot NAS. SETN equips a template network with an evaluator. The template network contains all possible candidate convolutional neural networks (CNNs) and shares its parameters (template parameters) with all candidates. We train this template network in a stochastic strategy: during one iteration, we uniformly sample one candidate and only optimize partial template parameters for this sampled candidate. In this way, after training, each candidate CNN can directly use the corresponding template parameters without additional training. Some previous methods coupled the shared parameters with a learnable distribution of architectures, such as <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>. These methods will introduce the bias into the template parameters. Since some shallow and light-weight network will quickly converged, the learnable distribution will bias to these "simple" networks, and other networks may not have a chance to be updated. In contrast to them, our uniformly stochastic training strategy allows each candidate to be treated equally. In this way, each candidate with shared template parameters would be trained fully, and their validation accuracy would be closer to the ground truth validation accuracy.</p><p>The evaluator learns to indicate the probability of each individual candidate CNN being likely to have a lower validation loss. We train this evaluator on the validation data with the assistance of the template network. After training, according to the learned probability, our evaluator can pick up low-validation-loss candidates for one-shot evaluation. As a result, the probability of including the best CNN in the search space can be dramatically improved. Compared to previous random sampling algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>, SETN can potentially search for a CNN with a higher accuracy.</p><p>In experiments, SETN can discover a superior CNN on CIFAR-10 within two GPU days. This SETN-searched architecture achieves state-of-the-art performance on three benchmarks, i.e., CIFAR-10, CIFAR-100, and ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Automatically discovering effective networks has attracted more and more researchers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13]</ref>. Various kinds of searching algorithms have been proposed, such as RL-based <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b29">30]</ref>, EA-based <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, gradient-Search Eff. Share Eval. Gen. ResNet <ref type="bibr" target="#b17">[18]</ref> Manual <ref type="table">Table 1</ref>. We compare different algorithms with five aspects. "Search" shows the search algorithm type. "Eff." indicates efficient, and we consider the algorithm that can discover CNN within five GPU days as efficient. "Share" indicates whether the algorithm shares parameters over different candidate networks or not. "Eval." means whether the algorithm is able to quickly evaluate a network. "Gen." indicates the network sampling strategy during the one-shot evaluation procedure. "-" indicates not available, which means those methods do not have these steps.</p><formula xml:id="formula_0">× − − − MetaQNN [1] RL × × − − NASNet [44] RL × × − − AmoebaNet [31] EA × × − − H-NAS [24] EA × × − − EAS [7] RL √ √ − − PNAS [23] SMBO × × slow − SMASH [6] Gradient √ × quick random Understand [4] Gradient √ √ quick random DARTS [25] Gradient √ √ − − GDAS [14] Gradient √ √ − − Our SETN Gradient √ √ quick selective</formula><p>based <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> approaches. Each of these algorithm has its unique advantage. For simplicity, we summarize some ubiquitous algorithms in <ref type="table">Table 1</ref> regarding five aspects. Our SETN has advantages when compared to them, and we will introduce these related works at below. Note that we focus on searching CNN models, and thus approaches about recurrent neural network or downstream applications are out of the scope of this paper. Early approaches train a large amount of candidate networks by tens epochs and use the validation accuracy of these networks as the supervisory signal <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b31">32]</ref>. For example, Zoph et al. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> learn an RL policy to sample networks with high accuracy. Real et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref> utilize EA algorithms to mutate low-quality networks to highquality networks. Unfortunately, these approaches cost too much computational resources. Recent approaches aim to solve the searching problem in an affordable computation cost. Liu et al. <ref type="bibr" target="#b22">[23]</ref> progressively search CNN from simple to complex. Barker et al. <ref type="bibr" target="#b1">[2]</ref> accelerate the searching procedure by performance prediction. Pham et al. <ref type="bibr" target="#b29">[30]</ref> share the parameters of different networks. Liu et al. <ref type="bibr" target="#b24">[25]</ref> propose DARTS, allowing efficient search of the architecture using gradient descent. DARTS <ref type="bibr" target="#b24">[25]</ref> and GDAS <ref type="bibr" target="#b13">[14]</ref> choose the best architecture using the arg max over a continues architecture representation, while the performance of an architecture cannot be correctly estimated without fully training it. Our SETN can more accurately estimate the performance (validation loss) of all candidates without separately training each of them one by one.</p><p>Our SETN is one-shot NAS, and is closely related to previous one-shot approaches <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>. Brock et al. <ref type="bibr" target="#b5">[6]</ref> train hypernetworks <ref type="bibr" target="#b15">[16]</ref> to generate suitable weights for every network in the search space. Zhang et al. <ref type="bibr" target="#b40">[41]</ref> encode the network as a computation graph and use a graph neural network to predict weights. Bender et al. <ref type="bibr" target="#b3">[4]</ref> deliver thorough experimental analysis for one-shot architecture search. These approaches can estimate the network performance correctly without additional training, while the architecture candidates for evaluation are randomly picked with uneven quality <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>. Our approach can selectively sample low-validation-loss architecture candidates. In this way, our sampled candidates would have much lower validation loss (better performance) than that of the random strategy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>, and thus we can potentially discover a more effective CNN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>Early works search for the whole CNN structure <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b2">3]</ref>, whereas recent works propose that finding a good neural cell is more effective than finding a whole CNN <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b13">14]</ref>. Therefore, we also search for a good cell instead of a full CNN model. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, a cell is a fully convolutional structure, mapping a tensor I in ∈ R H×W ×C to another tensor I out ∈ R H ×W ×C . If we use the stride of 1, the cell is named normal cell, which has (H , W , F )=(H, W, F ); and if we use the stride of 2, the cell is named reduction cell, which has (H , W , F )=( H 2 , W 2 , 2F ). Each cell contains B nodes, where each of them is specified as a quadruple (I 1 , I 2 , f 1 , f 2 ) <ref type="bibr" target="#b22">[23]</ref>. Specifically, the i-th node in the cthe cell takes two inputs I 1 , I 2 ∈ I c i and generates a tensor</p><formula xml:id="formula_1">H c i = f 1 (I 1 ) + f 2 (I 2 )</formula><p>. f 1 , f 2 ∈ O are transformation functions to apply to inputs. The output of the c-th cell is the concatenation of each intermediate output tensor from each node, denoted as H c .</p><p>The set of possible inputs I c i is the output set of all previous nodes adding the outputs of two previous cells:</p><formula xml:id="formula_2">I c i = {H c 1 , ..., H c i−1 , H c−1 .H c−2 }.</formula><p>The candidate function set O contains several pre-defined functions. In this paper, we apply our SETN on the candidate function set O following previous methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref> as follows:</p><formula xml:id="formula_3">• 3x3 max pooling • 3x3 avg pooling • skip connection • 3x3 separable conv • 5x5 separable conv • 1x3 &amp; 3x1 conv</formula><p>We set the number of nodes in a cell as B = 4. Therefore, the number of candidates in the search space</p><formula xml:id="formula_4">of O is (1 × 3 × 6 × 10 × (6 2 ) 4 ) 2 = 9.1 × 10 16 .</formula><p>Once we obtain the topology structures of the normal cell and the reduction cell, we follow previous works to construct the overall CNN <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14]</ref>. For CIFAR, the </p><formula xml:id="formula_5">overall CNN is [image] → [N-Cell]×N → [R-Cell] → [N- Cell]×N → [R-Cell] → [N-Cell]×N → [Softmax]; and for ImageNet, the overall CNN is [image] → [a pair of 3x3 Conv] → [3x3 Conv] → [N-Cell]×N → [R-Cell] → [N- Cell]×N → [R-Cell] → [N-Cell]×N → [Softmax],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Template Network</head><p>The template network contains all candidate CNNs in the search space. The parameters of each candidate CNN are shared by a single template network. It is non-trivial to make billions of candidate CNNs perform well after optimizing one template network. To achieve this goal, we introduce a stochastic training strategy to optimize the template network by stochastically selecting the operations and inputs as below:</p><formula xml:id="formula_6">H c i = f1(I1) + f2(I2),<label>(1)</label></formula><formula xml:id="formula_7">s.t. {I1, I2} = UniformSample(I c i , 2) 1 ,<label>(2)</label></formula><formula xml:id="formula_8">{f1, f2} = OrderedUniformSample(O, 2) 2 ,<label>(3)</label></formula><p>where , at the i-th node, we randomly sample two inputs I 1 and I 2 from the set I c i with replacement; random sample two functions f 1 and f 2 from the set O by restricting the index of f 1 in O ≥ the index of f 2 . This sample strategy can avoid the redundant candidates in the search space. For example, (f 1 =3x3 conv,f 2 =5x5 conv,I 1 =H c 1 ,I 2 =H c 1 ) is the same architecture as (f 1 =5x5 conv,f 2 =3x3 conv,I 1 =H c 1 ,I 2 =H c 1 ), but these two combinations are considered as different architectures during searching in some previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>At each training iteration, the template network uniformly samples a candidate CNN, decided by Eq. (1)∼Eq. (3), and then it only optimizes template parameters of this sampled CNN. This strategy allows us to optimize each candidate with equal opportunity, thus avoiding the Matthew effect. As a result, each candidate CNN is more likely to be fully trained compared to that in previous joint optimization strategies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b7">8]</ref>. We use "the Matthew effect" to refer that some quickly-converged candidates will get more chances to be further optimized in some NAS algorithms <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>. Besides, if we increase the cardinality of the function set |O|, the search space will grow exponentially, but the size of the template network will grow only linearly. This property allows us to search over large search space but only using a relatively small template network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The Searching Algorithm of SETN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluator</head><p>We only optimize the template parameters on the training data, and a candidate CNN would thus be considered to generalize well if it can use learned template parameters to yield a low validation loss. To find the best CNN, a trivial solution is traversing all candidates and evaluate them one by one, yet it would cost unaffordable computation time to cross over 10 16 candidates. Some one-shot methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> uniformly select a small amount of candidates to evaluate, where most uniformly selected candidates are useless. To solve these issues, we design an evaluator to indicate the probability of each individual candidate CNN being likely to have a lower validation loss. To represent this probability of each candidate, we encode one candidate CNN as a set of quadruples, and then define the probabilities over these quadruples.</p><p>We introduce the encoding approach for the i-th node in the c-th cell, and one can easily infer the steps for other nodes. For simplicity, suppose we only search for one neural cell. At first, we encode the choices of I 1 and I 2 . Based on Eq. (2), there are |I| choices for I 1 and |I| choices for I 1 (we omit the subscript and superscript for simplicity). We thus use two vectors f ∈ R |I| and g ∈ R |I| to indicate the categorical choice for I 1 and I 2 , and use its softmaxnormalized value as the choice probability, which can be formulated as:</p><formula xml:id="formula_9">f = softmax(f ) ;ĝ = softmax(g),<label>(4)</label></formula><formula xml:id="formula_10">t ∼ T (f ) ; u ∼ T (ĝ),<label>(5)</label></formula><formula xml:id="formula_11">I1 = I (t) ; I2 = I (u) ,<label>(6)</label></formula><p>where T (f ) represents the categorical distribution drawn by the vector f , so as T (ĝ). I 1 and I 2 are chosen as the tth and u-th element in I. Similarly, there are |O|(|O|+1)/2 combination choices for f 1 and f 2 , and therefore, we leverage a vector h ∈ R |O|(|O|+1)/2 to indicate the categorical choice, which is formulated as:</p><formula xml:id="formula_12">h = softmax(h) → r ∼ T (ĥ), (7) r1 = min n ∈ [ |I| ] : n k=1 k ≥ r ; r2 = r − r1-1 k=1 k, (8) f1 = O (r1) ; f2 = O (r2) ,<label>(9)</label></formula><p>where </p><formula xml:id="formula_13">H c i = (|O||O|+1) 2 r=1ĥ (r) ×( |I| t=1f (r1) O (r1) (I (t) ) + |I| u=1ĝ (u) O (r2) (I (u) )),<label>(10)</label></formula><p>s.t. calculate r1 and r2 as Eq. <ref type="formula">(8)</ref>,</p><p>Based on Eq. (10), we can back-propagate gradients through the architecture encoding α. To enforce the learned evaluator being able to reflect the validation loss of an architecture candidate, our objective for this evaluator is to minimize the validation loss. Specifically, we forward the validation images through the template network with assistance of the evaluator via Eq. (10), and backward the validation loss <ref type="bibr" target="#b2">3</ref> to the evaluator's parameters. In this way, after optimizing the evaluator, candidates sampled by the learned probabilities (Eq. (5) and Eq. <ref type="formula">(7)</ref>) would be more likely to result in a high performance on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The Searching Algorithm of SETN</head><p>We use ω to denote the parameters of the template network, i.e., the parameters of candidate functions in each node of each cell. The searching algorithm of SETN should (1) optimize parameters of the template network ω and parameters of the evaluator α; (2) sample T =1000 lowvalidation-loss architecture candidates via the evaluator; and (3) evaluate these sampled candidates with the template parameters ω and choose the candidate with the lowest validation loss.</p><p>We show the overall searching algorithm in Algorithm 1, where (x, y) indicates the standard classification loss based on an input image x with its label y. We optimize α based on Eq. <ref type="bibr" target="#b9">(10)</ref> and ω based on Eq. (1) in an alternative way. The parameters of the template network are optimized on the training set, while the parameters of the evaluator are optimized on the validation set to guarantee the generalization ability of the searched model <ref type="bibr" target="#b24">[25]</ref>.</p><p>Note that we use different forward procedures for the template network and the evaluator: the template network uses Eq. (1) and can enable each candidate perform well with the shared template network; the evaluator uses Eq. (10) can help squeeze the candidate set for evaluation. After optimizing α and ω, we sample T =1000 lowvalidation-loss candidates and select the one with the best one-shot performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Connections with Other NAS Approaches</head><p>Our proposed SETN generalized over DARTS <ref type="bibr" target="#b24">[25]</ref> and one-shot NAS approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>. DARTS directly can pick the best architecture, while this network capacity can not be correctly estimated in the validation set without fully training it. Besides, the architecture found by DARTS yields a performance with a high variance. Comparatively, one-shot NAS can estimate the network performance correctly without additional training, while the architecture candidates are randomly picked with uneven quality instead of generating the "best" candidate <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>. Our SETN is a new framework, which generalizes the above two typical streams and assimilates their benefits of accurate evaluation and high-quality architecture candidates selection.</p><p>To analyze the difference of technique details between our approach and others <ref type="bibr" target="#b24">[25]</ref>, we consider the following variants of our methods. SETN: our proposed search algorithm. SETN-LR: use a stochastic strategy to train the template network with less randomness, where the indexes of the sampled inputs/functions are the same for different cells. SETN-NON: optimize the template network without randomness, in which H c i is the weighted sum of all possible function and input combinations as Eq. (10). SETN-RAND: randomly sample candidates for evaluation as previous one-shot approaches. SETN-NON is the same strategy as <ref type="bibr" target="#b24">[25]</ref>, and SETN-RAND is the same strategy as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>. We will show that SETN is superior to SETN-LR, SETN-NON, and SETN-RAND in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Datasets. CIFAR-10 <ref type="bibr" target="#b21">[22]</ref> contains 60,000 images categorized into 10 classes. The training set has 5000 images per class, 50,000 images in total. The test set contains 1000 images per class, 10,000 images in total. CIFAR-100 <ref type="bibr" target="#b21">[22]</ref> is similar to CIFAR-10. It contains 50,000 training and 10,000 test images, categorized into 100 classes. All images are 32x32 colored ones. ImageNet <ref type="bibr" target="#b32">[33]</ref> is a large-scale image classification dataset, containing 1000 classes, 1.28 million training images and 50,000 validation images.</p><p>Searching Setup. We search the normal CNN cell and the reduction CNN cell on CIFAR-10. For searching, the official training images are randomly split into the searching training set D train and the search validation set D val in Algorithm 1. D train contains 50% of the official CIFAR-10 dataset, i.e., 25,000 images. D val contains the rest images. The candidate function set O has 6 different operations as introduced in Section 3. The hyper-parameters to construct the whole CNN model are: the number of nodes in a cell B, the initial channels of the first layer C, the number of repeated normal cells N . By default, we set C = 16, B = 4, and N = 2 to search the CNN cells. Note that, the number of operations in O s is the same as ENAS <ref type="bibr" target="#b29">[30]</ref> and DARTS <ref type="bibr" target="#b24">[25]</ref>.</p><p>We train the template network and the evaluator with the batch size of 64 in 400 epochs. To optimize the parameters ω of template network, we use the SGD optimization. We start the learning rate of 0.025 and anneal it down to 0 following a cosine schedule. We use the momentum of 0.9 and the weight decay of 3e-4. We set the probability of path dropout as 0.1. To optimize the parameters α of the evaluator, we use the Adam optimization <ref type="bibr" target="#b19">[20]</ref> with the learning rate of 3e-3 and the weight decay of 1e-3. To avoid the gradient explosion, we clip the gradient for both W and A by 10 during training.</p><p>Computational Costs. Our SETN would take about 40 hours to optimize both template network and evaluator on a single NVIDIA Tesla V100 GPU. Evaluating T =1000 candidates costs less than three hours on a single GPU (about 13 seconds per candidate). Therefore, it requires about 43 GPU hours to obtain the final CNN structure. Note that when different NAS methods report their searching costs, they may use different hardware, e.g., NVIDIA GTX 1080Ti <ref type="bibr" target="#b24">[25]</ref> and NVIDIA P100 <ref type="bibr" target="#b43">[44]</ref>. We did not normalize the GPU cost of compared methods across different devices, and we use the numbers from their original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Compared with State-of-the-art Approaches</head><p>Experiments on CIFAR. After we discover outstanding cells in the search space, we use the discovered topology with C = 36 and N = 6 to construct the CNN model  <ref type="table">Table 2</ref>. We compare SETN and other algorithms on CIFAR-10 and CIFAR-100. The top block presents state-of-the-art architectures designed by human experts. The bottom block presents architectures that are automatically discovered by machine. "M " indicates the total number of cells in the CNN, and "C" denotes the number of the filter channel in the first cell. "CutOut" indicates the data argumentation approach <ref type="bibr" target="#b10">[11]</ref>. † denotes the results reproduced by ourself. The bottom five lines show results for different variants of our approach. We run each model three times and report the mean error (lower is better).</p><p>for CIFAR-10 and CIFAR-100 following <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>. We train this network by 600 epochs with the initial learning rate of 0.025. We anneal the learning rate down to 0 with the cosine schedule. The batch size is 96; the momentum is 0.9; and the weight decay is 5e-4. The ratio of drop path is 0.2. Following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14]</ref>, we use an auxiliary tower with the weight of 0.4 to train the network. We train each network three times and report the mean error.</p><p>Comparison with the state-of-the-art on CIFAR-10 and CIFAR-100. We compare the results of the found network with other state-of-the-art networks in <ref type="table">Table 2</ref>. "SETN (T =1K)" indicates the network found by our approach. First, our SETN is one of the most efficient algorithms, in which we complete the search procedure in 1.8 GPU days. Second, among those efficient NAS approaches <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25]</ref> (less than five GPU days), the network found by SETN achieves the lowest error with similar or fewer parameters. Other NAS approaches need more than 100 times computational costs than ours, whereas models found by most approaches have higher error with more parameters than our SETN. On CIFAR-100, our network achieves the best performance (a error of 17.25%) among all compared methods. On CIFAR-10, our network is slightly worse than NASNet-A (2.69% test error vs. 2.65% test error), however, NASNet-A needs more than 1000× computational costs than SETN.</p><p>Comparison with other SETN variants. In <ref type="table">Table 2</ref>, we compare several variants of SETN introduced in Section 4.4. SETN-NON is a straightforward approach to optimize the evaluator, however, the model found by SETN-NON leads to the highest error. SETN-LR finds the model with more parameters but is inferior to the model found by SETN. In conclusion, the proposed SETN is superior to its baselines, i.e., SETN-LR and SETN-NON. "SETN (T =1)" indicates that we directly choose the best architecture from the evaluator via arg max over f /h as <ref type="bibr" target="#b24">[25]</ref>. From <ref type="table">Table 2</ref>, "SETN (T =1)" finds a small CNN, however, this CNN yields a relatively higher error than our SETN. Therefore, the stochastic training strategy and the final candidate evaluation strategy are necessary to find a good architecture.</p><p>Scalability. We compare the search cost between the small space using the candidate function set O and the large space using another candidate function set O l . This set O l adds two more functions into O l : 3x3 dilated conv and 5x5 dilated conv. It has eight functions in total, and its search space has about 9.1×10 <ref type="bibr" target="#b17">18</ref>  large search space, training SETN costs 50 GPU hours using the default hyper-parameters, and evaluating T =1K candidates takes less than three hours. Therefore, even though the large search space is 100× larger than the small one, SETN needs only about 18% more GPU days to complete the search procedure. Besides, the network found with O l achieves a similar performance compared to that of O. This shows that our SETN can be successfully applied to much larger search space.</p><p>Experiments on ImageNet. We use the same cell structures found on the CIFAR-10 dataset to construct the CNN for ImageNet. We adjust hyper-parameters N and C to make the network align with the ImageNet-mobile setting, i.e., under 600M FLOPs. We train the network with a batch size of 256 over four GPUs in 250 epochs totally. We warmup at the first five epochs, start the learning rate with 0.1, and decrease it to 0 via the cosine scheduler <ref type="bibr" target="#b26">[27]</ref>. We set the momentum as 0.9 and the weight decay as 3e-5. Besides, the label smoothing is applied with a epsilon of 0.1. An auxiliary tower with the weight of 0.4 is applied during training.</p><p>Comparison with the state-of-the-art on ImageNet.</p><p>Since the training procedure of SETN does not use any Im-ageNet images, this experiment can investigate the transferability of the discovered network. We use the same CNN structure found on CIFAR-10 with different N and C configurations. These networks strictly matche the ImageNetmobile setting. We show the top-1 and top-5 accuracy in <ref type="table" target="#tab_4">Table 3</ref>. "SETN (N=2 &amp; C=58)" achieves a top-1 accuracy of 74.3% on ImageNet. Our network obtains competitive accuracy compared to efficient NAS approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>AmoebaNet <ref type="bibr" target="#b30">[31]</ref> achieves a similar accuracy than ours, but it costs 3150 GPU days, which is 1750× more than ours. In sum, our network is competitive to state-of-the-art networks, whereas SETN needs acceptable search costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head><p>In Section 5.2, we deliver a brief comparison between SETN and its variants w.r.t. the model size and the model accuracy. In this section, we will give a more comprehensive analysis for different aspects of SETN.</p><p>The quality of estimated candidates. There are four options to generate candidate CNNs. (a) the proposed SETN. (b) SETN-RAND <ref type="bibr" target="#b3">[4]</ref>. (c) SETN-LR, sharing the sampled indexes of operations and inputs for different cells (d) SETN-NON, directly training the template network in the standard classification fashion as <ref type="bibr" target="#b24">[25]</ref>. We use these four methods to generate 1000 candidate CNN and count their one-shot accuracy in a histogram. In each sub-figure of <ref type="figure" target="#fig_1">Figure 2</ref>, the x-axis indicates the one-shot validation accuracy and y-axis indicates the number of candidate CNNs. Several conclusion can be made. First, randomly generated candidates have much lower accuracy than all other compared strategies. Second, SETN-NON is better than the random approach but still inferior to SETN and SETN-LR. Third, the performance of SETN generated candidates is similar to that of SETN-LR. However, taking a close look at the histogram, SETN generate more accurate candidates than SETN-LR, e.g., there are more candidates with the accuracy between 85%∼90%.</p><p>Can the validation accuracy with template parameters reflect the ground truth validation accuracy? We want to investigate whether the validation accuracy using the template parameters can provide a robust relative ranking of different networks or not. To achieve, we randomly sample 2000 networks, i.e., 1000 pairs. We first evaluate these 2000 networks by using the template parameters on D val . We denote the obtained validation accuracy as acc t . Then we retrain these 2000 networks from scratch from scratch by 100 epochs on D train , and other hyperparameters are the same as experiments in <ref type="table">Table 2</ref>. We evaluate these re-trained networks on D val and denote the accuracy as acc r , indicating the ground truth accuracy. For each pair, if the comparison of acc t is the same with the comparison of acc r , we count this pair is good; otherwise, we count this pair is bad. Finally, we obtain more than 80% pairs are good.</p><p>The above two analysis paragraphs did not directly evaluate (1) how good the candidate sampled by the evaluator is and (2) how accurate the one-shot accuracy is. To answer these two questions, we need a NAS dataset with ground truth accuracy of each candidate, where NAS-Bench-101 <ref type="bibr" target="#b39">[40]</ref> is the only one. However, our SETN can not be directly evaluated on NAS-Bench-101 <ref type="bibr" target="#b39">[40]</ref> due to its limitation. We would investigate these open questions once a suitable NAS dataset being public.</p><p>The effect of the number of generated candidates. In <ref type="table">Table 2</ref>, we show that using T =1 finds a model with a higher error than using T =1K. A small number of T , e.g., 10, will cause a high variance of the accuracy of discovered CNN. If we increase the number of T , we could reduce the variance. In our experiments, T =1K is enough to discover a good network. If we increase T to 10K, we could potentially find better networks, but the search cost will be more than five GPU days. Some approaches apply a progressive strategy to select the final CNN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>. For example, they first select the top 10 networks (ranked by the validation accuracy) and then retrain these networks with more epochs to get a precise validation accuracy. These strategies are able to further reduce the performance variance of the discovered CNN.</p><p>Visualization. We visualize the discovered cells in <ref type="figure">Figure</ref> 3. Compared to manually designed cells <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, the automatically discovered cells are much more complex and difficult to be designed by human experts. Given the superior performance of NAS-discovered networks, it is necessary to devote more effort on this topic.</p><p>Discussions. One limitation of SETN is that when the search space is very large, e.g., 10 30 candidates, a small number of T may not be able to find a good model. In this case, we have to increase the number of T , which will also increase the corresponding evaluation cost. A more efficient training strategy for the template network and the evaluator can alleviate this problem. We leave such interesting extensions for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An overview of the self-evaluated template network (SETN). SETN consists of a template network and an evaluator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of the candidate networks generated by different strategies. 2(a) shows the generated candidates from the proposed SETN. 2(c), 2(d), and 2(b) show statistics of other three SETN variants. In each sub-figure, the x-axis and y-axis indicate the number of candidate networks and the validation accuracy, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The left figure is the normal CNN cell that SETN discovered on CIFAR-10. The right figure is the reduction CNN cell that SETN discovered on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where [N-Cell] and [R-Cell] indicate the normal and reduction cells, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Input: the whole available training data a template network with ω and an evaluator with α Split the whole available training data into the training set Dtrain and the validation set D val for searching while not converge do Optimize ω and α Sample training batch Dt = {(xi, yi)} batch i=1 from Dtrain Calculate train = D t (xi, yi) based on Eq. (1) Update ω via gradients from the training loss train Sample validation batch Dv = {(xi, yi)} batch i=1 from D val Calculate val = Dv (xi, yi) based on Eq. (10) Update α via gradients from the validation loss val end while After the above steps, we obtain the optimized ω and α. Evaluate all candidates in A with parameters extracted from ω Select the candidate with the lowest validation loss Output: the final selected candidate</figDesc><table><row><cell>Initialize A = φ</cell><cell>Obtain Low-Validation-Loss Candidates</cell></row><row><cell>for i=1; i ≤ T ; i++ do</cell><cell></cell></row><row><cell cols="2">Sample an architecture a using Eq. (4) to Eq. (9)</cell></row><row><cell>A = A ⊃ {a}</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>in a low validation loss, we need to optimize the parameters of this evaluator α = {(h, f , g)} on the validation set. Since Eq. (4) to Eq. (9) are discrete, we use continues relaxation to calculate the output H c</figDesc><table /><note>T (ĥ) represents the categorical distribution drawn by the vectorĥ. O (t) and O (u) are the t-th and u-th func- tions in O, respectively. Eq. (8) guarantees the indexes r2 ≤ r1, which is consistent with Eq. (3). Eq. (4) to Eq. (9) can sample one quadruple (r1, r2, t, u) for one node based on the probabilitiesf ,ĝ, andĥ, which are encoded by f , g, and h. The set of quadruples for all nodes {(r1, r2, t, u)} can represent one candidate architecture in the search space. To enable the evaluator being able to indicate whether a candidate could resulti of a node, as follows:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>candidates. With the Method GPU days Parameters +× (million) Top-1 Accuracy Top-5 Accuracy We compare networks found by SETN and other approaches on ImageNet. We report the model size, the computation cost, the top-1 accuracy, and the top-5 accuracy. The top block shows the manually designed CNNs. The bottom two blocks indicate the automatically design CNNs. +× indicates the number of multiply-add operations.</figDesc><table><row><cell></cell><cell>Inception-v1 [36]</cell><cell>−</cell><cell>6.6 MB</cell><cell>1448</cell><cell>69.8%</cell><cell>89.9%</cell></row><row><cell>Human</cell><cell>ResNet [18]</cell><cell>−</cell><cell>11.7 MB</cell><cell>1814</cell><cell>69.8%</cell><cell>89.1%</cell></row><row><cell>Expert</cell><cell>MobileNet-v2 [34]</cell><cell>−</cell><cell>3.4 MB</cell><cell>300</cell><cell>72.0%</cell><cell>−</cell></row><row><cell></cell><cell>ShuffleNet [42]</cell><cell>−</cell><cell>∼5 MB</cell><cell>524</cell><cell>73.7%</cell><cell>−</cell></row><row><cell>NAS with more than 100 GPU days</cell><cell>Progressive NAS [23] NASNet-A [44] NASNet-B [44] NASNet-C [44]</cell><cell>150 2000 2000 2000</cell><cell>5.1 MB 5.3 MB 5.3 MB 4.9 MB</cell><cell>588 564 488 558</cell><cell>74.2% 74.0% 72.8% 72.5%</cell><cell>91.9% 91.6% 91.3% 91.0%</cell></row><row><cell></cell><cell>DARTS [25]</cell><cell>4</cell><cell>4.9 MB</cell><cell>595</cell><cell>73.1%</cell><cell>91.0%</cell></row><row><cell></cell><cell>GHN [41]</cell><cell>0.84</cell><cell>6.1 MB</cell><cell>569</cell><cell>73.0%</cell><cell>91.3%</cell></row><row><cell>NAS with less than 5 GPU days</cell><cell>SNAS [39] GDAS [14] SETN (N=1 &amp; C=73) SETN (N=2 &amp; C=58)</cell><cell>1.5 0.84 1.8 1.8</cell><cell>4.3 MB 5.3 MB 5.2 MB 5.3 MB</cell><cell>522 581 597 600</cell><cell>72.7% 74.0% 73.3% 74.3%</cell><cell>90.8% 91.5% 91.4% 91.6%</cell></row><row><cell></cell><cell>SETN (N=3 &amp; C=49)</cell><cell>1.8</cell><cell>5.3 MB</cell><cell>584</cell><cell>74.1%</cell><cell>91.9%</cell></row><row><cell></cell><cell>SETN (N=4 &amp; C=44)</cell><cell>1.8</cell><cell>5.4 MB</cell><cell>599</cell><cell>74.3%</cell><cell>92.0%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">UniformSample(S, N ) indicates a set of N elements chosen randomly from set S with replacement via a uniform distribution.<ref type="bibr" target="#b2">3</ref> OrderedUniformSample(S, N ) indicates a set of N elements chosen randomly from set S via a uniform distribution, in the mean time, the index of a later sampled element should be not larger than the index of former element. In implementation, we uniformly random sample an integer r from [0, N(N+1)/2) and use Eq. (8) to compute these two indexes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">the validation loss could be a simple softmax with cross-entropy loss, and could also integrate other constrains about latency or memory size<ref type="bibr" target="#b7">[8]</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose the self-evaluated template network (SETN) to search for the CNN with higher accuracy. Compared to previous one-shot NAS approaches, SETN significantly improves the quality of architecture candidates for the oneshot evaluation procedure. In this way, the sampled candidates of SETN can cover better architectures, and thus can finally find an architecture with higher performance. In experiments, SETN can complete the search procedure within two GPU days. It finds a good CNN from more than 10 16    network possibilities, and this CNN achieves state-of-theart performance on three benchmarks.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal Cell</head><p>Reduction Cell </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) Workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural optimizer search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SMASH: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2787" to="2794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RE-NAS: Reinforced evolutionary neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisen</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4787" to="4796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">More is less: A more complicated network with less inference complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5840" to="5848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Network pruning via transformable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="760" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected search space for more flexible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10628" to="10637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno>2017. 3</idno>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6307" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning curve prediction with Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Prototype propagation networks (PPN) for weakly-supervised few-shot learning on category graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3015" to="3022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>2017. 7</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">NAS-Bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph hypernetworks for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
