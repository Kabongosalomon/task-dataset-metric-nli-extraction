<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic framework for solving Visual Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="201917-10-18">October 18, 2019 17 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badri</forename><forename type="middle">N</forename><surname>Patro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anupriy</surname></persName>
							<email>anupriy@iitk.ac.in</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
							<email>vinaypn@iitk.ac.in</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badri</forename><forename type="middle">N</forename><surname>Patro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anupriy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Kanpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic framework for solving Visual Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="201917-10-18">October 18, 2019 17 Oct 2019</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Journal</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNN</term>
					<term>LSTM</term>
					<term>Uncertainty</term>
					<term>Aleatoric uncertainty</term>
					<term>Epistemic Uncertainty Vision and Language</term>
					<term>Visual Dialog</term>
					<term>VQA</term>
					<term>Answer Generation</term>
					<term>Question Generation</term>
					<term>Bayesian Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a probabilistic framework for solving the task of 'Visual Dialog'. Solving this task requires reasoning and understanding of visual modality, language modality, and common sense knowledge to answer. Various architectures have been proposed to solve this task by variants of multi-modal deep learning techniques that combine visual and language representations. However, we believe that it is crucial to understand and analyze the sources of uncertainty for solving this task. Our approach allows for estimating uncertainty and also aids a diverse generation of answers. The proposed approach is obtained through a probabilistic representation module that provides us with representations for image, question and conversation history, a module that ensures that diverse latent representations for candidate answers are obtained given the probabilistic representations and an uncertainty representation module that chooses the appropriate answer that minimizes uncertainty. We thoroughly evaluate the model with a detailed ablation analysis, comparison with state of the art and visualization of the uncertainty that aids in the understanding of the method. Using the proposed probabilistic framework, we thus obtain an improved visual dialog system that is also more explainable.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has aided significant progress in solving various computer vision tasks such as object classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. The solution of more semantic tasks such as visual question answering <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and image captioning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> has also seen progress lately. A challenging problem that extends these is that of maintaining a dialog with a user <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> In this case, a system is required to maintain context concerning the history of the conversation while answering a question and this is be more challenging. A specific task in the visual context is that of the 'Visual Dialog' task <ref type="bibr" target="#b9">[10]</ref>. The aim here is that given an image, we need to train an agent to maintain a dialog. The motivation for this emerges from an interest in developing associative technologies for visually impaired persons or chat-bot based dialog agents.</p><p>Several methods have been proposed for solving the task, such as using various discriminative and generative encoder-decoder frameworks that aim to solve the task of generating dialog <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref>. In this paper we aim to extend the previous approaches by formulating a probabilistic approach towards solving this task. This approach is illustrated in <ref type="figure" target="#fig_0">figure 1</ref>. Through our approach we can obtain a principled model that we can train end-to-end while being able to have uncertainty estimates and the ability to evaluate and explain the model. Such an ability to explain the model is crucial, especially, when we consider that the method could be used by visually impaired people.</p><p>At any point in the method, the model can be probed to ensure that it is certain about • Diverse Latent Answer Generation Module: In this module, we use a variational autoencoder based latent representation that allows us to obtain latent representations from which we can sample answers.</p><p>• Uncertainty Representation Module: In this module, we propose a Reverse Uncertainty based Attention Map (RUAM) method by using Bayesian deep learning methods that allows us to minimize data uncertainty and model uncertainty.</p><p>To provide an overview of the technical contributions we make, the main idea is to consider incorporating a Gaussian prior for generating samples of answers. We minimize the KL divergence between the prior and the posterior distribution. The other contribution is to explicitly incorporate a loss to ensure that the correlation between different samples is minimal. We further use these losses along with a loss to minimize the uncertainty. A similar loss has been considered in another context by Patro et al. <ref type="bibr" target="#b10">[11]</ref>. In this work we are interested in a principled framework for minimizing uncertainty by sampling and generating diverse answers. Moreover, its use has not been considered for the problem of visual dialog. We evaluate each of the contributions in our work. The technical details mentioned here are discussed further in detail in the following sections. <ref type="figure">Figure 2</ref>: Results were showing the certainty of the correct class increases from baseline <ref type="bibr" target="#b8">[9]</ref> to our proposed uncertainty model <ref type="bibr">(PDUN)</ref>. In this figure, we show the top 2 class confidence score of the question, "Is this vision and language. One of the earliest such problems is that of image captioning where we aim to generate a sentence describing an image <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Further, the community moved on to answer questions based on an image in the visual question answering (VQA) task <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. Another interesting problem that has been addressed is that of visual question generation (VQG) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, where the aim is that given an image to generate natural questions similar to that asked by humans. The solution of the visual dialog problem builds up on the previous work conducted for solving the various problems described above.</p><p>Visual dialog task requires the agents to have meaningful dialog conversation about the visual content. This task was introduced by Das et al. <ref type="bibr" target="#b9">[10]</ref>. The authors have proposed three approaches, namely late fusion in which all the history rounds are concatenated, attention-based hierarchical LSTM which handles variable length history and memory-based method for performing results best in terms of accuracy for solving this task. Following up, <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> have proposed generator and discriminator based architecture. Of these, Lu et al. <ref type="bibr" target="#b34">[35]</ref> consider an attention based method to combine all history rounds to get a single representation. Further works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9]</ref> have proposed visual dialog as an image guessing game. The latest work on the visual dialog to obtain state of the art results has been proposed by Jain et al. <ref type="bibr" target="#b38">[39]</ref>. This work is based on discriminative question generation and answering. In another work, Jain et al. <ref type="bibr" target="#b31">[32]</ref> have proposed a method to bring diversity in the question generation from an image using Variational Auto-encoder (VAE). Wang et al. <ref type="bibr" target="#b39">[40]</ref> have proposed a similar kind of method to generate a caption from an image using VAE. In related works, <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> have captured diversity in the caption generation from an image using generative adversary network. In contrast to these earlier works, in our framework we consider a fully probabilistic framework for solving the task of visual dialog.</p><p>We use Bayesian CNN in our work for obtaining probabilistic image representations. Modeling distribution over CNN filters is still a difficult task. Due to the large number of parameters to be inferred, the posterior distribution becomes intractable. To approximate this posterior distribution, the variational inference is one of the existing approaches introduced by <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. Gaussian distribution is the simplest varia-tional approximation used to fit the model parameters to the true distribution of parameters, but it is computationally expensive <ref type="bibr" target="#b45">[46]</ref>. This can be overcome using Bernoulli approximation.</p><p>There has been some work done in terms of estimating uncertainty in the predictions using deep learning the work by <ref type="bibr" target="#b46">[47]</ref> estimates the predictive variance of the deep network with the help of dropout <ref type="bibr" target="#b47">[48]</ref>. <ref type="bibr" target="#b48">[49]</ref> has proposed a method to capture model uncertainty for image segmentation task. They observed that softmax probability function approximates relative probability between the class labels, but does not provide information about the model's uncertainty. Recently, <ref type="bibr" target="#b49">[50]</ref> has decomposed predictive uncertainty into two major types, namely aleatoric and epistemic uncertainty, which capture uncertainty <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> about the predicted model and uncertainty present in the data itself. Here, our objective is to generate diverse answer, to analyze and minimize the uncertainty in answer data, and to analyze the uncertainty of the model for the challenging visual dialog task. We build up on the techniques proposed in several such works to obtain a fully probabilistic framework for solving the visual dialog problem.</p><p>In the next section we consider the background in terms of Bayesian modeling required for obtaining our probabilistic framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background: Bayesian Approach of Model</head><p>Consider the distribution p(x, y) over the input features x and labels y. For the visual dialog classification task, x corresponding to joint encoding feature of image, history and query question and y answer class label. For the given observation, X and its corresponding output Y . In a Bayesian framework, the predictive uncertainty of the classification model (p(y * |x * , D)) 1 is trained on a finite set of training data</p><formula xml:id="formula_0">D = {x i , y i } N i=1 .</formula><p>The predictive uncertainty will result in data(aleatoric) uncertainty and the model(epistemic) uncertainty. The model estimates two kinds of uncertainty, i.e., data uncertainty and model uncertainty. The posterior distribution describes the data uncertainty over class labels, given set of model parameters w, and the model uncertainty is described by the posterior distribution over model parameters w, given input data. The predictive uncertainty for new example point x * by integrating over all possible set of parameters w is given by</p><formula xml:id="formula_1">p(y * |x * , X, Y ) = p(y * |x * , w) Data p(w|X, Y ) M odel dw<label>(1)</label></formula><p>Our main objective is to find the best set of weights of our model that will generate our data X, Y. One of the approaches to make Bayesian inference is to compute the posterior distribution overweights, i.e., p(w|X, Y ). This distribution captures the best plausible set of model parameters given our observed data.</p><formula xml:id="formula_2">p(w|X, Y ) = p(Y |X, W )p(W )/p(Y |X)</formula><p>It is challenging to perform inference over the Bayesian network because the marginal probability p(Y |X) of the posterior cannot be evaluated analytically. So, the posterior distribution p(w|X, Y ) is intractable. To approximate the intractable posterior distribution, various approximation approaches are proposed in <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b45">46]</ref>. Variational inference is one of the approximating technique, where the posterior p(w|X, Y ) is approximated by a simple distribution q θ (W ), where θ is the parameterized by variational parameter p(w|X, Y ) ≈ q θ (W ). We thus minimize the KullbackLeibler(KL) divergence between approximate distribution q θ (w) and the posterior p(w|X, Y ) w.r.t θ, which is denoted by KL(q θ (w)||p(w|X, Y )).</p><formula xml:id="formula_3">KL(q θ (w)||p(w|X, Y )) ∝ − q θ (w) log p(Y |X, w)dw + KL(q θ (w)||p(w)) = − N i=1 q θ (w) log p(y i |fŴ (x i ))dw + KL(q θ (w)||p(w))</formula><p>Minimizing the KL divergence is equivalent to maximizing the log evidence lower bound <ref type="bibr" target="#b52">[53]</ref> with respect to the variational parameters defining q θ (w),</p><formula xml:id="formula_4">L V I = q θ (w) log p(Y |X, w)dw − KL(q θ (w)||p(w))<label>(2)</label></formula><p>The intractable posterior problem i.e., averaging over all the weight of the BNN, is replaced by the simple distribution function. Now we need to optimize the parameter of simple distribution function instead of optimizing the original neural network's parameters. Furthermore the integral in equation 1 (predictive posterior) is also intractable for the neural network, which is approximated via sampling using Monte Carlo dropout <ref type="bibr" target="#b46">[47]</ref> or Langevin Dynamics <ref type="bibr" target="#b53">[54]</ref> or explicit ensembling <ref type="bibr" target="#b54">[55]</ref>. So we approximate the integral with Monte Carlo sampling.</p><formula xml:id="formula_5">p(y * = c|x * , X, Y ) = p(y * = c|x * , w)p(w|X, Y )dw ≈ p(y * = c|x * , w)q θ (w)dw ≈ 1 M M i=1 p(y * = c|x * , w (i) )q(w (i) )<label>(3)</label></formula><p>where w (i) ∼ q(w (i) ), which is modeled by the dropout distribution and M samples of w (i) is obtained. , each p(y * |x * , w (i) ) in an ensemble p(y * |x * , w (i) ) M i=1 obtained sampled from q(w (i) ). In the following section, we have discussed Bayesian CNN and Bayesian LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bayesian CNN</head><p>One way to define a Bayesian neural network <ref type="bibr" target="#b46">[47]</ref> is to place a prior distribution over neural network weights, w = (W i ) L i=1 . Given weight matrix W i and bias b i for ith layer, we use standard Gaussian prior distribution over the weight matrix p 0 (W i ) = N (W i ; 0, 1). The variational Bayesian approximation in a Bayesian neural network can be interpreted as adding stochastic regularization in the deterministic neural network. The stochastic regularization technique is equivalent to multiplying random noise i with neural network weight matrices M i .</p><formula xml:id="formula_6">W i = M i · diag([ i,j ] Ki j=1 ) i,j ∼ Bernoulli(p i ), i = {1, ., L}, j = {1, ., K i−1 }<label>(4)</label></formula><p>where, i,j is a Bernoulli distributed random variable with probability p i . The diag(.)</p><p>operator maps vectors to diagonal matrices, whose diagonal elements are the elements of the vectors. The set of variational parameters M i is now the set of matrices θ =</p><formula xml:id="formula_7">{M i } L i=1 .</formula><p>The binary variable i,j = 0 indicates the corresponding element j in the layer i − 1 is dropped out as an input to layer i. In CNN with dropout <ref type="bibr" target="#b55">[56]</ref>, the forward propagation is formulated as,</p><formula xml:id="formula_8">m i k ∼ Bernoulli(p i ) a i k = a i k * m i k z i+1 j = n (l) k=1 Conv(W l+1 j ,â i k )<label>(5)</label></formula><p>Here a i k denotes the activations of feature map k (k = 1, 2, , n (l) ) at layer l. The mask matrix m l k consists of independent Bernoulli variables m l k (i). This mask is sampled and multiplied with activations in kth feature map at layer l, to produce dropoutmodified activationsâ i k . These modified activations are convolved with filter W l+1 j to produce convolved features z i+1 j . The function f is applied element wise to the convolved features to get the activations of convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bayesian LSTM</head><p>The conventional LSTM is a neural network that maps LSTM state s t (at time step t) and input x t to a new LSTM state s t+1 , f l : (s t , x t ) → s t+1 . The state of LSTM is</p><formula xml:id="formula_9">given by s t = (c t , h t ),</formula><p>where c t is a memory state and h t is the output of the hidden state. To train a input sequence of length T , x 1 , x 2 ..., x T , the LSTM cell is unrolled T times in to a feed forward network with initial state s 0 and can be represented by</p><formula xml:id="formula_10">s j = f l (s j−1 , x j )</formula><p>In Bayesian LSTM, let p(y * |w, x * ) be the likelihood of the neural network, then the posterior of the network is approximated to q(w) by minimizing the variational free energy L(w) <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>. Minimizing the variational free energy is equivalent to maximizing the likelihood log p(y|x, w) subject to KL divergence, which regularizes the parameters of the network.</p><formula xml:id="formula_11">L(w) = −E q(w) [ log p(y * 1:T |x * 1:T , w)] + KL(q(w)||p(w))</formula><p>Here log p(y 1:T |x 1:T , w) is the likelihood function of the sequence and the expectation in the previous equation is approximated by the Monte Carlo sampling. The predictive posterior for LSTM is calculated just as in equation 3 by,</p><formula xml:id="formula_12">p(y * 1:T |x * 1:T , X, Y ) = p(y * 1:T |x * 1:T , w)p(w|X, Y )dw ≈ 1 M M m=1 p(y * 1:T |x * 1:T ,ŵ m )dw (6) withŵ m ∼ q θ (w), where q θ (w)</formula><p>is called the dropout distribution for LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>Visual dialog task is introduced by [10]. The visual dialog task is defined as, given image I, a caption C, a dialog history till t−1 rounds,</p><formula xml:id="formula_13">H = {C, (q 1 , a 1 ), ....(q t−1 , a t−1 )</formula><p>and the following question q t at round t. The objective of the visual dialog agent is to predict a natural language answer to the question q t . The visual dialog problem can be solved into two possible ways; one is by using a generative model and the other by using a discriminative model. In a generative model, given the embeddings of image, history, and question(q t ), a generative model is trained to maximize the likelihood function to predict ground truth answer sequence. The discriminative model receives embedding of an image, history, and question(q t ) along with 100 candidate answers</p><formula xml:id="formula_14">A t = {a 1 t , .</formula><p>..a 100 t } and effectively learns to rank the list of candidate answers. One aspect of previous approaches tends to be a lack of diverse generations of answers; for instance, the tendency to correlate the animal 'zebra' with black and white stripes. In contrast, during conversations, a conversation is interesting if an unexpected or novel observation is raised. In our method, we hope to produce such insights. To do that we need an ability to characterize the space of all possible answers. We do that by using a Gaussian prior for the generation of answers. This allows us to generate samples of plausible answers. We then further use a diversity loss that would penalize correlations between the multiple samples. The final task then lies to choose an appropriate retort or response. To do so, we rely on minimizing uncertainty while generating the answer. We now consider the proposed approach in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our method consists of three parts viz. 1) Probabilistic Representation Module, 2)</head><p>Latent feature-based Diverse Answer Generation Module, and 3) Uncertainty Module as illustrated in figure 3 : </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Probabilistic Representation Module</head><p>We adopt a probabilistic representation module that has been previously considered in Patro et al. <ref type="bibr" target="#b10">[11]</ref> for the VQA task. However, using this representation for visual dialog requires us to also consider the history of previous dialogs as a new input. By using a probabilistic representation, we are able to investigate the uncertainty in any part of the full proposed model. To obtain this, we use the methods of Bayesian CNN and Bayesian LSTM that has been discussed in the background section. Given an input image x i , we obtain an image embedding g i by using a Bayesian CNN that we parameterized through a function G(x i , W i ), where W i are the weights of the Bayesian CNN.</p><p>We extract g i ∈ R w×h×c dimensional CONV-5 feature from Bayesian CNN network as shown in <ref type="figure" target="#fig_1">figure 3</ref>. We obtain g q , g h encoding feature for given question and history, after passing through an LSTM (Bayesian LSTM Network), which is parameterized using the function G q (X W E , θ l ), where θ l are the weights of the LSTM as shown in <ref type="figure" target="#fig_1">figure 3</ref>. Similarly, we obtain answer embedding G a parameterised by G(X a , θ a ).</p><p>After this, the question and answer embedding are combined to obtain a history embedding. To model the Bayesian CNN <ref type="bibr" target="#b58">[59]</ref>, we use pre-trained CNN layers and put dropout layer with dropout rate p, before each CNN layer. Similarly for Bayesian:</p><p>LSTM <ref type="bibr" target="#b57">[58]</ref>, we add dropout on each input and a hidden layer of the LSTM cell. These are input to an attention network that combines question-answer pair with previous history embedding using a weighted softmax function and produces a weighted output attention vector g f . There are various ways of modeling the attention network. In this paper, we have evaluated the network proposed in SAN <ref type="bibr" target="#b59">[60]</ref>. In the last round, we combine image embedding with the last history embedding to get a dialog context vector. At each round, we attend over the question representation with the previous history (combined question-answer representation). In the first round, the previous history is an encoded caption feature. In the final round, we attend to image representation with the appropriate history representation to obtain an attentive encoder feature, g f . The attention mechanism is illustrated as follows:</p><formula xml:id="formula_15">g a = tanh(W c g i + W q (g q ||g h ) + b c ) α = Softmax(W a g a + b a )<label>(7)</label></formula><p>where || means concatenation, W a , W c , W q , b c , b a are the weights and bias of different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Latent feature based Diverse Answer Generation Module</head><p>This module mainly focuses on representing a latent representative vector from attentive encoder module and generate a diverse answer using answer generator. We use the VAE <ref type="bibr" target="#b60">[61]</ref> based generative framework to generate diverse answer from the attentive encoder. We obtain mean, µ = W µ g f and log variance, log σ 2 = W σ g f , where µ and σ are the parameters of a multivariate Gaussian distribution. We train this network to learn a variational distribution which is close to a prior defined by the normal distribution with zero mean and unit variance i.e., N (0, 1). Then we obtain a latent vector representation z by using the reparameterization trick z = µ + σ.</p><p>The major concern for answer generation is the spread of the variance in the latent representation. Our main objective is to increase the spread in the Gaussian as much as possible for generating diverse answers. If the spread of the Gaussian is too low (∼ 0), then we have sampled similar answers, and if the variance is too high (∼ ∞), then we have sampled from a uniform distribution. Hence, we want to put some constraints on variance such that our sampled latent representations are as diverse as possible. A diversity loss which minimizes the correlation between the latent representations is introduced to ensure this. Let us define z 1 and z 2 as the two latent vectors randomly sampled from the N (µ, σ), z 1 = µ + 1 σ, z 2 = µ + 2 σ, where 1 and 2 are sampled from N (0, 1) and µ and σ are Gaussian parameters. The diversity loss is given by</p><formula xml:id="formula_16">L diverse = (z 1 − α), (z 2 − α) max(||z 1 − α|| 2 · ||z 1 − α|| 2 , γ)<label>(8)</label></formula><p>Where γ = 10 −8 is used to avoid division by zero, and α is the average of all the z samples. Similarly, we obtain an average loss for k sample points (in our experiments, we choose k to be 100) randomly sampled from the latent distribution. This loss ensures that these latent vectors are as far as possible.</p><p>Finally, the diverse latent feature is input to an LSTM based answer decoder module to generate diverse answers. The softmax probability for the predicted answer token at different time steps is given by the following equations:</p><formula xml:id="formula_17">h 0 = Z i = N (µ, σ) x t = W e * a t , ∀t ∈ {0, 1, 2, ...T − 1} h t+1 = LSTM(x t , h t ), ∀t ∈ {0, 1, 2, ...T − 1} y t+1 = sof tmax(W o * h t+1 ) L CE = − 1 C C j =1 y j log p(ŷ j |f o )</formula><p>whereŷ t+1 is the predicted answer class and f o is the context. Now, we classify the generated diverse answer among 100 classes in order to rank them with 100 ground truth answer options. We use a reasoning network to perform reasoning by predicting an answer and comparing it with the ground truth answer to obtain the final score. A similar approach has been used by Das et al. <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Uncertainty Representation Module</head><p>Through the previous module, we obtain the ability to generate diverse answers.</p><p>The task then is to choose an appropriate answer that is correct. To do that, we use an uncertainty representation module that characterizes the uncertainty among the diverse set of candidate answers (i.e., the classes present in the answers). We want to be certain about the response that is chosen. That is, we would like to minimize the uncertainty.</p><p>We do that by using an explicit loss for reducing uncertainty.</p><p>In this work, we also incorporate the attention regions, which specifies the spatially distributed weights given to a specific region embedding while generating the answer.</p><p>To obtain the best embedding, we consider the ground-truth answer and through attention, consider the corresponding spatial location. This region is multiplied with the uncertainty for generating the spatial attention weighted uncertainty corresponding to the ground-truth answer. We want to increase the weight for the spatial attention corresponding to generating the ground-truth answer and minimize the uncertainty for the same. At the same time, we would like to increase the uncertainty for all other answers and minimize the weight given in terms of attention to all other regions. We achieve this through a reverse uncertainty based attention map (RUAM) that is shown in figure 4. Reverse Uncertainty based Attention Map (RUAM): Patro et al. <ref type="bibr" target="#b10">[11]</ref> have proposed a model to estimate aleatoric and predictive uncertainty for Visual Question Answering task, where the gradient of uncertainty loss and gradient of classification is multiplied to improve attention feature. Kurmi et al. <ref type="bibr" target="#b61">[62]</ref> have also proposed a similar kind of network in the domain adaption task, where they train the discriminator network to reduce uncertainty in source and target domain. We follow a similar type of network in a visual dialog task to reduce uncertainty in the attention mask with the help of a predicted answer in the dialog turns. We stress more on those attention regions whose uncertainty is less and vice-versa. The aleatoric uncertainty occurs due to corruption in the feature or noise in the attention regions. These regions are the main source of predicting the wrong answer in the visual dialog.</p><p>We adopt a Bayesian framework to predict answer classification uncertainty efficiently.</p><p>We make our answer classifier as Bayesian and perform probabilistic inference over the classifier to obtain the final answer score. We adopt a Bayesian classifier as considered in several works <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b10">11]</ref>. The Bayesian classifier is obtained by applying dropout after every fully connected (FC) layer in the classifier and the</p><p>Bayesian classifier predicts answer class logits y i,d and aleatoric uncertainties. These are obtained as follows:</p><formula xml:id="formula_18">y i,d = G y (G d (f i )), σ i,d = G v (G d (f i )) (9)</formula><p>where f i is the attention feature for input image sample x i , question sample x q and history sample x h , which is obtained by the attention feature extractor G f : Finally, we measure the uncertainty for our answer prediction and found it to be lower.</p><formula xml:id="formula_19">f i = G f (G i (x i ), G q (x q ), G h (x h ),</formula><p>We learn and estimate observational noise parameter σ i,d to capture the uncertainty present in the input data (Image, History and Question). This can be achieved by corrupting the logit value (y i,d ) with the Gaussian noise with variance σ i,d (diagonal matrix with one element for each logits value) before the softmax layer. We used a Logit Reparameterization Trick (LRT) <ref type="bibr" target="#b62">[63]</ref>, which combines two outputs y i,d , σ i,d and then we obtain a loss with respect to ground truth. That is, after combining we get N (y i,d , (σ i,d ) 2 ) which is expressed as:</p><formula xml:id="formula_20">y i,t,d = y i,d + t,d σ i,d , where t,d ∼ N (0, 1)<label>(10)</label></formula><formula xml:id="formula_21">L u = i log 1 T t exp (ŷ i,t,M − log M ' expŷ i,t,M ' )<label>(11)</label></formula><p>where M is a discrete word token present in each sample sentence. y i,t . M is a discrete word token present in the ground truth sentence, L v is minimized for the true work token M , and t ∈ T is the number of Monte Carlo simulations. σ i,d is the standard</p><formula xml:id="formula_22">deviation, ( σ i,d = √ v i,d ).</formula><p>Now, we obtain uncertainty for attention map α att ∈ R u×v of width u and height v using following steps such as, we first compute gradient of the predictive uncertainty σ 2 g of our generator with respect to the features f i . This gradient of the uncertainty loss L u with respect to the attention feature f i is given by ∂Lv ∂fi . Now we pass the uncertainty gradient through a gradient reversal layer to reverse the gradient to get certainty mask for the attention map. This is given by</p><formula xml:id="formula_23">∇ u = −γ * ∂L u ∂f i</formula><p>We perform an element-wise multiplication of the forward attention feature map and reverse uncertainty gradients to get an enhanced attention feature map i.e.</p><formula xml:id="formula_24">α u,v = −γ * ∂L u ∂f i * α u,v<label>(12)</label></formula><p>The positive sign of the gradient γ indicates that the aleatoric certainty is activated on these regions and vice-versa. We apply a ReLU activation function on the product of gradients of the attention map and the gradients of aleatoric certainty as we are only interested in attention regions that have a positive influence for a corresponding answer class, i.e. attention pixels whose intensity should be increased in order to increase y c ,</p><p>where negative values are multiplied by γ (large negative number). Negative attention pixels are likely to belong to other categories in the image.</p><formula xml:id="formula_25">α u,v = ReLU (α u,v ) + γReLU (−α u,v )<label>(13)</label></formula><p>Images with higher aleatoric uncertainty correspond to lower certainty. Therefore the certain regions of these images should have lower attention values. We use residual connection to obtain the final attention feature by combining original attention feature with the reverse uncertainty map α u,v . This is given by:</p><formula xml:id="formula_26">α new = α u,v + α u,v * α u,v f i = u,v g i * α new<label>(14)</label></formula><p>Where, g i ∈ G i (x i ). The final attention feature (f i ) can be obtained by combining attention feature (f i ) with RUAM based new attention feature (f i ).</p><formula xml:id="formula_27">f i = f i + f i<label>(15)</label></formula><p>We show here, that using reverse uncertainty based attention Map (RUAM) results in an improved attention network and the attention confidence also increases. The entropy and predicted variance of the sampled logit's probability can be calculated as:</p><formula xml:id="formula_28">H(ŷ i,t ) = − M m=1 p(ŷ i,t = M ) * log p(ŷ i,t = M )<label>(16)</label></formula><p>The predictive uncertainty is the combination of entropy and variance of T sample outputs (of randomly masked model weights).</p><formula xml:id="formula_29">σ 2 p = 1 T T t=1 H(ŷ i,t ) + 1 T T t=1 v 2 i,t<label>(17)</label></formula><p>Where H(ŷ i,t ) is the entropy of the probability p(ŷ c i,t ), which depends on the spread of the probabilities and the variance captures both the spread and the magnitude of outcome valuesŷ i,t . Algorithm-1 explains details about reverse uncertainty map for attention mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Cost Function</head><p>Finally, we trained our complete PDUN model with the help of answer generation loss and uncertainty loss. The answer generation loss L gen is the combination of cross entropy loss L CE , to generate each and every token in the answer sequence, KL divergence loss L KL , to bring the approximate posterior closer to N (0, 1), and the diversity loss L div 8, to ensure diverse answer generation. The cost function used for obtaining the parameters θ f of the attention network, θ c of the classification network, θ y of the prediction network and θ u for uncertainty network is as follows:</p><formula xml:id="formula_30">C(θ f , θ c , θ y , θ u ) = 1 n n j=1 L j y (θ f , θ c , θ y )+L j KL (θ f , θ c )+L j div (θ f , θ c )+ηL j u (θ f , θ c , θ u )</formula><p>where n is the number of examples, and η is a hyper-parameter that is fine-tuned using validation set and L c is standard cross entropy loss. We train the model with this cost function till it converges so that the parameters (θ f ,θ c ,θ y ,θ u ) deliver a saddle point </p><formula xml:id="formula_31">function (θ f ,θ c ,θ y ,θ u ) = arg max θ f ,θc,θy,θu (C(θ f , θ c , θ y , θ u ))<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the proposed method in the following steps: First, we evaluate our proposed uncertainty model against other variants described in section 5.2. Second, we have shown analysis results for epistemic uncertainty in figure-5 and aleatoric uncertainty in <ref type="figure" target="#fig_6">figure-6</ref>  Input: Image X I , Question X Q , History X H</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Output: Answer y 4:</p><p>while loop do 5:</p><p>Attention</p><formula xml:id="formula_32">features G f (G i (X I ), G q (X Q ), G H (X H )) ← f i 6: Answer Logit G y (G d (f i )) ←ŷ 7: Data Uncertainty G u (G d (f i )) ← σ 2 A 8: σ 2 W = σ 2 A + H(ŷ i,t ), (Ref: eq-4) 9:</formula><p>Ans cross entropy L y ← loss(ŷ, y)</p><p>10:</p><p>Variance Equalizer <ref type="bibr" target="#b63">[64]</ref> L V E := ReLU (exp σ 2 w − exp I ), <ref type="bibr">11:</ref> while t = 1 : #M C − Samples do 12:</p><formula xml:id="formula_33">Sample w t ∼ N (0, σ 2 W ) 13:</formula><p>Distorted Logits:ŷ i,t = w t +ŷ i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>Gaussian Cross Entropy <ref type="bibr" target="#b63">[64]</ref> L GCE = − y log p(ŷ d |F (.))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>Distorted Loss :L UDL = exp(L y − L GCE ) 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Aleatoric uncertainty loss L u = L GCE + L VE + L UDL</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>Compute Reverse Gradients w.r.t f i , ∇ u = −λ * ∂L U New attended feature:</p><formula xml:id="formula_34">f i = u,v f i * α new</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>Final attended feature: We further compare our network with state-of-the-art methods such as visdial <ref type="bibr" target="#b9">[10]</ref> model. Finally, we have provided some qualitative results of our visual dialog model.</p><formula xml:id="formula_35">f i = f i + f i 23: update θ f ← θ f − η∇</formula><p>The quantitative evaluation is conducted using standard retrieval metrics, namely (1) mean rank, (2) recall @k, (3) mean reciprocal rank (MRR) of the human response in the returned sorted list.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>We evaluate our proposed approach by conducting experiments on Visual Dialog dataset <ref type="bibr" target="#b9">[10]</ref>, which contains human annotated questions based on images of MS-COCO dataset. This dataset was developed by pairing two subjects on Amazon Mechanical</p><p>Turk to chat about an image. One person was assigned the job of a 'questioner' and the other person act as an 'answerer'. The questioner sees only the text description of an image which is present in caption from MS-COCO dataset. The image remains hidden to the questioner. Their task is to ask questions about this hidden image to "imagine the scene better". The answerer sees the image and caption and answers the questions asked by the questioner. The two of them can continue the conversation by asking and answering questions for 10 rounds at max. We have performed experiments on "Vis-Dial 1.0" version of the dataset. "VisDial v1.0" contains 123k dialogs on COCO-train and 2k on "VisualDialog val2018" images for val and 8k on "VisualDialog test2018"</p><p>for test-standard set. The caption is considered to be the first round in the dialog history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Analysis on Model Parameter for Uncertainty</head><p>Aleatoric Cross Entropy consists of distorted (Gaussian Cross Entropy (GCE)),  We see an improvement of around 4% in R@1, 8% in R@5 score, 9% in R@10 score and 5% in MRR score from the baseline score. The combination of GCE loss and CE performs best among all the 3 cases. The third block takes into consideration all the loss functions ACE (GCE+CE+VE) and we see an improvement of around 6% in R@1, 10% in R@5 score, 10% in R@10 score and 7% in MRR score from the baseline score. The behaviour of dialog turn for a particular example is shown in 6. The first part of the <ref type="figure" target="#fig_6">figure 6</ref> shows, how aleatoric uncertainty loss varies over different turns in visual dialog. As dialog progress the width of the dialog turn decreases. There is an eventual decreasing trend. The second part of the figure shows how variance equalizer loss varies over different turns in visual dialog. There is an eventual increasing trend.</p><p>We can observe the third and fourth turn is more uncertain. This indicates that to have a successful dialog, it basically depend on the central part of the dialog not only start and end tuns of the dialog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Analysis of Epistemic Uncertainty</head><p>One of the main purposes of the Bayesian deep learning is that it improves both the predictions and the uncertainty estimates of the model. We have measured uncertainty score in terms of mean and variance for all the dialog prediction in "val-v1.0" dataset.</p><p>We have also measured uncertainty for a single dialog in the dataset. Here, we split our training data into three parts. In first part the model is trained with 50% of the training data. Then, second part is trained by 75% of training data and third part is trained by full dataset as shown in second block of the  <ref type="table">Table 3</ref>: Ablation study on Noise parameters. <ref type="figure">Figure 7</ref>: Bayesian CNN experiment based on dropout, Max-pooling and average pooling. We found out that Architecture 5 works best and we used it throughout our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Analysis of Aleatoric Uncertainty</head><p>Here, we have captured data uncertainty by checking contribution of each terms in aleatoric uncertainty as shown in first block of the to see which answer is more uncertain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Analysis of Noise in Aleatoric &amp; Epistemic uncertainty</head><p>We have performed another ablation study for change in uncertainty based on noise value. We estimated both aleatoric and epistemic uncertainty for visual dialog dataset.</p><p>We variance is decreasing as it goes through more and more epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Analysis of Epistemic Uncertainty : Dropout</head><p>We experimented with various dropout ratios and use the following values for the same. For implementing Bayesian CNN, we used dropout ratio of (0.1, 0. 0.5) for each stack of convolutional layers respectively and 0.5 for FC layers. As the number of neurons increase in subsequent layers, we increase the dropout ratio for better generalization. For Bayesian LSTM, we use dropout ratio 0.3 for input &amp; hidden layers <ref type="bibr" target="#b65">[66]</ref> and for output layer we have used 0.5 dropout ratio similar to <ref type="bibr" target="#b56">[57]</ref>.</p><p>We further experimented with different ways of placing the dropout layer in the CNN architecture and observe that putting dropout after Max pooling layer works best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5.">Loss Visualization</head><p>We  <ref type="figure" target="#fig_8">Figure 9</ref>. Also we have seen same type of behavior in the variance plot. The variance decreases for all the losses as training progresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Diversity</head><p>We used Singular value decomposition (SVD) based evaluation metric to demonstrate diversity across various generated answers in a dialog. Here, we have randomly selected 400 dialogs. For each dialog, we sampled m number of latent embedding feature using the attentive encoder. Each one is having n-dimensional feature vector.   A, where = diag(σ 0 , σ 1 ....σ n−1 ); U and V T are m × m and n × n unitary matrices respectively. The overall variance in all dimensions is l 1 -norm, σ o = n−1 i=0 |σ i |. A large variance indicates very less correlation among the generated answers, which further implies large diversity among the answers as shown in table 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with state-of-the-art (SOTA)</head><p>The comparison of PDUN method with various state-of-the-art methods for visual dialog dataset v0.9 and v1.0 are provided in table 4. The first block of the <ref type="table" target="#tab_8">table 4</ref> consists of the state-of-the-art methods, second and third block consist of our methods.</p><p>We compared our results with baseline results of the model 'Late-fusion-QIH' <ref type="bibr" target="#b9">[10]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Result</head><p>We provide qualitative results, which easily distinguishes between results of Baseline dialog model with our aleatoric dialog model for two dialog generation example in <ref type="figure" target="#fig_0">figure 10</ref>. We can clearly see that our proposed method are able to capture uncertainty and minimize it, which further improves dialog results. For example, in the first image, the question is "Is this in a park ?". The baseline model's main focus is on the chair, where the uncertainty is very high. But our model explains about field, plant and background image, which provides the extra information about the query that eventually decreases the uncertainty as shown in <ref type="figure" target="#fig_0">figure 10</ref>. We visualize the certainty activation map of other two dialogs whose uncertainty score decrease over turns.  We provide qualitative results, which easily distinguishes between results of baseline dialog model with our aleatoric dialog model for three dialog generation example in <ref type="figure" target="#fig_0">figure 11</ref>. We can clearly see that our proposed method is able to capture uncertainty and minimize it, which further improves dialog results. Also, we have measured epistemic and aleatoric uncertainty and showed how uncertainty decreases as dialog turns in <ref type="figure" target="#fig_0">figure 12</ref>. We visualize the uncertainty by taking multiple samples and show how does it change as per samples as shown in <ref type="figure" target="#fig_0">Figure 13</ref>. We also made the GIF version of this visualization with name 'aleatoric uncertainty ques gradcam 100.gif' and other visualisation file present in the following link 2 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Evaluation Protocol</head><p>We have followed the evaluation protocol mentioned in <ref type="bibr" target="#b9">[10]</ref>. We use a retrieval setting to evaluate the responses at each round in the dialog. Specifically, every question in VisDial is coupled with a list of 100 candidate answers, which the models are asked to sort for evaluation purposes. Models are evaluated on standard retrieval metrics <ref type="formula" target="#formula_1">(1)</ref> mean rank, (2) recall @k and (3) mean reciprocal rank (MRR) of the human response in the returned sorted list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Preprocessing</head><p>We truncate captions/questions/answers longer than 24/16/8 words respectively.</p><p>We then build a vocabulary of words that occur at least 5 times in train, resulting in 8964 words. In our experiments, all 3 Bayesian LSTMs are single layer with 512dimensional hidden state. For Bayesian CNN we use pretrained VGG-19 <ref type="bibr" target="#b0">[1]</ref> with dropout to get the representation of image. We first re-scale the images to 448 × 448 pixels and take the output of FC7 layer which is 4096-dimensional as image feature.</p><p>We use the Adam optimizer with a base learning rate of 4e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel probabilistic architecture that is termed as the probabilistic diversity and uncertainty network (PDUN), for solving visual dialog. The main parts in the proposed architecture are the modules that capture uncertainty and diversity. We captured aleatoric and epistemic uncertainty that provide us with uncertainty estimates and these are further reduced using appropriate loss functions. We have particularly shown that the performance in the visual dialog is improved around 3.5% by the proposed network. Further the use of the diversity module obtained through a variational autoencoder allows us to generate diverse answers. We validate that indeed the diversity of the proposed network is high as compared to variants of the method.</p><p>These two contributions enable us to obtain a significantly improved model for solving the challenging visual dialog task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgment</head><p>We acknowledge the help provided by our DelTA Lab members and our family who have supported us in our research activity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Proposed Probabilistic Diversity and Uncertainty Network (PDUN) consists of three parts, viz. a) Probabilistic Representation Module encodes image feature with a question and history feature in an attentive manner. b) Diversity module captures the diversity, and diverse answer is generated using Variational Auto-Encoder. c) Uncertainty module predicts uncertainty of the network. bilistic representations for image, question, and history of the conversation using Bayesian CNN and Bayesian RNN modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Probabilistic Diversity Uncertainty Network(PDUN), Bayesian CNN/LSTM is used to obtain the embeddings g i , f i , h i which is then fused using the Fusion Module to get e f . Then correlation is found between fused embedding with answer option embedding. Finally, variance and logits output are obtained using MLP, which is then used in Logits Reparameterization Trick(LRT) to get final softmax output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Reverse Uncertainty based Attention Map (RUAM): We obtain attention embedding f i from the attention network G f using image, question and history embeddings g i , gq, g h . Then we classify into answer class and obtain the uncertainty present in the data. Then we obtain reverse uncertainty map with will combine with attention map to get better confidence on the attention map as shown in thefigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where G y and G v are the logits and aleatoric variance predictor of the classifier G d respectively. The whole model is trained with the help of uncertainty loss (More details are present in 4.4) and cross-entropy loss. The uncertainty loss helps the classifier to make the classifier features more robust for prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>This shows the measurement of Entropy, Variation, Softmax scores and Bayesian loss for Bayesian model with dropout value 0.5 and 0.1 in first and second plot respectively for capturing Epistemic Uncertainty</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>∂fi 18 : 19 : 20 :</head><label>181920</label><figDesc>Certainty Activation for attention α u,v = ∇ u * α u,v Certainty Activation for attention α u,v = ReLU (α u,v )+γ * ReLU (−α u,v ) New Attention gradient α new = α u,v + α u,v * α u,v 21:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Upper Graph: This shows how aleatoric uncertainty loss varies over different turns in visual dialog. There is an eventual decreasing trend. Lower Graph: This shows how variance equalizer loss varies over different turns in visual dialog. There is an eventual increasing trend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>randomly selected 200 examples on val dataset and applied noise to the image and question responses and observed that uncertainty value changes on seeing noise image and noise question. So we applied noise value γ of 0.8 to decrease pixel value and γ = 1.2 to increase pixel value i.e. there is inverse proportionality. Mean and standard deviation of uncertainties are recorded in the table. From table 3, it can be observed that aleatoric uncertainty is very small as compared to epistemic uncertainty. The aleatoric uncertainty changes much rapidly as noise changes in comparison to that of epistemic uncertainty. (a)Distorted loss(GCE) (b) Undistorted loss(CE) (c) Variance Equalizer loss(VE) This figure shows role of different types of Losses over Epochs. From the plot we observed that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>2, 0.3, 0.4, (a)Distorted Loss(GCE) (b) Undistorted loss(CE) (c) Variance Equalizer loss(VE) We have shown the variance flow plots over Epochs. This shows role of different type of loss over epoch. from the plot we observed that variance is decreasing as it goes through more and more epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>have analyzed the significance of Distorted loss (Gaussian Cross Entropy (GCE) Loss), Undistorted loss (Cross Entropy(CE) Loss) and Variance Equalizer (VE) loss as shown in the Figure 8. It is clear form the figure that all the losses converges as epoch progresses. Variance flow in the various losses is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure showsthe difference between aleatoric dialog results and baseline dialog results. In this figure, the first row refers to Grad-CAM visualization of first example for baseline visual dialog model and second row refers to Grad-CAM visualization of first example for Aleatoric visual dialog model and same scheme is followed for next 2 rows. The first column indicates target Image and corresponding caption and starting from second column is the visualization of rounds of dialog from round 1 to 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Difference Between Aleatoric dialog results and Baseline dialog results are shown in the figure. In this figure,The first row refer to Grad-CAM visualization of first example for baseline visual dialog model and second row refer to Grad-CAM visualization of first example for aleatoric visual dialog model and so on.. The first column indicates target Image and corresponding caption, second column indicates visualization of dialog round 1, third column refer to visualization of dialog round 2 and so on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>This figure provide aleatoric and epidemic variance and visualize the aleatoric uncertainty using Grad-CAM for a particular Dialog.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>We visualize the multiple outputs from the Bayesian neural network. We took 100 sample from the posterior distribution of dialog model for particular image, particular question. It shows how Grad-CAM is flowing for particular image, particular question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and in table 2. Third, we further analyze effect of noise in aleatoric and epistemic uncertainty in table 3. Fourth, we compare diversity score for different Algorithm 1 Reverse Uncertainty based Attention Map (RUAM)</figDesc><table><row><cell>1: procedure RUAM(I, Q, H)</cell></row><row><cell>2:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>variants of our model in table 5. Fifth, we compare our network with state-of-the-art methods such as 'visdial'<ref type="bibr" target="#b9">[10]</ref> in table 4. Then, we have shown the Grad-CAM<ref type="bibr" target="#b64">[65]</ref> visualization of activation due to aleatoric uncertainty and baseline model (late fusion).</figDesc><table /><note>y</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>undistorted (Cross Entropy (CE)) loss, and Variance Equalizer (VE) loss.. The first block of the table 1 analyses individual loss function and its comparison is provided in that table. We use these models as our baseline and compare other variations of</figDesc><table><row><cell>Type of Uncertainty</cell><cell>Mean</cell><cell>Std</cell></row><row><cell>Aleatoric (with CE)</cell><cell>0.0051</cell><cell>8.677</cell></row><row><cell>Aleatoric (with VE)</cell><cell>0.0044</cell><cell>7.353</cell></row><row><cell>Aleatoric (with GCE)</cell><cell>0.0039</cell><cell>3.431</cell></row><row><cell>Aleatoric (with ACE)</cell><cell>0.0032</cell><cell>2.119</cell></row><row><cell>Epistemic (50% training)</cell><cell cols="2">0.6680 66.9321</cell></row><row><cell>Epistemic (75% training)</cell><cell cols="2">0.6310 42.8923</cell></row><row><cell>Epistemic (100% training)</cell><cell cols="2">0.5520 36.8110</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Aleatoric &amp; Epistemic uncertainty measurement score. our model with the best single loss function. The GCE loss performs best among all the 3 losses. This is reasonable as GCE can guide the loss function to minimize the variance in the data. The second block of table 1 depicts the models which uses combination of the loss function as variations of our method such as GCE, VE or CE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>table 2 .</head><label>2</label><figDesc>It is observed the epistemic uncertainty decreases as training data increases.</figDesc><table><row><cell>Type of Uncertainty</cell><cell>Mean</cell><cell>Std</cell></row><row><cell>Aleatoric (original)</cell><cell cols="2">0.0067 8.956</cell></row><row><cell>Aleatoric (γ = 0.8)</cell><cell cols="2">0.0123 11.717</cell></row><row><cell>Aleatoric (γ = 1.2)</cell><cell cols="2">0.0034 6.353</cell></row><row><cell>Epistemic (original)</cell><cell cols="2">0.671 70.213</cell></row><row><cell>Epistemic (γ = 0.5)</cell><cell cols="2">0.701 71.893</cell></row><row><cell>Epistemic (γ = 0.8)</cell><cell cols="2">0.646 69.117</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>table 2</head><label>2</label><figDesc></figDesc><table><row><cell>. From the measurements,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>To measure the variance, all the answer embedding features can be concatenated into a feature matrix A ∈ R m×n . σ i can be obtained by SVD, L = U V T of matrix</figDesc><table><row><cell>Models</cell><cell>R1</cell><cell cols="2">R5 R10 MRR Mean</cell></row><row><cell>LF [10]</cell><cell cols="2">43.8 74.6 84.0 0.580</cell><cell>5.78</cell></row><row><cell>HRE [10]</cell><cell cols="2">44.8 74.8 84.3 0.586</cell><cell>5.65</cell></row><row><cell>MN [10]</cell><cell cols="2">45.5 76.2 85.3 0.596</cell><cell>5.46</cell></row><row><cell>HCIAE [35]</cell><cell cols="2">48.4 78.7 87.5 0.622</cell><cell>4.81</cell></row><row><cell>SF-1 [39]</cell><cell cols="2">48.1 78.6 87.5 0.620</cell><cell>4.79</cell></row><row><cell>AMEM [67]</cell><cell cols="2">48.5 78.6 87.4 0.622</cell><cell>4.85</cell></row><row><cell>NMN [68]</cell><cell cols="2">50.9 80.1 88.8 0.641</cell><cell>4.45</cell></row><row><cell>ECE (ours)</cell><cell cols="2">44.3 76.1 85.9 0.590</cell><cell>5.51</cell></row><row><cell>ACE (ours)</cell><cell cols="2">49.0 80.5 89.3 0.629</cell><cell>4.32</cell></row><row><cell cols="3">PDUN (ours) 49.2 81.0 90.5 0.634</cell><cell>4.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Results on dataset-v0.9 for Visual dialog</figDesc><table><row><cell cols="2">Model Diversity(σ 2 d )</cell></row><row><cell>VE</cell><cell>6.41</cell></row><row><cell>CE</cell><cell>22.231</cell></row><row><cell>GCE</cell><cell>27.845</cell></row><row><cell>ECE</cell><cell>24.980</cell></row><row><cell>ACE</cell><cell>32.12</cell></row><row><cell>PDUN</cell><cell>34.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Answer Diversity results for Visual dialog.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Results on dataset v1.0 for Visual dialog We observe that in terms of @R10 score, we obtain an improvement of around 10% from the baseline &amp; 3.5% from SOTA (NMN [68]) method. In terms of NDGC score 9% from base model &amp; 0.5% from SOTA model and in term of MRR, 7% from the base model &amp; 1% from SOTA (NMN [68]) model using our proposed method. .</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Standard shorthand notation for p(y = y * |x * , X, Y ) = p(y * |x * , D)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://delta-lab-iitk.github.io/PDUN/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object detection with deep learning: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>VQA: Visual Question Answering</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning cooperative visual dialog agents with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Visual dialog</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">U-cam: Visual explanation using uncertainty based class activation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lunayach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06306</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<title level="m">Matching Words and Pictures</title>
		<imprint/>
	</monogr>
	<note>submitted to JMLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th CVPR</title>
		<meeting>the 24th CVPR</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="776" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2953" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4613" to="4621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Differential attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1571" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep attention neural tensor network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning to count objects in natural images for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prügel-Bennett</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06059</idno>
		<title level="m">Generating natural questions about an image</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03493</idno>
		<title level="m">Creativity: Generating diverse questions using variational autoencoders</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multimodal differential network for visual question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4002" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning semantic sentence embeddings using sequential pair-wise discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Patro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2715" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="314" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Are you talking to me? reasoned visual dialog generation through adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">End-to-end optimization of goal-driven and visually grounded dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pietquin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05423</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two can play this game: Visual dialog with discriminative question generation and answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5756" to="5766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards diverse and natural image descriptions via a conditional gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2989" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00861</idno>
		<title level="m">Generating diverse and accurate visual captions by comparative adversarial learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Computational learning theory (COLT)</title>
		<meeting>of the Conference on Computational learning theory (COLT)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Ensemble learning in bayesian neural networks</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="215" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05424</idno>
		<title level="m">Weight uncertainty in neural networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<title level="m">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Understanding measures of uncertainty for adversarial example detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08533</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06455</idno>
		<title level="m">Bayesian uncertainty estimation for batch normalized deep networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern recognition and machine learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards dropout training for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02798</idno>
		<title level="m">Bayesian recurrent neural networks</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02158</idno>
		<title level="m">Bayesian convolutional neural networks with bernoulli approximate variational inference</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Attending to discriminative certainty for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dorman</surname></persName>
		</author>
		<ptr target="https://github.com/kyle-dorman/bayesian-neural-network-blogpost" />
		<title level="m">Bayesian neural network blogpost</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Visual reference resolution using attention memory for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3719" to="3729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<title level="m">Visual coreference resolution in visual dialog using neural module networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="153" to="169" />
		</imprint>
	</monogr>
	<note>Proceedings of the European Conference on Computer Vision (ECCV)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
