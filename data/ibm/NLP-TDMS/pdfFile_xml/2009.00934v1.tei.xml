<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SELF-SUPERVISED SMOOTHING GRAPH NEURAL NETWORKS A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-02">2 Sep 2020 September 3, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
							<email>lu.yu@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Pei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Brandeis University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhong</forename><surname>Ding</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Ant Financial Services Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longfei</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Ant Financial Services Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
							<email>xiangliang.zhang@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SELF-SUPERVISED SMOOTHING GRAPH NEURAL NETWORKS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-02">2 Sep 2020 September 3, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies learning node representations with GNNs for unsupervised scenarios. We make a theoretical understanding and empirical demonstration about the non-steady performance of GNNs over different graph datasets, when the supervision signals are not appropriately defined. The performance of GNNs depends on both the node feature smoothness and the graph locality. To smooth the discrepancy of node proximity measured by graph topology and node feature, we proposed KS2L -a novel graph Knowledge distillation regularized Self-Supervised Learning framework, with two complementary regularization modules, for intra-and cross-model graph knowledge distillation. We demonstrate the competitive performance of KS2L on a variety of benchmarks. Even with a single GCN layer, KS2L has consistently competitive or even better performance on various benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) have been leading an effective framework of learning graph representations, and have been demonstrated powerful in numerous tasks <ref type="bibr" target="#b24">[25]</ref>. The key of GNNs roots at the repeated aggregation over local neighbors to obtain smoothing node representations, making close nodes have similar representations by filtering out noise existing in the raw node features. Learning GNN models to maintain local smoothness usually depends on supervision signals such as node labels or self-supervision signals extracted from the input graph. Whereas, labels are not always available in many scenarios. Besides, the trained GNNs in supervised and semi-supervised ways are not universal applicable, and only serve the defined learning tasks, e.g., node classification when the GNNs were trained with node labels, or graph classification when the GNNs are trained with graph labels. The real-world application scenarios might necessitate that GNNs are trained in an unsupervised way, without any requirement of labels. However, the key challenge is the definition of the self-supervision signals from the input graph.</p><p>There are a group of unsupervised node representation learning models in the spirit of self-supervised learning. They construct supervision signals from the graph structure (e.g., local structure) to enforce the local nodes to have similar representations. There already exist several successful solutions based on random walks (e.g., DeepWalk <ref type="bibr" target="#b15">[16]</ref>, Node2Vec <ref type="bibr" target="#b4">[5]</ref>) or unified factorization method (e.g. NetMF <ref type="bibr" target="#b16">[17]</ref>) to enforce nodes in the same local context (e.g., neighbors in a window size) to be close (i.e., smoothness). As an initial GNN study, GraphSage <ref type="bibr" target="#b5">[6]</ref> directly applies the unsupervised loss function borrowed from DeepWalk <ref type="bibr" target="#b15">[16]</ref>. It has been shown that its performance downgraded in certain graphs that have weak locality measured by averaged clustering coefficient <ref type="bibr" target="#b18">[19]</ref>. For example, PubMed is such a graph with weak locality, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. GraphSage and its variants were hard to make significant performance improvements when making usage of the learned node representation in PubMed.</p><p>The story behind the inefficacy is that the self-supervising signals in DeepWalk are designed to extract local structure dependency, but discard the fact that the representations quality of GNNs depend on both node feature smoothness and the locality of the graph structure, referring to Theorem 1. We also empirically found that most of the neural encoders (e.g., GNNs, even simple MLPs) performed well on node classification in graphs like CS, which has a strong locality and large feature smoothness (defined as the inverse of average pairwise distance among the node features <ref type="bibr" target="#b6">[7]</ref>), shown in <ref type="figure" target="#fig_0">Figure 1</ref>. However, for graphs with a strong locality but a low node feature smoothness (e.g., "Computers", "Photo"), unsupervised methods directly optimizing structure proximity can have significant better performance than those (semi-)supervised methods working on both node features and structure proximity. Based on our theoretical understanding in Theorem 1 and empirical observation, we conjecture that the discrepancy of node proximity measured by graph topology and node feature is the key reason of such non-steady performance of GNNs over different graph datasets, when the supervision signals are not appropriately defined.</p><p>We propose a new self-supervised learning strategy to smooth the discrepancy of node proximity measured by graph topology and node feature, especially in the cases that either the node feature smoothness is low or the graph locality is weak. We introduced a global-level consistency regularization module, known as intra-and cross-model graph knowledge distillation. The intra-model regularization aims at forcing the learnt representations and node features to have consistent subgraph-level relation distribution. Through stacking the cross-model distillation module, we implicitly mimic the deep smoothing operation with a shallow GNN (e.g. only a single GNN layer), while avoiding to bring noisy information from high-order neighbors since each GNN only depends on the neighbors at low-order. Thus our proposed model KS2L -a novel graph Knowledge distillation regularized Self-Supervised Learning framework can learn shallow but powerful GNN. Even with a single GCN layer, it has consistently competitive or even better performance on various benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph Neural Networks. In recent years, we have witnessed a fast progress of graph neural network in both methodology study and its applications. As one of the pioneering research, Defferrard et al. <ref type="bibr" target="#b1">[2]</ref> generalizes the convolution operation to non-Euclidean graph data through bridging the gap between graph spectral theory and deep learning methods. Kipf et al. <ref type="bibr" target="#b8">[9]</ref> reduce the computation complexity to 1-order Chebyshev approximation with an affined assumption. Different sampling strategies <ref type="bibr" target="#b0">[1]</ref> are explored to control the receptive field (i.e. the number of neighbors) while keeping the estimated aggregated embedding unbiased. NT et al. <ref type="bibr" target="#b14">[15]</ref> justify that classical graph convolution network (GCN) and its variants are just low-pass filter. At the same time, both of studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10]</ref> propose to replace standard GCN layer with a normalized high-order low-pass filters (e.g. personalized PageRank, heats kernel). This conclusion can help to answer why simplified GCN proposed by Wu et al. <ref type="bibr" target="#b23">[24]</ref> has competitive performance with complicated multi-layer GNNs. Besides GCNs, many novel GNNs have been proposed, such as multi-head attention models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13]</ref>, graph U-Net <ref type="bibr" target="#b3">[4]</ref> etc.</p><p>Self-supervised Learning Strategies in Graph Representation Learning. In addition to the line of work following Deepwalk for constructing self-supervising signals, mutual information (MI) maximization <ref type="bibr" target="#b22">[23]</ref> over the input and output representations shows up as an alternative line. In analogy to distinguishing that output image representation is generated from the input image patch or noisy image, Veličković et al. <ref type="bibr" target="#b22">[23]</ref> propose deep graph infomax (DGI) criterion to maximize the mutual information between a high-level "global" graph summary vector and a "local" patch representation. However, the graph-level vector summarizes the global context, which might not be shared by all nodes, and thus can easily transfer noisy information into the interactions among nodes. Another line of selfsupervised learning methods like InfoGraph <ref type="bibr" target="#b19">[20]</ref> and pre-training graph neural networks <ref type="bibr" target="#b7">[8]</ref> are designed for learning representations for a whole graph, not for nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Let G = {V, E, X} denote an attribute graph where V is the node set {v i ∈ V}, E is the edge set, and X ∈ R N ×F is the node feature matrix where each row x i stands for the node feature of v i . We use A represents the node relation matrix where a ij = 1 if there existing a link between node v i and v j , i.e., e ij ∈ E, otherwise a ij = 0. We define the degree matrix D = diag(d 1 , d 2 , · · · , d N ) where each element equals to the row-sum of adjacency matrix d i = j a ij .</p><p>Our goal is to learn an encoder Φ(X, A) = H, where H ≡ {h 1 , h 2 , · · · , h N } is the representation learned for nodes in V. The vector h i ∈ R F ′ can be used for various downstream tasks, e.g., node classification and link prediction. GNNs generate h i through repeated aggregation over local neighbors' features. Formally the l-th layer of a GNN can be defined as</p><formula xml:id="formula_0">h l i = COM BIN E(h l−1 i , AGGREGAT E({(h l−1 i , h l−1 j ) : j ∈ N (i)})),<label>(1)</label></formula><p>where h l i denotes the representation of node v i at the l-th layer, N (i) is the set of neighbors of node v i . The COM BIN E can be addition or concatenation operations, and AGGREGAT E can be weighted sum over neighbors. The vector h i actually summarizes a graph patch centered around node v i , the size of the summarized region exponentially grows as the increasing of l. In what follows, we refer to h i as patch representations. Let X = XW denote the low-level node feature which can be regarded as the 0-th layer. Since the low-level feature x i only depends on node v i itself, we refer to it as the node feature. Besides the node and patch representations, one can also summarize a graph-level representation through applying a READOUT operation over H. Typical operations include mean or max pooling functions or more sophisticated ones <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1. Suppose that a GNN aggregates node representations as h</head><formula xml:id="formula_1">l i = j∈N (i)∩vi α ij h l−1 j ,</formula><p>where α ij can be generated through attention operation or the element of a normalized relation matrix. The graph neural operator approximately equals to a second-order proximity graph regularization over the patch representations, whose smoothness depends on both node feature smoothness and second-order node proximity.</p><p>The proof of Theorem 1 is provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-Supervised Learning of GNNs</head><p>The weights in COMBINE/AGGREGATE functions of GNNs are learned to generate H to be used in downstream applications. For (semi-)supervised learning (e.g., node classification), the loss function of such GNNs can be defined by the classification loss. When there is no labeled data available, self-supervised learning as a brunch of unsupervised learning sheds light to design learning tasks with supervised signal from input graph itself. Contrastive learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14]</ref>, one of the self-supervised learning strategies, can be adopted to distinguish the connected (positive sample) and unconnected (negative sample) nodes in the graph. The objective is to enforce the predicted proximity derived from h i , h j for existing edges e ij ∈ E to be larger than that defined on h i , h k for non-existing edges e ik / ∈ E. Despite theoretically reasonable, the loss function formed by the constrastive samples in practice (e.g., academic networks) has several limitations. First, h i and h j are significantly closer than h i and h k in the embedding space if v i and v k have a longer shortest path than v i and v j . Such discrepancy is already exhibited by the distance on graph. The representation h optimized to discriminate such constrastive samples captures more the feature of their location difference on graph topology, than the node attribute difference. In other words, the learned h can be dominated by the difference on graph location, and make content difference under-represented.</p><p>To overcome the above-discussed issue in contrastive loss, we propose to compare h with x, rather than comparing two of h. Concretely, instead of constructing contrastive learning loss over the patch representations h at the same GNN layer, we turn to maximize the predicting probability between a patch representation h and its input node features x in its neighbors. Formally, for a sample set {v i , v j , v k } where e ij ∈ E but e ik / ∈ E, the loss ℓ i jk is defined on the pairwise  comparison of (h i , x j ) and (h i , x k )). Therefore, our self-supervised learning for GNN has the loss function defined below,</p><formula xml:id="formula_2">L ssl = eij ∈E e ik / ∈E −ℓ i jk (ψ(h i , x j ), ψ(h i , x k )) + λR(G),<label>(2)</label></formula><p>where ℓ() can be an arbitrary contrastive loss function, ψ is a scoring function, and R is the regularization function with weight λ for implementing graph structural constraints (to be introduced in next section). There are lots of candidates for contrastive loss ℓ(), typically including logistic pairwise loss ln σ</p><formula xml:id="formula_3">(ψ(h i , x j ) − ψ(h i , x k )), or cross-entropy loss ln σ(ψ(h i , x j )) + ln σ(−ψ(h i , x k )), where σ(x) = 1 1+exp(−x)</formula><p>. Discussion: It's noted that the proposed objective function is related to Deep Graph Infomax (DGI) <ref type="bibr" target="#b22">[23]</ref>. Both of them are designed for learning node representations in unsupervised ways. DGI focuses on maximizing mutual information between the graph summary vector and patch representation. By doing so, the graph summary representation shared by all of nodes may transfer noisy information to among unrelated nodes. While the proposed strategy here emphasizes more on the dependency between a patch representation and its input node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Knowledge Distillation Regularization</head><p>The objective function defined in Equation <ref type="formula" target="#formula_2">(2)</ref> models the interactions between output patch representations h and input node features x, which can be regarded as an intra-model knowledge distillation process from smoothed node embeddings to denoise low-level node features. However, the raised contrastive samples over edge connectivity might fail to represent a whole picture of node representation distribution. To supplement the loss defined on the individual pairwise samples, we introduce a regularization term ensuring the distribution consistency on the relations between the learned patch representations H and the node features X over a set of randomly sampled nodes. Let LS = {LS 1 , LS 2 , · · · , LS N } denote the randomly sampled pseudo relation graph, where LS i ⊂ V and |LS i | = d is the number of sampled pseudo local neighbors for center node v i . The estimated proximity for each node in i-th local structure LS i is computed by</p><formula xml:id="formula_4">S t ij = exp(ψ(h i , x j )) vj ∈LSi exp(ψ(h i , x j )) , S s ij = exp(ψ( x i , x j )) vj ∈LSi exp(ψ( x i , x j ))<label>(3)</label></formula><p>where S t ij and S s ij denote the similarity estimated from different node representations between node v i and v j . The S t ij will act as the teacher signal to guide the node features X = { x 1 , x 2 , · · · , x N } to agree on the relation distribution over a random sampled graph. For the node v i , the relation distribution similarity can be measured as</p><formula xml:id="formula_5">S i = CrossEntropy(S t [i,·] , S s [i,·]</formula><p>). Then we can compute the relation similarity distribution over all the nodes as</p><formula xml:id="formula_6">R g = N i=1 S i ,<label>(4)</label></formula><p>where R g acts as a regularization term generated from the intra-model knowledge, and to push the learned patch representation H and node features X being consistent at a subgraph-level.</p><p>The second regularization is to introduce the cross-model distillation for addressing the over-smoothing issue that limits most of GNNs to go deep. The cross-model distillation can guide the target GNN model by transferring the learned self-supervised knowledge. Through stacking the cross-model distillation module, we implicitly mimic the deep smoothing operation with a shallow GNN (e.g. only a single GNN layer), while avoiding to bring noisy information from high-order neighbors, since each GNN only depends on the neighbors at low-order. The overall cross-model distillation framework is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Working with a sequence of M GNN models</p><formula xml:id="formula_7">{Φ 1 , Φ 2 , · · · , Φ M }, each Φ m (X, A) = {H m , X m } distills knowledge from the previous model Φ m−1 .</formula><p>Since no labels are available, we propose to implement representation distillation <ref type="bibr" target="#b20">[21]</ref> with the constraint of graph structure. The knowledge distillation module consists of two parts, defined as</p><formula xml:id="formula_8">R m cross = KD(H m−1 , X m |G) + KD(H m−1 , H m |G)<label>(5)</label></formula><p>where H m−1 is the patch representations from GNN model Φ m−1 , and X m = X m W. To define module KD(·), we should meet several requirements: 1) this function should be easy to compute and friendly to back-propagation strategy; and 2) it should stick to the graph structure constraint. We resort to the conditional random field (CRF) <ref type="bibr" target="#b10">[11]</ref> to capture the pairwise relationship between different nodes. For a general knowledge distillation module KD(Y, Z|G), the dependency of Z on Y can be given following the CRF model:</p><formula xml:id="formula_9">P (Z|Y ) = 1 C(Y ) exp(−E(Z|Y )),<label>(6)</label></formula><p>where C(·) is the normalization factor and E(·) stands for the energy function, defined as follows:</p><formula xml:id="formula_10">E(Z i |Y i ) = ψ u (Z i , Y i ) + ψ p (Z i , Z j , Y i , Y j ) = α||Z i − Y i || 2 2 + (1 − α) j∈N (i) β ij ||Z i − Z j || 2 2 ,<label>(7)</label></formula><p>where ψ u and ψ p are the unary and pairwise energy function, respectively. The parameter α ∈ [0, 1] is to control the importance of two energy functions. When Z is the node feature of student model and Y is the patch representation from teacher model Φ m−1 , the energy function defined in Equation <ref type="formula" target="#formula_10">(7)</ref> enforces the node v i representation from student model to be close to that in the teacher model and its neighbor nodes. After obtaining the energy function, we can resolve the CRF objective with the mean-field approximation method by employing a simple distribution Q(Z) to approximate the distribution P (Z|Y ). Specifically distribution Q(Z) can be initialized as the product marginal distributions as Q(Z) = Π N i=1 Q i (Z i ). Through minimizing the KL divergence between these two distributions as follows:</p><p>arg min KL(Q(Z)||P (Z|Y )). (8) Then we can get the optimal Q * i (Z i ) as follows:</p><formula xml:id="formula_11">ln Q * i (Z i ) = E j =i [ln P (Z j |Y j )] + const.<label>(9)</label></formula><p>According to Equation <ref type="formula" target="#formula_9">(6)</ref> and <ref type="formula" target="#formula_10">(7)</ref>, we can get</p><formula xml:id="formula_12">Q * i (Z i ) ∼ exp(α||Z i − Y i || 2 2 + (1 − α) j∈N (i) β ij ||Z i − Z j || 2 2 ),<label>(10)</label></formula><p>which shows that Q * i (Z i ) is a Gaussian function. By computing its expectation, we have the optimal solution for Z i as follows:</p><formula xml:id="formula_13">Z * i = αY i + (1 − α) j∈N (i) β ij Z j α + (1 − α) j∈N (i) β ij<label>(11)</label></formula><p>Then we can get the cross-model knowledge distillation rule by enforcing the node representations from student model to have minimized metric distance to Z * i . After replacing the random variables Y i as the node representation h m−1 i of teacher model Φ m−1 , then we can get the final distillation regularization as follows: where x m i denotes the feature of node v i from m-th GNN model, and h m i denotes the output patch representation for node v i of m-th GNN model. In terms of β ij in Equation <ref type="formula" target="#formula_0">(11)</ref>, we have many choices such as attentive weight, or mean pooling etc. In this work, we simply initialize it with mean-pooling operation over the node representations.</p><formula xml:id="formula_14">KD(H m−1 , X m |G) = || x m i − αh m−1 i + (1 − α) j∈N (i) β ij x m j α + (1 − α) j∈N (i) β ij || 2 2 KD(H m−1 , H m |G) = ||h m i − αh m−1 i + (1 − α) j∈N (i) β ij h m j α + (1 − α) j∈N (i) β ij || 2 2<label>(12)</label></formula><p>The overall self-supervised learning objective in this work can be extended as follows by taking the two regularization terms:</p><formula xml:id="formula_15">L ssl = M m=1 eij ∈E e ik / ∈E −ℓ i jk (ψ(h m i , x m j ), ψ(h m i , x m k )) + λ(R g + R m cross ),<label>(13)</label></formula><p>where Φ 0 can be initialized through optimizing the proposed self-supervised learning without the cross-model distillation regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>Dataset and Baselines. To evaluate the proposed self-supervised training method for GNN, we compare it with various state-of-the-art methods on six datasets including three citation networks (Cora, Citeseer, Pubmed), two Amazon product co-purchase networks (Computers, Photo), and one co-author network subjected to a subfield Computer Science (CS). The citation networks can be obtained from the official implementation 1 of GCN <ref type="bibr" target="#b8">[9]</ref>. The Amazon and co-author graphs are benchmark datasets officially available at pytorch_geometric <ref type="bibr" target="#b2">[3]</ref>. The data statistics can be found in <ref type="table" target="#tab_1">Table 1</ref>. The baselines we compared include Label Propagation (LP) <ref type="bibr" target="#b27">[28]</ref>, Planetoid <ref type="bibr" target="#b25">[26]</ref>, Chebyshev filters (ChebNet) <ref type="bibr" target="#b1">[2]</ref>, MLP <ref type="bibr" target="#b8">[9]</ref>, Graph Convolution Network (GCN) <ref type="bibr" target="#b8">[9]</ref>, Simplified GCN (SGC) <ref type="bibr" target="#b23">[24]</ref>, Disentangled Graph Convolution Network (DisenGCN) <ref type="bibr" target="#b12">[13]</ref>, Graph Attention Network (GAT) <ref type="bibr" target="#b21">[22]</ref>, Graph Markov Neural Network (GMNN) <ref type="bibr" target="#b17">[18]</ref>, Deep Graph Infomax (DGI) <ref type="bibr" target="#b22">[23]</ref>.</p><p>Experiment Configuration. LP and Planetoid are two basic methods. We quote their performance results from <ref type="bibr" target="#b8">[9]</ref>. In terms of the rest of baselines, we use the official code provided by the authors. If the results are not previously reported, we implement them and conduct a hyper-parameter search according to the original paper. We use a one-layer GCN as the base model Φ, for the proposed self-supervised method (KS2L) to demonstrate its capability to learn a shallow GNN with competitive or even better performance than deep GNNs. The specific model configuration can be found from the supplementary materials. The results shown in this work are average value over 10 runs for each task.</p><p>For the node classification, the experimental configuration for the three citation data is the same as <ref type="bibr" target="#b8">[9]</ref>, and we use the default hyper-parameters for the baselines suggested by the authors. To make a fair comparison on the other co-purchase and co-author networks, we randomly select 20% nodes for supervised training GNNs, 10% node as validation set, and the rest of nodes are left for testing. For the unsupervised methods (i.e. DGI and the proposed KS2L), they can access to the complete graph without using node labels.</p><p>For the link prediction task, we use leave-one-out strategy to randomly select one edge for each node as testing, and the rest as the training set. All baselines involved in this experiment only work on the graph built from the training set. For the supervised learning models like GCN, GAT, DisenGCN, GMNN, we still use the same configure as the node classification task to select labelled nodes to train the model, which means the nodes in the selected testing edges can still be labelled nodes for the supervised training strategy. While the unsupervised methods like DGI and KS2L can only access to the edges in the training set.</p><p>Node Classification. The GNN methods compared here use either convolution or attentive neural networks. For a fair comparison, the proposed KS2L is applied to a one-layer SGC with the second-order adjacent matrix. <ref type="table" target="#tab_2">Table 2</ref> shows that the performance of simple GCN learned by the proposed method KS2L consistently and significantly outperforms the other baselines learned by supervised and unsupervised objectives. It's noted that DisenGCN iteratively applies   attentive routing operation to dynamically reshape node relationships in each layer. By default, it has 5-layers and iteratively applies 6 times, which typically is a deep GNN. Our proposed model KS2L can empower the single-layer GNN through iterative cross-model knowledge distillation. The results shown in the <ref type="table" target="#tab_2">Table 2</ref> are obtained with M = 5 iterations on Citeseer data, but less than M = 3 iterations for the other data.</p><p>Link Prediction. The goal of this task is to answer the question about whether the learned node representations can keep the node proximity. The performance of each method is measured with AUC value. All of the methods in this experiment have the same model configuration as the node classification task. From the results shown in <ref type="table" target="#tab_3">Table 3</ref>, we can see that KS2L still outperforms the baselines including GNNs learned with supervised and unsupervised objectives.</p><p>Linking <ref type="figure" target="#fig_0">Figure 1</ref> to the results in <ref type="table" target="#tab_2">Table 2</ref>-3, we see that classification of nodes in graph CS is indeed an easy task, and most of the GNNs models have similar good performance. However, for link prediction, unsupervised models (DGI and our model) learned better h than those with supervision information, obviously because the supervision information is for node classification, not for link prediction. On Computers and Photo dataset, our KS2L-GCN made significant improvement on the node classification accuracy (around 9.2% and 3.7% over the second best, respectively), and good improvement on link prediction as well (5% and 1.4% over the second best, respectively). These two datasets with strong locality but low node feature smoothness are in fact not GNN-friendly. Our self-supervised strategy well addresses the limits which GNNs are suffering. PubMed is another typical graph which has high feature smoothness but weak locality. Such discrepancy challenges the GNNs on learning high-quality h, especially for link prediction. Our model improves the best supervised models by 7.6%, and DGI by 5.3%. Robustness Against Incomplete Graphs. With the same experimental configuration as link prediction task, we also validate the performance of learned node embeddings from incomplete graph for node classification task. According to the results in <ref type="table" target="#tab_4">Table 4</ref>, KS2L still outperforms baseline methods in most cases, demonstrating the robustness of KS2L performance.</p><p>Ablation Study. We conduct node classification experiments to validate the contribution of each component of the proposed KS2L, which are denoted by l i jk , R g and R m cross in Equation <ref type="bibr" target="#b12">(13)</ref>. From <ref type="table" target="#tab_5">Table 5</ref>, we can see that the intraand cross-model knowledge distillation jointly improve the performance of the proposed self-supervised learning for smoothing GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose a self-supervised learning method (KS2L) regularized by graph structure to learn unsupervised node representations for downstream tasks. We conduct thorough experiments on node classification and link prediction tasks to evaluate the unsupervised node representations. Experimental results show that KS2L helps to learn competitive shallow GNN outperforming the state-of-the-art GNN learned with supervised or unsupervised objectives. GNN has already been applied to diverse applications. In the future, we'd like to explore potential applications of self-supervised learning method for few-shot tasks, and draw lessons from unsupervised network representations to develop more robust and efficient self-supervised learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our work presents a new design of self-supervised learning GNN model. The model is generally applicable on attributed graphs, such as academic citation graphs, co-author graphs, social graphs, and recommendation bipartite graphs. Since GNN is an active research topic, a broad group of researchers will be benefited from our work, because this is a new model for the first time addressing the discrepancy of node proximity measured by graph topology and node attributes. The datasets used in our experimental evaluation are widely used benchmark graphs, based on which we made a fair comparison between our proposed model and baseline models. Our implementation code will be available in public for the verification and reproducibility, and the usage by others.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>0.05 0.10 0.15 0.20 0.25 0.30 0.35 0Locality and node feature smoothness in the six graphs used in experimental evaluation. The larger feature smoothness value is, the more smoothing feature we have.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture of the proposed self-supervised learning GNN with Intra-and Cross-model Distillation (top); The pioneering works deep graph infomax and InfoGraph (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of datasets used for experimental analysis</figDesc><table><row><cell>Data</cell><cell>Type</cell><cell>#Nodes</cell><cell>Edges</cell><cell cols="3">Features Labels Label Rate</cell></row><row><cell>Cora</cell><cell>Citation</cell><cell>2,708</cell><cell>5,429</cell><cell>1,403</cell><cell>7</cell><cell>0.052</cell></row><row><cell>Citeseer</cell><cell>Citation</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell><cell>0.036</cell></row><row><cell>Pubmed</cell><cell>Citation</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell><cell>0.003</cell></row><row><cell cols="2">Computers Co-purchase</cell><cell>13,752</cell><cell>287,209</cell><cell>767</cell><cell>10</cell><cell>0.2</cell></row><row><cell>Photo</cell><cell>Co-purchase</cell><cell>7,650</cell><cell>143,663</cell><cell>745</cell><cell>8</cell><cell>0.2</cell></row><row><cell>CS</cell><cell>Co-author</cell><cell>18,333</cell><cell>81,894</cell><cell>6,805</cell><cell>15</cell><cell>0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of node classification (in %).</figDesc><table><row><cell>Input</cell><cell>Methods</cell><cell cols="5">Cora Citeseer PubMed Computers Photo</cell><cell>CS</cell></row><row><cell></cell><cell>ChebNet</cell><cell>81.2</cell><cell>69.8</cell><cell>74.4</cell><cell>70.5</cell><cell>76.9</cell><cell>92.3</cell></row><row><cell></cell><cell>MLP</cell><cell>55.1</cell><cell>46.5</cell><cell>71.4</cell><cell>55.3</cell><cell>71.1</cell><cell>85.5</cell></row><row><cell></cell><cell>LP</cell><cell>68.0</cell><cell>45.3</cell><cell>63.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Planetoid</cell><cell>75.7</cell><cell>64.7</cell><cell>77.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>X,A,Y</cell><cell>GCN SGC</cell><cell>81.5 81.0</cell><cell>70.3 71.9</cell><cell>79.0 78.9</cell><cell>*77.6 55.7</cell><cell>85.2 69.7</cell><cell>92.4 92.3</cell></row><row><cell></cell><cell>GAT</cell><cell>83.0</cell><cell>72.5</cell><cell>79.0</cell><cell>71.4</cell><cell>86.1</cell><cell>92.2</cell></row><row><cell></cell><cell>DisenGCN</cell><cell>*83.7</cell><cell>*73.4</cell><cell>80.5</cell><cell>52.7</cell><cell cols="2">*88.7 *93.3</cell></row><row><cell></cell><cell>GMNN</cell><cell>83.7</cell><cell>73.1</cell><cell>*81.8</cell><cell>68.8</cell><cell>85.9</cell><cell>92.7</cell></row><row><cell>X,A</cell><cell>DGI KS2L-GCN (ours)</cell><cell>82.3 84.6</cell><cell>71.8 74.2</cell><cell>76.8 83.8</cell><cell>68.2 86.8</cell><cell>78.2 92.4</cell><cell>92.4 *93.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>AUC of link prediction (in %).</figDesc><table><row><cell>Input</cell><cell>Methods</cell><cell cols="5">Cora Citeseer PubMed Computers Photo</cell><cell>CS</cell></row><row><cell></cell><cell>GCN</cell><cell>82.4</cell><cell>84.1</cell><cell>89.2</cell><cell>75.3</cell><cell>77.5</cell><cell>88.8</cell></row><row><cell></cell><cell>GAT</cell><cell>81.5</cell><cell>85.6</cell><cell>79.8</cell><cell>78.8</cell><cell>*86.0</cell><cell>90.8</cell></row><row><cell>X,A,Y</cell><cell>DisenGCN</cell><cell>85.6</cell><cell>87.5</cell><cell>87.8</cell><cell>*80.9</cell><cell>81.8</cell><cell>91.2</cell></row><row><cell></cell><cell>GMNN</cell><cell>85.2</cell><cell>85.9</cell><cell>89.6</cell><cell>70.6</cell><cell>70.6</cell><cell>91.1</cell></row><row><cell>X,A</cell><cell cols="2">DGI KS2L-GCN (ours) *93.9 94.4</cell><cell>*94.8 95.4</cell><cell>*91.9 97.2</cell><cell>68.8 85.9</cell><cell>66.1 87.4</cell><cell>*93.7 94.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy (in %) of node classification after leave-one-out neighbor removal. The symbol ↓ means that the classification accuracy downgrades comparing with the results shown inTable 2. ↓ 11.4% 68.4 ↓ 6.4% *80.5 ↓ 1.5% 66.8 ↓ 21.8% *82.6 ↓ 4.8% 92.2 ↓ 0.5% DGI 69.1 ↓ 16.1% *70.8 ↓ 1.4% 68.3 ↓ 11.1% 52.9 ↓ 22.4% 57.1 ↓ 27.1% 91.8 ↓ 0.6%</figDesc><table><row><cell>Methods</cell><cell>Cora</cell><cell>Citeseer</cell><cell>PubMed</cell><cell>Computers</cell><cell>Photo</cell><cell>CS</cell></row><row><cell>GCN</cell><cell>76.2 ↓ 6.5%</cell><cell>68.7 ↓ 2.2%</cell><cell cols="4">76.6 ↓ 3.0% 65.5 ↓ 15.5% 71.2 ↓ 16.4% 92.1 ↓ 0.3%</cell></row><row><cell>GAT</cell><cell cols="2">72.1 ↓ 13.1% 68.8 ↓ 5.1%</cell><cell cols="4">75.0 ↓ 5.1% *70.1 ↓ 1.8% 56.5 ↓ 34.3% 91.7 ↓ 0.5%</cell></row><row><cell cols="2">DisenGCN *77.1 ↓ 8.0%</cell><cell>68.6↓ 9.8%</cell><cell>75.5 ↓6.5%</cell><cell>48.6 ↓ 7.8%</cell><cell>80.6 ↓ 9.1%</cell><cell>92.8 ↓ 0.4%</cell></row><row><cell cols="2">GMNN 74.1 KS2L-GCN 80.1 ↓ 5.5%</cell><cell>71.7 ↓ 2.9%</cell><cell>82.7 ↓1.3%</cell><cell>84.5↓ 2.6%</cell><cell>89.0↓3.7%</cell><cell>*92.4 ↓ 0.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of the distillation component influence in node classification accuracy (in %).</figDesc><table><row><cell>Distillation</cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell>l i jk jk + Rg l i cross jk + R m l i jk + Rg + R m l i cross</cell><cell>82.4 83.5 83.2 84.6</cell><cell>72.8 73.3 73.1 74.2</cell><cell>82.3 83.5 83.6 83.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The best model configuration for reproducing the experimental results of node and link classification tasks.</figDesc><table><row><cell>Data</cell><cell>α</cell><cell>λ</cell><cell>M</cell><cell>F</cell></row></table><note>′ for each Φ m</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tkipf/gcn</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Proof for Theorem 1 Definition 1 (Feature Smoothness <ref type="bibr" target="#b6">[7]</ref>). We define the feature smoothness λ f over the normalized space X = [0, 1] F as follows:</p><p>where || · || 1 denotes the Manhattan norm.</p><p>Noted that we use normalized 1 λ f in Introduction section for better showing the influence of feature smoothness and locality on the model performance.</p><p>Definition 2 (Second-order Graph Regularization). The objective of second-order graph regularization is to minimize the following equation</p><p>where s ij is the second-order similarity which can be defined as cosine similarity s ij = c∈N (i)∩N (j) αic·αjc ||αi·||2||αj·||2 , and h i denotes the node representation.</p><p>Proof. Here we mainly focus on analyzing GNNs whose aggregation operator mainly roots on weighted sum over the neighbors, i.e. h l = j∈N (i)∩vi α ij h l−1 j . Typical examples include but limited to GCN <ref type="bibr" target="#b8">[9]</ref> where α ij can be the element of normalized adjacent matrixÃ = D − 1 2 (A + I)D − 1 2 , GAT <ref type="bibr" target="#b21">[22]</ref> where α ij can be the attentive weight for each head. The node representation h l i can be divided into three parts: the node representations α ii h l−1 i , the sum of common neighbor representations S i = c∈N (i)∩N (j) α ic h l−1 c , the sum of non-common neighbor representations</p><p>The distance between the representations h l i and h l j is:</p><p>From Equation <ref type="bibr" target="#b14">15</ref> we can see that the upper bound of similarity of a pair of nodes is mainly influenced by local feature smoothness and structure proximity. According to the Definition 2 and the proof shown above, if a pair of node (v i , v j ) has smoothed local features and similar structure proximity with many common similar neighbors (i.e.α ic ≈ α jc ), the obtained node representation of a GNN will also enforce their node representations to be similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Configuration</head><p>The proposed method has hyper-paramters including regularization weight λ, α for balancing the contribution of teacher model and local neighbors, M. M = 0 means that we only train the given GNN without cross-model distillation module. If M &gt; 0, we use the last model (i.e.Φ M ) as the target model to generate the node representations. Since we use the mean pooling operation, then the β equals to 1 N (i) by default. The GNN we used in this paper can be defined as:</p><p>where W ∈ R F ×F ′ and σ(·) denotes the activation function. For each hyper-parameter, we use grid search method to valid the best configuration from pre-defined search space, specifically, α ∈ {0, 0.1, 0.5, 1.0}, λ ∈ {0.0, 0.1, 0.5, 1.0}, M ∈ {0, 1, · · · , 5}, and F ′ ∈ {2048, 1024, 512, 256}.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<title level="m">Fast graph representation learning with pytorch geometric</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring and improving the use of graph information in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaili</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Chang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13333" to="13345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Label efficient semi-supervised learning via graph filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9582" to="9591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4212" to="4221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gmnn</surname></persName>
		</author>
		<title level="m">Graph markov neural networks. In ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5241" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalizations of the clustering coefficient to weighted complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jari</forename><surname>Saramäki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Kivelä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka-Pekka</forename><surname>Onnela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimmo</forename><surname>Kaski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janos</forename><surname>Kertesz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">27105</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
