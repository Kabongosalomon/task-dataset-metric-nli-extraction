<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Contrast Learning for Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<email>gbli@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
							<email>yzyu@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Contrast Learning for Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Salient object detection has recently witnessed substantial progress due to powerful features extracted using deep convolutional neural networks (CNNs). However, existing CNN-based methods operate at the patch level instead of the pixel level. Resulting saliency maps are typically blurry, especially near the boundary of salient objects. Furthermore, image patches are treated as independent samples even when they are overlapping, giving rise to significant redundancy in computation and storage. In this paper, we propose an end-to-end deep contrast network to overcome the aforementioned limitations. Our deep network consists of two complementary components, a pixel-level fully convolutional stream and a segment-wise spatial pooling stream. The first stream directly produces a saliency map with pixel-level accuracy from an input image. The second stream extracts segment-wise features very efficiently, and better models saliency discontinuities along object boundaries. Finally, a fully connected CRF model can be optionally incorporated to improve spatial coherence and contour localization in the fused result from these two streams. Experimental results demonstrate that our deep model significantly improves the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual saliency aims at identifying the most visually distinctive parts in an image, and has received increasing interest in recent years. Though early work primarily focused on predicting eye-fixations in images, research has shown that salient object detection, which emphasizes object-level integrity of saliency prediction results, is more useful and can serve as a pre-processing step for a variety of computer vision and image processing tasks including content-aware image editing <ref type="bibr" target="#b3">[3]</ref>, object detection <ref type="bibr" target="#b38">[36]</ref>, image classification <ref type="bibr" target="#b47">[45]</ref>, person re-identification <ref type="bibr" target="#b4">[4]</ref> and video summarization <ref type="bibr" target="#b34">[32]</ref>. Despite recent progress, salient object detection remains a challenging problem that calls for more accurate solutions.</p><p>Results from perceptual research <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b39">37]</ref> indicate that visual contrast is the most important factor in visual saliency. Various conventional saliency detection algorithms based on local or global contrast cues <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b50">48]</ref> have been successfully developed. In previous work, visual contrast is exemplified by contrast in various types of handcrafted low-level features (e.g., color, intensity and texture) at the pixel or segment level. Though handcrafted features tend to perform well in standard scenarios, they are not sufficiently robust for all challenging cases. For example, local contrast features may fail to detect homogenous regions inside salient objects while global contrast suffers from complex background. Although machine learning based saliency models have been developed <ref type="bibr" target="#b33">[31,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b35">33]</ref>, they are primarily for integrating different handcrafted features <ref type="bibr" target="#b21">[20]</ref> or fusing multiple saliency maps generated from different methods <ref type="bibr" target="#b35">[33]</ref>.</p><p>To obtain more robust features than handcrafted ones for salient object detection, deep convolutional neural networks (CNNs) have recently been employed, achieving substantially better results than previous state of the art <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b45">43]</ref>. In addition to improved robustness, features extracted using CNNs contain more high-level semantic information since those CNNs were typically pre-trained on datasets for visual recognition tasks. However, in all these methods, CNNs are all operated at the patch level instead of the pixel level, and each pixel is simply assigned the saliency value of its enclosing patch. As a result, saliency maps are typically blurry without fine details, especially near the boundary of salient objects. Furthermore, all image patches are treated as independent data samples for classification or regression even when they are overlapping. As a result, these methods usually have to run a CNN at least thousands of times (once for every patch) to obtain a complete saliency map. This gives rise to significant redundancy in computation and storage, and makes both training and testing very space and time consuming. For example, training a patch-oriented CNN model for saliency detection takes over 2 GPU days and requires hundreds of gigabytes of storage for the 5000 images in the MSRA-B dataset.</p><p>In this paper, inspired by a recent trend of developing fully convolutional neural networks for pixel labeling prob-lems <ref type="bibr" target="#b32">[30,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b48">46]</ref>, we propose an end-to-end deep contrast network to overcome the aforementioned limitations of recent CNN-based saliency detection methods. Here, "endto-end" means that our deep network only needs to be run on the input image once to produce a complete saliency map with the same pixel resolution as the input image. Our deep network consists of a pixel-level fully convolutional stream and a segment-level spatial pooling stream. In the fully convolutional stream, we design a multi-scale fully convolutional network (MS-FCN), which takes the raw image as input and directly produces a saliency map with pixellevel accuracy. Our MS-FCN can not only generate effective semantic features across different scales, but also capture subtle visual contrast among multi-scale feature maps for saliency inference. The segment-level spatial pooling stream generates another saliency map at the superpixel level by performing spatial pooling and saliency estimation over superpixels. This stream extracts segment-wise features very efficiently from MS-FCN by masking an intermediate feature map computed for the entire image. The saliency maps from both streams are fused at the end.</p><p>In summary, this paper has the following contributions:</p><p>• We introduce an end-to-end deep contrast network for salient object detection. It consists of a fully convolutional stream and a segment-wise spatial pooling stream. A training scheme is designed to learn the weights in both streams of this deep network. The fused saliency map from these two streams is further refined with a fully connected CRF for better spatial coherence and contour localization.</p><p>• We propose a multi-scale fully convolutional network as the first stream in our deep contrast network to infer a pixel-level saliency map directly from the raw input image. This model can not only infer semantic properties of salient objects, but also capture visual contrast among multi-scale feature maps.</p><p>• We also design a segment-wise spatial pooling stream as the second stream in our framework. This stream efficiently extracts segment-wise features, and accurately models visual contrast between regions and saliency discontinuities along region boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Salient object detection can be performed either in a bottom-up fashion using low-level features <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b40">38,</ref><ref type="bibr" target="#b50">48,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b52">50,</ref><ref type="bibr" target="#b7">7]</ref> or in a top-down fashion via the incorporation of high-level knowledge <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b42">40,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b28">27]</ref>. Since this paper is focused on visual saliency based on deep learning, we discuss relevant work in this context below.</p><p>Recently, machine learning and artificial intelligence have been revolutionized by deep convolutional neural networks, which have set new state of the art on a number of visual recognition tasks, including image classification <ref type="bibr" target="#b25">[24]</ref>, object detection <ref type="bibr" target="#b15">[15]</ref>, scene classification <ref type="bibr" target="#b49">[47]</ref> and scene parsing <ref type="bibr" target="#b12">[12]</ref>, closing the gap to human-level performance. There have also been attempts to apply deep learning to salient object detection. Li et al. <ref type="bibr" target="#b26">[25]</ref> trained a deep neural network for deriving a saliency map from multiscale features extracted using deep convolutional neural networks. Wang et al. <ref type="bibr" target="#b45">[43]</ref> adopted a deep neural network (DNN-L) to learn local patch features for each centered pixel. In <ref type="bibr" target="#b51">[49]</ref>, both global context and local context are utilized and integrated into a deep learning based pipeline for saliency detection. However, all these methods treat local image patches as independent training and testing samples. Since sharing computation among overlapping patches is not considered, there is a great deal of redundancy in feature computation, which gives rise to high computational cost for both training and testing. This limitation can be potentially overcome by recent end-to-end deep networks, which have been proven a success in semantic segmentation <ref type="bibr" target="#b32">[30,</ref><ref type="bibr" target="#b6">6]</ref>. However, directly applying existing fully convolutional network architecture to salient object detection would not be most appropriate because a standard fully convolutional model is not particularly good at capturing subtle visual contrast in an image. Therefore, our paper focuses on discovering high-level visual contrast in an endto-end mode, and experimental results demonstrate that our proposed deep model can significantly improve the current state of the art. This paper can be viewed as the first piece of work that aims to discover visual contrast information inside an image using end-to-end convolutional neural networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Contrast Network</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the architecture of our deep contrast network for salient object detection consists of two complementary components, a fully convolutional stream and a segment-wise spatial pooling stream. The fully convolutional stream is a multi-scale fully convolutional network (MS-FCN), which generates a saliency map S 1 with one eighth resolution of the raw input image by exploiting visual contrast across multiscale convolutional layers. The segment-wise spatial pooling stream generates a saliency map at the superpixel level by performing spatial pooling and saliency estimation over individual superpixels. The saliency maps from both streams are fused at the end through an extra convolutional layer with 1 × 1 kernels in our deep network to produce the final saliency map. The weights in this fusion layer are learned during training.</p><formula xml:id="formula_0">· CONV1_1+RELU · CONV1_2+RELU POOLING_1 · CONV2_1+RELU · CONV2_2+RELU POOLING_2 · CONV3_1+RELU · CONV3_2+RELU · CONV3_3+RELU POOLING_3 · CONV4_1+RELU · CONV4_2+RELU · CONV4_3+RELU POOLING_4 · CONV5_1+RELU · CONV5_2+RELU · CONV5_3+RELU</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Scale Fully Convolutional Network</head><p>In the fully convolutional stream, we aim to design an end-to-end convolutional network that can be viewed as a regression network mapping an input image to a pixel-level saliency map. To conceive such an end-to-end architecture, we have the following considerations. First, the network should be deep enough to produce multi-level features for detecting salient objects at different scales. Second, the network should be able to discover subtle visual contrast across multiple maps holding deep features at different scales. Last but not the least, fine-tuning an existing deep model is much desired since we do not have enough training images to train such a deep network from scratch.</p><p>We chose VGG16 <ref type="bibr" target="#b43">[41]</ref> as our pre-trained network and modified it to meet our requirements. To re-purpose it into a dense image saliency prediction network, the two fully connected layers of VGG16 are first converted into convolutional ones with 1×1 kernel as described in <ref type="bibr" target="#b32">[30]</ref>. However directly evaluating the resulting network in a convolutional manner yields a very sparse prediction map with a 32-pixel stride since the original VGG16 network has 5 pooling layers each of which has stride 2. To make the prediction map denser, we skip subsampling in the last two max-pooling layers to maintain an 8-pixel stride after the last pooling layer. To retain the original receptive field in the convolutional layers that follow, we use the "hole algorithm" to introduce zeros to increase the size of their convolutional kernels. The "hole algorithm", which is also calledà trous algorithm, was originally developed for efficient computation of the undecimated wavelet transform <ref type="bibr" target="#b36">[34]</ref>, and has recently been implemented in Caffe <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b27">26]</ref> to efficiently compute dense CNN feature maps at any target subsampling rate without introducing any approximation. This hole algorithm helps us keep the kernels intact, and a convolution now sparsely samples the input feature map using a stride of 2 or 4 pixels (2-pixel stride in the three convolutional layers after the penultimate pooling layer and 4-pixel stride in the last two converted 1 × 1 convolutional layers after the final pooling layer). For our experiments, we followed the implementation of the published DeepLab code <ref type="bibr" target="#b6">[6]</ref> and added the option to sparsely sample the underlying feature map to the 'im2col' function. 'im2col' is a function implemented in Caffe to convert multi-channel feature maps to vectorized patches for improving the efficiency of convolutions.</p><p>VGG16 has five pooling and downsampling layers, each of which has an increasingly larger receptive field containing contextual information. To design a deep network that is capable of discovering visual contrast crucial in saliency inference, we further develop a multiscale version of the above fully convolutional extension of VGG16. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we connect three extra convolutional layers to each of the first four max-pooling layers of VGG16. The first extra layer has 3 × 3 kernels and 128 channels, the second extra layer has 1 × 1 kernels and 128 channels, and the third extra layer (output feature map) has a 1 × 1 kernel and a single channel. To make the output feature maps of the four sets of extra convolutional layers have the same size (8× subsampled resolution), the stride of the first layer in these four sets are set to 4, 2, 1, and 1, respectively. Although the resulting four output maps have the same size, they are generated using receptive fields with different sizes and hence represent contextual features at 4 different scales. We further stack these four feature maps together with the final output map of the above end-to-end extension. The stacked feature maps (5 channels) are fed into a final convolutional layer with a 1 × 1 kernel and a single output channel, which is the inferred saliency map. The sigmoid activation function is used in the final layer. Although the output saliency map is of 8× subsampled resolution, they are smooth enough and allow us to use simple bilinear interpolation to make their resolution the same as that of the original input image at a negligible computational cost. We call this resized saliency map S 1 .</p><p>Note that the method in <ref type="bibr" target="#b32">[30]</ref> does not use the "hole algorithm" and produces very coarse maps (subsampled by a factor of 32), which motivate the use of trained deconvolution layers. The incorporation of deconvolution layers significantly increases the complexity and training time of their network. Experimental results also show that convolution with the "hole algorithm" can generate better results than trained deconvolution layers <ref type="bibr" target="#b6">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Segment-Level Saliency Inference</head><p>Salient objects often have irregular shapes and the corresponding saliency map has discontinuities along object boundaries. Our multiscale fully convolutional network operates at a subsampled pixel level without explicitly modeling such saliency discontinuities. To better model visual contrast between regions and visual saliency along region boundaries, we design a segment-wise spatial pooling stream in our network.</p><p>We first decompose the raw input image into a set of superpixels, and call each superpixel a segment. A mask is computed for every segment in the feature map generated from the last true convolutional layer (Conv5 3) of MS-FCN as follows. Since each activation in Conv5 3 is controlled by a receptive field in the input image, we first project every activation to the center of its receptive field as in <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b9">9]</ref>. For each segment in the input image, we first generate a binary mask with the same size as its bounding box. In this mask, pixels inside the segment are labeled '1' while others are labeled '0'. Each label in the binary mask is first assigned to the nearest center of receptive field and then backprojected onto Conv5 3. Thus, each activation in Conv5 3 collects multiple binary labels backprojected from its receptive field. The collected binary labels at each activation are first averaged and then thresholded by 0.5, yielding a corresponding binary segment mask on Conv5 3, where pixels within the segment can be easily identified according to this mask. Note that feature maps generated from Conv5 3 have 8-pixel strides in our MS-FCN instead of 32pixel ones in the original VGG16 network since subsampling was skipped in the last two max-pooling layers as described in Section 3.1. Therefore, the resolution of the feature map generated from Conv5 3 is sufficient for segment masking.</p><p>Since segments on Conv5 3 have variable size, to produce a fixed-length feature vector, we further perform spatial pooling (SP) over a fixed grid as with <ref type="bibr" target="#b17">[17]</ref>. We divide the bounding box of a segment on Conv5 3 into h × w cells. Let the size of the bounding box be H × W . Spatial pooling is performed within each cell with H/h × W/w pixels. Afterwards, the aggregated feature vector of each segment has h × w × C dimensions, where C is the number of channels of the feature map generated by Conv5 3.</p><p>To discover segment-level visual contrast, for each segment, we obtain three spatially aggregated feature vectors from three nested and increasingly larger windows, which are respectively the bounding box of the considered segment, the bounding box of its immediate neighboring segments, and the entire map from Conv5 3 (with the considered segment masked out to indicate the position of the segment in the map). Finally, the three aggregated feature vectors are concatenated and fed into two fully connected layers. The output of the second fully connected layer is fed into the output layer, which uses the sigmoid function to perform logistic regression to produce a distribution over binary saliency labels. We call the saliency map generated in this way S 2 .</p><p>This segment-wise spatial pooling stream of our network is in fact an accelerated version of the method in <ref type="bibr" target="#b26">[25]</ref>. Although they share similar strategies for multiscale feature extraction, our method is much more efficient because convolutional feature maps only need to be computed once for the entire image and afterwards, local features for thousands of segments from the same image can be masked out instantaneously. Moreover, our model also achieves better results as segment features are extracted from our multiscale fully convolutional network, which has been fine-tuned for salient object detection, instead of from the original VGG16 model for image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deep Contrast Network Training</head><p>Given training images and their superpixels, we first train the neural network in the second stream alone to obtain its initial weights. Segment features are extracted using the original VGG16 network pre-trained over the ImageNet dataset <ref type="bibr" target="#b10">[10]</ref>. After this initialization, we fine-tune the two streams of our deep contrast network in an alternating manner. We first fix the parameters in the second stream and train the first stream for one epoch. During this process, the weights for fusing the saliency maps (S 1 and S 2 ) from the two streams as well as the parameters in the multiscale fully convolutional network are updated using stochastic gradient descent. Then we fix the parameters in the first stream and fine-tune the neural network in the second stream for one epoch using groundtruth saliency maps. Segment features are extracted using the updated VGG16 network embedded in the first stream. We typically alternate the above two steps 8 times (16 epochs in total) before the whole finetuning process converges.</p><p>The loss function for fine-tuning the deep contrast network (the first stream) and the fusing weights is the cross entropy between the ground truth and the fused saliency map (S):</p><formula xml:id="formula_1">L = − β i |I| i=1 G i log P (S i = 1|I i , W ) − (1 − β i ) |I| i=1 (1 − G i ) log P (S i = 0|I i , W ) ,<label>(1)</label></formula><p>where G is the groundtruth label, W denotes the collection of all network parameters in MS-FCN and the fusion layer, β i is a weight balancing the number of salient pixels and unsalient ones, and |I|, |I| and |I| + denote the total number of pixels, unsalient pixels and salient pixels in image I, respectively. Then β i = |I| |I| and 1 − β i = |I|+ |I| . When fine-tuning the second stream, its parameters are updated by minimizing the squared prediction errors accumulated over all segments from all training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Complete Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Superpixel Segmentation</head><p>We aim to decompose the input image into nonoverlapping segments. In this paper, we use a slightly modified version of the SLIC algorithm <ref type="bibr" target="#b2">[2]</ref>, which uses geodesic image distance <ref type="bibr" target="#b8">[8]</ref> during K-means clustering in the CIELab color space. As discussed in <ref type="bibr" target="#b46">[44]</ref>, geodesic distance based superpixels can guarantee connectivity while well preserve edges in the image. In our experiments, we have found that the final saliency detection performance does not vary much when the number of superpixels is between 200 and 300. And the performance becomes slightly worse when the number of superpixels is fewer than 200 or more than 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Spatial Coherence</head><p>Since both streams in our deep contrast network assign saliency scores to individual pixels or segments without considering the consistency of saliency scores among neighboring pixels and segments, we propose a pixelwise saliency refinement model based on a fully connected CRF <ref type="bibr" target="#b24">[23]</ref> to improve spatial coherence. This model solves a binary pixel labeling problem, and employs the following energy function,</p><formula xml:id="formula_2">E (L) = − i log P (l i ) + i,j θ ij (l i , l j ) ,<label>(2)</label></formula><p>where L represents a binary label (salient or not salient) assignment for all pixels. P (l i ) is the probability of pixel x i having label l i , which indicates the likelihood of pixel x i being salient. Initially, P (1) = S i and P (0) = 1 − S i , where S i is the saliency score at pixel x i from the fused saliency map S. θ ij (l i , l j ) is a pairwise potential and defined as follows,</p><formula xml:id="formula_3">θ ij = µ (l i , l j ) ω 1 exp − p i − p j 2 2σ 2 α − I i − I j 2 2σ 2 β + ω 2 exp − p i − p j 2 2σ 2 γ ,<label>(3)</label></formula><p>where µ (l i , l j ) = 1 if l i = l j , and zero otherwise. θ ij involves two kernels. The first kernel depends on pixel positions (p) and pixel intensities (I). This kernel encourages nearby pixels with similar colors to take similar saliency scores. The degree of influence by color similarity and spatial closeness is controlled by three parameters (σ α and σ β ), respectively. The second kernel aims at removing small isolated regions.</p><p>Energy minimization is based on a mean field approximation to the CRF distribution, and high-dimensional filtering can be utilized to speed up the computation. In this paper, we use the publicly available implementation of <ref type="bibr" target="#b24">[23]</ref> to minimize the above energy, and it takes less than 0.5 second on an image with 300×400 pixels. At the end of energy minimization, we generate a saliency map using the posterior probability of each pixel being salient. We call the generated saliency map S crf . As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the saliency maps generated from the proposed method without CRF are fairly coarse and the contours of salient objects may not be well preserved. The proposed saliency refinement model can not only generate smoother results with pixelwise accuracy but also well preserve salient object contours. A quantitative study of the effectiveness of the saliency refinement model can be found in Section 5.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head><p>We evaluate the performance of our method on five public datasets: MSRA-B <ref type="bibr" target="#b31">[29]</ref>, PASCAL-S <ref type="bibr" target="#b28">[27]</ref>, DUT-OMRON <ref type="bibr" target="#b50">[48]</ref>, HKU-IS <ref type="bibr" target="#b26">[25]</ref> and SOD <ref type="bibr" target="#b37">[35]</ref>. The MSRA-B dataset contains 5,000 images with a variety of image   <ref type="table">Table 1</ref>. Comparison of quantitative results including maximum F-measure (larger is better) and MAE (smaller is better). The best three results are shown in red, blue, and green , respectively.</p><p>contents. Most of the images has a single salient object. PASCAL-S was built using the validation set of the PAS-CAL VOC 2010 segmentation challenge. It contains 850 images with the ground truth labeled by 12 subjects. We threshold the masks at 0.5 to obtain binary masks as suggested in <ref type="bibr" target="#b28">[27]</ref>. Dut-OMRON contains 5,168 challenging images, each of which has one or more salient objects and relatively complex backgrounds. We have noticed that many saliency annotations in this dataset may be controversial among different human observers. As a result, none of the existing saliency models has achieved a high accuracy on this dataset. HKU-IS is another large dataset containing 4447 challenging images, most of which have either low contrast or multiple salient objects. The SOD dataset contains 300 images and it was originally designed for image segmentation. Many images in this dataset have multiple salient objects with low contrast. All the datasets contain manually annotated groundtruth saliency maps. To facilitate a fair comparison against other methods, we divide the MSRA-B dataset into three parts as in <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b26">25]</ref>, 2500 for training, 500 for validation and the remaining 2000 images for testing. To test the adaptability of trained saliency models to other different datasets, we use the models trained on the MSRA-B dataset and test them over all other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Evaluation Criteria</head><p>We evaluate the performance using precision-recall (PR) curves, F-measure and mean absolute error (MAE). The precision and recall of a saliency map is computed by converting a continuous saliency map to a binary mask using a threshold and comparing the binary mask against the ground truth. The PR curve of a dataset is obtained from the average precision and recall over saliency maps of all images in the dataset. The F-measure is defined as  <ref type="figure">Figure 5</ref>. Comparison of precision-recall curves of 11 saliency detection methods on 3 datasets. Our DCL and DCL + (DCL with CRF) consistently outperform other methods across all the testing datasets. Note that MC <ref type="bibr" target="#b51">[49]</ref> and LEGS <ref type="bibr" target="#b45">[43]</ref> are overrated on the MSRA-B dataset and LEGS <ref type="bibr" target="#b45">[43]</ref> is also overrated on the PASCAL-S dataset. where β 2 is set to 0.3 to weigh precision more than recall as suggested in <ref type="bibr" target="#b0">[1]</ref>. We report the maximum F-measure (maxF) computed from the PR curve. We also report the average precision, recall and F-measure using an adaptive threshold for generating a binary saliency map. The adaptive threshold is determined to be twice the mean value of a saliency map. In addition, MAE <ref type="bibr" target="#b40">[38]</ref> represents the average absolute per-pixel difference between an estimated saliency map and its corresponding ground truth. MAE is meaningful in evaluating the applicability of a saliency model in a task such as object segmentation.</p><formula xml:id="formula_4">F β = (1 + β 2 ) · P recision · Recall β 2 · P recision + Recall ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Implementation</head><p>Our proposed deep contrast network has been implemented on the basis of Caffe <ref type="bibr" target="#b20">[19]</ref>, an open source framework for CNN training and testing. We resize all the images to 321 × 321 pixels for training, and set the initial learning rate to 0.01 for all newly added layers with one channel and 0.001 for all other layers. The momentum parameter is set to 0.9 and the weight decay is 0.0005. For the segmentlevel stream, the number of superpixels is set to 400 with 3 different scales (200, 150 and 50 respectively). A 2 × 2 grid is used for spatial pooling over each segment. Thus the aggregated feature for each segment has 6144 dimensions, and this feature is further fed into two fully connected layers each of which has 300 neurons. The parameters of the fully connected CRF are determined through cross validation as in <ref type="bibr" target="#b24">[23]</ref> on the validation set and finally the paramters of w 1 , w 2 , σ α , σ β , and σ γ are set to 3.0, 5.0, 3.0, 50.0 and 3.0 respectively in our experiments.</p><p>We use DCL to denote our saliency model based on deep contrast learning only without CRF-based post-processing, and DCL + to denote the saliency model that includes CRFbased refinement. While it takes around 25 hours to train our deep contrast network using the MSRA-B dataset, it only takes 1.5 seconds for the trained model (DCL) to detect salient objects in a testing image with 400x300 pixels on a PC with an NVIDIA Titan Black GPU and a 3.4GHz Intel processor. Note that this is far more efficient than the latest deep learning based methods which treat all image patches as independent data samples for saliency regression. CRF-based post-processing requires additional 0.8 second per image. Experimental results will show that DCL alone without CRF-based post-processing already outperforms existing state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with the State of the Art</head><p>We compare our saliency models (DCL and DCL + ) against eight recent state-of-the-art methods, including SF <ref type="bibr" target="#b40">[38]</ref>, GC <ref type="bibr" target="#b7">[7]</ref>, DRFI <ref type="bibr" target="#b21">[20]</ref>, PISA <ref type="bibr" target="#b44">[42]</ref>, BSCA <ref type="bibr" target="#b41">[39]</ref>, LEGS <ref type="bibr" target="#b45">[43]</ref>, MC <ref type="bibr" target="#b51">[49]</ref> and MDF <ref type="bibr" target="#b26">[25]</ref>. The last three are the latest deep learning based methods. For fair comparison, we use either the implementations or the saliency maps provided by the authors. In addition, we also train a fully convolutional neural network (FCN) (the FCN-8s network proposed in <ref type="bibr" target="#b32">[30]</ref>) for comparison. To train the FCN saliency model, we simply replace its last softmax layer with a sigmoid cross-entropy layer for saliency inference, and finetune the revised model using the training sets in the aforementioned saliency datasets.</p><p>A visual comparison is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. As can be seen, our method generates more accurate saliency maps in various challenging cases, e.g., objects touching the image boundary (the first two rows), multiple disconnected salient objects (the middle two rows) and low contrast between object and background (the last two rows). It is necessary to point out that the performance of MC <ref type="bibr" target="#b51">[49]</ref> is overrated on the MSRA-B dataset and the performance of LEGS <ref type="bibr" target="#b45">[43]</ref> is overrated on both the MSRA-B dataset and the PASCAL-S dataset because most testing images in the corresponding datasets were used as training samples for the publicly released trained models of MC and LEGS used in our comparison.</p><p>Our method significantly outperforms all existing salient object detection algorithms across the aforementioned public datasets in terms of PR curve ( <ref type="figure">Fig. 5</ref>) and average precision, recall and F-measure ( <ref type="figure" target="#fig_6">Fig. 6</ref>). Refer to the supplemental materials for the results on the PASCAL-S and SOD datasets. Moreover, we report a quantitative comparison w.r.t. maximum F-measure and MAE in <ref type="table">Table 1</ref>. Our complete model (DCL + ) improves the maximum Fmeasure achieved by the best-performing existing algorithm by 3.5%, 5.0%, 7.7%, 7.6% and 6.0% respectively on MSRA-B (skipping MC and LEGS on this dataset), HKU-IS, DUT-OMRON, PASCAL-S (skipping LEGS on this dataset) and SOD. And at the same time, our model lowers the MAE by 28.8%, 35.5%, 9.1%, 25.5% and 18.7% respectively on MSRA-B (skipping MC and LEGS on this dataset), HKU-IS, DUT-OMRON, PASCAL-S (skipping LEGS on this dataset) and SOD. We can also see that our model without CRF (DCL) significantly outperforms all evaluated salient object detection algorithms across all the considered datasets. Our model also significantly outperforms the FCN adapted from a model originally designed for semantic segmentation <ref type="bibr" target="#b32">[30]</ref> because we explicitly perform deep contrast learning, which is critical for saliency detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Effectiveness of Deep Contrast Network</head><p>Our deep contrast network consists of a fully convolutional stream and a segment-wise spatial pooling stream. To show the effectiveness and necessity of these two components, we compare the saliency map S 1 generated from the first stream (MS-FCN), the saliency map S 2 from the second segment-level stream and the fused saliency map from S 1 and S 2 (DCL) using testing images in the MSRA-B dataset. As shown in <ref type="figure" target="#fig_7">Fig. 7</ref>, the fused saliency map (DCL) consistently achieves the best performance on average precision, recall and F-measure, and the fully convolutional stream (MS-FCN) has more contribution to the fused result than the segment-wise spatial pooling stream. These two streams are complementary to each other, and our trained deep contrast network is capable of discovering and understanding subtle visual contrast among multi-scale feature maps as well as between neighboring segments. To demonstrate the effectiveness of MS-FCN, we also generate saliency maps from the last scale of MS-FCN (the best performing scale) for comparison. The last scale of MS-FCN is in fact the fully convolutional version of the original VGG16 network. As shown in <ref type="figure" target="#fig_7">Fig. 7</ref>, this single scale of MS-FCN (called SC MSFCN) performs much worse than the complete version of MS-FCN in terms of the PR curve as well as the average precision, recall and F-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Effectiveness of CRF</head><p>In Section 4.2, a fully connected CRF is incorporated to improve the spatial coherence of the saliency maps from our deep contrast network. To validate its effectiveness, we have also evaluated the performance of our final saliency model with and without the CRF using the testing images in the MSRA-B dataset. The results are also shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. It is evident that the CRF improves the accuracy of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have introduced an end-to-end deep contrast network for salient object detection. Our deep network consists of two complementary components, a pixellevel fully convolutional stream and a segment-level spatial pooling stream. A fully connected CRF model can be optionally incorporated to further improve spatial coherence and contour localization in the fused result from these two streams. Experimental results demonstrate that our deep model can significantly improve the state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fea_s1Figure 1 .</head><label>1</label><figDesc>Two streams of our deep contrast network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of multi-scale fully convolutional network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of saliency detection results with and without CRF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparison of saliency maps generated from state-of-the-art methods, including our DCL and DCL + . The ground truth (GT) is shown in the last column. DCL + consistently produces saliency maps closest to the ground truth. Data Set Metric SF GC DRFI PISA BSCA LEGS MC MDF FCN DCL DCL + maxF 0.700 0.719 0.845 0.837 0.830 0.870 0.894 0.885 0.864 0.905 0.916 MSRA-B MAE 0.166 0.159 0.112 0.102 0.130 0.081 0.054 0.066 0.096 0.052 0.047 maxF 0.590 0.588 0.776 0.753 0.723 0.770 0.798 0.861 0.867 0.892 0.904 HKU-IS MAE 0.173 0.211 0.167 0.127 0.174 0.118 0.102 0.076 0.087 0.054 0.049 maxF 0.495 0.495 0.664 0.630 0.617 0.669 0.703 0.694 0.681 0.733 0.757 DUT-OMRON MAE 0.147 0.218 0.150 0.141 0.191 0.133 0.088 0.092 0.131 0.084 0.080 maxF 0.493 0.539 0.690 0.660 0.666 0.752 0.740 0.764 0.793 0.815 0.822 PASCAL-S MAE 0.240 0.266 0.210 0.196 0.224 0.157 0.145 0.145 0.128 0.113 0.108 maxF 0.516 0.526 0.699 0.660 0.654 0.732 0.727 0.785 0.795 0.829 0.832 SOD MAE 0.267 0.284 0.223 0.223 0.251 0.195 0.179 0.155 0.158 0.129 0.126</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of precision, recall and F-measure (computed using a per-image adaptive threshold) among 11 different methods on 3 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Componentwise efficacy of the proposed deep contrast network and the effectiveness of the CRF model.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The first author is supported by Hong Kong Postgraduate Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ieee conference on</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Slic superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Seam carving for contentaware image resizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person re-identification using multiple experts with random subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Image and Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusing generic objectness and visual saliency for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="914" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Geodesic image and video editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3992" to="4000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Does luminance-contrast contribute to a saliency map for overt visual attention?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Einhäuser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koènig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1089" to="1097" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bottom-up saliency is a discriminant process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contextaware saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Category-independent object-level saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1761" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Salient region detection by ufo: Uniqueness, focusness and objectness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1976" to="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Center-surround divergence of feature statistics for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2214" to="2219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.5644</idno>
		<title level="m">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Highly efficient forward and backward propagation of convolutional neural networks for pixelwise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4526</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive partial differential equation learning for visual saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning optimal seeds for diffusion-based salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2790" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A user attention model for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM international conference on Multimedia</title>
		<meeting>the tenth ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="533" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency aggregation: a data-driven approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1131" to="1138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A wavelet tour of signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Eighth IEEE International Conference on</title>
		<meeting>Eighth IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An integrated model of top-down and bottom-up attention for optimizing detection speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Navalpakkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling the role of salience in the allocation of overt visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parkhurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Saliency detection via cellular automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A unified approach to salient object detection via low rank matrix recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pisa: Pixelwise image saliency by aggregating complementary appearance contrast measures with edge-preserving coherence. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3019" to="3033" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Structure-sensitive superpixels via geodesic distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scale: Supervised and cascaded laplacian eigenmaps for visual object recognition based on nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06375</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Holistically-nested edge detection. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hd-cnn: Hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>De-Coste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2740" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
