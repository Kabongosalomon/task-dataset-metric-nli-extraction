<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Automatic Face-to-Face Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 21-25, 2019. 2019. October 21-25, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K R</forename><surname>Prajwal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrabha</forename><surname>Mukhopadhyay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerin</forename><surname>Philip</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Jha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Namboodiri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IIIT</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Automatic Face-to-Face Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 27th ACM International Conference on Multimedia (MM &apos;19)</title>
						<meeting>the 27th ACM International Conference on Multimedia (MM &apos;19) <address><addrLine>Nice, France ACM Reference Format; Nice</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 21-25, 2019. 2019. October 21-25, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3343031.3351066</idno>
					<note>jerin.philip@research.iiit.ac.in IIIT Hyderabad Abhishek Jha abhishek.jha@research.iiit.ac.in IIIT Hyderabad Vinay Namboodiri vinaypn@iitk.ac.in IIT Kanpur C. V. Jawahar jawahar@iiit.ac.in IIIT Hyderabad * Both authors contributed equally to this research. *Equal Contribution., France. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Computer vision</term>
					<term>Machine translation</term>
					<term>Learning from critiques KEYWORDS Lip Synthesis</term>
					<term>Translation systems</term>
					<term>Cross-language talking face generation</term>
					<term>Neural Machine Translation</term>
					<term>Speech to Speech Trans- lation</term>
					<term>Voice Transfer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: In light of the increasing amount of audio-visual content in our digital communication, we examine the extent to which current translation systems handle the different modalities in such media. We extend the existing systems that can only provide textual transcripts or translated speech for talking face videos to also translate the visual modality i.e. lip and mouth movements. Consequently, our proposed pipeline produces fully translated talking face videos with corresponding lip synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as "Faceto-Face Translation". As today's digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact in multiple real-world applications. First, we</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards "Face-to-Face Translation" by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Communicating effectively across language barriers has always been a major aspiration for humans all over the world. In recent years, there has been tremendous progress by the research community towards this goal. Neural Machine Translation (NMT) systems have become increasingly competent <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref> in automatically translating foreign languages without the need for a human in the loop. The success of the recent NMT systems not only impacts plain text-to-text translation but also plays a pivotal role in speechto-speech translation systems. The latter problem is also of great interest because a large part of our communication with others is oral. By cascading speech recognition, neural machine translation and speech synthesis modules, current systems can generate a translated speech output for a given source speech input <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>. In this work, we argue that it is possible to extend this line of research further with a visual module that can greatly broaden the scope and enhance the user experience of existing speech translation systems.</p><p>The motivation to incorporate a visual module into a translation system arises from the fact that the majority of the information stream today, is increasingly becoming audio-visual. YouTube, the world's largest online video sharing platform generates 300 hours of video content every minute <ref type="bibr" target="#b0">1</ref> . The meteoric rise of video conferencing <ref type="bibr" target="#b21">[22]</ref> also exemplifies the preference for rich audio-visual communication. Existing systems can only translate such audiovisual content at a speech-to-speech level and hence possess some major limitations. Firstly, the translated voice sounds very different from the original speaker's voice. But, more importantly, the generated speech when directly overlaid on the original video produces unsynchronized lip movements with respect to the speech, leading to poor user experience. Thus, we build upon the speech-to-speech translation systems and propose a pipeline that can take a video of a person speaking in a source language and output a video of the same speaker speaking in a target language such that the voice style and lip movements justifies the target language. By doing so, the translation system becomes holistic, and as shown by our human evaluations in this paper, significantly improves the user experience in creating and consuming translated audio-visual content.</p><p>Our pipeline is made up of five modules. In the scope of this paper, we work with two widely spoken languages: English and Hindi. For speech to speech translation, we use an automatic speech recognizer <ref type="bibr" target="#b2">[3]</ref> to transcribe text from the original speech in language L A . We adapt state-of-the-art neural machine translation and textto-speech models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref> to work for Indian languages and generate translated speech in language L B . We also personalize the voice <ref type="bibr" target="#b13">[14]</ref> 1 https://merchdope.com/youtube-stats/ generated by the TTS model to closely match the voice of the target speaker.</p><p>Finally, to generate talking faces conditioned on the translated speech, we design a novel generative adversarial network, LipGAN in which we employ an adversary that measures the extent of lip synchronization in the frames generated by the generator. Furthermore, our system is capable of handling faces in random poses without the need for realignment to a template pose. Our intuitive approach yields realistic talking face videos from any audio with no dependence on language. We achieve state-of-the-art scores on the LRW test set across all quantitative metrics. Using our complete pipeline, we show a proof-of-concept on multiple applications and also propose future directions in this novel research problem. Different resources for this work along with demo videos are available publicly 2 . In summary, our contributions are as follows:</p><p>(1) For the first time, we design and train an automatic pipeline for the novel problem of face-to-face translation. Our system can automatically translate a talking face of a person into a given target language, with realistic lip synchronization. <ref type="bibr" target="#b1">(2)</ref> We propose a novel model, LipGAN, for generating realistic talking faces conditioned on audio in any language. Our model outperforms existing works in both quantitative and human-based evaluation. (3) In the process of creating a face-to-face translation pipeline, we also achieve state of the art neural machine translation results in the Hindi-English language pair by incorporating recent advancements in the area.</p><p>The rest of the paper is organized as follows: In section 2, we survey the recent developments in speech, vision and language research which enable our work. Following this, the adaptation of the existing methods in speech and language to our problem setting is described in section 3. Section 4 explains in detail our novel contributions to bring improvements in lip synchronization. We produce a few applications deriving from our work in Section 5 and conclude our findings in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Given a video of a speaker speaking in language L A , our aim is to generate a lip-synchronized video of the speaker speaking in language L B . Our system brings together multiple modules from speech, vision, and language to achieve face to face translation for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Automatic Speech Recognition</head><p>We make use of recent works on Automatic Speech Recognition (ASR) <ref type="bibr" target="#b2">[3]</ref> to convert the speech of the source language L A into the corresponding text. Speech recognition for English has been extensively investigated, owing to the existence of large open-source speech recognition datasets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref> and trained models <ref type="bibr" target="#b2">[3]</ref>. We employ the DeepSpeech 2 model to perform English speech recognition in this work. <ref type="figure">Figure 2</ref>: Block diagram of the overall pipeline of our network. In our case, L A is English and L B is Hindi. We decompose our problem into: (1) recognize speech in the source language L A , (2) translate the recognized text in L A to a target language L B , (3) synthesize speech from the translated text (5) generate realistic talking faces in language L B from the synthesized speech. Additionally, to obtain personalized speech for a speaker, we employ a Voice transfer module (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Machine Translation</head><p>NMT is often modelled as a sequence to sequence problem which was first introduced with neural networks in Sutskever et al. <ref type="bibr" target="#b27">[28]</ref>. Further improvements were brought about with attention mechanisms by Bahdanau et al. <ref type="bibr" target="#b4">[5]</ref> and Luong et al. <ref type="bibr" target="#b19">[20]</ref>. More recently, Vaswani et al. <ref type="bibr" target="#b30">[31]</ref> introduced transformer network which relies only on an attention mechanism to draw global dependencies between input and output. The transformer network outperforms its predecessors by a healthy margin and hence we decided to adopt this into our pipeline. It has also been observed in works like Johnson et al. <ref type="bibr" target="#b12">[13]</ref> that training multilingual translation systems also improve the performance especially for low resource languages. Thus, in this work, we also follow a similar path where we use state of the art architectures extended to multilingual learning setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Text to Speech</head><p>There has been a lot of work in the area of text-to-speech (TTS) synthesis, starting with the most commonly used HMM-based models <ref type="bibr" target="#b33">[34]</ref>. These models can be trained with lesser data to produce fairly intelligible speech, but fail to capture aspects like prosody that is evident in natural speech. Recently, researchers have achieved natural TTS by training neural network-based architectures <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> to map character sequences to mel-spectrograms. We adopt this approach, and train Deep Voice 3 <ref type="bibr" target="#b24">[25]</ref> based models to achieve highquality text-to-speech synthesis in our target language L B . Our implementation of DeepVoice 3 also makes use of a recent work on guided-attention <ref type="bibr" target="#b29">[30]</ref> allowing it to achieve high-quality alignment and faster convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Voice Transfer in Audio</head><p>Multiple recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref> make use of multi-speaker TTS models to generate voice conditioned on speaker embeddings. While these systems offer the advantage of being able to generate novel TTS voice samples given a few seconds of reference audio, the quality of TTS is inferior <ref type="bibr" target="#b24">[25]</ref> compared to single-speaker TTS models. In our system, we employ another recent work <ref type="bibr" target="#b13">[14]</ref> that uses a CycleGAN architecture to achieve good voice transfer between two human speakers with no loss in linguistic features. We train this model to perform a cross-language transfer of a synthetic TTS voice to a natural target speaker voice. We evaluate our models and show that by using just about ten minutes of a target speaker's audio samples, we can emulate the speaker's voice and significantly improve the experience of a listener.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Talking Face Synthesis from Audio</head><p>Lip synthesis from a given audio track is a fairly long-standing problem, first introduced in the seminal work of Bregler et al. <ref type="bibr" target="#b5">[6]</ref>. However, realistic lip synthesis in unconstrained real-life environments was only made possible by a few recent works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>. Typically, these networks predicted the lip landmarks conditioned on the audio spectrogram in a time window. However, it is important to highlight that these networks fail to generalize to unseen target speakers and unseen audio. A recent work by Chung et al. <ref type="bibr" target="#b7">[8]</ref> treated this problem as learning a phoneme-to-viseme mapping and achieved generic lip synthesis. This leads them to use a simple fully convolutional encoder-decoder model. Even more recently, a different solution to the problem was proposed by Zhou et al. <ref type="bibr" target="#b34">[35]</ref>, in which they use audio-visual speech recognition as a probe task for associating audio-visual representations, and then employ adversarial learning to disentangle the subject-related and speech-related information inside them. However, we observed two major limitations in their work. Firstly, to train using audio-visual speech recognition, they use 500 English word-level labels for the corresponding spoken audio. We observed that this makes their approach language-dependent. It also becomes hard to reproduce this model for other languages as collecting large video datasets with careful word-level annotated transcripts in various languages is infeasible. Our approach is a fully self-supervised approach that learns a phoneme-viseme mapping, making it language independent. Secondly, we observe that their adversarial networks are not conditioned on the corresponding input audio. As a result, their adversarial training setup does not directly optimize for improved lip-sync conditioned on audio. In contrast, our LipGAN directly optimizes for improved lip-sync by employing an adversarial network that measures the extent of lip-sync between the frames generated by the generator and the corresponding audio sample. Additionally, both Zhou et al. <ref type="bibr" target="#b34">[35]</ref> and Chung et al. <ref type="bibr" target="#b7">[8]</ref> normalize the pose of the input faces to a canonical pose, thus making it difficult to blend the generated faces in the original input video. Proposed LipGAN tackles this problem by providing additional information about the pose of the target face as an input to the model thus making the final blending of the generated face in the target video fairly straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SPEECH-TO-SPEECH TRANSLATION</head><p>In the previous section, we surveyed the possibility of using state of the art models in speech and language to suit our problem setting. There are not many existing systems reported for speech recognition, machine translation and speech synthesis available for Indian languages. In this section, we describe the current state of the art architectures we use for text and speech, and how we adapt them to our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recognizing speech in source language L A</head><p>We use publicly available state-of-the-art ASR systems for generating text in language L A . A publicly available pre-trained model using Deep Speech 2 is used for speech recognition in English. This model was trained on LibriSpeech dataset and achieves WER% of 5.22% on the LibriSpeech test set. Once we have text, recognized in a source language, we translate it into a target language using an NMT model, which we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Translating to target language L B</head><p>We use the re-implementation of Transformer-Base <ref type="bibr" target="#b30">[31]</ref> available in fairseq-py <ref type="bibr" target="#b2">3</ref> . The language pairs we attempt our problem on contains a low resource language, Hindi. To create a nmt system which works well for Hindi as well as English, we resort to training a multiway model to maximize learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>. We closely follow Johnson et al. <ref type="bibr" target="#b12">[13]</ref> in training a multi-way model whose parameters are shared across all seven languages -Hindi, English, Telugu, Malayalam, Tamil, Telugu, Urdu. Details of the translation system has been reported in <ref type="bibr" target="#b23">[24]</ref>. In <ref type="table">Table 1</ref> for language directions which are within the scope of this paper. We indicate the size of training data used and the evaluated scores using the widely used Bilingual Evaluation Under Study (BLEU) obtained on the test split of IIT-Bombay Hindi-English Parallel Corpus <ref type="bibr" target="#b17">[18]</ref>. We compare against Google Translate 4 in this test set, which is indicated in <ref type="table">Table 1</ref> as Online-G. We achieve an increase of 3 BLEU points on the test set compared to Google Translate.</p><p>Next, we describe our methods of generating speech from the target text in L B , obtained after translating source text in language L A . <ref type="bibr" target="#b2">3</ref> https://github.com/pytorch/fairseq 4 compared in the first week of April, 2019</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating Speech in language L B</head><p>For our Hindi text-to-speech model, we adapt a re-implementation of the DeepVoice 3 model proposed by Ping et al. <ref type="bibr" target="#b24">[25]</ref>. Due to the lack of publicly available large scale dataset for Hindi, we curate a dataset similar to LJSpeech by recording Hindi sentences from crawled news articles.</p><p>We adopt the nyanko-build 5 implementation of DeepVoice 3 to train our Hindi TTS model. We trained on about 10, 000 audio-text pairs and evaluated on 100 unseen test sentences. Griffin-Lim algorithm <ref type="bibr" target="#b10">[11]</ref> was used to generate waveforms from the spectrograms produced by our model. We evaluate this model by conducting a user study with 25 participants using our unseen test set. The average Mean Opinion Scores (MOS) scores with 95% confidence intervals are reported in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Personalizing speaker voice</head><p>Voice of a speaker is one of the key elements of her acoustic identity. As our TTS model only generates audio samples in a single voice, we personalize this voice to match the voice of different target speakers. As collecting parallel training data for the same speaker across languages is infeasible, we adopt the CycleGAN architecture <ref type="bibr" target="#b13">[14]</ref> to work around this problem.</p><p>For a given speaker we collect about 10 minutes of audio clips, which can be easily obtained as we need only a non-parallel dataset. Using our trained TTS model, we generate 5000 samples amounting to about 3 hours worth of synthetic TTS speech. For each speaker, we train a CycleGAN for about 50K iterations with a batch size of 16. The other hyperparameters are the same as used in Kaneko and Kameoka <ref type="bibr" target="#b13">[14]</ref>. During inference, given a TTS generated audio sample, the model preserves the linguistic features and generates speech in the voice of the speaker it was trained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speaker</head><p>Quality  <ref type="table">Table 3</ref>: MOS scores for Voice Transfer of Hindi TTS on various target speakers. Using the CycleGAN approach, we are able to consistently achieve reasonable cross-language voice transfer from the TTS generated voice to a given speaker.</p><p>We evaluate our Voice Transfer models in a similar fashion to Kaneko and Kameoka <ref type="bibr" target="#b13">[14]</ref>, with the help of 30 participants. We use 20 generated TTS samples each of which are voice transferred across 5 famous personalities. <ref type="table">Table 3</ref> reports the result of this study.</p><p>In the next section, we describe how we generate realistic talking face videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TALKING FACE GENERATION</head><p>Given a face image I containing a subject identity and a speech A divided into a sequence of speech segments {A 1 , A 2 , ...A k } , we would like to design a model G, that generates a sequence of frames {S 1 , S 2 , ...S k } that contains the face speaking the audio A with proper lip synchronization. Additionally, the model must work for unseen languages and faces during inference. As collecting annotated data for various languages is tedious, the model must also be able to learn in a self-supervised fashion. <ref type="table" target="#tab_3">Table 4</ref> compares our model against recent state-of-the-art approaches for talking face generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Works for any face?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross -language</head><p>No manual labeled data</p><p>Smooth blending into target video Suwajanakorn et al. <ref type="bibr" target="#b28">[29]</ref> × × ✓ ✓ Kumar et al. <ref type="bibr" target="#b16">[17]</ref> × × × ✓ Zhou et al. <ref type="bibr" target="#b34">[35]</ref> ✓ × × × Chung et al. <ref type="bibr" target="#b7">[8]</ref> ✓ </p><formula xml:id="formula_0">✓ ✓ × LipGAN (Ours) ✓ ✓ ✓ ✓</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Formulation</head><p>We formulate our talking face synthesis problem as "learning to synthesize by testing for synchronization". Concretely, our setup contains two networks, a generator G that generates faces by conditioning on audio inputs and a discriminator D that tests whether the generated face and the input audio are in sync. By training these networks together in an adversarial fashion, the generator G learns to create photo-realistic faces that are accurately in sync with the given input audio. The setup is illustrated in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generator network</head><p>The generator network is a modification of Chung et al. <ref type="bibr" target="#b7">[8]</ref> and contains three branches: (i) Face encoder, (ii) Audio encoder and a (iii) Face Decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.1</head><p>The Face Encoder. We design our face encoder a bit differently from Chung et al. <ref type="bibr" target="#b7">[8]</ref>. We observe that during the training process of the generator in <ref type="bibr" target="#b7">[8]</ref>, a face image of random pose and its corresponding audio segment is given as input and the generator is expected to morph the lip shape. However, the ground-truth face image used to compute the reconstruction loss is of a completely different pose, and as a result, the generator is expected to change the pose of the input image without any prior information. To mitigate this, along with the random identity face image I , we also provide the desired pose information of the ground-truth as input to the face encoder. We mask the lower half of the ground truth face image and concatenate it channel-wise with I . The masked ground truth image provides the network with information about the target pose while ensuring that the network never gets any information about the ground truth lip shape.  <ref type="bibr" target="#b7">[8]</ref> employ only 2 skip connections between the face encoder and the decoder, we employ 6 skip connections, one after every upsampling operation to ensure that the fine-grained input facial features are preserved by the decoder while generating the face.</p><p>As we feed the desired pose as input during training, the model generates a morphed mouth shape that matches the given pose. Indeed, in our results, it can be seen that we preserve the face pose and expression better than Chung et al. <ref type="bibr" target="#b7">[8]</ref> and only change the mouth shape. This allows us to seamlessly paste the generated face crop into the given video without any artefacts, which was not possible with Chung et al. <ref type="bibr" target="#b7">[8]</ref> due to the random uncontrollable pose variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discriminator network</head><p>While using only an L2 reconstruction loss for the generator can generate satisfactory talking faces <ref type="bibr" target="#b7">[8]</ref>, employing strong additional supervision can help the generator learn robust, accurate phonemeviseme mappings and also make the facial movements more natural. Zhou et al. <ref type="bibr" target="#b34">[35]</ref> employed audio-visual speech recognition as a probe task to associate the acoustic and visual information. However, this makes the setup language-specific and offers only indirect supervision. We argue that directly testing whether the generated face synchronizes with the audio provides a stronger supervisory signal to the generator network. Accordingly, we create a network that encodes an input face and audio into fixed representations and computes the L2 distance d between them. The face encoder and audio encoder are the same as used in the generator network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Joint training of the GAN framework</head><p>Our training process is as follows. We randomly select a T millisecond window from an input video sample and extract its corresponding audio segment A, resampled at a frequency F Hz. We choose the middle video frame S in this window as the desired ground-truth. We mask the mouth region (assumed to be the lower-half of the image) of a person in the ground truth frame to get S m . We also sample a negative frame S ′ , i.e., a frame outside this window which is expected to not be in sync with the chosen audio segment A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3:</head><p>We train our LipGAN network in an intuitive GAN setup. The generator generates face images conditioned on the audio input. The discriminator checks whether the generated frame and the input audio are in sync. Note that while training the discriminator, we also feed extra ground-truth synced / unsynced samples to ensure that the discriminator learns to specifically check for superior lip-sync and not just the image quality.</p><p>At each training batch to the generator, the unsynced face S ′ concatenated channel wise with the masked ground truth face S m and the target audio segment A is provided as the input. The generator is expected to generate the synced face G([S ′ ; S m ], A) ≈ S. Each training batch to the discriminator contains three types of samples: (i) Synthetic samples from the generator (G(S ′ , A), A); y i = 1, (ii) Actual frames synced with audio (S, A); y i = 0 and (iii) Actual frames out of sync with audio (S ′ , A); y i = 1. The third sample type is particularly important to force the discriminator to take into account the lip synchronization factor while classifying a given input pair as real / synthetic. Without the third type of sample, the discriminator would simply be able to ignore the audio input and make its decision solely on the quality of the image. The discriminator learns to detect synchronization by minimizing the following contrastive loss:</p><formula xml:id="formula_1">L c (d i , y i ) = 1 2N N i=1 (y i · d i 2 + (1 − y i ) · max(0, m − d i ) 2 ) (1)</formula><p>where m is the margin, which we set to 2. The generator learns to reconstruct the face image by minimizing the L1 reconstruction loss:</p><formula xml:id="formula_2">L Re (G) = 1 N N i=1 ||S − G(S ′ , A)|| 1<label>(2)</label></formula><p>We train the generator G and discriminator D using the following GAN objective function:</p><formula xml:id="formula_3">L real = E z,A [L c (D(z, A), y)] (3) L fake = E S ′ ,A [L c (D(G([S ′ ; S m ], A), A), y = 1)] (4) L a (G, D) = L real + L fake<label>(5)</label></formula><p>where z ∈ {S, S ′ }. Here, G tries to minimize L a and L Re and D tries to maximize L a . Thus, the final objective function is:</p><formula xml:id="formula_4">G * = arg min G max D L a (G, D) + L Re<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Implementation Details</head><p>We use the LRS 2 dataset <ref type="bibr" target="#b0">[1]</ref> which contains over 29 hours of talking faces in the provided train split in the dataset. We train on four NVIDIA TITAN X GPUs with a batch size of 512. We extract 13 MFCC features from each audio segment (T = 350, F = 100) and discard the first feature similar to Chung et al. <ref type="bibr" target="#b7">[8]</ref>. We detect faces in our input frame using dlib <ref type="bibr" target="#b14">[15]</ref> and resize the face crops to 96x96x3. We use the Adam <ref type="bibr" target="#b15">[16]</ref> optimizer with an initial learning rate of 1e−3 and train for about 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results and Evaluation</head><p>We evaluate our novel LipGAN architecture quantitatively and also with subjective human evaluation. During inference, the model generates the talking face video of the target speaker frame-byframe. The visual input is the current frame concatenated with the same current frame with the lower-half masked. That is, during inference, we expect the model to morph the input shape and preserve other aspects like pose and expression. Along with each of the visual inputs, we feed a T = 350ms audio segment. In <ref type="figure" target="#fig_0">Figure  4</ref>, we compare the talking faces generated by 3 models on audio segments actually spoken by Narendra Modi and Elon Musk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Quantitative evaluation.</head><p>To evaluate our lip synthesis quantitatively, we use the LRW test set <ref type="bibr" target="#b8">[9]</ref>. We follow the same inference method mentioned above, but with one change. Instead of feeding the current frame as input as mentioned above, we feed a random input frame of the speaker, concatenated with the masked current frame for the pose prior. This is done to ensure we do not leak any lip information to the model while computing the quantitative metrics. In <ref type="table">Table 5</ref>, we report the scores obtained using standard metrics: PSNR, SSIM <ref type="bibr" target="#b31">[32]</ref> and Landmark distance <ref type="bibr" target="#b6">[7]</ref>. As can be seen in <ref type="table">Table 5</ref>, our model significantly outperforms existing works across all quantitative metrics. These results highlight the superior quality of our generated faces (judged by PSNR) and also a highly accurate lip synthesis (LMD, SSIM). The noted increase in SSIM and the decrease in LMD can be attributed to the direct lipsynchronization supervision provided by the discriminator, which is absent in prior works.</p><p>Algorithm PSNR SSIM LMD Chung et al. <ref type="bibr" target="#b7">[8]</ref> 28.06 0.460 2.22 Zhou et al. <ref type="bibr" target="#b34">[35]</ref> 26.80 0.884 -LipGAN (Ours) <ref type="bibr">33.4</ref> 0.960 0.60 <ref type="table">Table 5</ref>: Our proposed LipGAN model achieves significant improvements over existing competitive approaches across all standard quantitative metrics.</p><p>4.6.2 Importance of the lip sync discriminator. To illustrate the effect of employing a discriminator in the LipGAN network that tests whether the generator faces are in sync, we conduct the following experiment. We train the talking face generator network separately on the same train split of LRS 2 without changing any of the other hyperparameters. We feed the unseen test images shown in <ref type="figure" target="#fig_1">Figure  5</ref> along with unseen audio segments as input to our LipGAN network and the plain generator network that was trained without the discriminator. We plot the activations of the penultimate layer of the generator in both these cases. From the heatmaps in <ref type="figure" target="#fig_1">Figure 5</ref>, it is evident that our LipGAN network learns to attend strongly on the lip and mouth regions compared to the one that is not trained with a lip-sync discriminator. These findings also concur with the significant increase in the quantitative metrics as well as the natural movement of the lip regions in the generated faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3">Human evaluation.</head><p>Talking face generation is primarily done for direct human consumption. Hence, alongside the quantitative metrics, we also subject it to human evaluation. We choose 10 audio samples, with an equal number of English and Hindi speech   videos. For each audio sample, we generate talking faces using three different models for 5 popular identities to yield a total of 150 samples. We compare the faces generated by three different models: (i) Chung et al. <ref type="bibr" target="#b7">[8]</ref>, (ii) Zhou et al. <ref type="bibr" target="#b34">[35]</ref> and (iii) Our LipGAN model. We conduct a user study with the help of 20 participants who are asked to rate each of the videos on a scale of 1 to 5 based on the extent of lip synchronization and realistic nature. As shown in <ref type="table" target="#tab_6">Table 6</ref>, our model obtains significantly higher scores compared to existing works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Evaluating the complete pipeline</head><p>Finally, our Face-to-Face translation pipeline with all the components put together is evaluated based on its impact on the end-user experience. We choose 5 famous identities and generate talking face videos in Hindi of Andrew Ng, Obama, Modi, Elon Musk and Chris Anderson using our complete pipeline. We do this by choosing short videos of each of the above speakers speaking in English. We use our ASR and NMT modules to recognize the speech in English and translate it to Hindi. We use our Hindi TTS model to obtain speech in Hindi. We convert this speech to the voices of each of the above speakers using our CycleGAN models. Using these final voices, we generate talking face videos using our LipGAN network. We compare these videos against videos with (i) English speech and automatically translated subtitles (ii) automatic dubbing to Hindi (iii) automatic dubbing with voice transfer and (iv) automatic dubbing with voice transfer + lip synchronization. Additionally, we also benchmark human performance for speech-to-speech translation: (v) Manual dubbing and (vi) Manual Dubbing + automatic lip synchronization. We ask the users to rate the videos on a scale of 1 − 5 for two attributes. First one is "Semantic consistency" to check whether the automatic pipelines preserve the meaning of the original speech and the second attribute is the "Overall user experience", where the user considers factors such as the realistic nature of the talking face and his/her comfort level. The results of this study are reported in <ref type="table">Table 7</ref>.  <ref type="table">Table 7</ref>: User ratings for different ways to consume crosslanguage multimedia content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The results present three major takeaways. Firstly, we observe that there are significant scopes for improvement in each of the modules of automatic speech-to-speech translation systems. Future improvements in each of the speech and text translation systems will improve the user study scores. Secondly, the increase in user scores by using lip synchronization after manual dubbing again validates the effectiveness of the LipGAN model. Finally, note that adding each of our automatic modules increases the user experience score, emphasizing the need for each of them. Our complete proposed system improves the overall user experience over traditional text-based and speech-based translation systems by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATIONS</head><p>Our face-to-face translation framework can be used in a lot of applications. The demo video available here 6 demonstrates a proofof-concept for each of these applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Movie dubbing</head><p>Movies are generally dubbed by dubbing artists manually. The dubbed audio is then overlaid to the original video. This causes the actors' lips to be out of sync with the audio, thus affecting the viewer experience. Our pipeline can be used to automate this process at different levels with different trade-offs. We demonstrate that we can synthesize and synchronize lips in manually dubbed videos, thus automatically correcting any dubbing errors.</p><p>We also show a proof-of-concept for performing automatic dubbing using our translation pipeline. That is, given a movie scene in a particular language our system can potentially be used to dub it to a different language. However, as shown by the user study scores in <ref type="table">Table 7</ref>, significant improvements to the speech-to-speech pipeline is necessary to achieve realistic dubbing of complex speech present in movies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Educational videos</head><p>As reported before in <ref type="bibr" target="#b11">[12]</ref>, a large amount of online educational content is present in English in the form of video lectures. They are often aided with subtitles of foreign languages. But this increases the cognitive load of the viewer. Dubbing these videos with just speech-to-speech systems creates a visual discrepancy between the lip motion and the dubbed audio. However, with the help of our face to face translation system, it is possible to automatically translate such educational video content and also ensure lip synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Television news and interviews</head><p>Automatic Face-to-Face Translation systems can potentially allow viewers to access and consume important information from across the globe irrespective of the underlying language. For example, a Hindi or German viewer can watch an English interview of Obama in the language of his/her choice with lip synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We extend the problem of automatic machine translation to face to face translation with a focus on audio-visual content, i.e., where input and output are talking face videos. Beyond demonstrating the feasibility of a Face-to-Face translation pipeline, we also introduce a novel approach for talking face generation. We also contribute towards several language processing tasks (such as textual machine translation) for resource-constrained languages. Finally, we manifest our work on practical applications such as automatically dubbing educational videos, movie clips and interviews. Our attempt of "Face-to-Face Translation" also opens up a number of research directions in computer vision, multimedia processing, and machine learning. For instance, the duration of the speech gets naturally modified upon translation. This demands the transformation of the corresponding gestures, expressions, and background content. In addition to improving the existing individual modules, we believe that the above directions are all open for exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparison of faces generated by different models when they try to speak specific segments of the words shown in the last row. The audio segments corresponding to these word segments are extracted from the guiding video and fed into each of the models compared above. From top to bottom row: (a) Zhou et al. [35] (b) Chung et al. [8] and Our LipGAN model. While (a) achieves poor lipsync across languages, and (b) generates unnatural lip movements, our LipGAN model produces consistent accurate, natural talking faces across languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Activation heatmaps from the penultimate layer of two generator networks, one trained without a lip-sync discriminator (A) and the LipGAN network (ours) with a discriminator (B). Our network with the discriminator is highly attentive towards lip and mouth regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 2 :</head><label>22</label><figDesc>In the next section, we describe how we can modify the voice of the TTS model to a given target speaker. The MOS for our Hindi TTS is comparable to the same architecture trained on the LJSpeech English TTS dataset.</figDesc><table><row><cell>Sample Type</cell><cell>MOS</cell></row><row><cell>DeepVoice 3 Hindi</cell><cell>3.56</cell></row><row><cell cols="2">Ground truth Hindi 4.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Comparison of recent works on talking face syn- thesis against our LipGAN model. Ours is the first model that that generates realistic in-the-wild talking face videos across languages without the need for any manually labeled data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Thus our final input to the face encoder is a H xH x6 image. The encoder consists of a series of residual blocks with intermediate down-sampling layers and it embeds the given input image into a face embedding of size h.</figDesc><table><row><cell>4.2.2 Audio Encoder. The audio encoder is a standard CNN that</cell></row><row><cell>takes a Mel-frequency cepstral coefficient (MFCC) heatmap of size</cell></row><row><cell>MxT x1 and creates an audio embedding of size h. The audio embed-</cell></row><row><cell>ding is concatenated with the face embedding to produce a joint</cell></row><row><cell>audio-visual embedding of size 2xh.</cell></row><row><cell>4.2.3 Face Decoder. This branch produces a lip-synchronized face</cell></row><row><cell>from the joint audio-visual embedding by inpainting the masked</cell></row><row><cell>region of the input image with an appropriate mouth shape. It</cell></row><row><cell>contains a series of residual blocks with a few intermediate decon-</cell></row><row><cell>volutional layers that upsample the feature maps. The output layer</cell></row><row><cell>of the Face decoder is a sigmoid activated 1x1 convolutional layer</cell></row><row><cell>with 3 filters</cell></row></table><note>, resulting in a face image of H xH x3. While Chung et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>LipGAN achieves significantly higher scores for both realistic rate and the extent of lip synchronization</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://cvit.iiit.ac.in/research/projects/cvit-projects/facetoface-translation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/r9y9/deepvoice3_pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://cvit.iiit.ac.in/research/projects/cvit-projects/facetoface-translation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Oriol Vinyals, and Andrew Zisserman</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00089</idno>
		<title level="m">Massively Multilingual Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural voice cloning with a few samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sercan</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10040" to="10050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video Rewrite: driving visual speech with audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Siggraph</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="520" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02966</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">You said that? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft speech language translation (mslt) corpus: The iwslt 2016 release for english, french and german</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time Fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-Language Speech Dependent Lip-Synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>To appear in. ICASSP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">GoogleâĂŹs multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Parallel-data-free voice conversion using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuhiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Kameoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11293</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Obamanet: Photo-realistic lip-sync from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01442</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Alexandre de Brébisson, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The IIT Bombay English-Hindi Parallel Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skype Translator: Breaking Down Language and Hearing Barriers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Translating and the Computer (TC37)</title>
		<meeting>Translating and the Computer (TC37)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rapid Adaptation of Neural Machine Translation to New Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="875" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Percent of Millennial Smartphone Owners Use their Device for Video Calling, According to The NPD Group</title>
		<idno>NPD. 2016. 52</idno>
		<ptr target="https://www.npd.com/wps/portal/npd/us/news/press-releases/2016/52-percent-of-millennial-smartphone-owners-use-their-device-for-video-calling-according-to-the-npd-group/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A Baseline Neural Machine Translation System for Indian Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerin</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12437</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep voice 3: Scaling text-tospeech with convolutional sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07654</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TED-LIUM: an Automatic Speech Recognition dedicated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Deléglise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Esteve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rj</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideyuki</forename><surname>Tachibana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuya</forename><surname>Uenoyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Aihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4784" to="4788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Statistical parametric speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiichi</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1039" to="1064" />
		</imprint>
	</monogr>
	<note>speech communication</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07860</idno>
		<title level="m">Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
