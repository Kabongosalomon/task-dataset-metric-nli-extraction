<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Dual-Source Approach for 3D Human Pose Estimation from a Single Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Doering</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Krüger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
						</author>
						<title level="a" type="main">A Dual-Source Approach for 3D Human Pose Estimation from a Single Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>SUBMITTED TO COMPUTER VISION AND IMAGE UNDERSTANDING. 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D human pose estimation</term>
					<term>motion capture</term>
					<term>3D reconstruction</term>
					<term>articulated pose estimation !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we address the challenging problem of 3D human pose estimation from single images. Recent approaches learn deep neural networks to regress 3D pose directly from images. One major challenge for such methods, however, is the collection of training data. Specifically, collecting large amounts of training data containing unconstrained images annotated with accurate 3D poses is infeasible. We therefore propose to use two independent training sources. The first source consists of accurate 3D motion capture data, and the second source consists of unconstrained images with annotated 2D poses. To integrate both sources, we propose a dual-source approach that combines 2D pose estimation with efficient 3D pose retrieval. To this end, we first convert the motion capture data into a normalized 2D pose space, and separately learn a 2D pose estimation model from the image data. During inference, we estimate the 2D pose and efficiently retrieve the nearest 3D poses. We then jointly estimate a mapping from the 3D pose space to the image and reconstruct the 3D pose. We provide a comprehensive evaluation of the proposed method and experimentally demonstrate the effectiveness of our approach, even when the skeleton structures of the two sources differ substantially.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>3D human pose estimation has a vast range of applications such as virtual reality, human-computer interaction, activity recognition, sports video analytics, and autonomous vehicles. The problem has traditionally been tackled by utilizing multiple images captured by synchronized cameras capturing the person from multiple views <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b60">[61]</ref>. In many scenarios, however, capturing multiple views is infeasible which limits the applicability of such approaches. Since 3D human pose estimation from a single image is very difficult due to missing depth information, depth cameras have been utilized for human pose estimation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b18">[19]</ref>. However, current depth sensors are also limited to indoor environments and cannot be used in unconstrained scenarios. Therefore, estimating 3D pose from single, in particular, unconstrained images is a highly relevant task.</p><p>One approach to address this problem is to follow a fullysupervised learning paradigm, where a regression model <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b53">[54]</ref> or a deep neural network <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b37">[38]</ref> can be learned to directly regress the 3D pose from single images. This approach, however, requires a large amount of training data where each 2D image is annotated with a 3D pose. In contrast to 2D pose estimation, manual annotation of such training data is not possible due to ambiguous geometry and body part occlusions. On the other hand, automatic acquisition of accurate 3D pose for an image requires a very sophisticated setup. The popular datasets like HumanEva <ref type="bibr" target="#b46">[47]</ref> or Human3.6M <ref type="bibr" target="#b22">[23]</ref> synchronized cameras with a commercial marker-based system to obtain 3D poses for images. This requires a very expensive hardware setup and the requirements for marker-based system like studio environment and attached markers limits the applicability of such systems primarily to indoor laboratory environments. Some recent approaches such as EgoCap <ref type="bibr" target="#b42">[43]</ref> allows to capture 3D poses in outdoor environments, but image data in such cases is restricted only to ego-centric views of the person. In this work, we propose a dual-source method that does not require training data consisting of pairs of an image and a 3D pose, but rather utilize 2D and 3D information from two independent training sources as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The first source is accurate 3D motion capture data containing a large number of 3D poses, and is captured in a laboratory setup, e.g., as in the CMU motion capture dataset <ref type="bibr" target="#b15">[16]</ref> or the Human3.6M dataset <ref type="bibr" target="#b22">[23]</ref>. Whereas, the second source consists of images with annotated 2D poses as they are provided by 2D human pose datasets, e.g., MPII Human Pose <ref type="bibr" target="#b3">[4]</ref>, Leeds Sports Pose <ref type="bibr" target="#b24">[25]</ref> and MSCOCO <ref type="bibr" target="#b30">[31]</ref>. Since 2D poses can be manually annotated for images, they do not impose any restriction regarding the environment from where the images are taken. In fact any image from the Internet can be annotated and used. Since both sources are captured independently, we do not know the 3D pose for any training image. In order to bring the two sources together, we map the motion capture data into a normalized 2D pose space to allow for an efficient retrieval based on 2D body joints. Concurrently, we learn a 2D pose estimation model from the 2D images based on convolutional neural networks. During inference, we first estimate the 2D pose and retrieve the nearest 3D poses using an effective approach that is robust to 2D pose estimation errors. We then jointly estimate the projection from the 3D pose space to the image and reconstruct the 3D pose.</p><p>A preliminary version of this work was presented in <ref type="bibr" target="#b61">[62]</ref>. In this work we leverage the recent progress in 2D pose estimation <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b14">[15]</ref>, arXiv:1705.02883v2 [cs.CV] 6 Sep 2017 and improve the performance of <ref type="bibr" target="#b61">[62]</ref> by a large margin. We further show that with the availability of better 2D pose estimates, the approach <ref type="bibr" target="#b61">[62]</ref> can be largely simplified. We extensively evaluate our approach on two popular datasets for 3D pose estimation namely Human3.6M <ref type="bibr" target="#b22">[23]</ref> and HumanEva <ref type="bibr" target="#b46">[47]</ref>. On both datasets, our approach performs better or on par with the state-of-the-art. We provide an in-depth analysis of the proposed approach. In particular, we analyze the impact of different MoCap datasets, the impact of the similarity of the training and test poses, the impact of the accuracy of the used 2D pose estimator, and also the differences of the skeleton structure between the two training sources. Finally, we also provide qualitative results for images taken from the MPII Human Pose dataset <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Earlier approaches for 3D human pose estimation from single images <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b32">[33]</ref> utilize discriminative methods to learn a mapping from hand-crafted local image features (e.g., HOG, SIFT, etc.) to 3D human pose. Since local features are sensitive to noise, <ref type="bibr" target="#b25">[26]</ref> proposed an approach based on a 3D pictorial structure model that combines generative and discriminative methods to obtain robustness to noise. For this, regression forests are trained to estimate the probabilities of 3D joint locations and the final 3D pose is inferred by the pictorial structure model. Since inference is performed in 3D, the bounding volume of the 3D pose space needs to be known and the inference requires a few minutes per frame. In addition to the local image features, the approach <ref type="bibr" target="#b21">[22]</ref> also utilizes body part segmentation with a second order hierarchical pooling process to obtain robust image descriptors. Instead of computing low level image features, the approach <ref type="bibr" target="#b36">[37]</ref> uses boolean geometric relationships between body joints to encode body pose appearance. These features are then used to retrieve semantically similar poses from a large corpus of 3D poses.</p><p>With the advances in deep learning, more recent approaches learn end-to-end CNNs to regress the 3D joint locations directly from the images <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b37">[38]</ref>. In this direction, the work <ref type="bibr" target="#b28">[29]</ref> is one of the earliest methods that presents an end-to-end CNN architecture, where a multi-task loss is proposed to simultaneously detect body parts in 2D images and regress their locations in 3D space. In <ref type="bibr" target="#b29">[30]</ref> a max-margin loss is incorporated with a CNN architecture to efficiently model joint dependencies. Similarly, <ref type="bibr" target="#b63">[64]</ref> enforces kinematic constraints by introducing a differentiable kinematic function that can be combined with a CNN. The approach <ref type="bibr" target="#b51">[52]</ref> uses auto-encoders to incorporate dependencies between body joints and combines them with a CNN architecture to regress 3D poses. Approaches for data augmentation have also been proposed in <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b13">[14]</ref> where synthetic training images are generated to enlarge the training data. <ref type="bibr" target="#b31">[32]</ref> proposes to encode 3D pose using an Euclidean distance matrix formulation that implicitly incorporates body joint relations and allows to regress 3D poses in form of a distance matrix using a very small network architecture. The approaches <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b37">[38]</ref> leverage the information about the locations of 2D body joints to aid 3D human pose estimation. While <ref type="bibr" target="#b34">[35]</ref> directly uses the 2D joint coordinates to regularize the training of a CNN, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b37">[38]</ref> use confidence scoremaps of 2D body joints obtained using a CNN as additional features for 3D pose regression. All these approaches demonstrate very good performances for 3D pose estimation, but require a large amount of training data containing pairs of images and ground-truth 3D poses to train deep network architectures. This limits their applicability to the environments of the training data.</p><p>Estimating 3D human pose from a given 2D pose by exploiting motion capture data has also been addressed in the literature <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b54">[55]</ref>. While early approaches <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b62">[63]</ref> used manually annotated 2D joint locations, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b57">[58]</ref> are one of the first approaches that estimate the 3D pose from estimated 2D poses. With the progress in 2D pose estimation methods <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b14">[15]</ref>, the number of approaches in this category also rose <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b54">[55]</ref>. All these approaches have the benefit that they do not require training data containing images with annotated 3D poses, but rather only utilize 3D pose data to build their models.</p><p>In <ref type="bibr" target="#b62">[63]</ref>, the 2D pose is manually annotated in the first frame and tracked in a video. A nearest neighbor search is then performed to retrieve the closest 3D poses. In <ref type="bibr" target="#b40">[41]</ref> a sparse representation of 3D human pose is constructed from a MoCap dataset and fitted to manually annotated 2D joint locations. The approach has been extended in <ref type="bibr" target="#b57">[58]</ref> to handle poses from an off-the-shelf 2D pose estimator <ref type="bibr" target="#b59">[60]</ref>, and subsequently to videos in <ref type="bibr" target="#b16">[17]</ref>. The information about the 2D body joints is used in <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b48">[49]</ref> to constrain the search space of 3D poses. In <ref type="bibr" target="#b49">[50]</ref> an evolutionary algorithm is used to sample poses from the pose space that correspond to the estimated 2D joint positions. This set is then exhaustively evaluated according to some anthropometric constraints. The approach is extended in <ref type="bibr" target="#b48">[49]</ref> such that the 2D pose estimation and 3D pose estimation are iterated. In contrast to <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b49">[50]</ref>, the approach <ref type="bibr" target="#b48">[49]</ref> deals with 2D pose estimation errors.</p><p>An expectation maximization algorithm is presented in <ref type="bibr" target="#b64">[65]</ref> to estimate 3D poses from monocular videos. Additional smoothness constraints are used to exploit the temporal information in videos. In addition to the 3D pose, <ref type="bibr" target="#b9">[10]</ref> also estimates the 3D shape of the person. The approach exploits a high-quality 3D human body model and fits it to estimated 2D joints using an energy minimization objective. The approach is improved further in <ref type="bibr" target="#b27">[28]</ref> by introducing an extra fitting objective and generating additional training data. In <ref type="bibr" target="#b12">[13]</ref> a non-parametric nearest neighbor model is used to retrieve 3D exemplars that minimize the reprojection error from the estimated 2D joint locations. More recently, <ref type="bibr" target="#b54">[55]</ref> proposes a probabilistic 3D pose model and combines it with a multi-staged CNN, where the CNN incorporates evidences from the 2D body part locations and projected 3D poses to sequentially improve 2D joint predictions which in turn also results in better 3D pose estimates.</p><p>Action specific priors learned from motion capture data have also been proposed for 3D pose tracking <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b4">[5]</ref>. These approaches, however, are more constrained by assuming that the type of motion is known in advance and therefore cannot deal with a large and diverse pose dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head><p>In this work, we aim to predict the 3D pose from an RGB image. Since acquiring 3D pose data in natural environments is impractical and annotating 2D images with 3D pose data is infeasible, we do not assume that our training data consists of images annotated with 3D pose. Instead, we propose an approach that utilizes two independent sources of training data. The first source consists of motion capture data, which is publicly available in large quantities and that can be recorded in controlled environments. The second source consists of images with annotated 2D poses, which is also available and can be easily provided by humans. Since we do not assume that we know any relations between the sources except that the motion capture data includes the poses we are interested in, we preprocess the sources first independently as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. From the image data, we learn a CNN to predict 2D poses from images. This will be discussed in Section 4. The motion capture data is prepared to efficiently retrieve 3D poses that could correspond to a 2D pose. This part is described in Section 5.1. We then estimate the 3D pose by minimizing the projection error under the constraint that the solution is close to the retrieved poses (Section 5.2). The source code of the approach is publicly available. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Sources</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">2D POSE ESTIMATION</head><p>In this work, we use the convolutional pose machines (CPM) <ref type="bibr" target="#b58">[59]</ref> for 2D pose estimation, but other CNN architectures, e.g. stacked hourglass <ref type="bibr" target="#b33">[34]</ref> or multi-context attention models <ref type="bibr" target="#b14">[15]</ref>, could be used as well. Given an image I, we define the 2D pose of the person as x = {x j } j∈J , where x j ∈ R 2 is the 2D coordinates of body joint j, and J is the set of all body joints. CPM consists of a multi-staged CNN architecture, where each stage t ∈ {1 . . . T } produces a set of confidence scoremaps s t = {s j t } j∈J , where s j t ∈ R w×h is the confidence score map of body joint j at stage t, and w and h are the width and the height of the image, respectively. Each stage of the network sequentially refines the 2D pose estimates by utilizing the output of the preceding stage and also the features extracted from the raw input image. The final 2D pose x is obtained as</p><formula xml:id="formula_0">x = arg max x ={x j } j∈J j∈J s j T (x j ).<label>(1)</label></formula><p>.</p><p>In our experiments we will show that training the network on publicly available dataset for 2D pose estimation in-the-wild, such as MPII Human Pose dataset <ref type="bibr" target="#b3">[4]</ref>, is sufficient to obtain state-ofthe-art results with our proposed method. <ref type="bibr" target="#b0">1</ref>. http://pages.iai.uni-bonn.de/iqbal_umar/ds3dpose/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">3D POSE ESTIMATION</head><p>While the CNN for 2D pose estimation is trained on the images with 2D pose annotations as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we now describe an approach that makes use of a second dataset with 3D poses in order to predict the 3D pose from an image. Since the two sources are independent, we first have to establish relations between 2D poses and 3D poses. This is achieved by using an estimated 2D pose as query for 3D pose retrieval (Section 5.1). The retrieved poses, however, contain many wrong poses due to errors in 2D pose estimation, 2D-3D ambiguities and differences of the skeletons in the two training sources. It is therefore necessary to fit the 3D poses to the 2D observations. This will be described in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">3D Pose Retrieval</head><p>In order to efficiently retrieve 3D poses for a 2D pose query, we preprocess the motion capture data. We first normalize the poses by discarding orientation and translation information from the poses in our motion capture database. We denote a 3D normalized pose with X and the 3D normalized pose space with Ψ. As in <ref type="bibr" target="#b62">[63]</ref>, we project the normalized poses X ∈ Ψ to 2D using orthographic projection. We use 144 virtual camera views with azimuth angles spanning 360 degrees and elevation angles in the range of 0 and 90 degree. Both angles are uniformly sampled with a step size of 15 degree. We further normalize the projected 2D poses by scaling them such that the y-coordinates of the joints are within the range of [−1, 1]. The normalized 2D pose space is denoted by ψ and does not depend on a specific camera model or coordinate system. This step is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. After a 2D pose is estimated by the approach described in Section 4, we first normalize it according to ψ, i.e., we translate and scale the pose such that the y-coordinates of the joints are within the range of [−1, 1], then use it to retrieve 3D poses. The distance between two normalized 2D poses is given by the average Euclidean distance of the joint positions. The Knearest neighbors in ψ are efficiently retrieved by a kd-tree <ref type="bibr" target="#b26">[27]</ref>. The retrieved normalized 3D poses are the corresponding poses in Ψ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">3D Pose Estimation</head><p>In order to obtain the 3D pose X, we have to estimate the unknown projection M from the normalized pose space Ψ to the image. To this end, we minimize the energy</p><formula xml:id="formula_1">E(X, M) = E p (X, M) + αE r (X)<label>(2)</label></formula><p>consisting of the two terms E p and E r , where α is a weighting parameter. The first term E p (X, M, s) measures the projection error of the 3D pose X and the projection M:</p><formula xml:id="formula_2">E p (X, M) =   j∈J M (X j ) − x j 2   1 2 ,<label>(3)</label></formula><p>where x j is the joint position of the predicted 2D pose and X j is the 3D joint position of the unknown 3D pose.</p><p>The second term enforces that the pose X is close to the retrieved 3D poses X k :</p><formula xml:id="formula_3">E r (X) = k   j∈J X k j − X j 2   1 2 .<label>(4)</label></formula><p>Minimizing the energy E(X, M) (2) over the continuous parameters X and M would be expensive. We therefore propose to obtain an approximate solution where we estimate the projection M first. For the projection, we assume that the intrinsic parameters are given and only estimate the global orientation and translation. The projectionM is estimated by minimizinĝ</p><formula xml:id="formula_4">M = arg min M K k=1 E p (X k , M)<label>(5)</label></formula><p>using non-linear gradient optimization. Given the estimated pro-jectionsM, we minimizê</p><formula xml:id="formula_5">X = arg min X E(X,M)<label>(6)</label></formula><p>to obtain the 3D pose X. In our experiments, we will also evaluate the case when camera orientation and translation are also known. In this case the projection M reduces to a rigid transformation of 3D pose X from normalized pose space Ψ to the camera coordinate system. The dimensionality of X can be reduced by applying PCA to the retrieved 3D poses X k . Reducing the dimensions of X helps to decrease the optimization time without loss in accuracy, as we will show in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We evaluate the proposed approach on two publicly available datasets, namely Human3.6M <ref type="bibr" target="#b22">[23]</ref> and HumanEva-I <ref type="bibr" target="#b46">[47]</ref>. Both datasets provide accurate 3D poses for each image and camera parameters. For all cases, 2D pose estimation is performed by convolutional pose machines <ref type="bibr" target="#b58">[59]</ref> trained on the MPII Human Pose dataset <ref type="bibr" target="#b3">[4]</ref>, and no fine-tuning is performed, unless stated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation on Human3.6M Dataset</head><p>For evaluation on the Human3.6M dataset, a number of protocols have been proposed in the literature. The protocol originally proposed for the Human3.6M dataset <ref type="bibr" target="#b22">[23]</ref>, which we denote by Protocol-III, uses the annotated bounding boxes and the training data only from the action class of the test data. This simplifies the task due to the small pose variations for a single action class and the known person bounding box. Other protocols have been therefore proposed in <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b9">[10]</ref>. In order to compare with other existing approaches, we report results for all three protocols <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Human3.6M Protocol-I</head><p>Protocol-I, which was proposed by <ref type="bibr" target="#b25">[26]</ref>, is the most unconstrained protocol. It does not make any assumption about the location and activity labels during testing, and the training data comprises all action classes. The training set consists of six subjects (S1, S5, S6, S7, S8 and S9), whereas the testing is performed on every 64 th frame taken from the sequences of S11. For evaluation, we use the 3D pose error as defined in <ref type="bibr" target="#b49">[50]</ref>. The error measures the accuracy of the relative pose up to a rigid transformation. To this end, the estimated skeleton is aligned to the ground-truth skeleton by a rigid transformation and the average 3D Euclidean joint error after alignment is measured, where the body skeleton consists of 14 body joints namely head, neck, ankles, knees, hips, wrists, elbows and shoulders. In order to comply with the protocol, we do not use ground truth person bounding boxes, but instead use an off-the-shelf person detector <ref type="bibr" target="#b41">[42]</ref> to detect the person's bounding box required for 2D pose estimation. We consider two sources for the motion capture data, namely the Human3.6M and the CMU motion capture dataset. We first evaluate the impact of the parameters of our approach and the impact of different MoCap datasets. We then compare our approach with the state-of-the-art and evaluate the impact of the 2D pose estimation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters</head><p>Nearest Neighbors. The impact of the number of nearest neighbors K used during 3D pose reconstruction is evaluated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Increasing the number of nearest neighbors improves 3D pose estimation. This, however, also increases the reconstruction time. In the rest of this paper, we use a default value of K = 256 that provides a good trade-off between accuracy and run-time. We can see that using the CMU MoCap dataset results in a higher error as compared to the Human3.6M dataset. We will evaluate the impact of different MoCap datasets in more details later in this section.  PCA. PCA can be used to reduce the dimension of X. While in <ref type="bibr" target="#b61">[62]</ref> a fixed number of principal components is used, we use a more adaptive approach and set the number of principal components based on the captured variance. The number of principal components therefore varies for each image.</p><p>The impact of the threshold on the minimum amount of variation can be seen in <ref type="figure" target="#fig_2">Fig. 3</ref>. If the threshold is within a reasonable range, i.e. between 0.8 and 1, the accuracy is barely reduced while the runtime decreases significantly compared to 1, i.e. without PCA. In this work, we use the minimum number of principle components that explain at least 80% of the variance of the retrieved 3D poses X k .</p><p>Energy Terms. The impact of the weight α in (2) is reported in <ref type="figure" target="#fig_3">Fig. 4</ref>. If α = 0, the term E r is ignored and the error is very high. This is expected since E r constrains the possible solution while E p ensures that the estimated 3D pose projects onto the estimated 2D pose. In our experiments, we use α = 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MoCap Data</head><p>Impact of MoCap dataset size. We evaluate the impact of the size of the MoCap dataset in <ref type="figure" target="#fig_4">Fig. 5</ref>. Using the entire 469K 3D poses of the Human3.6M training set as motion capture data results in a 3D pose error of 68.8mm. Reducing the size of the MoCap data to 329K by removing similar poses reduces the error to 66.85mm. We consider two poses as similar when the Euclidean distance between both poses is less than a certain threshold, which is 20mm in this case. Removing similar poses ensures that the retrieved nearest neighbors for each test image embody a sufficient variety of 3D poses. However, decreasing the size of the MoCap dataset even further degenerates the performance. In the rest of our experiments, we use the MoCap dataset from Human3.6M with 329K 3D poses, where a threshold of 20mm is used to remove similar poses.</p><p>CMU Motion Capture Dataset. Since we do not assume that the images are annotated by 3D poses but use motion capture data as a second training source. We therefore evaluate our approach using the CMU motion capture dataset <ref type="bibr" target="#b15">[16]</ref> for our 3D pose retrieval. We use one third of the CMU dataset and downsample the CMU dataset from 120Hz to 30Hz, resulting in 360K 3D poses. We remove similar poses using the same threshold (20mm) as used <ref type="bibr">MoCap</ref>   for Human3.6M which results in a final MoCap dataset with 303K 3D poses. <ref type="figure" target="#fig_5">Fig. 6</ref> compares the pose estimation accuracy using both datasets, while the results for each activity can be seen in Tab. 1. As expected the error is higher due to the differences of the datasets.</p><note type="other">data Direction Discuss Eating Greeting Phoning Posing Purchases Sit SitDown Human3.6M 59.</note><p>To analyze the impact of the motion capture data more in detail, we have evaluated the pose error for various modifications of the MoCap data in Tab. 1. First, we remove all poses of a selected activity from the MoCap data and evaluate the 3D pose error for the test images corresponding to the removed activity. The error increases since the dataset does not contain poses related to the removed activity anymore. While the error still stays comparable for many activities, e.g. Direction, Discussion, etc., a significant increase in error can be seen for activities that do not share similar poses with other activities e.g. SitDown. However, even if all poses related to the activity of the test images are removed, the results are still good and better compared to the CMU dataset. This indicates that the error increase for the CMU dataset cannot only be explained by the difference of poses, but also other factors like different motion capture setups seem to influence the result. We will investigate the impact of the skeleton structure between two datasets in Section 6.2.</p><p>We also evaluate the case when MoCap data only consist of the poses of a specific activity. This also results in an increased mean pose estimation error and shows that having a diverse MoCap dataset is helpful to obtain good performance. Finally, we also report the error when the 3D poses of the test sequences are added to the MoCap dataset. In this case, the error is reduced from 61.53mm to 51.29mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-art</head><p>Tab. 2 compares the performance of the proposed method with the state-of-the-art approaches <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b54">[55]</ref> using both MoCap datasets. Our approach outperforms the other approaches. In particular, the 3D pose error reported in <ref type="bibr" target="#b61">[62]</ref> is reduced from 108.3mm to 66.9mm when using the Human3.6M MoCap dataset. A similar decrease in error can also be seen for the CMU dataset (124.8mm vs. 91.0mm). The main improvement compared to <ref type="bibr" target="#b61">[62]</ref> stems from the better 2D pose estimation model. Our approach also outperforms the recent methods <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b54">[55]</ref>. While <ref type="bibr" target="#b31">[32]</ref> relies on pair of images and 3D poses as training data, <ref type="bibr" target="#b54">[55]</ref> learns a deep CNN model using the 2D pose data from Human3.6M. We on the other hand do not use pairs of images and 3D pose for training and only utilize a pre-trained model trained on the MPII Human Pose Dataset <ref type="bibr" target="#b3">[4]</ref> for 2D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of 2D Pose</head><p>We also investigate the impact of the accuracy of the estimated 2D poses. If we initialize the approach with the 2D ground-truth poses, the 3D pose error is significantly reduced as shown in Tab. 3. This indicates that the 3D pose error can be further reduced by improving the used 2D pose estimation method. We also report the 3D pose error when both 3D and 2D ground-truth poses are available. In this case the error reduces even further which shows the potential of further improvements for the proposed method. We also compare our approach to <ref type="bibr" target="#b61">[62]</ref> and <ref type="bibr" target="#b12">[13]</ref>, which also report the accuracy for ground-truth 2D poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Human3.6M Protocol-II</head><p>The second protocol, Protocol-II, has been proposed in <ref type="bibr" target="#b9">[10]</ref>. The dataset is split using five subjects (S1, S5, S6, S7, S8) for training and two subjects (S9 and S11) for testing. We follow <ref type="bibr" target="#b27">[28]</ref> and perform testing on every 5 th frame of the sequences from the frontal camera (cam-3) and trial-1 of each activity. The evaluation is performed in the same way as in Protocol-I with a body skeleton consisting 14 joints. In contrast to Protocol-I, the ground-truth bounding boxes are, however, used during testing. Tab. 4 reports the comparison of the proposed method with the state-of-the-art approaches <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Our approach achieves comparable results to the state-of-the-art, while also outperforming other methods on some activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Human3.6M Protocol-III</head><p>The third protocol, Protocol-III, is the most commonly used protocol for Human3.6M. Similar to Protocol-II, the dataset is split by using subjects S1, S5, S6, S7 and S8 for training and subjects S9 and S11 for testing. The sequences are downsampled from the original frame-rate of 50fps to 10fps, and testing is performed on the sequences from all cameras and trials. The evaluation is performed without a rigid transformation, but both the ground-truth and estimated 3D poses are centered with respect to the root joint. We therefore have to use the provided camera parameters such that the estimated 3D pose is in the coordinate system of the camera. The training and testing is often performed on the same activity. However, some recent approaches also report results by training only once for all activities. In this work, we report results under both settings. In this protocol, a body skeleton with 17 joints is used and the ground-truth bounding boxes are used during testing. Note that even though the 3D poses contain   17 joints, we still use the 2D poses with 14 joints for nearest neighbor retrieval and only use the corresponding joints for optimizing objective <ref type="bibr" target="#b1">(2)</ref>. Tab. 5 provides a detailed comparison of the proposed approach with the state-of-the-art methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>Finally, we present some qualitative results in <ref type="figure">Fig. 7</ref>. As it can be seen, our approach shows very good performance even for highly articulated poses and under severe occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation on HumanEva-I Dataset</head><p>We follow the same protocol as described in <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b25">[26]</ref> and use the provided training data to train our approach while using the validation data as test set. As in <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b25">[26]</ref>, we report our results on every 5 th frame of the sequences walking (A1) and jogging (A2) for all three subjects (S1, S2, S3) and camera C1. The 3D pose error is computed as in Protocol-I for the Human3.6M dataset.</p><p>We perform experiments with the 3D pose data from the HumanEva and CMU MoCap datasets. For HumanEva, we use the entire 49K 3D poses of the training data as MoCap dataset. Since the joint positions of skeleton used for HumanEva differs from the marked joint positions used for the MPII Human Pose dataset, we fine-tune the 2D pose estimation model on the HumanEva dataset using the provided 2D pose data. For fine-tuning, we run 500 iterations with a learning rate of 0.00008.</p><p>We also have to adapt the skeleton structure of the CMU dataset to the skeleton structure of the HumanEva dataset. As in <ref type="bibr" target="#b61">[62]</ref>, we re-target the 3D poses in the CMU dataset to the skeleton of the HumaEva dataset using linear regression. To this end, we first search for each 3D pose in the CMU dataset the nearest neighbor in the HumanEva dataset and then select pairs of 2D  nearest neighbors that have a distance less than a certain threshold. The selected pairs are then used to learn a linear regressor for each joint. We analyze the impact of the difference between the skeletons of both datasets in Tab. 6. Using HumanEva as MoCap dataset results in a 3D pose error of 31.5mm, whereas using CMU as MoCap dataset increases the error significantly to 80.0mm. Re-targeting the skeletons of the CMU dataset to the skeleton of HumanEva reduces the error from 80.0mm to 50.5mm, and re-targeting the skeleton of HumanEva to CMU increases the error from 31.5mm to 58.4mm. This shows that the difference of the skeleton structure between the the two sources can have a major impact on the evaluation. This is, however, not an issue for an application where the MoCap dataset defines the skeleton structure.</p><p>We also compare our approach with the state-of-the-art approaches <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b31">[32]</ref> in Tab. 7. Our method outperforms all other methods except of the two recent approaches <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Both approaches use pairs of images and 3D poses to learn a neural network model, while our approach considers them as independent sources and is therefore trained with less supervision.</p><p>Finally, we present qualitative results for a few realistic images taken from the MPII Human Pose dataset <ref type="bibr" target="#b3">[4]</ref> in <ref type="figure">Fig. 8</ref>. The results show that the proposed approach generalizes very well to complex unconstrained images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we have proposed a novel dual-source method for 3D human pose estimation from monocular images. The first source is a MoCap dataset with 3D poses and the other source are images with annotated 2D poses. Due to the separation of the two sources, our approach needs less supervision compared to approaches that are trained from images annotated with 3D poses, which is difficult to acquire under real conditions. The proposed approach therefore presents an important step towards accurate 3D pose estimation in unconstrained images. Compared to the preliminary work, the proposed approach achieves a substantial lower pose estimation error. This is achieved by utilizing the strengths of recent 2D pose estimation methods and combining them with an efficient and robust method for 3D pose retrieval. We have performed a thorough experimental evaluation and demonstrated that our approach achieves competitive results in comparison to the stateof-the-art, even when the training data are from very different sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Pose</head><p>View-    Comparison with other state-of-the-art approaches on the HumanEva-I dataset. The average 3D pose error (mm) are reported for all three subjects (S1, S2, S3) and camera C1. * denotes a different evaluation protocol.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overview. Our approach relies on two training sources. The first source is a motion capture database that contains only 3D poses. The second source is an image database with annotated 2D poses. The motion capture data is processed by pose normalization and projecting the poses to 2D using several virtual cameras. This gives many 3D-2D pairs where the 2D poses serve as features. The image data is used to learn a CNN model for 2D pose estimation. Given a test image, the CNN predicts the 2D pose which is then used to retrieve the normalized nearest 3D poses. The final 3D pose is then estimated by minimizing the projection error under the constraint that the solution is close to the retrieved poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Impact of the number of nearest neighbors K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Impact of PCA. The number of principle components are selected based on the minimum number of components that explain a given percentage of variation. The x-axis corresponds to the threshold for the cumulative amount of variation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Impact of α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Impact of the size of the MoCap dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Comparison of 3D pose error using different MoCap datasets. The plot represent the percentage of estimated 3D poses with an error below a specific threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Umar Iqbal, Andreas Doering and Juergen Gall are with the Computer Vision Group, University of Bonn, Germany. Email: uiqbal@iai.unibonn.de, s6andoer@uni-bonn.de, gall@iai.uni-bonn.de,</figDesc><table /><note>*authors contributed equally•• Hashim Yasin is with the National University of Science and Technology, Pakistan Email: hashim.yasin@nu.edu.pk,• Björn Krüger is with the Gokhale Method Institute, Stanford, USA. Email: kruegerb@cs.uni-bonn.de,• Andreas Weber is with the Multimedia, Simulation, Virtual Reality Group, University of Bonn, Germany. Email: weber@cs.uni-bonn.de.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Impact of the MoCap dataset. While for Human3.6M \ Activity we removed all poses from the dataset that correspond to the activity of the test sequence, Human3.6M ∈ Activity only contains the poses of the activity of the test sequence. For Human3.6M + GT 3D Poses, we include the ground-truth 3D poses of the test sequences to the MoCap dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art on the Human3.6M dataset using Protocol-I.</figDesc><table><row><cell>Method</cell><cell>Direction</cell><cell>Discuss</cell><cell>Eat</cell><cell>Greet</cell><cell>Phone</cell><cell>Pose</cell><cell>Purchase</cell><cell>Sit</cell><cell>SitDown</cell></row><row><cell>Ours</cell><cell>59.5</cell><cell>52.4</cell><cell>75.5</cell><cell>67.0</cell><cell>58.8</cell><cell>64.9</cell><cell>58.2</cell><cell>68.4</cell><cell>89.7</cell></row><row><cell>Ours + GT 2D</cell><cell>51.9</cell><cell>45.3</cell><cell>62.4</cell><cell>55.7</cell><cell>49.2</cell><cell>56.0</cell><cell>46.4</cell><cell>56.3</cell><cell>76.6</cell></row><row><cell>Ours + GT 2D + GT 3D</cell><cell>40.9</cell><cell>35.3</cell><cell>41.6</cell><cell>44.3</cell><cell>36.6</cell><cell>43.7</cell><cell>38.0</cell><cell>40.3</cell><cell>53.4</cell></row><row><cell>Yasin [62] + GT 2D</cell><cell>60.0</cell><cell>54.7</cell><cell>71.6</cell><cell>67.5</cell><cell>63.8</cell><cell>61.9</cell><cell>55.7</cell><cell>73.9</cell><cell>110.8</cell></row><row><cell>Chen &amp; Ramanan [13] + GT 2D</cell><cell>53.3</cell><cell>46.8</cell><cell>58.6</cell><cell>61.2</cell><cell>56.0</cell><cell>58.1</cell><cell>48.9</cell><cell>55.6</cell><cell>73.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(MoCap from CMU dataset)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours + GT 2D</cell><cell>67.8</cell><cell>58.7</cell><cell>90.3</cell><cell>72.1</cell><cell>78.2</cell><cell>75.7</cell><cell>71.9</cell><cell>103.2</cell><cell>132.8</cell></row><row><cell>Method</cell><cell>Smoke</cell><cell>Photo</cell><cell>Wait</cell><cell>Walk</cell><cell>WalkDog</cell><cell>WalkPair</cell><cell>Mean</cell><cell>Median</cell><cell>-</cell></row><row><cell>Ours</cell><cell>73.0</cell><cell>88.5</cell><cell>67.7</cell><cell>52.1</cell><cell>73.0</cell><cell>54.1</cell><cell>66.9</cell><cell>61.5</cell><cell>-</cell></row><row><cell>Ours + GT 2D</cell><cell>58.8</cell><cell>79.1</cell><cell>58.9</cell><cell>35.6</cell><cell>63.4</cell><cell>46.3</cell><cell>56.1</cell><cell>51.9</cell><cell>-</cell></row><row><cell>Ours + GT 2D + GT 3D</cell><cell>44.2</cell><cell>56.6</cell><cell>45.9</cell><cell>26.9</cell><cell>45.8</cell><cell>31.4</cell><cell>41.6</cell><cell>39.1</cell><cell>-</cell></row><row><cell>Yasin [62] + GT 2D</cell><cell>78.9</cell><cell>96.9</cell><cell>67.9</cell><cell>47.5</cell><cell>89.3</cell><cell>53.4</cell><cell>70.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Chen &amp; Ramanan [13] + GT 2D</cell><cell>60.3</cell><cell>76.1</cell><cell>62.2</cell><cell>35.8</cell><cell>61.9</cell><cell>51.1</cell><cell>57.5</cell><cell>51.9</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">(MoCap from CMU dataset)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours + GT 2D</cell><cell>91.3</cell><cell>91.6</cell><cell>84.7</cell><cell>70.9</cell><cell>81.2</cell><cell>76.7</cell><cell>83.7</cell><cell>75.6</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 :</head><label>3</label><figDesc>Impact of 2D pose estimation. GT 2D denotes that the ground-truth 2D pose is used. GT 3D denotes that the 3D poses of the test images are added to the MoCap dataset as in Tab. 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc>Comparison with the state-of-the-art on the Human3.6M dataset using Protocol-II.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Some qualitative results from the MPII Human Pose Dataset.</figDesc><table><row><cell>LinKDE [23]* Li et al. [29]* Tekin et al. [54]* Tekin et al. [52]* Tekin et al. [53]* Zhou et al. [66]* Zhou et al. [64] Du et al. [17]* Ours* Sanzari et al. [45] Tome et al. [55] Moreno-Noguer [32] Chen &amp; Ramanan [13] Ours Ours LinKDE [23]* Li et al. [29]* Tekin et al. [54]* Tekin et al. [52]* Tekin et al. [53]* Zhou et al. [66]* Zhou et al. [64] Du et al. [17]* Ours* Sanzari et al. [45] Tome et al. [55] Moreno-Noguer [32] Chen &amp; Ramanan [13] Ours Ours</cell><cell>1 Discussion 183.6 136.9 158.5 129.1 108.8 109.3 102.4 112.7 107.4 56.3 73.5 80.2 97.6 98.4 148.0 Smoking 162.1 -118.2 -85.1 107.4 106.9 120.0 111.4 97.8 85.0 89.7 106.7 112.4 Fig. 8: Directions 132.7 -102.4 -85.0 87.4 91.8 85.1 97.0 48.8 65.0 69.5 89.9 90.9 139.4 SitDown 243.0 -205.7 -170.4 199.2 159.0 226.9 155.4 129.6 173.9 113.9 240.1 150.1 186.7 154.8</cell><cell>View-2 Eating 132.4 96.9 88.0 91.4 84.4 87.1 97.0 104.9 97.2 96.0 76.8 78.2 90.0 98.2 (MoCap from CMU dataset) 2D Pose Greeting Phoning 164.4 162.1 124.7 -126.8 118.4 121.7 -98.9 119.4 103.2 116.2 98.8 113.4 122.1 139.1 128.9 126.2 84.8 96.5 86.4 86.3 87.0 100.8 107.9 107.3 118.3 118.0 148.3 165.2 161.7 Waiting WalkDog Walk 170.7 177.1 96.6 -132.2 70.0 146.7 128.1 65.9 -130.5 65.8 116.9 113.7 62.1 118.1 114.2 79.4 94.4 126.1 79.0 117.7 137.4 99.3 128.4 115.9 84.2 65.9 130.5 92.6 85.8 86.3 71.4 98.5 82.4 79.2 106.2 114.1 87.0 113.5 109.2 89.1 (MoCap from CMU dataset) 154.4 163.7 140.9</cell><cell>View-1 Posing 150.6 -114.7 -98.5 106.9 90.0 105.9 102.5 66.3 68.9 76.0 93.6 95.9 138.6 WalkTogether Mean Photo 205.9 168.7 185.0 162.2 95.7 143.3 125.2 135.9 137.0 105.6 110.7 102.7 139.2 130.5 170.1 127.9 162.1 --77.2 125.3 --94.8 100.1 97.7 113.0 99.0 107.3 106.5 126.5 90.8 117.1 102.2 93.2 73.1 88.4 77.2 87.3 90.6 114.2 88.4 111.8 160.3 157.3</cell><cell>View-2 Purchases 171.3 -107.6 -93.8 99.8 93.9 166.2 116.5 107.4 74.8 69.7 136.1 112.1 168.2 Media --------98.5 ---93.1 95.3 141.7</cell><cell>Sit 151.6 -136.2 -73.8 124.5 132.2 117.5 149.3 116.9 110.2 104.7 133.1 146.1 168.5 ----------------</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 5 :</head><label>5</label><figDesc>Comparison with the state-of-the-art on the Human3.6M dataset using Protocol-III. (*perform testing and training on the same activity.)</figDesc><table><row><cell>MoCap Data</cell><cell cols="2">Walking (A1, C1) Jogging (A2, C1) Average S1 S2 S3 S1 S2 S3</cell></row><row><cell>HumanEva</cell><cell>27.4 28.6 32.5 39.9 29.4 31.4</cell><cell>31.5</cell></row><row><cell>CMU</cell><cell>68.4 81.6 88.3 70.1 81.6 89.9</cell><cell>80.0</cell></row><row><cell>CMU → HumanEva</cell><cell>39.5 47.3 61.4 53.5 48.3 53.1</cell><cell>50.5</cell></row><row><cell>HumanEva → CMU</cell><cell>45.1 54.9 59.1 58.6 63.1 69.7</cell><cell>58.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 6 :</head><label>6</label><figDesc>Impact of different skeleton structures. The symbol → indicates retargeting of the skeleton structure of one dataset to the skeleton of another dataset.</figDesc><table><row><cell>Methods</cell><cell>S1</cell><cell>Walking (A1, C1) S2</cell><cell>S3</cell><cell>S1</cell><cell>Jogging (A2, C1) S2</cell><cell>S3</cell><cell>Average</cell></row><row><cell>Kostrikov et al. [26]</cell><cell>44.0</cell><cell>30.9</cell><cell>41.7</cell><cell>57.2</cell><cell>35.0</cell><cell>33.3</cell><cell>40.3</cell></row><row><cell>Wang et al. [58]</cell><cell>71.9</cell><cell>75.7</cell><cell>85.3</cell><cell>62.6</cell><cell>77.7</cell><cell>54.4</cell><cell>71.3</cell></row><row><cell>Radwan et al. [39]</cell><cell>75.1</cell><cell>99.8</cell><cell>93.8</cell><cell>79.2</cell><cell>89.8</cell><cell>99.4</cell><cell>89.5</cell></row><row><cell>Simo-Serra et al. [49]</cell><cell>65.1</cell><cell>48.6</cell><cell>73.5</cell><cell>74.2</cell><cell>46.6</cell><cell>32.2</cell><cell>56.7</cell></row><row><cell>Simo-Serra et al. [50]</cell><cell>99.6</cell><cell>108.3</cell><cell>127.4</cell><cell>109.2</cell><cell>93.1</cell><cell>115.8</cell><cell>108.9</cell></row><row><cell>Bo et al. [8]*</cell><cell>38.2</cell><cell>32.8</cell><cell>40.2</cell><cell>42.0</cell><cell>34.7</cell><cell>46.4</cell><cell>39.1</cell></row><row><cell>Yasin et al. [62]</cell><cell>35.8</cell><cell>32.4</cell><cell>41.6</cell><cell>46.6</cell><cell>41.4</cell><cell>35.4</cell><cell>38.9</cell></row><row><cell>Popa et al. [38]</cell><cell>27.1</cell><cell>18.4</cell><cell>39.5</cell><cell>37.6</cell><cell>28.9</cell><cell>27.6</cell><cell>29.9</cell></row><row><cell>Moreno-Noguer [32]</cell><cell>19.7</cell><cell>13.5</cell><cell>26.5</cell><cell>34.6</cell><cell>17.9</cell><cell>20.1</cell><cell>22.0</cell></row><row><cell>Ours</cell><cell>27.4</cell><cell>28.6</cell><cell>32.5</cell><cell>39.9</cell><cell>29.4</cell><cell>31.4</cell><cell>31.5</cell></row><row><cell></cell><cell></cell><cell cols="3">MoCap from CMU dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yasin et al. [62]</cell><cell>52.2</cell><cell>51.0</cell><cell>62.8</cell><cell>74.5</cell><cell>72.4</cell><cell>56.8</cell><cell>61.6</cell></row><row><cell>Ours</cell><cell>39.5</cell><cell>47.3</cell><cell>61.4</cell><cell>53.5</cell><cell>48.3</cell><cell>53.1</cell><cell>50.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 7 :</head><label>7</label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d human pose from silhouettes by relevance vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering 3d human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A datadriven approach for real-time full body pose reconstruction from a depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Twin gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internation Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="52" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast algorithms for large scale conditional 3d prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Synthesizing training images for boosting human 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multicontext attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Carnegie mellon university graphics lab: Motion capture database</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note>mocap.cs.cmu.edu</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Marker-less 3d human motion capture with monocular image sequence and height-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonlinear body pose estimation from depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Iterated Second-Order Label Sensitive Pooling for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pose for action -action for pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3d human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast local and global similarity searches in large motion capture databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tautges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Symposium on Computer Animation</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recovering 3d human body configurations using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1052" to="1062" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DeepCut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep multitask architecture for integrated 2d and 3d human sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monocular image 3d human pose estimation under self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Egocap: egocentric markerless motion capture with two fisheye cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">162</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bayesian image based 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ntouskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internation Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Loose-limbed people: Estimating 3d human pose and motion using non-parametric belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internation Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A Joint Model for 2D and 3D Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Single Image 3D Human Pose Estimation from Noisy Observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alenyà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discriminative density propagation for 3d human motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Fusing 2d uncertainty and 3d cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Márquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05708</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Predicting people&apos;s 3d poses from short sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Suna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wanga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fuaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3d people tracking with gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Coupled action recognition and pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="37" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Model based full body human motion reconstruction from video data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision/Computer Graphics Collaboration Techniques and Applications</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
