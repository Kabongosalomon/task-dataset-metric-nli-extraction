<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016">2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
						</author>
						<title level="a" type="main">Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<biblScope unit="page">1</biblScope>
							<date type="published" when="2016">2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-nonlinear reaction diffusion</term>
					<term>loss specific training</term>
					<term>image denoising</term>
					<term>image super resolution</term>
					<term>JPEG deblocking !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image restoration is a long-standing problem in low-level computer vision with many interesting applications. We describe a flexible learning framework based on the concept of nonlinear reaction diffusion models for various image restoration problems. By embodying recent improvements in nonlinear diffusion models, we propose a dynamic nonlinear reaction diffusion model with time-dependent parameters (i.e., linear filters and influence functions). In contrast to previous nonlinear diffusion models, all the parameters, including the filters and the influence functions, are simultaneously learned from training data through a loss based approach. We call this approach TNRD -Trainable Nonlinear Reaction Diffusion. The TNRD approach is applicable for a variety of image restoration tasks by incorporating appropriate reaction force. We demonstrate its capabilities with three representative applications, Gaussian image denoising, single image super resolution and JPEG deblocking. Experiments show that our trained nonlinear diffusion models largely benefit from the training of the parameters and finally lead to the best reported performance on common test datasets for the tested applications. Our trained models preserve the structural simplicity of diffusion models and take only a small number of diffusion steps, thus are highly efficient. Moreover, they are also well-suited for parallel computation on GPUs, which makes the inference procedure extremely fast.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I MAGE restoration is the process of estimating uncorrupted images from noisy or blurred ones. It is one of the most fundamental operations in image processing, video processing, and low-level computer vision. For several decades, image restoration remains an active research topic and hence new approaches are constantly emerging. There exists a huge amount of literature addressing the topic of image restoration problems, see for example <ref type="bibr" target="#b40">[41]</ref> for a survey.</p><p>In recent years, the predominant approaches for image restoration are non-local methods based on patch modeling, for example, image denoising with (i) Gaussian noise <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b45">[46]</ref>, (ii) multiplicative noise <ref type="bibr" target="#b13">[14]</ref>, or (iii) Poisson noise <ref type="bibr" target="#b23">[24]</ref>, image interpolation <ref type="bibr" target="#b46">[47]</ref>, image deconvolution <ref type="bibr" target="#b19">[20]</ref>, etc. Most state-ofthe-art techniques mainly concentrate on achieving utmost image restoration quality, with little consideration on the computational efficiency, e.g., <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b46">[47]</ref>, despite the fact that it is a critical factor for real applications. However, there are a few exceptions. For example, there are two notable exceptions for the task of Gaussian denoising, BM3D <ref type="bibr" target="#b14">[15]</ref> and the recently proposed Cascade of Shrinkage Fields (CSF) <ref type="bibr" target="#b51">[52]</ref> model, which simultaneously offer high efficiency and high image restoration quality.</p><p>It is well-known that BM3D is a highly engineered Gaussian image denoising algorithm. It involves a block matching process, which is challenging for parallel computation on GPUs, alluding to the fact that it is not straightforward to accelerate BM3D algorithm on parallel architectures. In contrast, the recently proposed CSF model offers high levels of parallelism, making it well suited for GPU implementation, thus owning high computational efficiency.</p><p>In this paper, we propose a flexible learning framework to generate fast and effective models for a variety of image restoration problems. Our approach is based on learning optimal nonlinear reaction diffusion models. The learned models preserve the structural simplicity of these models and hence it is straightforward to implement the corresponding algorithms on massive parallel hardware such as GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Nonlinear diffusion for image restoration</head><p>Partial differential equation (PDEs) have become a standard approach for various problems in image processing. On the one hand they come along with a sound mathematical framework that allow to make clear statements about the existence and regularity of the solutions. On the other hand, efficient numerical algorithms have been developed, that allow to compute the solution of PDEs in very short time <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b54">[55]</ref> . Although recent PDE approaches have shown good performance for a number of image processing task, they still fail to produce state-of-the-art quality for classical image restoration tasks.</p><p>In the seminal work <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr">Perona and Malik (PM)</ref> proposed a nonlinear diffusion model, which is given as the following PDE ∂u ∂t = div(g(|∇u|)∇u) u t=0 = f , <ref type="bibr" target="#b0">(1)</ref> arXiv:1508.02848v2 [cs.CV] 20 Aug 2016</p><p>where ∇ is the gradient operator, t denotes the time, f is a initial image. The function g is known as edge-stopping function <ref type="bibr" target="#b6">[7]</ref> or diffusivity function <ref type="bibr" target="#b54">[55]</ref>, and a typical g-function is given by g(z) = 1/(1 + z 2 ). The proposed PM diffusion model <ref type="bibr" target="#b0">(1)</ref> leads to a nonlinear anisotropic 1 diffusion model which is able to preserve and enhance image edges. Hence, it is well suited for a number of image processing tasks such as image denoising and segmentation.</p><p>1.1.1 Improvements from the side of PDEs A first variant of the PM model is the so-called biased anisotropic diffusion (also known as reaction diffusion) proposed by Nordström <ref type="bibr" target="#b42">[43]</ref>, which introduces a bias term (forcing term) to free the user from the difficulty of specifying an appropriate stopping time for the PM diffusion process. This additional term reacts against the strict smoothing effect of the pure PM diffusion, therefore resulting in a nontrivial steady-state. Subsequent works consider modifications of the diffusion or the reaction term for the reaction diffusion model <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b0">[1]</ref>, e.g., Acton et al. <ref type="bibr" target="#b0">[1]</ref> exploited a more complicated reaction term to enhance oriented textures; <ref type="bibr" target="#b3">[4]</ref> proposed to replace the ordinary diffusion term with a flow equation based on mean curvature. A notable work is the forward-backward diffusion model proposed by Gilboa et al. <ref type="bibr" target="#b22">[23]</ref>, which incorporates explicit inverse diffusion with negative diffusivity coefficient by carefully choosing the diffusivity function. The resulting diffusion processes can adaptively switch between forward and backward diffusion processes. In subsequent work <ref type="bibr" target="#b55">[56]</ref>, the theoretical framework for discrete forward-and-backward diffusion filtering has been investigated. Researchers also propose to exploit higher-order nonlinear diffusion filtering, which involves larger linear filters, e.g., fourthorder diffusion models <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Meanwhile, theoretical properties about the stability and local feature enhancement of higher-order nonlinear diffusion filtering are established in <ref type="bibr" target="#b15">[16]</ref>.</p><p>It should be noted that the above mentioned diffusion models are handcrafted, including elaborate selections of the diffusivity functions, optimal stopping times and proper reaction forces. It is a generally difficult task to design a good-performing PDE for a specific image processing problem, as good insights into this problem and a deep understanding of the behavior of the PDEs are usually required. Therefore, an attempt to learn PDEs from training data via an optimal control approach was made in <ref type="bibr" target="#b38">[39]</ref>, where the PDEs to be trained have the form of</p><formula xml:id="formula_0">∂u ∂t = κ(u) + a(t) O(u) u t=0 = f .<label>(2)</label></formula><p>Coefficients a(t) are free parameters (i.e., combination weights) to train. κ(u) is related to the TV regularization <ref type="bibr" target="#b48">[49]</ref> and O(u) denotes a set of operators (invariants) over u, e.g., ∇u 2 2 = u 2</p><p>x + u 2 y .</p><p>1.1.2 Improvements from the side of image statistics/regularization As shown in <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b49">[50]</ref>, there exist a strong connection between anisotropic diffusion models and variational models adopting image priors derived from the statistics of natural images. Let us consider the discrete version of the PM model <ref type="formula">(1)</ref>, where images are represented as column vectors, i.e., u ∈ R N . The discrete PM model is formulated as the following discrete PDE with an explicit finite difference scheme</p><formula xml:id="formula_1">u t+1 − u t ∆t = − i={x,y} ∇ i Λ(u t )∇ i u t . = − i={x,y} ∇ i φ(∇ i u t ) ,<label>(3)</label></formula><p>where matrices ∇ x and ∇ y ∈ R N ×N are finite difference approximation of the gradient operators in x-direction and y-direction, respectively and ∆t denotes the time step. Λ(u t ) ∈ R N ×N is defined as a diagonal matrix</p><formula xml:id="formula_2">Λ(u t ) = diag g (∇ x u t ) 2 p + (∇ y u t ) 2 p p=1,··· ,N ,</formula><p>where function g is the edge-stopping function mentioned before. If ignoring the coupled relation between ∇ x u and ∇ y u, the PM model can also be written in the form φ(</p><formula xml:id="formula_3">∇ i u) = (φ(∇ i u) 1 , · · · , φ(∇ i u) N ) ∈ R N with function φ(z) = zg(z),</formula><p>known as influence function <ref type="bibr" target="#b6">[7]</ref> or flux function <ref type="bibr" target="#b54">[55]</ref>. In this paper, we stick to this discrete and decoupled formulation, as it is the starting point of our approach. As shown in previous works, e.g., <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b58">[59]</ref>, the diffusion step (3) corresponds to a gradient descent step to minimize the energy functional given as</p><formula xml:id="formula_4">R(u) = i∈{x,y} N p=1 ρ((k i * u) p ) ,<label>(4)</label></formula><p>the functions ρ (e.g., ρ(z) = log(1 + z 2 )) is the so-called penalty function. <ref type="bibr" target="#b1">2</ref> It is worthwhile to mention that the matrixvector product ∇ x u can be interpreted as a 2D convolution of u with the linear filter k x = [−1, 1] (∇ y corresponds to the linear filter k y = [−1, 1] ). The energy functional (4) can be also understood from the aspects of image statistics, image prior and image regularization. As a consequence, a lot of efforts listed below have been made to improve the capability of model (4). a) More filters of larger kernel size were considered in <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b2">[3]</ref>, instead of relatively small kernel size, such as usually used pair-wise kernels. The resulting regularization model leads to the so-called fields of experts (FoE) <ref type="bibr" target="#b47">[48]</ref> image prior, which works well for many image restoration problems. b) Instead of hand-crafted ones with fixed shape, more flexible penalty functions were exploited, and they were learned from data <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Especially, as shown in <ref type="bibr" target="#b58">[59]</ref> that those unusual penalties such as inverted penalties (i.e., ρ(z) decreasing as a function of |z|) were found to be necessary. c) In order accelerate the inference phase related to the model (4), in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>, it was proposed to truncate the gradient descent procedure to fixed iterations/stages, and then train this truncated optimization algorithm based on the FoE prior model. d) In order to further increase the flexibility of multi-stage models, Schmidt and Roth <ref type="bibr" target="#b51">[52]</ref> considered varying parameters per stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Motivations and Contributions</head><p>In this paper we concentrate on nonlinear diffusion process due to its high efficiency. Taking into consideration the improvements mentioned in Sec. 1.1.2, we propose a trainable nonlinear diffusion 2. Note that ρ (z) = φ(z).</p><p>model with (1) fixed iterations (also referred to as stages), (2) more filters of larger kernel size, (3) flexible penalties in arbitrary shapes, (4) varying parameters for each iteration, i.e., time varying linear filters and penalties. Then all the parameters (i.e., linear filters and penalties) in the proposed model are simultaneously trained from training data in a supervised way. The proposed approach results in a novel learning framework to train effective image diffusion models. It turns out that the trained diffusion processes leads to state-of-the-art performance, while preserve the property of high efficiency of diffusion based approaches. In summary, our proposed nonlinear diffusion process offers the following advantages: 1) It is conceptually simple as it is merely a standard nonlinear diffusion model with trained filters and influence functions; 2) It has broad applicability to a variety of image restoration problems. In principle, all the diffusion based models can be revisited with appropriate training; 3) It yields excellent results for several tasks in image restoration, including Gaussian image denoising,single image super resolution and JPEG deblocking; 4) It is highly computationally efficient, and well suited for parallel computation on GPUs.</p><p>A shorter paper has been presented as a conference version <ref type="bibr" target="#b11">[12]</ref>. In this paper, we incorporate additional contents listed as follows 1) We investigate more details of the training phase, such as the influence of (a) initialization, (b) the model capacity and (c) the number of training samples; 2) We consider more detailed analysis of trained models, such as how the trained models generate the patterns; 3) We exploit an additional application of single image super resolution to further illustrate the potential breadth of our proposed learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROPOSED REACTION DIFFUSION PROCESS</head><p>In this section, we first describe our learning based reaction diffusion model for image restoration, and then we show the relations between the proposed model and existing image restoration models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Proposed nonlinear diffusion model</head><p>The fundamental idea of our proposed learning based reaction diffusion model is described in Sec. 1.2. In addition, we incorporate a reaction term in order to apply our model for different image processing problems, as shown later. As a consequence, our proposed nonlinear reaction diffusion model is formulated as</p><formula xml:id="formula_5">u t − u t−1 ∆t = − N k i=1 K t i φ t i (K t i u t−1 ) diffusion term − ψ t (u t−1 , f ) reaction term ,<label>(5)</label></formula><p>where K i ∈ R N ×N is a highly sparse matrix, implemented as 2D convolution of the image u with the filter kernel k i , i.e., K i u ⇔ k i * u, K i is a set of linear filters and N k is the number of filters.</p><p>In practice, we set ∆t = 1, as we can freely scale the functions φ t i and ψ t on the right hand side. Note that our proposed diffusion process is truncated after a few stages, usually less than 10. Moreover, the linear filters and influence functions are adjustable and vary across stages. As our proposed approach is inspired by nonlinear reaction diffusion model but with trainable filters and influence functions, we coin our method Trainable Nonlinear Reaction Diffusion (TNRD).</p><p>In practice, we train the proposed nonlinear diffusion model <ref type="bibr" target="#b4">(5)</ref> for specific image restoration problem by exploiting application specific reaction terms ψ(u). For classical image restoration problems, such as Gaussian denoising, image deblurring, image super resolution and image inpainting, we can set the reaction term to be the gradient of a data term, i.e. ψ(u) = ∇ u D(u).</p><p>For</p><formula xml:id="formula_6">example, if we choose D t (u, f ) = λ t 2 Au − f 2 2 , we have ψ t (u) = λ t A (Au − f ), where f is the degraded input image,</formula><p>A is the associated linear operator, and λ t is related to the strength of the reaction term. In the case of Gaussian denoising, A is the identity matrix; for image super resolution, A is related to the down sampling operation and for image deconvolution, A corresponds to the linear blur kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A more general formulation of the proposed diffusion model</head><p>Note that the data term D(u) related to (5) should be differentiable. In order to handle the problems involving a nondifferentiable data term, e.g., the JPEG deblocking problem investigated in Section 6, we consider a more general form of our proposed diffusion model as follows</p><formula xml:id="formula_7">ut = Prox G t ut−1 − N k i=1 (K t i ) φ t i (K t i ut−1) + ψ t (ut−1, f ) ,<label>(6)</label></formula><p>where Prox G t (û) is the proximal mapping operation <ref type="bibr" target="#b43">[44]</ref> related to the non-differentiable function G t , given as</p><formula xml:id="formula_8">Prox G t (û) = min u u −û 2 2 2 + G t (u) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Related image restoration models</head><p>As mentioned in Sec. 1.1.2, there exist natural connection between anisotropic diffusion and image regularization based energy functional. Therefore, Eq. (6) can be interpreted as a forward-backward step 3 <ref type="bibr" target="#b36">[37]</ref> at u t−1 for the energy functional given by</p><formula xml:id="formula_9">E t (u, f ) = N k i=1 R t i (u) + D t (u, f ) + G t (u, f ) ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_10">R t i (u) = N p=1 ρ t i ((K t i u) p )</formula><p>are the regularizers. Since the parameters {K t i , ρ t i } vary across the stages, (7) is a dynamic energy functional, which changes at each iteration.</p><p>If {K t i , ρ t i } keep the same across stages, the functional (7) with G = 0 corresponds to the FoE prior regularized variational model for image restoration <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref>. In our work, we do not exactly solve this minimization problem anymore. In contrast, we run the gradient descent step for several iterations, and each gradient descent step is optimized by training. More importantly, we are allowed to investigate more generalized penalties.</p><p>To the best of our knowledge, Zhu and Mumford <ref type="bibr" target="#b58">[59]</ref> were the first to consider learning reaction diffusion models from data. The linear filters appearing in the prior were chosen (not fully trained) from a general filter bank by minimizing the entropy of probability distributions of natural images. The associated penalty functions were learned based on the maximum entropy principle. <ref type="bibr" target="#b2">3</ref>. The forward step is a gradient descent step w.r.t the function D and the backward step is the proximal operation w.r.t the function G.  However, our proposed diffusion model is a multi-stage diffusion process with multiple image priors, where the filters and penalties are fully trained from clean/degraded pairs. Some more works have also been dedicated to train the penalties in a diffusion model. In <ref type="bibr" target="#b33">[34]</ref>, the authors trained the penalty functions in the way that they first computed the image statistics appropriate to certain chosen filters (e.g., horizontal and vertical image derivatives), and then considered mixture models of fixed shape having a peak near zero and two heavy tails in both sides to fit to image statistics. Analogously, in <ref type="bibr" target="#b49">[50]</ref>, potential functions of fixed shape are chosen to resemble zero mean filter responses.</p><p>In very recent work <ref type="bibr" target="#b51">[52]</ref>, Schmidt and Roth exploited an additive form of half-quadratic optimization to solve problem <ref type="bibr" target="#b6">(7)</ref>. The resulting model shares similarities with classical wavelet shrinkage and hence it is termed "cascade of shrinkage fields" (CSF). The CSF model relies on the assumption that the data term in <ref type="bibr" target="#b6">(7)</ref> is quadratic and the operator A can be interpreted as a convolution operation, such that the corresponding subproblem can be solved in closed-form using the discrete Fourier transform (DFT). However, our proposed diffusion model does not have this restriction on the data term. In principle, any smooth data term is appropriate. Moreover, as shown in the following sections, we can even handle the case of non-smooth data terms.</p><p>As already mentioned, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref> also proposed to train an optimized gradient descent algorithm for the energy functional similar to <ref type="bibr" target="#b6">(7)</ref>. However, their model is much more constrained, since they exploited the same filters for each gradient descent step and more importantly, they make use of a hand-selected influence function. This clearly restricts the model capability, as demonstrated in Sec. 4.</p><p>Comparing our model to the model of <ref type="bibr" target="#b38">[39]</ref> (cf. <ref type="formula" target="#formula_0">(2)</ref>), one can see that this approach learns only a linear model based on predefined image invariants. Therefore, this model can be interpreted as a simplified version of our model (5) with fixed linear filters and influence functions, and only the weight of each term is optimized.</p><p>The proposed diffusion model also bears an interesting link to convolutional networks (CNs) applied to image restoration problems in <ref type="bibr" target="#b31">[32]</ref>. One can see that each iteration (stage) of our proposed diffusion process involves convolution operations with a set of linear filters, and thus it can be treated as a convolutional network. The architecture of our proposed diffusion model is shown in <ref type="figure" target="#fig_1">Figure 1</ref>, where it is represented as a common feedforward network. We refer to this network in the following as diffusion network. Our diffusion network can also be interpreted as a CN with a feedback step, which makes it different from conventional feed-forward networks. Due to the feedback step, it can be categorized into recurrent networks <ref type="bibr" target="#b24">[25]</ref>.</p><p>However, we can introduce a feedback step to explicitly illustrate the special architecture of our diffusion network that we subtract "something" from the input image. Therefore, our diffusion model can be represented in a more compact way in <ref type="figure">Figure 2</ref>, where one can see that the structure of our CN model is different from conventional feed-forward networks. Due to this feedback step, it can be categorized into recurrent networks <ref type="bibr" target="#b24">[25]</ref>. It should be noted that the nonlinearity (i.e., influence functions in the context of nonlinear diffusion) in our proposed network are trainable. However, conventional CNs make use of fixed activation function, e.g., the ReLU function <ref type="bibr" target="#b41">[42]</ref> or sigmoid functions <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING FRAMEWORK</head><p>We train our diffusion networks in a supervised manner, namely we firstly prepare the input/output pairs for a certain image processing task, and then exploit a loss minimization scheme to learn the model parameters Θ t for each stage t of the diffusion process. The training dataset consists of S training samples {f s , u s gt } S s=1 , where f s is a noisy observation and u s gt is the corresponding ground truth clean image. The model parameters Θ t of each stage include the parameters of (1) the reaction force weight λ, (2) linear filters and (3) influence functions, i.e., Θ t = {λ t , φ t i , k t i }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall training model</head><p>In the supervised manner, a training cost function is required to measure the difference between the output of the diffusion network and the ground-truth image. As our goal is to train a diffusion network with T stages, the cost function is formulated as</p><formula xml:id="formula_11">L(Θ 1,··· ,T ) = S s=1 (u s T , u s gt ) ,<label>(8)</label></formula><p>where u T is the output of the final stage T . In our work we exploit the usual quadratic loss function <ref type="bibr" target="#b3">4</ref> , defined as</p><formula xml:id="formula_12">(u s T , u s gt ) = 1 2 u s T − u s gt 2 2 .<label>(9)</label></formula><p>As a consequence, the training task is generally formulated as the following optimization problem</p><formula xml:id="formula_13">                 min Θ L(Θ) = S s=1 1 2 u s T − u s gt 2 2 s.t.          u s 0 = I s 0 u s t = Prox G t u s t−1 − N k i=1 (K t i ) φ t i (K t i u s t−1 ) + ψ t (u s t−1 , f s ) , t = 1 · · · T ,<label>(10)</label></formula><p>where Θ = {Θ t } t=T t=1 and I 0 is the initial status of the diffusion process. Note that the above loss function only depends on the output of the final stage T , i.e., the parameters in all stages are simultaneously trained such that the output of the diffusion process -u T is optimized. We call this training scheme joint training similar to <ref type="bibr" target="#b51">[52]</ref>. The joint training strategy is a minimization problem with respect to the parameters in all stages</p><formula xml:id="formula_14">{Θ 1 , Θ 2 , · · · , Θ T }.</formula><p>One can see that our training model is also a deep model with many stages (layers). It is well-known that deep models are usually sensitive to initialization, and therefore training from scratch is prone to getting stuck at bad local minima. As a consequence, people usually consider a greedy layer-wise pre-training <ref type="bibr" target="#b4">[5]</ref> to provide a good initialization for the joint training (fine tune).</p><p>In our work, we also consider a greedy training scheme similar to <ref type="bibr" target="#b51">[52]</ref>, to pre-train our diffusion network stage-by-stage, where each stage is greedily trained such that the output of each stage is optimized, i.e., for stage t, we minimize the cost function</p><formula xml:id="formula_15">L(Θ t ) = S s=1 (u s t , u s gt ) ,<label>(11)</label></formula><p>where u s t is the output of stage t of the diffusion process. Note that this is a minimization problem only with respect to the parameters Θ t in stage t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameterizing the influence functions φ t</head><p>i , linear filters k t i and weights λ t In this paper, we aim to investigate arbitrary influence functions. In order to conduct a fast and accurate training, an effective function parameterization method is required. Following the work of <ref type="bibr" target="#b51">[52]</ref>, we parameterize the influence function via standard radial basis functions (RBFs), i.e., each function φ is represented as a weighted linear combination of a family of RBFs as follows</p><formula xml:id="formula_16">φ t i (z) = M j=1 w t ij ϕ |z − µ j | γ j ,<label>(12)</label></formula><p>4. This loss function is related to the PSNR quality measure. Note that as shown in <ref type="bibr" target="#b32">[33]</ref>, other quality measures, such as structural similarity (SSIM) and mean absolute error (MAE) can be chosen to define the loss function. At present we only consider the quadratic loss function due to its simplicity.</p><p>where ϕ represents different RBFs. In this paper, we exploit RBFs with equidistant centers µ j and unified scaling γ j . We investigate two typical RBFs <ref type="bibr" target="#b30">[31]</ref>: (1) Gaussian radial basis ϕ g and (2) triangular-shaped radial basis ϕ t , given as</p><formula xml:id="formula_17">ϕ g (z) = ϕ |z − µ| γ = exp − (z − µ) 2 2γ 2 and ϕ t (z) = ϕ |z − µ| γ = 1 − |z−µ| γ |z − µ| ≤ γ 0 |z − µ| &gt; γ</formula><p>respectively. The basis functions are shown in <ref type="figure" target="#fig_3">Figure 3</ref>, together with an example of the function approximation by using two different RBF methods. In our work, we have investigated both function approximation methods, and we find that they lead to similar results. We only present the results obtained by the Gaussian RBF in this paper. In our work, the linear kernels k t i related to the linear operators K t i are defined as a linear combination of Discrete Cosine Transform (DCT) basis kernels b r , i.e.,</p><formula xml:id="formula_18">k t i = r ω t i,r b r ω t i 2 ,</formula><p>where the kernels k t i are normalized to get rid of an ambiguity appearing in the proposed diffusion model. More details can be found in the supplemental material. The kernels are formed in this way in order to keep the expected property of zero-mean.</p><p>The weights λ t in our model are constrained to be positive. To this end, we set λ t ← e λ t in the training phase for our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computing gradients</head><p>For both greedy training and joint training, we make use of gradient-based algorithms (e.g., the L-BFGS algorithm <ref type="bibr" target="#b37">[38]</ref>) for optimization. The key point is to compute the gradients of the loss function with respect to the training parameters. In greedy training, the gradient of the loss function at stage t with respect to the model parameters Θ t is computed using standard chain rule, given as</p><formula xml:id="formula_19">∂ (u t , u gt ) ∂Θ t = ∂u t ∂Θ t · ∂ (u t , u gt ) ∂u t ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_20">∂ (ut,ugt) ∂ut = u t − u gt is directly derived from (9), ∂ut ∂Θt</formula><p>is computed from the diffusion process for specific task. For the applications exploited in this paper, such as image denoising with Gaussian noise, single image super resolution and JPEG deblocking, we present the detailed derivations of ∂ut ∂Θt in the supplemental material.</p><p>In the joint training, we compute the gradients of the loss function with respect to Θ t by using the standard back-propagation technique widely used in the neural networks learning <ref type="bibr" target="#b34">[35]</ref>, namely, ∂ut ∂Θt is computed by using</p><formula xml:id="formula_21">∂ (u T , u gt ) ∂Θ t = ∂u t ∂Θ t · ∂u t+1 ∂u t · · · ∂ (u T , u gt ) ∂u T .</formula><p>Compared to the greedy training, we additionally need to calculate ∂ut+1 ∂ut . For the investigated image processing problems in this paper, we provide all necessary derivations in the supplemental material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental setup and implementation details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Boundary condition of the convolution operations</head><p>In our convolution based diffusion network, the image size stays the same when an image goes through the network, and we use the symmetric boundary condition for convolution calculation. In our original diffusion model <ref type="formula" target="#formula_7">(6)</ref>, there is matrix transpose K v, which exactly corresponds to the convolution operationk * v (k is obtained by rotating the kernel k 180 degrees) in the cases of periodic and zero-padding boundary conditions. It should be noted that in the case of symmetric boundary condition used in this paper, this result holds only in the central image region. However, we still want to explicitly use the formulationk * v to replace K v, because the former can significantly simplify the derivation of the gradients required for training. We find that the direct replacement introduces some artifacts at the image boundary. In order to avoid these artifacts, we symmetrically pad the input image before it is sent to the diffusion network, and then we discard those padding pixels in the final output. More details are found in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">RBF kernels</head><p>Images exploited in this paper have the dynamic range in [0, 255], and the filters have unit norm. In order to cover most of the filter response, we consider influence functions in the range [−310, 310]. We use 63 Gaussian RBFs with equidistant centers at [−310 : 10 : 310], and set the scaling parameter γ = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Experimental setup</head><p>Model capacity: In our work, we train the proposed diffusion network with at most 8 stages to observe its saturation behavior after certain stages. We first greedily train T stages of our model with specific model capacity, then conduct a joint training to refine the parameters of the whole T stages.</p><p>In this paper, we mainly consider four different diffusion networks with increasing capacity: Training and test dataset: In order to make a fair comparison to previous works, we make use of the same training datasets used in previous works for our training, and then evaluate the trained models on commonly used test datasets. For image processing problems investigated in this paper, i.e., Gaussian denoising, single image super resolution and JPEG deblocking, we consider the following training and test datasets, respectively. a) Gaussian denoising. Following <ref type="bibr" target="#b51">[52]</ref>, we use the same 400 training images, and crop a 180 × 180 region from each image, resulting in a total of 400 training samples of size 180 × 180, i.e., roughly 13 million pixels. We then evaluate the denoising performance of a trained model on a standard test dataset of 68 natural images, which is suggested by <ref type="bibr" target="#b47">[48]</ref>, and later widely used for Gaussian denoising testing.  <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b18">[19]</ref> accomplish their comparison based on this framework. Therefore, we also use the same 91 training images. We crop 4-5 sub-images of size 150 × 150 from each training image, according to its size, and this finally gives us 421 training samples. We then evaluate the performance of the trained models on the Set5 and Set14 dataset. c) JPEG deblocking. We train the diffusion models using the same training images as in the case of Gaussian denoising. In the test phase, we follow the test procedure in <ref type="bibr" target="#b32">[33]</ref> for performance evaluation. The test images are converted to gray-value, and scaled by a factor of 0.5, resulting 200 images of size 240×160.</p><formula xml:id="formula_22">TNRD T 3×3 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Approximate training time</head><p>Note that the calculation of the gradients of the loss function in (13) is very efficient even using a simple Matlab implementation, since it mainly involves 2D convolutions. The training time varies greatly for different configurations. Important factors include (1) model capacity, (2) number of training samples, (3) number of iterations taken by the L-BFGS algorithm, and (4) number of Gaussian RBF kernels used for function approximation. We report the most time consuming cases as follows.</p><p>In training, computing the gradients ∂L ∂Θ with respect to the parameters of one stage for 400 images of size 180 × 180 takes about 35s (TNRD 5×5 ), 75s (TNRD 7×7 ) or 165s (TNRD 9×9 ) using Matlab implementation on a server with CPUs: Intel(R) Xeon E5-2680 @ 2.80GHz (eight parallel threads using parfor in Matlab, 63 Gaussian RBF kernels for the influence function parameterization). We typically run 200 L-BFGS iterations for optimization. Therefore, the total training time, e.g., for the TNRD 5 7×7 model is about 5 × (200 × 75)/3600 = 20.8h. Code for learning and inference is available on the authors' homepage www.GPU4Vision.org <ref type="bibr" target="#b4">5</ref> . For the training of the Gaussian denoising task, we have also accomplished a GPU implementation, which is about 4-5 times faster than our CPU implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRAINING FOR GAUSSIAN DENOISING</head><p>For the task of Gaussian denoising, we consider the following energy functional</p><formula xml:id="formula_23">min u E(u) = N k i=1 ρ i (k i * u) + λ 2 u − f 2 2 .</formula><p>By setting D(u) = λ 2 u − f 2 2 and G(u) = 0, we arrive at the following diffusion process with u 0 = f</p><formula xml:id="formula_24">u t = u t−1 − N k i=1k t i * φ t i (k t i * u t−1 ) + λ t (u t−1 − f ) ,<label>(15)</label></formula><p>where we explicitly use a convolution kernelk i (obtained by rotating the kernel k i 180 degrees) to replace the K i for the sake of model simplicity, but we have to pad the input image. The gradients ∂ut ∂Θt and ∂ut ∂ut−1 required in training are computed from this equation. Detailed derivations are presented in the supplemental material.</p><p>We started with the training for TNRD T 5×5 . We first considered the greedy training phase to train a diffusion process up to 8 stages (i.e., T ≤ 8), in order to observe the asymptotic behavior of the diffusion network. In the greedy training, only parameters in one stage were trained at a time. We exploited a plain initialization to start the training, namely linear filters and influence functions were initialized from the modified DCT filters and the function φ(z) = 2z/(1 + z 2 ), respectively.</p><p>After the greedy training was completed, we conducted joint training for a diffusion model of certain stages (e.g., T = 2, 5, 8), to simultaneously tune the parameters in all stages. We initialized the joint training using parameters learned in greedy training, as this is guaranteed not to degrade the training performance.</p><p>We first trained our diffusion models for the Gaussian denoising problem with standard deviation σ = 25. The noisy training 5. http://gpu4vision.icg.tugraz.at/binaries/nonlinear-diffusion.zip#pub94 images were generated by adding synthetic Gaussian noise with σ = 25 to the clean images. Once we obtained a trained model, we evaluated its denoising performance on 68 natural images following the same test protocol as in <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b9">[10]</ref>. <ref type="figure" target="#fig_5">Figure 4</ref> shows a denoising example for noise level σ = 25 to illustrate the denoising process of our learned TNRD 5 5×5 diffusion network. We present the final results of the joint training in <ref type="table">Table  1</ref>, together with a selection of recent state-of-the-art denoising algorithms, namely BM3D <ref type="bibr" target="#b14">[15]</ref>, LSSC <ref type="bibr" target="#b39">[40]</ref>, EPLL-GMM <ref type="bibr" target="#b59">[60]</ref>, opt-MRF <ref type="bibr" target="#b9">[10]</ref>, RTF model <ref type="bibr" target="#b32">[33]</ref>, the CSF model <ref type="bibr" target="#b51">[52]</ref> and WNNM <ref type="bibr" target="#b25">[26]</ref>, as well as two similar approaches ARF <ref type="bibr" target="#b2">[3]</ref> and opt-GD <ref type="bibr" target="#b17">[18]</ref>, which also train an optimized gradient descent inference. We downloaded these algorithms from the corresponding author's homepage, and used them as is. Unfortunately, we are not able to present comparisons with <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b31">[32]</ref>, as their codes are not available.</p><p>From <ref type="table">Table 1</ref>, one can see that (1) the performance of the TNRD T 5×5 model saturates after stage 5, i.e., in practice, 5 stages are typically enough; (2) our TNRD 5 5×5 model has achieved significant improvement (28.78 vs.28.60), compared to a similar model CSF 5 5×5 , which has the same model capacity and (3) moreover, our TNRD 8 5×5 model is on par with so far the bestreported algorithm -WNNM. It turns out that our trained models perform surprisingly well for image denoising. Then, a natural question arises: what is the critical factor for the effectiveness of the trained diffusion models?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Understanding the proposed diffusion models</head><p>There are actually two main aspects in our training model: (1) the linear filters and (2) the influence functions. In order to have a better understanding of the trained models, we went through a series of experiments to investigate the impact of these two aspects.</p><p>Concentrating on the model capacity of 24 filters of size 5 × 5, we considered the training of a diffusion process with 10 steps, i.e., T = 10 for the Gaussian denoising of noise level σ = 25. We exploited two main classes of configurations: (A) the parameters of every stage are the same and (B) every diffusion stage is different from each other. In both configurations, we consider two cases: (I) only train the linear filters with fixed influence function φ(z) = 2z/(1 + z 2 ) and (II) simultaneously train the filters and influence functions.</p><p>Based on the same training dataset and test dataset, we obtained the following results: (A.I) every diffusion step is the same, and only the filters are optimized with fixed influence function.  , learned by our proposed method in the TNRD 5 5×5 model. A major finding in this paper is that our learned penalty functions significantly differ from the usual penalty functions adopted in partial differential equations and energy minimization methods. In contrast to their usual robust smoothing properties which is caused by a single minimum around zero, most of our learned functions have multiple minima different from zero and hence are able to enhance certain image structures. See Sec. 4.3 for more information.</p><p>This is a similar configuration to previous works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The trained model achieves a test performance of 28.47dB. (A.II) with additional tuning of the influence functions, the resulting performance is boosted to 28.60dB. (B.I) every diffusion step can be different, but only the linear filters are trained with fixed influence functions. The corresponding model obtains a result of 28.56dB, which is equivalent to the variational model <ref type="bibr" target="#b9">[10]</ref> with the same model capacity. Finally (B.II) with additional optimization of the influence functions, the trained model leads to a significant improvement with the result of 28.86dB.</p><p>The analytical experiments demonstrate that without the training of the influence functions, there is no chance to achieve significant improvements over previous works, no matter how hard we tune the linear filters. Therefore, we believe that the most critical factor of our proposed training model lies in the adjustable influence functions. A closer look at the learned influence functions of the TNRD 5 5×5 model in Sec.4.2 strengthens our argument. Comparing our proposed TNRD model to the CSF model <ref type="bibr" target="#b51">[52]</ref>, one can see that the degree of freedom is in principle the same, since in both models the filters and the non-linear functions can be learned. Therefore, one would expect a similar performance of both models in practice. However, it turns out the performance of the CSF model is inferior to our TNRD model in the case of Gaussian denoising task. The reason for the performance gap is still unclear and we plane to investigate it in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learned influence functions</head><p>A close inspection of the learned 120 penalty functions 6 ρ in the TNRD 5 5×5 model demonstrated that most of the penalties resemble <ref type="bibr" target="#b5">6</ref>. The penalty function ρ(z) is integrated from the influence function φ(z) according to the relation φ(z) = ρ (z) four representative shapes shown in <ref type="figure" target="#fig_7">Figure 5</ref>. At first glance, the learned penalty functions (except (a)) differ significantly from the usually adopted penalty functions used in PDE and energy minimization methods. However, it turns out that they have a clear meaning for image regularization. Regarding the penalty function (b), there are two critical points (indicated by red triangles). When the magnitude of the filter response is relatively small (i.e., less than the critical points), probably it is stimulated by the noise and therefore the penalty function encourages smoothing operation as it has a local minimum at zero. However, once the magnitude of the filter response is large enough (i.e., across the critical points), the corresponding local patch probably contains a real image edge or certain structure. In this case, the penalty function encourages to increase the magnitude of the filter response, alluding to an image sharpening operation. Therefore, the diffusion process controlled by the influence function (b), can adaptively switch between image smoothing (forward diffusion) and sharpening (backward diffusion). We find that the learned influence function (b) is closely similar to an elaborately designed function in a previous work <ref type="bibr" target="#b22">[23]</ref>, which leads to an adaptive forward-and-backward diffusion process.</p><p>Similar forms of the learned penalty function in (c) with a concave shape are also observed in previous work on image prior learning <ref type="bibr" target="#b58">[59]</ref>. This penalty function also encourages to sharpen the image edges. Concerning the learned penalty function (d), as it has local minima at two specific points, it prefers specific image structure, implying that it helps to form certain image structure. We also find that this penalty function is exactly the type of bimodal expert functions for texture synthesis employed in <ref type="bibr" target="#b29">[30]</ref>.</p><p>Therefore, our learned penalty functions confirmed existing robust penalties based prior models and many priors exploiting some unusual penalties, which can produce patterns and enhance preferred features. As a consequence, the diffusion process involving the learned influence functions does not perform pure image smoothing any more for image processing. In contrast, it leads to a diffusion process for adaptive image smoothing and sharpening, distinguishing itself from previous commonly used image regularization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pattern formation using the learned influence functions</head><p>In the previous work on Gibbs reaction diffusion 7 <ref type="bibr" target="#b58">[59]</ref>, it is shown that those unconventional penalty functions such as <ref type="figure" target="#fig_7">Figure  5</ref>(c) have significant meaning in visual computation, as they can produce patterns. We also find that those unconventional penalty functions learned in our models can produce some interesting image patterns. <ref type="bibr" target="#b6">7</ref>. The terminology of "reaction diffusion" in <ref type="bibr" target="#b58">[59]</ref> is a bit different from ours. In our formulation, "reaction term" is related to the data term, while in <ref type="bibr" target="#b58">[59]</ref>, it means the diffusion term controlled by those downright penalty functions.  <ref type="figure">Fig. 6</ref>. Well distributed gradients ∂L ∂Θ over stages for the TNRD 5 5×5 model at the initialization point Θ 0 with a plain setting. One can see that the "vanishing gradient" phenomenon <ref type="bibr" target="#b5">[6]</ref> in the back-propagation phase of a conventional deep model does not appear in our training model. <ref type="figure">Fig. 7</ref>. Patterns synthesized from uniform noise using our learned diffusion models. (a) is generated by (16) using the parameters (linear filters and influence functions) in a stage of our learned TNRD 5 5×5 for image denoising, (b) is generated by <ref type="bibr" target="#b15">(16)</ref> using the parameters in a stage of our learned TNRD 5 7×7 for image super resolution and (c) is also from a stage of our learned TNRD 5 7×7 for image super resolution.</p><formula xml:id="formula_25">(a) (b) (c)</formula><p>We consider the following diffusion process involving our learned linear filters and the corresponding influence functions</p><formula xml:id="formula_26">u t − u t−1 ∆t = − N k i=1k i * φ i (k i * u t−1 ) ,<label>(16)</label></formula><p>where the filters k i and influence functions φ i are chosen from a certain stage of the learned models. Note that we do not incorporate a reaction term in this diffusion model. We run (16) from starting points u 0 (uniform noise images in the range [0, 255]), and it converges to a local minimum <ref type="bibr" target="#b7">8</ref> . Some synthesized patterns are shown in <ref type="figure">Figure 7</ref>. One can see that the diffusion model with our learned influence functions and filters can produce edge-like image structure and repeated patterns from fully random images. This kind of diffusion model is known as Gibbs reaction diffusion in <ref type="bibr" target="#b58">[59]</ref>. We provide another example in <ref type="figure" target="#fig_1">Figure 13</ref> to demonstrate how our learned diffusion models can generate meaningful patterns for image super resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Important aspects of the training framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Influence of initialization</head><p>Our training model is also a deep model with many stages (layers), but we find that it is not very sensitive to initialization. Based on the training for Gaussian denoising, we conducted experiments with fully random initializations and some plain settings. <ref type="bibr" target="#b7">8</ref>. The corresponding diffusion processes are unstable, and therefore we have to restrict the image dynamic range to [0, 255].</p><p>We firstly investigated the case of greedy training where the model was trained stage by stage and the parameters of one stage were trained at a time. We initialized the parameters using fully random numbers in the range [−0.5, 0.5]. It turns out that the resulting models with different initializations lead to a deviation within 0.01dB in the test phase. That is to say, the greedy training strategy is not sensitive to initialization.</p><p>Then, we considered the case of joint training, where all the parameters in all stages were trained at a time. We also initialized the training with fully random numbers in the range [−0.5, 0.5]. In this case, it turns out that the resulting models lead to inferior results, e.g., in the case of TNRD 5 5×5 (28.61 vs.28.78). However, plain initializations can generate equivalent results. For example, we considered a plain initialization (all stages were initialized from the modified DCT filters and an unified influence function φ(z) = 2z/(1 + z 2 )), the resulting models performed almost the same as those models trained from some good initializations such as parameters obtained from greedy training, e.g.,TNRD 5 5×5 , (28.75 vs.28.78) and TNRD 5 7×7 , (28.91 vs.28.92). We believe that this appealing property of our training framework is attributed to the well-distributed gradients across stages. We show in <ref type="figure">Figure 6</ref> an example to illustrate the gradients of the training loss function with respect to the parameter of all stages. One can see that the well-known phenomenon of "vanishing gradient" <ref type="bibr" target="#b5">[6]</ref> in the back-propagation phase of a usual deep model does not appear in our training model. We believe that the reason for the well-distributed gradients is that our training model is more constrained. For example, in a more general sense, the rotated kernelk i in our formulation is not necessary to be the rotated version of the kernel k i , and it can be an arbitrary kernel. However, we stick to this form, as it has a clear meaning derived from energy minimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Influence of the number of training samples</head><p>In our training, we do not consider any regularization for the training parameters, and we finally reach good-performing models. A probable reason is that we have exploited sufficient training samples (400 samples of size 180 × 180). Thus an interesting question arises: how many samples are sufficient for our training?</p><p>In order to answer this question, we re-train the TNRD 5 5×5 model using different size of training dataset, and then evaluate the test performance of trained models. We summarize the results in <ref type="figure">Figure 8</ref>. One can see that <ref type="formula">(1)</ref>    model for the noise level σ = 25. We can find first, second and higherorder derivative filters, as well as rotated derivative filters along different directions. These filters are effective for image structure detection, such as image edge and texture.</p><p>40 images) will clearly lead to over-fitting, thus inferior test performance, and (2) 200 images are typically enough to prevent over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Influence of the filter size</head><p>In our model, the size of involved filters is a free parameter. In principle, we can exploit filters of any size, but in practice, we need to consider the trade-off between run time and accuracy.</p><p>In order to investigate the influence of the filter size, we increase the filter size to 7×7 and 9×9. We find that increasing the filter size from 5 × 5 to 7 × 7 brings a significant improvement of 0.14dB ( TNRD 5 7×7 vs.TNRD 5 5×5 ) as show in <ref type="table">Table 1</ref>. However, when we further increase the filter size to 9 × 9, the resulting TNRD 5 9×9 only leads to a performance of 28.96dB (a slight improvement of 0.05dB relative to the TNRD 5 7×7 model). We can conjecture that further increasing the filter size to 11 × 11 might bring negligible improvements.</p><p>Note that the above conclusion is drawn from a relatively small training data set of 400 images of size 180 × 180. It should be mentioned that when the size of the model increase, the size of training data set should also increase to avoid overfitting. However, our current CPU implementation for training prevents us from training with larger model and large-scale data sets (millions). A faster implementation on GPUs together with the stochastic gradient descent optimization strategy is left to future work.</p><p>We also consider a model with smaller filters, 3 × 3. We summarize the results of different model capacities in <ref type="figure">Figure 9</ref>. In practice, we prefer the TNRD 5 7×7 model as it provides the best trade-off between performance and computation time. Therefore, in later applications, we only consider TNRD T 7×7 models. <ref type="figure" target="#fig_1">Fig. 10</ref> shows the trained filters of the TNRD 5 7×7 model in the first and last stage for the task of Gaussian denoising. One can find many edge and image structure detection filters along different directions and in different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training for different noise levels and comparison to recent state-of-the-arts</head><p>The above training experiments are based on Gaussian noise of level σ = 25. We also trained diffusion models for the noise levels σ = 15 and σ = 50. The test performance is summarized in <ref type="table">Table 1</ref>, together with comparison to very recent state-of-theart denoising algorithms. In experiments, we observed that joint training can always gain an improvement of about 0.1dB over the greedy training for the cases of T ≥ 5.</p><p>From <ref type="table">Table 1</ref>, one can see that for all noise levels, the resulting TNRD 7×7 model achieves the highest average PSNR. The TNRD 5 7×7 model outperforms the benchmark -BM3D method by 0.35dB in average. This is a notable improvement as few methods can surpass BM3D more than 0.3dB in average <ref type="bibr" target="#b35">[36]</ref>. Moreover, the TNRD 5 7×7 model also surpasses the best-reported algorithm -WNNM method, which is quite slow as shown in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Run time</head><p>The algorithm structure of our TNRD model is similar to the CSF model, which is well-suited for parallel computation on GPUs. We implemented our trained models on GPU using CUDA programming to speed up the inference procedure, and finally it leads to significantly improved performance, see <ref type="table">Table 2</ref>. We make a run time comparison to other denoising algorithms based on strictly enforced single-threaded CPU computation ( e.g., start Matlab with -singleCompThread) for a fair comparison, see <ref type="table">Table  2</ref>. We only present the results of some selective algorithms, which either have the best denoising result or run time performance. We refer to <ref type="bibr" target="#b51">[52]</ref> for a comprehensive run time comparison of various algorithms 9 . <ref type="bibr" target="#b8">9</ref>. LSSC, EPLL, opt-MRF and RTF 5 methods are much slower than BM3D on the CPU, cf. <ref type="bibr" target="#b51">[52]</ref>.  <ref type="bibr" target="#b47">[48]</ref> for image denoising with σ = 15, 25 and 50. All the TNRD models are jointly trained. Note that among those algorithms similar to our model, opt-MRF 7×7 , ARF 4 5×5 and opt-GD 10 5×5 only train the filters with fixed penalty function log(1 + z 2 ). In the opt-MRF 7×7 model, 48 filters of size 7 × 7 (2304 free parameters), for ARF 4 5×5 , 13 filters of size 5 × 5 (325 free parameters) and for the opt-GD 10 5×5 algorithm, 24 filters of size 5 × 5 (600 free parameters) are trained. The CSF model and our approach train both the filters and nonlinearities, thus involving more parameters, e.g., the TNRD We see that our TNRD model is generally faster than the CSF model with the same model capacity. It is reasonable, because in each stage the CSF model involves additional DFT and inverse DTF operations, i.e., our model only requires a portion of the computation of the CSF model. Even though the BM3D is a non-local model, it still possesses high computational efficiency. In contrast, another non-local model -WNNM achieves compelling denoising results at the expense of huge computation time. Moreover, the WNNM algorithm is hardly applicable for high resolution images (e.g., 10 mega-pixels) due to its huge memory requirements. Note that our model can be also easily implemented with multi-threaded CPU computation.</p><p>In summary, our TNRD 5 7×7 model outperforms these recent state-of-the-arts, meanwhile it is the fastest method even with a CPU implementation. We present an illustrative denoising example in <ref type="figure" target="#fig_1">Figure 11</ref> on an image from the test dataset. More denoising examples can be found in the supplemental material based on images from the test dataset and a megapixel-size natural image of size 1050 × 1680.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SINGLE IMAGE SUPER RESOLUTION (SISR)</head><p>As demonstrated in the last section that our trained diffusion model can lead to explicit backward diffusion process, which sharpens image structures like edges. This is the very property demanded for the task of image super resolution. Therefore, we are motivated to investigate the SISR problem with our proposed approach.</p><p>We start with the following energy functional</p><formula xml:id="formula_27">min u E(u) = N k i=1 ρ i (k i * u) + λ 2 Au − f 2 2 ,<label>(17)</label></formula><p>where the linear operator A is a bicubic interpolation which links the high resolution (HR) image h to the low resolution (LR) image f via f = Ah. Casting D(u) = λ 2 Au − f 2 2 and G(u) = 0, the energy functional (17) suggests the following diffusion process We considered the model capacity of TNRD 5 7×7 , and trained diffusion models for three upscaling factors ×2, ×3 and ×4, using exactly the same 91 training images as in previous works <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b52">[53]</ref>. The trained models are evaluated on two different test data sets: Set5 and Set14. Following previous works <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b52">[53]</ref>, the trained models are only applied to the luminance component of an image, and a regular bicubic upscaling method is applied to the color components.</p><formula xml:id="formula_28">u t = u t−1 − N k i=1k t i * φ t i (k t i * u t−1 ) + λ t A (Au t−1 − f ) ,<label>(18)</label></formula><p>The test results are summarized in <ref type="table" target="#tab_6">Table 3</ref> and <ref type="table" target="#tab_8">Table 4</ref>. One can see that in terms of average PSNR, our trained diffusion model TNRD 5 7×7 leads to significant improvements over very recent state-of-the-arts in all cases, meanwhile it is still among the fast algorithms <ref type="bibr" target="#b9">10</ref> . A SISR example is shown in <ref type="figure" target="#fig_1">Figure 12</ref> to illustrate its effectiveness. One can see that our approach can obtain more accurate image edges, as shown in the zoom-in parts. More SISR examples can be found in the supplemental material.</p><p>We apply the learned diffusion parameters to the diffusion equation <ref type="bibr" target="#b15">(16)</ref>. It turns out that the diffusion process can also generate some interesting patterns from random images, as shown in <ref type="figure">Figure 7</ref>. We believe that this ability to generate image patterns from weak evidence is the main reason for the superiority of our trained model for the SISR task. In order to further validate our argument, we carry out a toy SISR experiment based on a synthesized image with repeated hexagons. The results are shown in <ref type="figure" target="#fig_1">Figure 13</ref>, where one can see that our trained model can better reconstruct those repeated image structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">JPEG DEBLOCKING EXPERIMENTS</head><p>In order to further demonstrate the applicability of our proposed framework for those problems with a non-smooth data term, we investigate the JPEG deblocking problem -suppressing the block artifacts in the JPEG compressed images, which is formulated as a <ref type="bibr" target="#b9">10</ref>. Note that our approach is a Matlab implementation, while some of other algorithms are based on C++ implementations, such as SR-CNN.   non-smooth optimization problem. Motivated by <ref type="bibr" target="#b7">[8]</ref>, we consider the following variational model based on the FoE image prior</p><formula xml:id="formula_29">min u E(u) = N k i=1 ρ i (k i * u) + I Q (Du) ,<label>(19)</label></formula><p>where I Q is a indicator function over the set Q (quantization constraint set). In JPEG compression, information loss happens in the quantization step, where all possible values in the range [d − 0.5, d + 0.5] (d is an integer) are quantized to a single number d. Given a compressed data, we only know d. Therefore, all possible values in the interval [d − 0.5, d + 0.5] define a convex set Q which is a box constraint. The sparse matrix D ∈ R N ×N denotes the block DCT transform. We refer to <ref type="bibr" target="#b7">[8]</ref> for more details. By setting D(u) = 0 and G(u) = I Q (Du), we obtain the following diffusion process</p><formula xml:id="formula_30">u t = D proj Q D u t−1 − N k i=1k t i * φ t i (k t i * u t−1 ) ,<label>(20)</label></formula><p>where proj Q (·) denotes the orthogonal projection onto Q. More details can be found in the supplemental material.</p><p>We also trained diffusion models for the JPEG deblocking problem. We followed the test procedure in <ref type="bibr" target="#b32">[33]</ref> for performance evaluation. We distorted the images by JPEG block-   <ref type="figure" target="#fig_1">Fig. 12</ref>. A super resolution example for the "Monarch" image from Set14 with an upscaling factor ×3. Note the differences in the highlighted region that our model achieves more clean and sharp image edges. Best viewed on screen and zoom in.  <ref type="figure" target="#fig_1">Fig. 13</ref>. A toy experiment on a synthesized image with repeated hexagons for the upscaling factor ×3.</p><p>ing artifacts. We considered three compression quality settings q = 10, 20 and 30 for the JPEG encoder.</p><p>We trained three nonlinear diffusion TNRD 7×7 models for different compression parameter q. We found that for JPEG deblocking, 4 stages are already enough. Results of the trained models are shown in <ref type="table" target="#tab_10">Table 5</ref>, compared with several representative deblocking approaches. We see that our trained TNRD 4 7×7 outperforms all the competing approaches in terms of PSNR. Concerning the run time, our model takes about 11.2s to handle an image of size 1024 × 1024 with CPU computation, while the strongest competitor (in terms of run time) -SADCT consumes about 56.5s <ref type="bibr" target="#b10">11</ref> . Furthermore, our model is extremely fast on GPUs. For the same image size the GPU implementation takes about 0.095s. See the supplemental material for JPEG deblocking examples. <ref type="bibr" target="#b10">11</ref>. RTF is slower than SADCT, as it depends on the output of SADCT. q JPEG decoder TGV <ref type="bibr" target="#b7">[8]</ref> Dic-SR <ref type="bibr" target="#b8">[9]</ref> SADCT <ref type="bibr" target="#b21">[22]</ref> RTF <ref type="bibr" target="#b32">[33]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION, SUMMARY AND FUTURE WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Summary</head><p>We have proposed a trainable nonlinear reaction diffusion framework for effective image restoration. Its critical point lies in the additional training of the influence functions. We have trained our models for the problem of Gaussian denoising, single image super resolution and JPEG deblocking. Based on standard test datasets, the trained models result in the best-reported results. We believe that the effectiveness of the trained diffusion models is attributed to the following desired properties of the models • Anisotropy. In the trained filters, we can find rotated derivative filters in different directions, cf. <ref type="figure" target="#fig_1">Fig 10,</ref> which will make the diffusion happen in some special directions. • Higher order. The learned filters contain first, second and higher-order derivative filters, cf. Meanwhile, the structure of trained models is very simple and well-suited for parallel computation on GPUs. As a consequence, the resulting algorithms are significantly faster than all competing algorithms and hence are also applicable to the restoration of high resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Discussion</head><p>One possible limitation of the proposed TNRD approach is that one has to define the ground truth -the expected output of the diffusion network during training. For image restoration applications in this paper, this is not a problem as we have a clear choice for the ground truth. However, for those applications with ambiguous ground truth, e.g., image structure extraction <ref type="bibr" target="#b56">[57]</ref>, we will have to make efforts to define the ground truth.</p><p>Furthermore, the trained diffusion networks will only perform well in the way they are trained. For example, the trained model based on noise level σ = 25 will break for an input image with noise σ = 50, and the trained model for upscaling factor ×3 will also lead to inferior performance when it is applied to the SISR problem of upscaling factor ×2. It is generally hard to train a universal diffusion model to handle all the noise levels or all upscaling factors.</p><p>Our approach is to optimize a time-discrete PDE, which is inspired by FoE based model, but we do not aim to minimize a series of FoE based energies. Our model directly learns an optimal trajectory for a certain possibly unknown energy functional, the minimizer of which provides a good estimate of the demanded solution. Probably, such a functional can not be modeled by a single FoE energy, while our learned gradient descent/forwardbackward steps provide good approximation to the local gradients of this unknown functional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Future work</head><p>From an application point of view, we think that it will be interesting to consider learned nonlinear reaction diffusion based models also for other image processing tasks such as image inpainting, blind image deconvolution, optical flow. Moreover, since learning the influence functions turned out to be crucial, we believe that learning optimal nonlinearities (i.e., activation functions) in standard CNs could lead to a similar performance increase. There are actually two recent works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b28">[29]</ref> to investigate a parameterized ReLU function in standard deep convolutional networks, which indeed brings improvements even with little freedom to tune the activation functions. Finally, it will also be interesting to investigate the unconventional penalty functions learned by our approach in usual energy minimization approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of our proposed diffusion model with a reaction force ψ t = λ t A (Au − f ) and G = 0. It is represented as a feed-forward network. Note that the additional convolution step with the rotated kernelsk i (cf. Equ. 15) does not appear in conventional feed-forward CNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 2. Our diffusion network can also be interpreted as a CN with a feedback step, which makes it different from conventional feed-forward networks. Due to the feedback step, it can be categorized into recurrent networks [25].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Function approximation via Gaussian ϕg(z) or triangular-shaped ϕt(z) radial basis function, respectively for the function φ(z) = 2sz 1+s 2 z 2 with s = 1 20 . Both approximation methods use 63 basis functions equidistantly centered at [−310 : 10 : 310].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fully trained model with 8 filters of size 3 × 3 , TNRD T 5×5 , Fully trained model with 24 filters of size 5 × 5 , TNRD T 7×7 , Fully trained model with 48 filters of size 7 × 7 , TNRD T 9×9 , Fully trained model with 80 filters of size 9 × 9 , where TNRD T m×m denotes a nonlinear diffusion process of stage T with filters of size m × m. The filters number is m 2 − 1, if not specified. For example, TNRD T 7×7 model contains (48 × 48 (filters) + 48 × 63 (penalties) + 1 (λ)) · T = 5329 · T free parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Noisy u 0 (20.17) (b) Stage 1: u 1 (27.26) (c) Stage 2: u 2 (28.40) (d) Stage 3: u 3 (28.18) (e) Stage 4: u 4 (28.63) (f) Stage 5: u 5 (29.63) An image denoising example for noise level σ = 25 to illustrate how our learned TNRD 5 5×5 works. (b) -(e) are intermediate results at stage 1 -4, and (f) is the output of stage 5, i.e., the final denoising result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>The figure shows four characteristic influence functions (left plot in each subfigure) together with their corresponding penalty functions (right plot in each subfigure)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) Truncated convex penalty functions with low values around zero to promote smoothness. (b) Negative Mexican hat functions, which have a local minimum at zero and two symmetric local maxima. (c) Truncated concave functions with smaller values at the two tails. (d) Double-well functions, which have a local maximum (not a minimum any more) at zero and two symmetric local minima.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Influence of the number of training examples for the training model Influence of the filter size (based on a relatively small training data set of 400 images of size 180 × 180) (a) 48 filters of size 7 × 7 in stage 1 (b) 48 filters of size 7 × 7 in stage 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Trained filters (in the first and last stage) of the TNRD 5 7×7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Fig 10. • Adaptive forward/backward diffusion through the learned nonlinear functions. Nonlinear functions corresponding to explicit backward diffusion appear in the learned nonlinearity, cf. Fig 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>where the starting point u 0 is given by the direct bicubic interpolation of the LR image f . Computing the gradients ∂ut ∂Θt and</figDesc><table><row><cell>∂ut ∂ut−1 with respect to (18) can be done with little modifications</cell></row><row><cell>to the derivations for image denoising. Detailed derivations are</cell></row><row><cell>presented in the supplemental material.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Fig. 11. Denoising results on a test image of size 481 × 321 (σ = 25) by different methods (compared with BM3D<ref type="bibr" target="#b14">[15]</ref>, WNNM<ref type="bibr" target="#b25">[26]</ref> and CSF model<ref type="bibr" target="#b51">[52]</ref>), together with the corresponding computation time either on CPU or GPU. Note the differences in the highlighted region.</figDesc><table><row><cell cols="3">a) Noisy, 20.17dB</cell><cell></cell><cell></cell><cell cols="4">(b) BM3D, 27.53dB/CPU: 2.5s</cell><cell></cell><cell cols="4">(c) CSF 5 7×7 , 28.00dB/GPU: 0.55s</cell></row><row><cell cols="3">(d) WNNM, 27.94dB/CPU: 393.2s</cell><cell></cell><cell></cell><cell cols="4">(e) TNRD 5 5×5 , 28.16dB/GPU: 9.1ms</cell><cell></cell><cell cols="4">(f) TNRD 5 7×7 , 28.23dB/GPU: 20.3ms</cell></row><row><cell>Set5</cell><cell></cell><cell cols="2">Bicubic</cell><cell cols="2">K-SVD [58]</cell><cell cols="2">ANR [54]</cell><cell cols="2">SR-CNN [19]</cell><cell cols="2">RFL [53]</cell><cell cols="2">TNRD 5 7×7</cell></row><row><cell>images</cell><cell>scale</cell><cell>PSNR</cell><cell>Time</cell><cell>PSNR</cell><cell>Time</cell><cell>PSNR</cell><cell>Time</cell><cell>PSNR</cell><cell>Time</cell><cell>PSNR</cell><cell>Time</cell><cell>PSNR</cell><cell>Time</cell></row><row><cell>baby</cell><cell>2</cell><cell>37.07</cell><cell>-</cell><cell>38.25</cell><cell>8.21</cell><cell>38.44</cell><cell>1.39</cell><cell>38.30</cell><cell>0.38</cell><cell>38.39</cell><cell>1.31</cell><cell>38.51</cell><cell>1.52</cell></row><row><cell>bird</cell><cell>2</cell><cell>36.81</cell><cell>-</cell><cell>39.93</cell><cell>2.67</cell><cell>40.04</cell><cell>0.44</cell><cell>40.64</cell><cell>0.14</cell><cell>40.99</cell><cell>0.52</cell><cell>41.29</cell><cell>0.59</cell></row><row><cell>butterfly</cell><cell>2</cell><cell>27.43</cell><cell>-</cell><cell>30.65</cell><cell>2.14</cell><cell>30.48</cell><cell>0.38</cell><cell>32.20</cell><cell>0.10</cell><cell>32.46</cell><cell>0.41</cell><cell>33.16</cell><cell>0.56</cell></row><row><cell>head</cell><cell>2</cell><cell>34.86</cell><cell>-</cell><cell>35.59</cell><cell>2.46</cell><cell>35.66</cell><cell>0.41</cell><cell>35.64</cell><cell>0.13</cell><cell>35.70</cell><cell>0.48</cell><cell>35.71</cell><cell>0.60</cell></row><row><cell>woman</cell><cell>2</cell><cell>32.14</cell><cell>-</cell><cell>34.49</cell><cell>2.45</cell><cell>34.55</cell><cell>0.43</cell><cell>34.94</cell><cell>0.13</cell><cell>35.19</cell><cell>0.46</cell><cell>35.50</cell><cell>0.57</cell></row><row><cell>average</cell><cell>2</cell><cell>33.66</cell><cell>-</cell><cell>35.78</cell><cell>3.59</cell><cell>35.83</cell><cell>0.61</cell><cell>36.34</cell><cell>0.18</cell><cell>36.55</cell><cell>0.64</cell><cell>36.83</cell><cell>0.77</cell></row><row><cell>baby</cell><cell>3</cell><cell>33.91</cell><cell>-</cell><cell>35.08</cell><cell>3.77</cell><cell>35.13</cell><cell>0.79</cell><cell>35.01</cell><cell>0.38</cell><cell>35.04</cell><cell>0.79</cell><cell>35.28</cell><cell>1.52</cell></row><row><cell>bird</cell><cell>3</cell><cell>32.58</cell><cell>-</cell><cell>34.57</cell><cell>1.34</cell><cell>34.60</cell><cell>0.27</cell><cell>34.91</cell><cell>0.14</cell><cell>35.15</cell><cell>0.31</cell><cell>36.11</cell><cell>0.59</cell></row><row><cell>butterfly</cell><cell>3</cell><cell>24.04</cell><cell>-</cell><cell>25.94</cell><cell>1.08</cell><cell>25.90</cell><cell>0.24</cell><cell>27.58</cell><cell>0.10</cell><cell>27.18</cell><cell>0.25</cell><cell>28.90</cell><cell>0.56</cell></row><row><cell>head</cell><cell>3</cell><cell>32.88</cell><cell>-</cell><cell>33.56</cell><cell>1.35</cell><cell>33.63</cell><cell>0.24</cell><cell>33.55</cell><cell>0.13</cell><cell>33.68</cell><cell>0.29</cell><cell>33.78</cell><cell>0.60</cell></row><row><cell>woman</cell><cell>3</cell><cell>28.56</cell><cell>-</cell><cell>30.37</cell><cell>1.14</cell><cell>30.33</cell><cell>0.24</cell><cell>30.92</cell><cell>0.13</cell><cell>30.92</cell><cell>0.28</cell><cell>31.77</cell><cell>0.57</cell></row><row><cell>average</cell><cell>3</cell><cell>30.39</cell><cell>-</cell><cell>31.90</cell><cell>1.74</cell><cell>31.92</cell><cell>0.35</cell><cell>32.39</cell><cell>0.18</cell><cell>32.39</cell><cell>0.39</cell><cell>33.17</cell><cell>0.77</cell></row><row><cell>baby</cell><cell>4</cell><cell>31.78</cell><cell>-</cell><cell>33.06</cell><cell>2.63</cell><cell>33.03</cell><cell>0.59</cell><cell>32.98</cell><cell>0.38</cell><cell>33.05</cell><cell>0.60</cell><cell>33.29</cell><cell>1.52</cell></row><row><cell>bird</cell><cell>4</cell><cell>30.18</cell><cell>-</cell><cell>31.71</cell><cell>0.70</cell><cell>31.82</cell><cell>0.18</cell><cell>31.98</cell><cell>0.14</cell><cell>32.14</cell><cell>0.23</cell><cell>32.98</cell><cell>0.59</cell></row><row><cell>butterfly</cell><cell>4</cell><cell>22.10</cell><cell>-</cell><cell>23.57</cell><cell>0.54</cell><cell>23.52</cell><cell>0.14</cell><cell>25.07</cell><cell>0.10</cell><cell>24.44</cell><cell>0.19</cell><cell>26.22</cell><cell>0.56</cell></row><row><cell>head</cell><cell>4</cell><cell>31.59</cell><cell>-</cell><cell>32.21</cell><cell>0.66</cell><cell>32.27</cell><cell>0.16</cell><cell>32.19</cell><cell>0.13</cell><cell>32.31</cell><cell>0.22</cell><cell>32.57</cell><cell>0.60</cell></row><row><cell>woman</cell><cell>4</cell><cell>26.46</cell><cell>-</cell><cell>27.89</cell><cell>0.72</cell><cell>27.80</cell><cell>0.23</cell><cell>28.21</cell><cell>0.13</cell><cell>28.31</cell><cell>0.23</cell><cell>29.17</cell><cell>0.57</cell></row><row><cell>average</cell><cell>4</cell><cell>28.42</cell><cell>-</cell><cell>29.69</cell><cell>1.05</cell><cell>29.69</cell><cell>0.26</cell><cell>30.09</cell><cell>0.18</cell><cell>30.05</cell><cell>0.29</cell><cell>30.85</cell><cell>0.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 PSNR</head><label>3</label><figDesc>(dB) and run time (s) performance for upscaling factors ×2, ×3 and ×4 on the Set5 dataset. All the methods use the same 91 training images as in<ref type="bibr" target="#b53">[54]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4</head><label>4</label><figDesc>Upscaling factor ×3 performance in terms of PSNR(dB) and runtime (s) per image on the Set14 dataset.</figDesc><table><row><cell>(a) Bicubic / 29.43dB</cell><cell>(b) K-SVD [58] / 31.10dB</cell><cell>(c) ANR [54] / 31.09dB</cell></row><row><cell>(d) SR-CNN [19] / 32.39dB</cell><cell>(e) RFL [53] / 32.10dB</cell><cell>(f) TNRD 5 7×7 / 33.61dB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5</head><label>5</label><figDesc>JPEG deblocking results for natural images, reported with average PSNR values.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">. Anisotropic diffusion in this paper is understood in the sense that the smoothing induced by PDEs can be favored in some directions and prevented in others. The diffusivity is not necessary to be a tensor. It should be noted that this definition is different from Weickert's terminology<ref type="bibr" target="#b54">[55]</ref>, where anisotropic diffusion always involves a diffusion tensor, and the PM model is regarded as an isotropic model.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Oriented texture completion by AM-FM reaction-diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Acton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Prasad</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Havlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="885" to="896" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6830</idno>
		<title level="m">Learning activation functions to improve deep neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training an active random field for real-time image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2451" to="2462" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A well-balanced flow equation for noise removal and edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A Z</forename><surname>Barcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boaventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Silva</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="751" to="763" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Greedy layerwise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust anisotropic diffusion and sharpening of scalar and vector images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marimont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Image Processing</title>
		<meeting>IEEE Int&apos;l Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="263" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Artifact-free decompression and zooming of JPEG compressed images with total generalized variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bredies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Imaging and Computer Graphics. Theory and Application</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="242" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing artifact in JPEG decompression via a learned dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="718" to="728" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Revisiting loss-specific training of filter-based MRFs for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. German Conf. Pattern Recognition</title>
		<meeting>German Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Insights into analysis operator learning: From patch-based sparse models to higher order MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1060" to="1072" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On learning optimized reaction diffusion processes for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image processing through reaction combined with nonlinear diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-H</forename><surname>Cottet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Germain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="page" from="659" to="673" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast adaptive nonlocal SAR despeckling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parrilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Scarpa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sensing Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="524" to="528" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stability and local feature enhancement of higher order nonlinear diffusion filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Didas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Burgeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. German Conf. Pattern Recognition</title>
		<meeting>German Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Properties of higher order nonlinear diffusion filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Didas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Burgeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="208" to="226" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generic methods for optimization-based modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conf. Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="318" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image quantization using reaction-diffusion equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Esclarín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="175" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointwise shape-adaptive DCT for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Forward-and-backward diffusion processes for adaptive image enhancement and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sochen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="689" to="703" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sparsity-based Poisson denoising with dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5057" to="5069" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two enhanced fourth order diffusion models for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Longo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="188" to="198" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An anisotropic fourth-order diffusion filter for image noise removal. Int&apos;l</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Hajiaboli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="191" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning generative texture models with extended Fields-of-Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Handbook of neural network signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Loss-specific training of nonparametric image restoration models: A new state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="112" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Diffusion filtering without parameter tuning: Models and inference tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krajsek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2536" to="2543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Natural image denoising: Optimality and inherent bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2833" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Splitting algorithms for the sum of two nonlinear operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Lions</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mercier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="964" to="979" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning PDEs for image restoration via optimal control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="115" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;,l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;,l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A tour of modern image filtering: new insights and methods, both practical and theoretical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="128" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Machine Learning</title>
		<meeting>IEEE Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Biased anisotropic diffusion: a unified regularization and diffusion approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Niklas</forename><surname>Nordström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">iPiano: Inertial Proximal Algorithm for Nonconvex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1388" to="1419" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image denoising using the higher order singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajwade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="849" to="862" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Single image interpolation via adaptive nonlocal sparsity-based modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3085" to="3098" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fields of Experts. Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="229" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image statistics and anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Haussecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="840" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Relations between regularization and diffusion filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Scherzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="63" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Anisotropic diffusion in image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note type="report_type">Teubner Stuttgart</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Theoretical foundations for discrete forward-and-backward diffusion filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Scale Space and Variational Methods in Computer Vision</title>
		<meeting>Int&apos;l Conf. Scale Space and Variational Methods in Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="527" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Structure extraction from texture via relative total variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">139</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Prior learning and Gibbs reaction-diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1236" to="1250" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
