<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-13">13 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungkyu</forename><surname>Lee</surname></persName>
							<email>jungkyu.lee@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova Vision</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeryun</forename><surname>Won</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova Vision</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><forename type="middle">Kwan</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova Vision</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyemin</forename><surname>Lee</surname></persName>
							<email>hmin.lee@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova Vision</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geonmo</forename><surname>Gu</surname></persName>
							<email>geonmo.gu@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova Vision</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiho</forename><surname>Hong</surname></persName>
							<email>kiho.hong@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova Vision</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-13">13 Mar 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recent studies in image classification have demonstrated a variety of techniques for improving the performance of</head> Convolutional Neural Networks (CNNs)<p>. However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to basic CNN models (e.g., ResNet and MobileNet) can improve the accuracy and robustness of the models while minimizing the loss of throughput. Our proposed assembled ResNet-50 shows improvements in top-1 accuracy from 76.3% to 82.78%, mCE from 76.0% to 48.9% and mFR from 57.7% to 32.3% on ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. To verify the performance improvement in transfer learning, fine grained classification and image retrieval tasks were tested on several public datasets and showed that the improvement to backbone network performance boosted transfer learning performance significantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019 1 , and the source code and trained models will be made publicly available 2 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the introduction of AlexNet <ref type="bibr" target="#b21">[18]</ref>, many studies have mainly focused on designing new network architectures for image classification to increase accuracy. For example, new architectures such as Inception <ref type="bibr" target="#b33">[30]</ref>, ResNet <ref type="bibr" target="#b11">[8]</ref>, DenseNet <ref type="bibr" target="#b17">[14]</ref>, NASNet <ref type="bibr" target="#b41">[38]</ref>, MNASNet <ref type="bibr" target="#b34">[31]</ref> and Efficient-Net <ref type="bibr" target="#b35">[32]</ref> have been proposed. Inception introduced new modules into the network with convolution layers of different kernel sizes. ResNet utilized the concept of skip connection, and DenseNet added dense feature connections to boost the performance of the model. In addition, in the area of AutoML, network design was automatically decided to create models such as NASNet and MNASNet. Efficient-Net proposes an efficient network by balancing the resolution, height, and width of the network. The performance of EfficientNet for ILSVRC2012 top-1 accuracy was greatly improved relative to AlexNet.</p><p>Unlike these studies which focus on designing new network architecture, He et al. <ref type="bibr" target="#b12">[9]</ref> proposes different approaches to improve model performance. They noted that performance can be improved not only through changes in the model structure, but also through other aspects of network training such as data preprocessing, learning rate decay, and parameter initialization. They also demonstrate that these minor "tricks" play a major part in boosting model performance when applied in combination. As a result of using these tricks, ILSVRC2012 top-1 validation accuracy of ResNet-50 improved from 75.3% to 79.29% and MobileNet improved from 69.03% to 71.90%. This improvement is highly significant because it shows as much performance improvement as a novel network design does.</p><p>Inspired by <ref type="bibr" target="#b12">[9]</ref>, we conducted a more extensive and systematic study of assembling several CNN-related techniques into a single network. We first divided the CNNrelated techniques into two categories: network tweaks and regularization. Network tweaks are methods that modify the CNN architectures to be more efficient. (e.g., SENet <ref type="bibr" target="#b16">[13]</ref>, SKNet <ref type="bibr" target="#b22">[19]</ref>). Regularization includes methods that prevent overfitting by increasing the training data through data augmentation processes such as AutoAugment <ref type="bibr" target="#b7">[4]</ref> and Mixup <ref type="bibr" target="#b39">[36]</ref>, or by limiting the complexity of the CNN with processes such as Dropout <ref type="bibr" target="#b32">[29]</ref>, and DropBlock <ref type="bibr" target="#b9">[6]</ref>. We then systematically analyze the process of assembling these two types of techniques through extensive experiments and demonstrate that our approach leads to significant performance improvements.</p><p>In addition to top-1 accuracy, mCE, mFR and throughput were used as performance indicators for combining these various techniques. Hendrycks et al. <ref type="bibr" target="#b13">[10]</ref> proposed mCE Model Top-1 mCE mFR Throughput EfficientNet B4 <ref type="bibr" target="#b35">[32]</ref> + AutoAugment <ref type="bibr" target="#b7">[4]</ref> 83.0 60.7 -95 EfficientNet B6 <ref type="bibr" target="#b35">[32]</ref> + AutoAugment <ref type="bibr" target="#b7">[4]</ref> 84.2 60.6 -28 EfficientNet B7 <ref type="bibr" target="#b35">[32]</ref> + AutoAugment <ref type="bibr" target="#b7">[4]</ref> 84.5 59. <ref type="bibr">4 -16</ref> ResNet-50 <ref type="bibr" target="#b11">[8]</ref>   <ref type="table">Table 1</ref>. Summary of key results. Top-1 is ILSVRC2012 top-1 validation accuracy. mCE is mean corruption error and mFR is mean flip rate (Lower is better.) <ref type="bibr" target="#b13">[10]</ref>. The Throughput refers to how many images per second the model processes during inference.</p><p>(mean corruption error) and mFR (mean flip rate). mCE is a measure of network robustness against input image corruption, and mFR is a measure of network stability on perturbations in image sequences. Moreover, we used throughput (images/sec) instead of the commonly used measurement of FLOPS (floating point operations per second) because we observed that FLOPS is not proportional to the inference speed of the actual GPU device.</p><p>Our contributions can be summarized as follows:</p><p>1. By organizing the existing CNN-related techniques for image classification, we find techniques that can be assembled into a single CNN. We then demonstrate that our resulting model surpasses the state-of-the-art models with similar accuracy in terms of mCE, mFR and throughput <ref type="table">(Table 1)</ref>.</p><p>2. We provide detailed experimental results for the process of assembling CNN techniques and release the code for accessibility and reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Before introducing our approach, we describe default experimental settings and evaluation metrics used in Sections 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Training Procedure</head><p>We use the official TensorFlow [1] ResNet 3 as base code. The ILSVRC2012 <ref type="bibr" target="#b30">[27]</ref> dataset is used to train and evaluate models. All models were trained on a single machine with 8 Nvidia Tesla P40 GPUs compatible with the CUDA 10 platform and cuDNN 7.6. TensorFlow version 1.14.0 was used.</p><p>The techniques proposed by He et al. <ref type="bibr" target="#b12">[9]</ref> are basically applied to all our models described in Section 3. We briefly describe the default hyperparameters and training techniques as follows.</p><p>Preprocessing In the training phase, a rectangular region is randomly cropped using a randomly sampled aspect ratio 3 https://github.com/tensorflow/models from 3/4 to 4/3, and the fraction of cropped area over whole image is randomly chosen from 5% to 100%. Then, the cropped region is resized to 224 × 224 and flipped horizontally with a random probability of 0.5 followed by the RGB channel normalization. During validation, shorter dimension of each image is resized to 256 pixels while the aspect ratio is maintained. Next, the image is center-cropped to 224 × 224, and the RGB channels are normalized.</p><p>Hyperparameter We use 1,024 batch size for training which is close to the maximum size that can be received by a single machine with 8 P40 GPUs. Stochastic gradient descent with momentum 0.9 is used as the optimizer. The initial learning rate is 0.4 and the weight decay is set to 0.0001. The default number of training epochs is 120, but some techniques require different number of epochs. This is explicitly specified when necessary.</p><p>Learning rate warmup If the batch size is large, a high learning rate may result in numerical instability. To prevent this, Goyal et al. <ref type="bibr" target="#b10">[7]</ref> proposes a warmup strategy that linearly increases the learning rate from 0 to the initial value. The warm-up period is set to the first 5 epochs.</p><p>Zero γ We initialize γ = 0 for all batch-norm layers that sit at the end of residual blocks. Therefore, all the residual blocks only return their shortcut branch results in the early stages of training. This has the effect of shrinking the entire layer at the initial stage and helps training.</p><p>Mixed-precision floating point We use mixed-precision floating point in the training phase because mixed-precision accelerates the overall training speed if the GPU supports it <ref type="bibr" target="#b25">[22]</ref>. However, this does not result in the improvement of top-1 accuracy.</p><p>Cosine learning rate decay The cosine decay schedule <ref type="bibr" target="#b23">[20]</ref> reduces the initial learning rate to close to 0 at the end of training by following a cosine curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluation Metrics</head><p>The selection of metrics used to measure the performance of the model is important because it indicates the di-rection in which the model is developed. We use the following three metrics as key indicators of model performance.</p><p>Top-1 The top-1 is a measure of classification accuracy on the ILSVRC2012 <ref type="bibr" target="#b30">[27]</ref> validation dataset. The validation dataset consists of 50,000 images of 1,000 classes.</p><p>Throughput Throughput is defined as how many images are processed per second on the GPU device. We measured inference throughput for an Nvidia P40 1 GPU. For comparison with other models, we used FP32 instead of FP16 in our experiments, using a batch size of 64. mCE and mFR The mean corruption error (mCE) and the mean flip rate (mFR) were proposed by Hendrycks et al. <ref type="bibr" target="#b13">[10]</ref> to measure the performance of the classification model on corrupted images and network stability on perturbations in image sequences, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Assembling CNN</head><p>In this section, we introduce various network tweaks and regularization techniques to be assembled, and describe the details of the implementation. We also perform preliminary experiments to study the effect of different parameter choices. ResNet-D ResNet-D is a minor adjustment to the vanilla ResNet network architecture model proposed by He et al. <ref type="bibr" target="#b12">[9]</ref>. It is known to work well in practice and has little impact to computational cost <ref type="bibr" target="#b12">[9]</ref>. Three changes are added to the ResNet model. First, the stride sizes of the first two convolutions in the residual path have been switched. Second, a 2 × 2 average pooling layer with a stride of 2 is added before the convolution in the skip connection path. Last, a large 7 × 7 convolution is replaced with three smaller 3 × 3 convolutions in the stem layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Tweaks</head><p>Channel Attention We examine two tweaks in relation to channel attention. First, Squeeze and Excitation (SE) network <ref type="bibr" target="#b16">[13]</ref> focuses on enhancing the representational capacity of the network by modeling channel-wise relationships. SE eliminates spatial information by global pooling to get channel information only, and then two fully connected layers in this module learn the correlation between channels. Second, Selective Kernel (SK) <ref type="bibr" target="#b22">[19]</ref> is inspired by the fact that the receptive sizes of neurons in the human visual cortex are different from each other. SK unit has multiple branches with different kernel sizes, and all branches are fused using softmax attention.</p><p>The original SK generates multiple paths with 3 × 3 and 5 × 5 convolutions, but we instead use two 3 × 3 convolutions to split the given feature map. This is because two convolutions of the same kernel size can be replaced with one convolution with twice as many channels, thereby lowering the inference cost. <ref type="figure" target="#fig_1">Figure 2</ref> shows an SK unit where the original two branches are replaced with one convolution operation.  <ref type="table">Table 2</ref>. Result of channel attention with different configurations. R50 is a simple notation for ResNet-50. r is the reduction ratio of SK in the Fuse operation. The piecewise learning rate decay is used in these experiments. <ref type="table">Table 2</ref> shows the results for different configurations of channel attention. Compared with SK, SE has higher throughput but lower accuracy (C1 and C2 in <ref type="table">Table 2</ref>). Between C3 and C2, the top-1 accuracy only differs by 0.08% (78.00% and 77.92%), but the throughput is significantly different (326 and 382). Considering this trade-off between accuracy and throughput, we decide to use one 3 × 3 kernel with doubled channel size instead of 3×3 and 5×5 kernels. Comparing C3 and C4, we see that changing the setting of reduction ratio r for SK units from 2 to 16 yields a large degradation of top-1 accuracy relative to the improvement of throughput. Applying both SE and SK (C5) not only decreases accuracy by 0.42% (from 77.92% to 77.50%), but also decreases inference throughput by 37 (from 382 to 345). Overall, for a better trade-off between top-1 accuracy and throughput, the configuration of C3 is preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exp</head><p>Anti-Alias Downsampling (AA) CNN models for image classification are known to be very vulnerable to small amounts of distortion <ref type="bibr" target="#b37">[34]</ref>. Zhang et al. <ref type="bibr" target="#b40">[37]</ref> proposes AA to improve the shift-equivariance of deep networks. The max-pooling is commonly viewed as a competing downsampling strategy, and is inherently composed of two operations. The first operation is to densely evaluate the max operator and second operation is naive subsampling <ref type="bibr" target="#b40">[37]</ref>. AA is proposed as a low-pass filter between them to achieve practical anti-aliasing in any existing strided layer such as strided-conv. The smoothing factor can be adjusted by changing the blur kernel filter size, where a larger filter size results in increased blur. In <ref type="bibr" target="#b40">[37]</ref>, AA is applied to max-pooling, projection-conv, and strided-conv of ResNet. <ref type="table">Table 3</ref> shows the experimental results for AA. We observe that reducing the filter size from 5 to 3 maintains the  top-1 accuracy while increasing inference throughput (A1 and A2 in <ref type="table">Table 3</ref>). However, removing the AA applied to the projection-conv does not affect the accuracy (A3). We also observe that applying AA to max-pooling degrades throughput significantly (A1, A2, and A3) compared to A4. Based on the result, we apply AA only to strided-conv in our model (Green box in <ref type="figure" target="#fig_0">Figure 1</ref>).  <ref type="table">Table 3</ref>. Results for downsampling with anti-aliasing. The performance of the model was tested with different configurations for downsampling with anti-aliasing. The piecewise learning rate decay is used in these experiments.</p><p>Big Little Network (BL) BigLittleNet <ref type="bibr" target="#b6">[3]</ref> applies multiple branches (Big-Branch and Little-Branch) with different res-olutions while aiming at reducing computational cost and increasing accuracy. The Big-Branch has the same structure as the baseline model and operates at a low image resolution, whereas the Little-Branch reduces the convolutional layers and operates at same image resolution as the baseline model. BigLittleNet has two hyperparameters, α and β, which adjust the width and depth of the Little-Branch, respectively. We use α = 2 and β = 4 for ResNet-50 and use α = 1 and β = 2 for ResNet-152. The upper small branch in <ref type="figure" target="#fig_0">Figure 1</ref> represents the Little-Branch. The Little-Branch has one residual block and is smaller in width than the main Big-Branch. Since BigLittleNet saves budget in computation, the models can be evaluated with a larger input image scale for better performance while maintaining similar throughput <ref type="bibr" target="#b6">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Regularization</head><p>AutoAugment (Autoaug) AutoAugment <ref type="bibr" target="#b7">[4]</ref> is a data augmentation procedure which learns augmentation strategies from data. It uses reinforcement learning to select a sequence of image augmentation operations with the best accuracy by searching a discrete search space of their probability of application and magnitude. We borrow the augmentation policy of Autoaug on ILSVRC2012 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Smoothing (LS)</head><p>In the classification problem, class labels are expressed as one hot encoding. If CNN is trained to minimize cross entropy with this one hot encoding tar-get, the logits of the last fully connected layer of CNN grow to infinity, which leads to over-fitting <ref type="bibr" target="#b12">[9]</ref>. Label smoothing <ref type="bibr" target="#b28">[25]</ref> suppresses infinite output and prevents over-fitting. We set the label smoothing factor ǫ to 0.1.</p><p>Mixup Mixup <ref type="bibr" target="#b39">[36]</ref> creates one example by interpolating two examples of the training set for data augmentation. Neural networks are known to memorize training data rather than generalize from the data <ref type="bibr" target="#b38">[35]</ref>. As a result, the neural network produces unexpected outputs when it encounters data which are different from the distribution of the training set. Mixup mitigates the problem by showing the neural network interpolated examples, and this helps to fill up the empty feature space of the training dataset. Mixup has two types of implementation. The first type uses two mini batches to create a mixed mini batch. this type of implementation is suggested in the original paper <ref type="bibr" target="#b39">[36]</ref>. The second type uses a single mini batch to create the mixed mini batch by mixing the single mini batch with a shuffled clone of itself. The second type of implementation uses less CPU resources because only one mini batch needs to be preprocessed to create one mixed mini batch. However, experiments show that the second type of implementation reduces top-1 accuracy <ref type="table" target="#tab_4">(Table 4</ref>). Therefore, in later experiments, we use the first type of implementation. We set the Mixup hyperparameter α to 0.2.</p><p>DropBlock Dropout <ref type="bibr" target="#b32">[29]</ref> is a popular technique for regularizing deep neural networks. It prevents the network from being over-fitted to the training set by dropping neurons at random. However, Dropout does not work well for extremely deep networks such as ResNet <ref type="bibr" target="#b9">[6]</ref>. DropBlock <ref type="bibr" target="#b9">[6]</ref> can remove specific semantic information by dropping a continuous region of activation. Thus, it is efficient for the regularization of very deep networks. We borrow the same DropBlock setting used in the original paper <ref type="bibr" target="#b9">[6]</ref>. We apply DropBlock to Stage 3 and 4 of ResNet-50 and linearly decay the keep_prob hyperparameter from 1.0 to 0.9 during training.</p><p>Knowledge Distillation (KD) Knowledge Distillation <ref type="bibr" target="#b14">[11]</ref> is a technique for transferring knowledge from one neural network (teacher) to another (student). Teacher models are often complex with high accuracy, and a weak but light student model can improve its own accuracy by mimicking a teacher model. The T hyperparameter of KD was said to be optimal when set to 2 or 3 in the original paper <ref type="bibr" target="#b14">[11]</ref>, but we use T =1 for our model. Because our model uses Mixup and KD techniques together, the teacher network should also be applied to Mixup. This leads to better performance at lower temperatures because the teacher's signal itself is already smoothed by the Mixup <ref type="table">(Table 5</ref>). We used EfficientNet B7 as a teacher with 84.5% of ILSVRC2012 validation top-1 accuracy. In addition, the logits of the teacher were not computed during the training time, but computed offline before training. The saved teacher logits were then used during training. Although this offline implementation of KD cannot calculate the teacher logits of augmented data (e.g. AutoAugment) during training time, it worked well in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Configuration Top-1 R50D+SK LS+Mixup+DropBlock 81.40 R50D+SK LS+Mixup+DropBlock+KD <ref type="table">(T =2)</ref> 81.47 R50D+SK LS+Mixup+DropBlock+KD (T =1.5) 81.50 R50D+SK LS+Mixup+DropBlock+KD (T =1) 81.69 <ref type="table">Table 5</ref>. Result of the change of KD temperature. We apply KD by varying the temperature T to find the optimal T value. We choose T = 1 for next experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study on ResNet</head><p>In this section we will describe ablation study for assembling the individual network tweaks covered in Section 3.1 to find a better model. The results are shown in <ref type="table">Table 6</ref> Adding ResNet-D to the baseline model improves top-1 accuracy by 0.5% from 76.87% to 77.37% (T1 in <ref type="table">Table 6</ref>), and adding SK tweaks improves accuracy by 1.46% from 77.37% to 78.83% (T2). In <ref type="table">Table 2</ref>, We show that the accuracy is increased by 1.62% when SK is independently applied to ResNet from 76.30% to 77.92%. Stacking ResNet- The ablation study in <ref type="table" target="#tab_6">Table 7</ref> shows the impact of assembling the regularization techniques described in Section 3.2. The regularization techniques increase accuracy, mCE and mFR altogether, but the performance improvement of mCE and mFR is greater than the improvement of accuracy (E2, 3, 5, 7, and 11). It can be seen that regularization helps to make CNNs more robust to image distortions. Adding SE improves top-1 accuracy by 0.61% and improves mCE by 3.71% (E4). We confirm that channel attention is also helpful for robustness to image distortions. Replacing SE with SK improves top-1 and mCE by 1.0% and 4.3% (E6). In Table 2, when SE is changed to SK without regularization, the accuracy increases by 0.5%. With regularization, replacing SE with SK nearly doubles the accuracy improvement (E5 and E6). This means that SK is more complementary for regularization techniques than SE.</p><p>Changing the epochs from 270 to 600 improves performance (E8). Because data augmentation and regularization are stacked, they have a stronger effect of regularization, so longer epochs seems to yield better generalization performance. BL shows a performance improvement not only on top-1, but also on mCE and mFR without inference throughput loss (E9). AA also shows higher performance gain in mCE and mFR relative to top-1 (E10), which agrees with AA being used as a network tweak to make the CNN robust for image translations as claimed in <ref type="bibr" target="#b40">[37]</ref>.</p><p>The assembled model of all the techniques described so far has top-1 accuracy of 82.78%, mCE of 48.89% and mFR of 32.31%. This final model is listed in <ref type="table" target="#tab_6">Table 7</ref> as E11, and we call this model Assemble-ResNet-50. We also experiment with ResNet-152 for comparison as E12, we call this model Assemble-ResNet-152.</p><p>To further show that the boosted performance of the proposed ResNet-50 is not mainly due to the increase in network parameters, we compared ResNet-50 with network tweaks and regularizations (E9 in  <ref type="table">Table 8</ref>. Performance comparison among "ResNet-50D+SK+BL+Regularization", "ResNet-101+Regularization" and "ResNet-152+Regularization" on ILSVRC2012 dataset.</p><p>eters. As shown in <ref type="table">Table 8</ref>, ResNet-50 with network tweaks and regularizations shows approximately 1.2% better performance in top-1, and 2% in mCE compared to ResNet101 with regularizations, while having less parameters and FLOPS. Moreover, ResNet-50 with network tweaks and regularizations outperforms ResNet-152 with regularizations which have far larger parameters and FLOPS. These observations prove that the combination of network tweaks and regularizations in ResNet-50 creates a synergistic effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study on MobileNet</head><p>In this section, the results of applying CNN-related techniques to MobileNet-V1 <ref type="bibr" target="#b15">[12]</ref> are presented. MobileNet-V1, as its name suggests, is a baseline CNN model for use in mobile edge-devices. To follow the design principle of MobileNet, which prioritizes inference speed, we applied the aforementioned techniques such that the reduction in throughput is minimized. Therefore, among network tweaks, only SE was applied to MobileNet-V1 and boosted the accuracy by 1.69 % (M0, M1). The top-1 accuracy gain of using SE-MobileNet-V1 together with LS+Mixup+KD was 2.05% more than that of vanilla MobileNet-V1 with the same regularizations applied (M5, M6). In other words, the synergistic effect of using network tweaks and regularizations is also demonstrated in mobile-oriented models. Based on this, we reduced the reduction ratio r of the SE block from 16 to 2 to maximize synergy between network tweaks and regularization. By doing so, we could improve MobileNet-V1's top-1 accuracy by 1% with minimal throughput loss. However, unlike ResNet, the top-1 accuracy of SE-MobileNet-V1 decreased when DropBlock was applied. As the network capacity of MobileNet-V1 backbone is smaller than that of ResNet, more training epoch and the adjustment of keep_prob hyperparameter (from 1.0-0.9 to 1.0-0.95) are needed for DropBlock regularization to have a sufficient effect in MobileNet-V1 model as that in ResNet (M8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer Learning: FGVC</head><p>In this section, we investigate whether the improvements discussed so far can help with transfer learning. We first analyzed the contribution of transfer learning for each technique. An ablation study was performed on the Food-101 <ref type="bibr" target="#b5">[2]</ref> dataset, which is the largest public fine-grained visual classification (FGVC) dataset. The basic experiment setup and hyperparameters that differ from the backbone training are described in supplementary material.</p><p>As shown in <ref type="table">Table 10</ref>, stacking network tweaks and regularization techniques steadily improved both top-1 accuracy and mCE for the transfer learning task on the Food-101 dataset. In particular, comparing the experiments F4-F8 with experiments F9-F13 (in <ref type="table">Table 10</ref>) shows the effect of regularization on the backbone. We use the same network structure in F4-F13, but for F9-F13, they have regularization such as Mixup, DropBlock, KD and Autoaug on the backbone. This regularization of the backbone gives performance improvements for top-1 accuracy as expected. On the other hand, the aspect of mCE performance differed from the top-1 accuracy. Without regularization during finetuning such as in F4 and F9, the backbone with regularization leads to better mCE performance than backbone without regularization. However, adding regularization during fine-tuning narrows the mCE performance gap (F5-8 and F10-13). For convenience, we call the final F13 model in <ref type="table">Table 10</ref> as Assemble-ResNet-FGVC-50.</p><p>We also evaluated Assemble-ResNet-FGVC-50 in Table 10 on the following datasets: CARS196 (Stanford Cars) <ref type="bibr" target="#b20">[17]</ref>, Oxford 102 Flowers <ref type="bibr" target="#b26">[23]</ref>, FGVC-Aircraft <ref type="bibr" target="#b24">[21]</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b27">[24]</ref> and Food-101 <ref type="bibr" target="#b5">[2]</ref>. The statistics for each dataset are as shown in supplementary material. <ref type="table">Table 11</ref> shows the transfer learning performance. Compared to EfficientNet <ref type="bibr" target="#b35">[32]</ref> and AmoebaNet-B <ref type="bibr" target="#b18">[15]</ref> which are state-of-the-art models for image classification task, our Assemble-ResNet-FGVC-50 model achieves comparable accuracy with 20x faster inference throughput.  <ref type="table">Table 9</ref>. Ablation study for assembling the network tweaks and regularization with MobileNet on ILSVRC2012 dataset. In order to measure throughput, we use the standard TFLite Benchmark Tool. We measure the floating point and quantized performance using single-threaded large core of Google Pixel 3 with batch size 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exp. No.</head><p>Backbone  <ref type="table">Table 10</ref>. Ablation study of transfer learning with the Food-101 dataset. REG means that regularization techniques "LS+Mixup+DropBlock+KD+Autoaug" are applied during training backbone. The Food-101 mCE is not normalized by AlexNet's errors. We use the augmentation policy which is found by Autoaug on CIFAR-10 in these experiments <ref type="bibr" target="#b7">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Transfer Learning: Image Retrieval</head><p>We also conducted an ablation study on three public finegrained image retrieval (IR) datasets: Stanford Online Products (SOP) <ref type="bibr" target="#b31">[28]</ref>, CUB200 <ref type="bibr" target="#b36">[33]</ref> and CARS196 <ref type="bibr" target="#b20">[17]</ref>. We borrowed the zero-shot data split protocol from <ref type="bibr" target="#b31">[28]</ref>.</p><p>On top of that, cosine-softmax based losses were used for image retrieval. In this work, we use ArcFace <ref type="bibr" target="#b8">[5]</ref> loss with a margin of 0.3 and use generalized mean-pooling (GeM) <ref type="bibr" target="#b29">[26]</ref> for a pooling method without performing downsampling at Stage 4 of backbone networks because it has better performance for the image retrieval task. The basic experiment setup and hyperparameters are described in supplementary material.</p><p>In the case of SOP, the degree of the effect was examined by an ablation study with the results listed in <ref type="table">Table 12</ref>.</p><p>The particular combinations of network tweaks and regularizations that worked well on the SOP dataset were different from that for FGVC datasets. Comparing S2-4, we see that BL and AA did not work well on the SOP dataset. Among the regularizers, DropBlock works well, but Autoaug does not improve the recall at 1 performance (S2 and S5,6). Nevertheless, in the best configuration, there was a significant performance improvement of 3.0% compared to the baseline ResNet-50. The recall at 1 results for image retrieval datasets are reported in <ref type="table">Table 13</ref>. There is also a significant performance improvement on CUB200 and CARS196 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>The state-of-the-art Models ResNet-50 Assemble-ResNet-FGVC-50</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Food-101</head><p>EfficientNet B7 <ref type="bibr" target="#b35">[32]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we show that assembling various techniques for CNNs to single convolutional networks leads to improvements of top-1 accuracy, mCE and mFR on the ILSVRC2012 validation dataset. Synergistic effects have been achieved by using a variety of network tweaks and regularization techniques together in a single network. Our approach has also improved performance consistently on transfer learning such as FGVC and image retrieval tasks. More excitingly, our network is not frozen, but is still evolving, and can be further developed with future research. We expect that there will be further improvements if we change the vanilla backbone to a more powerful backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. FLOPS and throughput</head><p>We observe in several experiments that FLOPS is not proportional to the inference speed of the actual GPU. FLOPS and throughput for variations of EfficientNet <ref type="bibr" target="#b35">[32]</ref> and ResNet <ref type="bibr" target="#b11">[8]</ref> are described in <ref type="table" target="#tab_4">Table 14</ref>. For example, FLOPS of EfficientNet B0 is very small compared to that of ResNet-50, but throughput is rather lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Resolution FLOPS Throughput  <ref type="table" target="#tab_4">Table 14</ref>. FLOPS and throughput for variation of EfficientNet and ResNet. We use the TensorFlow official profiler code to measure FLOPS. EfficientNet's FLOPS is borrowed from <ref type="bibr" target="#b35">[32]</ref>. We measured inference throughput for an Nvidia P40 single GPU using a batch size of 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FGVC Task Configuration</head><p>In this section, we will describe experimental configurations for public fine-grained visual classification (FGVC) datasets: Food-101 <ref type="bibr" target="#b5">[2]</ref>, CARS196 <ref type="bibr" target="#b20">[17]</ref>, Oxford 102 Flowers <ref type="bibr" target="#b26">[23]</ref>, Oxford-IIIT Pets <ref type="bibr" target="#b27">[24]</ref> and FGVC-Aircraft <ref type="bibr" target="#b24">[21]</ref>.</p><p>The basic experimental setup and hyperparameters that differ from the backbone training are described as follows.</p><p>• Initial learning rate is 0.01.</p><p>• Weight decay is set to 0.001.</p><p>• Momentum for BN is set to (max(1 − 10/s, 0.9)).</p><p>• Keep probability of DropBlock starts at 0.9 and decreases linearly to 0.7 at the end of training We use the same hyperparameters for all datasets for transfer learning except for training epochs. The training epochs for each dataset are described in <ref type="table" target="#tab_12">Table 15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FGVC Dataset Training Epochs</head><p>Food-101 100 <ref type="bibr">CARS196</ref> 1,000 Oxford-Flowers 1,000 FGVC Aircraft 800 Oxford-IIIT Pets 1,300  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. IR Task Configuration</head><p>In this section, we will describe experimental configurations for three public fine-grained image retrieval (IR) datasets: Stanford Online Products (SOP) <ref type="bibr" target="#b31">[28]</ref>, CUB200 <ref type="bibr" target="#b36">[33]</ref> and CARS196 <ref type="bibr" target="#b20">[17]</ref>. The basic experimental setup and hyperparameters are described as follows.</p><p>• Image preprocessing resizes to 224 × 224 without maintaining aspect ratio with probability 0.5 and resizes to 256 × 256 and random crop to 224 × 224 with probability 0.5.</p><p>• Data augmentation includes random horizontal flip with 0.5 probability.</p><p>• Momentum for BN is set to max(1 − 10/s, 0.9).</p><p>• Weight decay is set to 0.0005.</p><p>• Feature size is set to 1536.</p><p>• The training epoch, batch size, learning rate decay and assembling configuration vary for each dataset.</p><p>The different parameter settings for each dataset are described in <ref type="table" target="#tab_6">Table 17</ref>. The best configurations for each dataset are specified.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>shows the overall flow of our final ResNet model. Various network tweaks are applied to vanilla ResNet. The network tweaks we use are as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Modified SK Unit. We use one 3 × 3 kernel with doubled output channel size instead of 5 × 5 and 3 × 3 kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>The training epoch varies for each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Output Skip Conn. +ResNet-D +SK +AA +DropBlock +ResNet-D +SK +AA +ResNet-D +SK +ResNet-D +SK +AA +DropBlock Down Sampling Block</head><label></label><figDesc></figDesc><table><row><cell cols="2">Little Branch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">from BigLittleNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Residual</cell><cell></cell><cell></cell><cell cols="2">Residual</cell><cell></cell><cell></cell><cell>Residual</cell><cell></cell></row><row><cell></cell><cell>Block</cell><cell></cell><cell></cell><cell>Block</cell><cell></cell><cell></cell><cell></cell><cell>Block</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Stage 1</cell><cell></cell><cell></cell><cell cols="2">Stage 2</cell><cell></cell><cell></cell><cell>Stage 3</cell><cell>Stage 4</cell></row><row><cell>Stem Layer</cell><cell>Down Sampling Block</cell><cell>Residual Blocks Residual Blocks</cell><cell cols="2">Down Sampling Block (+Anti Aliasing)</cell><cell cols="2">Residual Blocks Residual Blocks</cell><cell cols="2">Down Sampling Block (+Anti Aliasing)</cell><cell>Residual Blocks Residual Blocks</cell><cell>Down Sampling Block (+Anti Aliasing)</cell><cell>Blocks Blocks Residual Residual</cell><cell>FC Layer</cell></row><row><cell></cell><cell></cell><cell>Conv</cell><cell>Drop</cell><cell>SK Conv</cell><cell>Drop</cell><cell></cell><cell>Anti Aliasing</cell><cell>Conv</cell><cell>Drop</cell></row><row><cell></cell><cell></cell><cell>(1x1)</cell><cell>Block</cell><cell>(3x3)</cell><cell>Block</cell><cell cols="2">Downsampling</cell><cell>(1x1)</cell><cell>Block</cell></row><row><cell></cell><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>AvgPooling</cell><cell>Conv</cell><cell>Drop</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(2x2, S=2)</cell><cell>(1x1)</cell><cell>Block</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3x3 kernel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Fuse Op.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Figure 1. Assembling techniques into ResNet. We apply network tweaks such as ResNet-D, SK, Anti-alias, DropBlock, and BigLittleNet to vanilla ResNet. In more detail, ResNet-D and SK are applied to all blocks in all stages. Downsampling with anti-aliasing is only applied to the downsampling block from Stage 2 to Stage 4. DropBlock is applied to all blocks in Stage 3 and Stage 4. Little-Branch from BigLittleNet uses one residual block with smaller width.Softmax s z</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Configuration</cell><cell>Top-1</cell></row><row><cell>R50D</cell><cell>LS</cell><cell>77.37</cell></row><row><cell>R50D</cell><cell cols="2">LS + Mixup (type2) 78.85</cell></row><row><cell>R50D</cell><cell cols="2">LS + Mixup (type1) 79.10</cell></row></table><note>. Result of different Mixup implementation types.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Exp. No.</cell><cell>Model</cell><cell>Input Size</cell><cell cols="2">Top-1 Throughput</cell></row><row><cell>T0</cell><cell>R50 (baseline)</cell><cell>224</cell><cell>76.87</cell><cell>536</cell></row><row><cell>T1</cell><cell>R50D</cell><cell>224</cell><cell>77.37</cell><cell>493</cell></row><row><cell>T2</cell><cell>R50D+SK</cell><cell>224</cell><cell>78.83</cell><cell>359</cell></row><row><cell>T3</cell><cell>R50D+SK+BL</cell><cell>224</cell><cell>78.26</cell><cell>445</cell></row><row><cell>T4</cell><cell>R50D+SK+BL</cell><cell>256</cell><cell>79.27</cell><cell>359</cell></row><row><cell>T5</cell><cell>R50D+SK+BL+AA</cell><cell>256</cell><cell>79.39</cell><cell>312</cell></row><row><cell cols="5">Table 6. Performance comparison of stacking network tweaks.</cell></row><row><cell cols="5">By stacking the ResNet-D, Selective Kernel (SK), BigLittleNet</cell></row><row><cell cols="5">(BL) and downsampling with anti-aliasing (AA), we have steadily</cell></row><row><cell cols="5">improved the ResNet-50 model with some inference throughput</cell></row><row><cell cols="5">losses. The focus of each experiment is highlighted in boldface.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>Exp. No.</cell><cell>Model</cell><cell>Regularization Configuration</cell><cell>Train Epoch</cell><cell>Input Size</cell><cell cols="4">Top-1 mCE mFR Throughput</cell></row><row><cell></cell><cell>EfficientNet B0 [32]</cell><cell>Autoaug</cell><cell>-</cell><cell>224</cell><cell>77.3</cell><cell>70.7</cell><cell>-</cell><cell>510</cell></row><row><cell></cell><cell>EfficientNet B1 [32]</cell><cell>Autoaug</cell><cell>-</cell><cell>240</cell><cell>79.2</cell><cell>65.1</cell><cell>-</cell><cell>352</cell></row><row><cell></cell><cell>EfficientNet B2 [32]</cell><cell>Autoaug</cell><cell>-</cell><cell>260</cell><cell>80.3</cell><cell>64.1</cell><cell>-</cell><cell>279</cell></row><row><cell></cell><cell>EfficientNet B3 [32]</cell><cell>Autoaug</cell><cell>-</cell><cell>300</cell><cell>81.7</cell><cell>62.9</cell><cell>-</cell><cell>182</cell></row><row><cell></cell><cell>EfficientNet B4 [32]</cell><cell>Autoaug</cell><cell>-</cell><cell>380</cell><cell>83.0</cell><cell>60.7</cell><cell>-</cell><cell>95</cell></row><row><cell></cell><cell>EfficientNet B5 [32]</cell><cell>Autoaug</cell><cell>-</cell><cell>456</cell><cell>83.7</cell><cell>62.3</cell><cell>-</cell><cell>49</cell></row><row><cell></cell><cell>EfficientNet B6 [32]</cell><cell>Autoaug</cell><cell>-</cell><cell>528</cell><cell>84.2</cell><cell>60.6</cell><cell>-</cell><cell>28</cell></row><row><cell></cell><cell>EfficientNet B7 [32]</cell><cell>Autoaug</cell><cell>-</cell><cell>600</cell><cell>84.5</cell><cell>59.4</cell><cell>-</cell><cell>16</cell></row><row><cell>E0</cell><cell>R50 (baseline)</cell><cell></cell><cell>120</cell><cell>224</cell><cell cols="3">76.87 75.55 56.55</cell><cell>536</cell></row><row><cell>E1</cell><cell>R50D</cell><cell></cell><cell>120</cell><cell>224</cell><cell cols="3">77.37 75.73 58.17</cell><cell>493</cell></row><row><cell>E2</cell><cell>R50D</cell><cell>LS</cell><cell>120</cell><cell>224</cell><cell cols="3">78.35 74.27 54.75</cell><cell>493</cell></row><row><cell>E3</cell><cell>R50D</cell><cell>LS+Mixup</cell><cell>200</cell><cell>224</cell><cell cols="3">79.10 68.19 51.24</cell><cell>493</cell></row><row><cell>E4</cell><cell>R50D+SE</cell><cell>LS+Mixup</cell><cell>200</cell><cell>224</cell><cell cols="3">79.71 64.48 47.47</cell><cell>420</cell></row><row><cell>E5</cell><cell>R50D+SE</cell><cell>LS+Mixup+DropBlock</cell><cell>270</cell><cell>224</cell><cell cols="3">80.40 62.64 42.34</cell><cell>420</cell></row><row><cell>E6</cell><cell>R50D+SK</cell><cell>LS+Mixup+DropBlock</cell><cell>270</cell><cell>224</cell><cell cols="3">81.40 58.34 39.61</cell><cell>359</cell></row><row><cell>E7</cell><cell>R50D+SK</cell><cell>LS+Mixup+DropBlock+KD</cell><cell>270</cell><cell>224</cell><cell cols="3">81.69 57.08 38.15</cell><cell>359</cell></row><row><cell>E8</cell><cell>R50D+SK</cell><cell>LS+Mixup+DropBlock+KD</cell><cell>600</cell><cell>224</cell><cell cols="3">82.10 56.48 37.43</cell><cell>359</cell></row><row><cell>E9</cell><cell>R50D+BL+SK</cell><cell>LS+Mixup+DropBlock+KD</cell><cell>600</cell><cell>256</cell><cell cols="3">82.44 55.20 37.24</cell><cell>359</cell></row><row><cell>E10</cell><cell>R50D+BL+SK+AA</cell><cell>LS+Mixup+DropBlock+KD</cell><cell>600</cell><cell>256</cell><cell cols="3">82.69 54.12 36.81</cell><cell>312</cell></row><row><cell>E11</cell><cell>R50D+BL+SK+AA</cell><cell>LS+Mixup+DropBlock+KD+Autoaug</cell><cell>600</cell><cell>256</cell><cell cols="3">82.78 48.89 32.31</cell><cell>312</cell></row><row><cell>E12</cell><cell cols="2">R152D+BL+SK+AA LS+Mixup+DropBlock+KD+Autoaug</cell><cell>600</cell><cell>256</cell><cell cols="3">84.19 43.27 29.34</cell><cell>143</cell></row><row><cell cols="3">D and SK increases the top-1 accuracy gain almost in equal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">measure to the sum of the performance gains of applying</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ResNet-D and SK separately. The results show that the two</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tweaks can improve performance independently with little</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">effect on each other. Applying BL to R50D+SK decreases</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">top-1 accuracy from 78.83% to 78.26% , but throughput is</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">increased from 359 to 445 (T3). To achieve higher accu-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">racy by 0.44% while maintaining throughput similar to that</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">of the R50D+SK, we use 256 × 256 image resolution for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">inference, whereas we use 224 × 224 image resolution for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">training (T4). Applying AA to the R50D+SK+BL improves</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">top-1 accuracy by 0.12% from 79.27% to 79.39% and de-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">creases throughput by 47 from 359 to 312 (T5).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>. Ablation study for assembling the network tweaks and regularizations with ResNet-50 on ILSVRC2012 dataset. The top-1 accuracy and mCE scores for EfficientNet are borrowed from the official code in [16] and [34] respectively. As with other experiments, the inference throughput measurements of EfficientNet were performed on a single Nvidia P40 using official EfficientNet code [16].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 )</head><label>7</label><figDesc>to ResNet-101 with regularizations with a similar number of param-</figDesc><table><row><cell>Model</cell><cell cols="6">Regularization Configuration Input Size Top-1 mCE FLOPS Params Throughput</cell></row><row><cell>R50 (baseline)</cell><cell></cell><cell>224</cell><cell>76.87 75.55</cell><cell>4.1B</cell><cell>25.5M</cell><cell>536</cell></row><row><cell>R101</cell><cell></cell><cell>224</cell><cell>78.35 71.32</cell><cell>7.9B</cell><cell>44.6M</cell><cell>330</cell></row><row><cell>R152</cell><cell></cell><cell>224</cell><cell>78.51 68.95</cell><cell>11.6B</cell><cell>60.2M</cell><cell>233</cell></row><row><cell>R50D+SK+BL</cell><cell></cell><cell>256</cell><cell>79.27 67.59</cell><cell>5.4B</cell><cell>41.8M</cell><cell>359</cell></row><row><cell>R101</cell><cell>LS+Mixup+DropBlock+KD</cell><cell>224</cell><cell>81.26 57.26</cell><cell>7.9B</cell><cell>44.6M</cell><cell>330</cell></row><row><cell>R152</cell><cell>LS+Mixup+DropBlock+KD</cell><cell>224</cell><cell>81.96 54.99</cell><cell>11.6B</cell><cell>60.2M</cell><cell>233</cell></row><row><cell cols="2">R50D+SK+BL LS+Mixup+DropBlock+KD</cell><cell>256</cell><cell>82.44 55.20</cell><cell>5.4B</cell><cell>41.8M</cell><cell>359</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .Table 12 .Table 13 .</head><label>111213</label><figDesc>Transfer learning results for FGVC. Numbers in the table indicate top-1 accuracy. Ablation study of transfer learning with SOP dataset. REG means "LS+Mixup+DropBlock+KD". Transfer learning for IR task with our method. Assemble-ResNet-IR-50 represents the best configuration model for each dataset. The best configurations for each dataset are specified in supplementary material. Numbers in the table indicate re-call@1.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>93.0</cell><cell>87.0</cell><cell>92.5</cell></row><row><cell></cell><cell cols="2">CARS196</cell><cell></cell><cell>EfficientNet B7 [32] 94.7</cell><cell>89.1</cell><cell>94.4</cell></row><row><cell></cell><cell cols="3">Oxford-Flowers</cell><cell>EfficientNet B7 [32] 98.8</cell><cell>96.1</cell><cell>98.9</cell></row><row><cell></cell><cell cols="3">FGVC Aircraft</cell><cell>EfficientNet B7 [32] 92.9</cell><cell>78.8</cell><cell>92.4</cell></row><row><cell></cell><cell cols="4">Oxford-IIIT Pets AmoebaNet-B [15]</cell><cell>95.9</cell><cell>92.5</cell><cell>94.3</cell></row><row><cell>Exp. No.</cell><cell cols="2">Backbone</cell><cell cols="2">Regularization</cell><cell>Recall@1</cell></row><row><cell>S0</cell><cell>R50 (baseline)</cell><cell></cell><cell></cell><cell>82.9</cell></row><row><cell>S1</cell><cell>R50D</cell><cell></cell><cell></cell><cell>84.2</cell></row><row><cell>S2</cell><cell>R50D+SK</cell><cell></cell><cell></cell><cell>85.4</cell></row><row><cell>S3</cell><cell cols="2">R50D+SK+BL</cell><cell></cell><cell>85.2</cell></row><row><cell>S4</cell><cell cols="2">R50D+SK+BL+AA</cell><cell></cell><cell>85.1</cell></row><row><cell>S5</cell><cell>R50D+SK</cell><cell></cell><cell cols="2">DropBlock</cell><cell>85.9</cell></row><row><cell>S6</cell><cell>R50D+SK</cell><cell></cell><cell cols="2">DropBlock+Autoaug</cell><cell>83.7</cell></row><row><cell>S7</cell><cell cols="2">R50D+SK + REG</cell><cell></cell><cell>85.2</cell></row><row><cell>S8</cell><cell cols="2">R50D+SK + REG</cell><cell cols="2">DropBlock</cell><cell>85.9</cell></row><row><cell>S9</cell><cell cols="2">R50D+SK + REG</cell><cell cols="2">DropBlock+Autoaug</cell><cell>84.0</cell></row><row><cell></cell><cell cols="4">Dataset ResNet-50 Assemble-ResNet-IR-50</cell></row><row><cell></cell><cell>SOP</cell><cell>82.9</cell><cell></cell><cell>85.9</cell></row><row><cell></cell><cell>CUB200</cell><cell>75.9</cell><cell></cell><cell>80.3</cell></row><row><cell></cell><cell>CARS196</cell><cell>92.9</cell><cell></cell><cell>96.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 15 .</head><label>15</label><figDesc>Training configuration of FGVC datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Train Size Test Size # Classes</cell></row><row><cell>Food-101</cell><cell>75,750</cell><cell>25,250</cell><cell>101</cell></row><row><cell>CARS196</cell><cell>8,144</cell><cell>8,041</cell><cell>196</cell></row><row><cell>Oxford-Flowers</cell><cell>2,040</cell><cell>6,149</cell><cell>102</cell></row><row><cell>FGVC Aircraft</cell><cell>6,667</cell><cell>3,333</cell><cell>100</cell></row><row><cell>Oxford-IIIT Pets</cell><cell>3,680</cell><cell>3,669</cell><cell>37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 16 .</head><label>16</label><figDesc>Statistics of FGVC datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 17 .</head><label>17</label><figDesc>Different hyperparameter settings for IR tasks.</figDesc><table><row><cell>Dataset</cell><cell>Loss Fuction</cell><cell>Learning rate</cell><cell>Batch size</cell><cell>Training Epochs</cell></row><row><cell>SOP</cell><cell>Arcface</cell><cell>0.008</cell><cell>128</cell><cell>60</cell></row><row><cell>CUB200</cell><cell>Softmax</cell><cell>0.001</cell><cell>32</cell><cell>100</cell></row><row><cell cols="2">CARS196 Softmax</cell><cell>0.01</cell><cell>32</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 18 .</head><label>18</label><figDesc>Model configuration for IR tasks. REG means "LS+Mixup+DropBlock+KD"</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/tensorflow/models/tree/master/research/autoaugment</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">M1 SE-MobileNet-V1 (r=16) (baseline)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">M6 SE-MobileNet-V1 (r=16)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">M7 SE-MobileNet-V1 (r=2)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">M8 SE-MobileNet-V1 (r=2)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Big-little net: An efficient multi-scale feature representation for visual and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mallinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03848</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<ptr target="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet" />
	</analytic>
	<monogr>
		<title level="j">Google Inc. Efficientnet official code</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Ganesh Venkatesh, et al. Mixed precision training</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Finetuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09665</idno>
		<title level="m">Adversarial examples improve image recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
