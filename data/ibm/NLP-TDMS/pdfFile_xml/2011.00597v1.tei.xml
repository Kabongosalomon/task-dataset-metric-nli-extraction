<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ging</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland Baltimore County</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at https://github.com/gingsi/coot-videotext</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representation learning based on both vision and language has many potential benefits: visual grounding <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>; visual learning with a more natural, almost self-supervised annotation process; and direct applicability to cross-modal tasks, such as video retrieval by text <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, video summarization <ref type="bibr" target="#b9">[10]</ref>, and automated indexing. This research direction has recently boomed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>] also due to the success of self-attention in text analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> with its almost immediate applicability in the cross-modal context. Many different research foci are currently developing in this area, where some are concerned with large-scale pretraining to leverage the abundant data available <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> to learn a joint embedding space, and others to bring in more explicit structure <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> or new losses <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref> into the learning process.</p><p>In this paper, we focus on long-range temporal dependencies and propose a hierarchical model that can exploit long-range temporal context both in videos and text when learning the joint cross-modal embedding. For instance, the action of "making tea" involves boiling water, pouring it into a cup, and then adding a tea bag. This action can take a long time and may have lots of details that distinguish a particular style of making tea from other styles. To capture the whole temporal context, we leverage the idea of a hierarchical model with losses that enforce the interaction within and between different hierarchy levels. The idea of such a hierarchy is generic and has been explored by several works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> in the context of video-text learning. In addition, we use alignment losses from Zhang et al. <ref type="bibr" target="#b20">[21]</ref> and extend our baseline model with a new feature aggregation method for the intra-level interactions between features and a new transformer-based module for inter-level interactions (between local and global semantics). We consider three different levels of hierarchy: frame/word, clip/sentence and video/paragraph, visualized by the three blocks in <ref type="figure" target="#fig_0">Figure 1</ref>. To model intra-level cooperation, we introduce an attention-aware feature aggregation layer to focus on temporal interactions between low-level entities <ref type="figure" target="#fig_0">(Figure 1</ref>-Attention-FA).</p><p>This component replaces traditional sequence representation aggregation methods in transformers such as using a <ref type="bibr">[CLS]</ref> token <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref> or mean pooling <ref type="bibr" target="#b24">[25]</ref> with an attention-aware fusion. It leverages temporal context to encourage important entities to contribute more to the final representation of a sequence of frames or words.</p><p>For the inter-level cooperation, we introduce a contextual attention module, which enforces the network to highlight semantics relevant to the general context of the video and to suppress the irrelevant semantics. This is done by modeling the interaction between low-level (clips-sentences) and high-level entities (global contexts), as shown in <ref type="figure" target="#fig_0">Figure 1</ref>-green region.</p><p>In addition to this architectural contributions, we introduce a new cross-modal cycle-consistency loss to enforce interaction between modalities and encourage the semantic alignment between them in the learned common space. We show that enforcing two domains to produce consistent representations leads to substantially improved semantic alignment.</p><p>In summary, this paper contributes:</p><p>• a hierarchical transformer architecture with a new attention-aware feature aggregation layer and a new contextual attention module; • a cross-modal cycle-consistency loss that encourages semantic alignment between vision and text features in the joint embedding space; • state-of-the-art results on video-text retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Cooperative Hierarchical Transformer</head><p>Videos and text descriptions naturally involve different levels of granularity. Every paragraph contains multiple sentences, and each sentence is composed of several words. Similarly, videos have a hierarchical semantic structure, even if it is not as exactly defined as for text. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the overview of the COOT model which consists of three levels: 1) A temporal transformer that captures the relationships between frame/word features, 2) attention-aware feature aggregation to produce clip/sentence features (Section 2.2) and 3) a contextual transformer to produce final video and text embeddings (Sec. 2.3). We use alignment losses from Zhang et al. <ref type="bibr" target="#b20">[21]</ref> to align representations at different granularity levels. In addition, we introduce a new cross-model cycle-consistency loss to connect video and text (Sec. 3). In this section, we briefly summarize the alignment losses from Zhang et al. <ref type="bibr" target="#b20">[21]</ref> and the standard transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Semantic Alignment Losses. For the video-text alignment, Zhang et al. <ref type="bibr" target="#b20">[21]</ref> leverage a contrastive loss to enforce the positive samples to stay in a close neighborhood and negative samples far apart <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. Assuming the positive pair P = (x, y), two negative pairs (x, y ) and (x , y) expressed as N = {(x, y ), (x , y)}, and a margin α, they define the following loss:</p><formula xml:id="formula_0">L(P, N , α) = max(0, α + D(x, y) − D(x , y)) + max(0, α + D(x, y) − D(x, y ))<label>(1)</label></formula><p>where D(x, y) = 1 − x y/( x y ) is the cosine distance of two vectors.</p><p>To align representations at clip-sentence (ϑ k i , δ k i ), video-paragraph (ϑ k , δ k ) and global context (g v , g p ) levels, Zhang et al. <ref type="bibr" target="#b20">[21]</ref> use the following losses:</p><formula xml:id="formula_1">L align = k∈D,i,k =k,i =i L((ϑ k i , δ k i ), {(ϑ k i , δ k i ), (ϑ k i , δ k i )}, β) H align = k∈D,k =k L((ϑ k , δ k ), {(ϑ k , δ k ), (ϑ k , δ k )}, α) g align = k∈D,k =k L((g k v , g k p ), {(g k v , g k p ), (g k v , g k p )}, α g )<label>(2)</label></formula><p>Here, ϑ k i denotes the embedding for the i-th clip of the k-th video and similarly δ k i is the embedding of the i-th sentence of the k-th paragraph. α, α g and β are constant margins, and D is a dataset of videos with corresponding text descriptions. Zhang et al. <ref type="bibr" target="#b20">[21]</ref> employed an additional loss to model the clustering of low-level and high-level semantics in the joint embedding space:</p><formula xml:id="formula_2">cluster = k∈D,i,k =k,i =i L((1, 1), {(ϑ k i , ϑ k i ), (δ k i , δ k i )}, γ) + k∈D,k =k L((1, 1), {(ϑ k , ϑ k ), (δ k , δ k )}, η)<label>(3)</label></formula><p>where γ and η both are constant margins. The (1, 1) pairs denote that positive samples are not changed. In short, the goal of this loss is to push apart embeddings for negative samples.</p><p>Note. Due to the symmetrical design of the video and text branches in our model, from now on, we explain only the video branch. For simplicity, we assume a single head in transformer formulations. All transformers use residual connections.</p><p>Temporal Transformer. We use standard attention-blocks <ref type="bibr" target="#b17">[18]</ref> to learn frame and word representations, as shown in <ref type="figure" target="#fig_0">Fig 1-</ref>Right. We learn two temporal transformers (T-Transformer); one for the video branch and another one for the text branch. Both have the same architecture. All T-Transformers in each branch share their weights. This module draws the relationship between temporal features and yields improved representations as output. Given a video v k , we first encode all its frames to obtain the frame-level features {f k i,: } n i=1 , where f k i,: are all frame-level features of the i-th clip for video v k (orange parts in <ref type="figure" target="#fig_0">Figure 1</ref>). We also consider all frame features (f k : ) of a video as extra input for the global context computation (green parts in <ref type="figure" target="#fig_0">Figure 1</ref>). This yields {f k i,: } n i=1 andf k : .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intra-Level Cooperation</head><p>Standard feature fusion methods consider each feature independently by average pooling or max pooling. Hence, they miss the relationship between features to highlight the relevant features. Recent transformers use a <ref type="bibr">[CLS]</ref> token <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref> or average pooling <ref type="bibr" target="#b24">[25]</ref> to obtain the aggregated features. For example, when a person is cooking, objects on the table are more relevant than objects on the wall or in the background. Therefore, we need to attend to specific features depending on the context. There have been some attempts in other domains to design a context-aware feature fusion method <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. However, we introduce an attention-aware feature aggregation module (Attention-FA in <ref type="figure" target="#fig_0">Fig. 1</ref>  . This module (right) encourages the model to optimize the representations with respect to interactions between local and global context. In the third sentence, to know the type of dough (cookie) the model should have information about the general context of the video (making chocolate cookies). Likewise, in the second sentence, to know that she is the "same woman", the model must be aware of the person's identity throughout the video.</p><p>Suppose we have a sequence with T feature vectors, denoted by X = {x 1 , . . . , x T } (e.g.f k i,: = {f k i,1 , . . . ,f k i,T }). We set key K = X and utilize two learnable transformation weights W 2 and W 1 together with two biases b 1 and b 2 . The attention matrix A is computed as:</p><formula xml:id="formula_3">A = softmax(W 2 Q + b 2 ) T , Q = GELU (W 1 K T + b 1 ), K = X<label>(4)</label></formula><p>We compute the final feature asx = T i=1 a i x i , where denotes element-wise multiplication and a i is the i-th attention vector of A for the i-th feature. This module differs from attention <ref type="bibr" target="#b17">[18]</ref> in two aspects: (1) we use only two learnable weights for query (Q) and key (K) and then aggregate the values based on calculated scores; (2) the query equals to transformed keys (K) and then we apply the activation function GELU <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. We feed {f k i,: } n i=1 andf k : to this component and obtain the clip-level ({ϑ k i } n i=1 ) features and the global context for the video (g ν ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inter-Level Cooperation</head><p>By modeling the interactions between local and global context, the network learns to highlight semantics relevant to the general context of the video and to suppress the irrelevant ones: interactions between clip embeddings and the general context of the video; interactions between sentence embeddings and the general context of the text. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>-Left, without knowing the global context, just from observing the frame in the third clip, there is no information about what type of "dough" is involved. Also the "same woman" in the second clip could not be related to the woman seen in the first clip.</p><p>Thus, we propose a Contextual Transformer (CoT) in <ref type="figure" target="#fig_1">Figure 2</ref>-Right to model the interactions between low-level and high-level semantics. More formally, we build the Contextual Transformer with two modules F Local and F Global . We append the positional embedding to the inputs of F Local . The goal of F Local is to model the short-term interactions between low-level semantics ({ϑ k i } n i=1 ), whereas F Global models the interactions between local and global context (g ν ) to highlight the important semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given local representations {ϑ</head><formula xml:id="formula_4">k i } n i=1 ∈ R n×d ,</formula><p>where n is the number of clips and d indicates the feature dimension, F Local applies multi-head attention followed by a feed-forward layer and a normalization layer on top of both layers and produces embeddings {h i } n i=1 . We compute key (K)-value(V) pairs based on these embeddings {h i } n i=1 ∈ R n×d and query(Q) based on the global context g v . F Global produces the attention output as follows,</p><formula xml:id="formula_5">H attn = softmax( QK T √ d )V, Q = W q g v , K = W k {h i } n i=1 , V = W v {h i } n i=1<label>(5)</label></formula><p>where W q , W k , and W v are the embedding weights. H attn is a weighted sum of values (local semantics), where the weight of each value is calculated based on its interaction with the global context query Q. H attn is further encoded by a feed-forward layer to produce the contextual embedding H context . We calculate the mean of {h i } n i=1 and concatenate it with H context to obtain the final video embedding ϑ k = concat(mean({h i } n i=1 ), H context ); see <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-Modal Cycle Consistency</head><p>We introduce a cross-modal cycle-consistency loss to enforce the semantic alignment between clips and sentences, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. It replaces the cross-modal attention units used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. A pair of clip and sentence will be identified as semantically aligned if they are nearest neighbors in the learned common spaces. Consider as input a sequence of clip embeddings {ϑ i } n i=1 = {ϑ 1 , . . . , ϑ n } and sentence embeddings {δ i } m i=1 = {δ 1 , . . . , δ m }. As the sentences of a paragraph have a temporal order, given a sentence embedding δ i on this sequence, we first find its soft nearest neighbor <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref> </p><formula xml:id="formula_6">ϑ δi = n j=1 α j ϑ j where α j = exp(− δ i − ϑ j 2 ) n k=1 exp(− δ i − ϑ k 2 )<label>(6)</label></formula><p>in the clip sequence {ϑ i } n i=1 . α j is the similarity score of clip ϑ j to sentence δ i . We then cycle back fromθ δi to the sentence sequence {δ i } m i=1 and calculate the soft location</p><formula xml:id="formula_7">µ = m j=1 β j j where β j = exp(− θ − δ j 2 ) m k=1 exp(− θ − δ k 2 ) .<label>(7)</label></formula><p>The sentence embedding δ i is semantically cycle consistent if and only if it cycles back to the original location, i.e., i = µ. We penalize deviations from cycle-consistency for sampled sets of clips and sentences, which encourages the model to learn semantically consistent representations.  Starting from a sentence s i , we find its nearest neighbor in the clip sequence and again its neighbor in the sentence sequence. Deviations from the start index are penalized as alignment error.</p><p>Our objective is the distance between the source location i and the soft destination location µ.</p><formula xml:id="formula_8">CM C = i − µ 2 (8)</formula><p>Computing nearest neighbors as soft nearest neighbors makes the loss differentiable <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>. We can use this loss in both supervised and self-supervised scenarios. In the self-supervised case, we split each video uniformly into several clips and each paragraph into sentences. Beside the cycle-consistency from text to video, we also calculate CM C from video to text. Therefore, the final CM C loss includes both cycles.</p><p>The final training loss for the overall model is:</p><formula xml:id="formula_9">f inal = L align + H align + g align + cluster + λ CM C<label>(9)</label></formula><p>4 Experimental Setup Datasets. We evaluate our method on the datasets ActivityNet-captions <ref type="bibr" target="#b38">[39]</ref> and Youcook2 <ref type="bibr" target="#b39">[40]</ref>. ActivityNet-captions consists of 20k YouTube videos with an average length of 2 minutes, with 72k clip-sentence pairs. There are ∼10k, ∼5k and ∼5k videos in train, val1 and val2, respectively. Youcook2 contains 2000 videos with a total number of 14k clips. This dataset is collected from YouTube and covers 89 types of recipes. There are ∼9.6k clips for training and ∼3.2k clips for validation. For each clip there is a manually annotated textual description.</p><p>Evaluation Metrics. We measure the performance on the retrieval task with standard retrieval metrics, i.e., recall at K (R@K e.g. R@1, R@5, R@10) and Median Rank (MR).</p><p>Text encoding. We feed paragraphs consisting of several sentences into a pretrained "BERT-Base, Uncased" model <ref type="bibr" target="#b18">[19]</ref> and use the per-token outputs of the last 2 layers, resulting in 1536-d features.</p><p>Video encoding. For Activitynet-Captions, we use the 2048-d features provided by Zhang et al. <ref type="bibr">[</ref> For each clip as well as for the entire video, we sample up to 80 frame features. If needed, we split the frames into 80 equal length intervals and uniformly sample a frame from each interval during training or take the center frame during validation. Training. Similar to <ref type="bibr" target="#b20">[21]</ref> we set all margins α = α g = β = γ = µ = 0.2. We use a mini-batch size of 64 video/paragraph pairs and sample all corresponding clips and sentences. All possible combinations of embeddings with non-matching indices in a batch are used as negative samples for the contrastive loss. To apply the cycle-consistency loss, we found that sampling 1 clip per video and 1 sentence per paragraph works best. The optimal loss weight λ depends on architecture and dataset.</p><p>As activation function, we found GELU <ref type="bibr" target="#b33">[34]</ref> to perform best. We set the hidden size to 384 and use a pointwise linear layer to reduce the input feature dimension. We use one self-attention layer for the T-Transformer and one self-attention and one cross-attention layer for CoT. For further details on optimization and hyperparameters we refer the interested reader to the supplementary material.</p><p>Video-Language Retrieval. For video-text retrieval, the query is a paragraph and the task is to find the most relevant video from a database. Alternatively, the query can be a video and the task is to retrieve the most relevant paragraph. We follow the experimental protocol from Zhang et al. <ref type="bibr" target="#b20">[21]</ref> to evaluate the models. We use the final embedding output of our model (ϑ k , δ k ) to do the retrieval.</p><p>Clip-sentence retrieval. For Youcook2, we also evaluate the quality of our model when retrieving a short video clip given a single sentence. For this experiment, we use the intermediate low-level embeddings produced by our model (ϑ k i , δ k i ) to do the retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Influence of each component. We show results of a model ablation study in <ref type="table" target="#tab_2">Table 1</ref>. First, to validate the general effectiveness of the proposed cross-modal cycle consistency loss (CMC), we apply it to the HSE architecture <ref type="bibr" target="#b20">[21]</ref>. The CM C loss provides a significant boost in performance for both HSE and COOT, which indicates that it will be beneficial if plugged into other video-text representation learning methods. Second, the Attention-FA module shows better performance (7.2% average improvement on R@1 for paragraph =⇒ video and video =⇒ paragraph tasks) than common average pooling. Third, we observe that integrating the Contextual Transformer into the overall model improves the performance. This confirms that interactions between local and global context help the model to highlight the relevant semantics (more in supp. material).</p><p>Comparison to the state of the art. <ref type="table" target="#tab_3">Table 2</ref> summarizes the results of paragraph to video and video to paragraph retrieval tasks on the ActivityNet-captions dataset. For a fair comparison, our model utilizes the same video features as HSE <ref type="bibr" target="#b20">[21]</ref>. Our method significantly outperforms all previous methods across different evaluation metrics. COOT obtains on average 16.6% better R@1 in comparison to HSE <ref type="bibr" target="#b20">[21]</ref> while having fewer parameters. We believe the major gain comes from our attention-aware feature aggregation component and the CM C loss.</p><p>We further provide retrieval results on the Youcook2 <ref type="bibr" target="#b39">[40]</ref> dataset in <ref type="table" target="#tab_4">Table 3</ref>. We compare our model under two settings: (1) with features pretrained on classification (2) with features from a pretrained SOTA video-text model.  <ref type="bibr" target="#b15">[16]</ref> and HGLMM <ref type="bibr" target="#b43">[44]</ref> on both paragraph-to-video and sentence-to-clip tasks. This supports our rationale that modeling interactions between different hierarchy levels is crucial for capturing long-term semantics.</p><p>With HowTo100M pretrained features. In <ref type="table" target="#tab_4">Table 3</ref>, we compare our method with the recently proposed SOTA methods MIL-NCE <ref type="bibr" target="#b16">[17]</ref>, ActBERT <ref type="bibr" target="#b7">[8]</ref>, and Miech et al. <ref type="bibr" target="#b15">[16]</ref>, which utilize pretraining on the huge HowTo100M dataset. We use features (B) (Sec. 4) and train the model on the YouCook2 dataset. Note that the paragraph to video results of other methods are computed by us. Training our model with features of a model pretrained on the HowTo100M dataset clearly improves over training with features of a model pretrained on classification and over the state-of-the-art. We can see that our model outperforms MIL-NCE <ref type="bibr" target="#b16">[17]</ref> 16.4% on R@1 score for paragraph-to-video task, which verifies that COOT benefits from hierarchy interactions. This shows that the contributions of this paper are complementary to works that focus on large-scale pretraining.</p><p>Time complexity and number of parameters. The COOT model has 10.6M, parameters which is 60% less than the HSE method <ref type="table" target="#tab_2">(Table 1)</ref>. Training is fast and takes less than 3 hours on two GTX1080Ti GPUs (without data I/O).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Video Captioning</head><p>To show that the learned representations contain meaningful information for other tasks than retrieval, we use the learned representations for video captioning building upon the captioning model MART <ref type="bibr" target="#b44">[45]</ref>. The original method uses appearance (RGB) and optical flow features extracted from ResNet-200 <ref type="bibr" target="#b40">[41]</ref> and BN-Inception <ref type="bibr" target="#b45">[46]</ref>, respectively.</p><p>We use the clip (ϑ k i ) and optionally the video (ϑ k ) representation generated with our COOT model. In comparison to MART, we input about 100 times less features per video into the captioning model. We use the standard language evaluation metrics BLEU@3/4 <ref type="bibr" target="#b46">[47]</ref>, RougeL <ref type="bibr" target="#b47">[48]</ref>, METEOR <ref type="bibr" target="#b48">[49]</ref>, CIDEr-D <ref type="bibr" target="#b49">[50]</ref> and R@4 <ref type="bibr" target="#b50">[51]</ref> which measures the degree of n-gram repetition. Our results in <ref type="table" target="#tab_5">Table 4</ref> and <ref type="table" target="#tab_6">Table 5</ref> show that the MART method using our representations improves over using appearance and optical flow video features. Generated captions in <ref type="table" target="#tab_7">Table 6</ref> show that our video representations encapsulate richer information about the video while being more compact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Image and Language. Many self-supervised visual-language representation learning methods have focused on improving one representation with the help of the other <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref>. These approaches learn joint image-text embeddings or map images and sentences into a common space. Recently, there has been a surging interest in utilizing Transformers for image-text representation learning <ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref><ref type="bibr" target="#b64">[65]</ref><ref type="bibr" target="#b65">[66]</ref>.</p><p>ViLBERT <ref type="bibr" target="#b12">[13]</ref> and VisualBERT <ref type="bibr" target="#b11">[12]</ref> pretrain a BERT-like architecture on an image-text dataset and then transfer learned representations to different downstream tasks. LXMERT <ref type="bibr" target="#b13">[14]</ref> uses additional pretraining tasks and an object-relationship component. In contrast to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, VL-BERT <ref type="bibr" target="#b14">[15]</ref> does not utilize the task of sentence-image relationship prediction and additionally pretrains the model on text-only datasets.  Video and Language. The multi-modal nature of video is a great source of self-supervision.</p><p>Modalities such as audio, text and motion provide strong cues to learn richer spatio-temporal features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref>. Aytar et al. <ref type="bibr" target="#b69">[70]</ref> leverage natural synchronization to learn rich representations across vision, sound, and language. VideoBERT <ref type="bibr" target="#b10">[11]</ref> learns joint video-text representations based on predicting whether the linguistic sentence is temporally aligned with the visual sentence. These approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23</ref>] focus on self-supervised pretraining and require a large set of paired video clips and texts to learn a good representation model <ref type="bibr" target="#b16">[17]</ref>.</p><p>There has been growing interest in temporal localization of natural language in videos <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref><ref type="bibr" target="#b72">[73]</ref>. Moment localization identifies a time window given a text query <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b73">74]</ref>. Most related to our work are methods that focus on joint video-text embeddings and perform video-text retrieval or captioning <ref type="bibr">[4-6, 9, 21, 52, 69, 75, 76]</ref>. Several works tried to utilize the temporal structure of video and text for the alignment task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b78">[79]</ref>. Miech et al. <ref type="bibr" target="#b67">[68]</ref> proposed a mixture of experts approach to learn text-video representations. Likewise, CE <ref type="bibr" target="#b19">[20]</ref> proposes a mixture-of-experts model to aggregate information from pretrained experts (e .g. object, action, audio) with a gating mechanism.</p><p>In our work, we use a similar hierarchy as CMHSE <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> and extend their design by proposing three new components to learn the interactions between different levels of the hierarchy.</p><p>Cycle-Consistency. Cycle-Consistency uses transitivity as an objective for training <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b79">[80]</ref><ref type="bibr" target="#b80">[81]</ref><ref type="bibr" target="#b81">[82]</ref>.</p><p>The assumption of cyclical structure has been used in various works <ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref><ref type="bibr" target="#b84">[85]</ref>. Wang et al . <ref type="bibr" target="#b79">[80]</ref> obtain the supervision for visual correspondence by tracking forward and backward. Shah et al. <ref type="bibr" target="#b80">[81]</ref> enforce consistency between the generated and the original question in visual question answering. To prevent mode collapse, cycle-consistency is activated only after a certain number of training iterations <ref type="bibr" target="#b80">[81]</ref>. TCC <ref type="bibr" target="#b37">[38]</ref> employs cycle-consistency for temporal video alignment. In contrast to TCC, which works only in the video domain, we align video and text. To the best of our knowledge, this is the first work which introduces cycle-consistency to the video-text domain.  MART: A man is kneeling down on a floor. He is kneeling down on the ground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COOT (Ours):</head><p>A man is seen kneeling down on the ground and begins putting shoes on. The man continues to put on the shoes and ends by putting his shoes on. GT: A person is seen bending over a floor placing tiles down over the plaster. The person continues laying tiles down and pushing down on the floor to make sure it's sturdy.</p><p>MART: Heat up a pan and cook until golden brown. Add onions to the pan. Add flour salt and pepper to the pan. Add rice to the pan and stir. COOT (Ours): Boil the potatoes in water. Add chopped potatoes to the pan. Add butter and mash. Add some milk and mash.</p><p>GT: Boil some small pieces of potatoes in water. Mash the potato. Add some butter and salt and stir. Gradually add milk while stirring the potatoes.</p><p>MART: Cut the salmon into thin slices. Cut the salmon into thin slices. Cut the salmon into thin slices. Cut the salmon into thin slices. COOT (Ours): Cut the salmon in half. Cut the salmon in half. Cut the salmon into thin slices. Cut the salmon into thin slices. GT: Slice the fish into smaller pieces. Chop the tail end off. Cut the fish at an angle. Cut the fish into thin pieces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have presented a cooperative hierarchical transformer architecture for learning a joint video and text embedding space where similar semantics are aligned. The architecture is designed to encourage the use of long-range temporal context in a cross-level manner. Our approach uses two new components to model the interactions within and between hierarchy levels; an attention-aware feature aggregation module to model the interactions between frames and words, a contextual transformer to model the interactions between local contexts and global context. In addition, we have introduced a new cross-modal cycle-consistency loss which enforces the semantic alignment of clips and sentences. We have shown that both components contribute -jointly and individually -to an improved retrieval performance. As a result, our approach achieves state-of-the-art retrieval and captioning performance on two challenging datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This work contributes fundamental research and does not present any foreseeable societal consequence. In the long run, this line of research can contribute to services on video search and video organisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Implementation Details Hyperparameters. To select hyperparameters for our model, we used a combination of manual search and BOHB <ref type="bibr" target="#b85">[86]</ref> to explore the hyperparameters space. In <ref type="table" target="#tab_8">Table 7</ref>, we provide an overview of our hyperparameter search.</p><p>After testing different activation functions ReLU, SELU <ref type="bibr" target="#b86">[87]</ref>, ELU <ref type="bibr" target="#b87">[88]</ref> and GELU <ref type="bibr" target="#b33">[34]</ref> we found GELU to perform best. Increasing the capacity of the model by using a higher attention dimension can help improve the results, but comes at the cost of higher memory requirements and more difficult optimization. We schedule the Learning Rate with a Reduce on Plateau approach: Whenever our validation metric does not improve for a certain number of epochs, we reduce the learning rate by a factor of 10. After no improvements for 15 epochs, we terminate the training process. As relevant metric we defined the sum of R@1 Retrieval Score for video-paragraph and paragraph-video retrieval on Activitynet-Captions and the sum of R@1 Retrieval Score for clip-sentence and sentence-clip retrieval on Youcook2. Careful tuning of the optimizer parameters, using an automated search method like BOHB <ref type="bibr" target="#b85">[86]</ref> to search parts of the parameters space, was crucial to train the models properly.</p><p>Strength of the cross-modal cycle-consistency loss. For Activitynet we set λ = 0.01 and for Youcook2 we set λ = 0.001.</p><p>For weight initialization, we utilized Uniform, Normal and Truncated Normal distributions. The best results were obtained with initializing weights randomly from the Truncated Normal distribution with a standard deviation of 0.01, redrawing all samples with more than 2 standard deviations.</p><p>To cope with the overfitting problem, the different regularization methods (Dropout, Weight Decay, CMC-loss, Gaussian Noise on Frame Features) need to be traded off carefully to obtain good results (see <ref type="table" target="#tab_8">Table 7</ref>).</p><p>Preprocessing. For ActivityNet captions, we found it helpful to expand all clips to be at least 10 frames long. Expanding is done by iteratively adding frames to the start and end of the clip until we reach the desired length.  Retrieval. We L2-normalize the output embeddings of our model so the squared elements sum to 1. Retrieval is done by cosine similarity, e.g. given video embedding v, we retrieve paragraph embedding p = max p∈D v p (10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Ablation Studies</head><p>In this section, we provide ablation studies on the importance of low-level supervision, different text encoders performance, impact of different alignment losses in our final training loss and analysis on sequence pooling.  <ref type="table" target="#tab_2">Table 11</ref>: Evaluation of different averagepooling methods. We modify our exact approach to averagepooling in the high-level and evaluate the results. Impact of Alignment Losses. We study the effect of alignment losses on the performance in <ref type="table" target="#tab_10">Table 9</ref>. To give a more diverse picture, we evaluate the losses under different settings: We use three different low level pooling methods (Averagepool, Maxpool and Attention-aware Feature Aggregation) and selective disable the Cross-Modal Cycle Consistency loss and the global context attention in the Contextual Transformer.</p><p>We found that removing any or all of the three alignment losses significantly decreases performance.</p><p>In addition, we observed that clustering losses have a positive impact on the performance of the model.</p><p>Note that we also tried clustering the global context and found it to be not helpful. It might be a too strong constraint on our low-level embedding network.</p><p>Study on sequence pooling methods. In <ref type="table" target="#tab_2">Table 10</ref> we replace our low-level (frames, words) and high-level (clips, sentences) pooling methods and evaluate the performance. Interestingly, we get the best results with our AF module on the low level, while averagepooling outperforms it on the high level. Removing our components (CMC, CoT) and replacing AF with maxpooling provides a considerably strong baseline compared to our full model.</p><p>The sequence length is higher on the low level (e.g. up to 80 frames) than on the high level (on average 3.6 clips per video). Additionally, there are stronger temporal relationships between semantics in the low level. The AF module can learn to capture these relationships and improve the features. However on the high-level, the semantics have more independent meanings which makes it much harder for AF to model the temporal relationships between them.</p><p>Note that to give a more fair comparison, we change the optimizer setting when adding AF on the high level, as denoted with *. We observe that concatenating the output of 2 AF modules on the high level improves the performance, suggesting that the two modules learn to attend to different features.</p><p>We also vary our approach on averagepooling on the high level and report results In <ref type="table" target="#tab_2">Table 11</ref>. Working on variable length inputs, there are a number of design choices to make. We evaluate the following ones: 1) Summing over the unmasked sequence elements (nonzero inputs) only or summing over both sequence and padding elements (zero inputs). 2) Minimum padding to the maximum sequence length in the minibatch or to a length of at least 16. 3) Obtaining the average by dividing the sum by the length of nonzero elements or by the length of all elements.</p><p>On ActivityNet-captions (split val1, average sequence length 3.6), we show our non-standard approach of including padding tokens in the sum but dividing by the length of non-padding tokens works well. Note that in all other reported experiments, we use this version of averagepooling.</p><p>On Youcook2 (split val, average sequence length 7.6), we cannot reproduce this large gain in performance but the approach still works reasonably well. The good results when padding to a minimum length of 16 might be due to the average length being closer to 16 than in ActivityNetcaptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Retrieval on ActivityNet-captions (split val2)</head><p>We provide retrieval results for ActivityNet-Captions (val2 split) in <ref type="table" target="#tab_2">Table 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Qualitative Results</head><p>ActivityNet-Captions. To further check whether our COOT model can learn the semantic alignment of video and text well, we provide qualitative examples for the retrieval task on the ActivityNetcaption dataset (val1 split, 4917 video-paragraph pairs). Note that any spelling errors in the dataset are not corrected. As shown in <ref type="table" target="#tab_2">Table 13</ref> and <ref type="table" target="#tab_2">Table 14</ref>, the model learns to semantically align the video and paragraph embeddings. Even for imperfect rankings, the model retrieves semantically similar items.</p><p>YouCook2. We also present a set of qualitative clip-to-sentence and sentence-to-clip retrieval examples for the YouCook2 dataset (val split, 3492 clip-sentence pairs, 457 video-paragraph pairs). <ref type="table" target="#tab_2">Table 15</ref> and <ref type="table" target="#tab_2">Table 16</ref> show several examples where we can reasonably retrieve similar semantics, even when the wrong object is recognized <ref type="table" target="#tab_2">(Table 15</ref>-Right).</p><p>t-SNE Visualization of Embeddings. We project the video embeddings of Activitynet dataset to 2D space using t-SNE <ref type="bibr" target="#b90">[91]</ref> and visualize each point with a sample frame from the video. As shown in <ref type="figure">Figure 5</ref>, the embeddings are clustered semantically around activities and videos with similar content are in close neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Captioning Results</head><p>To expand upon the qualitative captioning results, we provide evaluation on samples that are not cherry-picked for Youcook2 (val split) and ActivityNet (ae-val and ae-test split) in <ref type="table" target="#tab_2">Tables 17, 18,  19</ref>.    <ref type="table" target="#tab_2">Table 16</ref>: Clip-to-Sentence Retrieval on Youcook2 val set. Left: The model gives high relative score to the relevant text but has problems visually distinguishing apples from potatoes. Right:: Wine is confused with oil and the herbs cannot be identified precisely to be bay leaves and thyme. Identical sentences can produce different results, since the Bert <ref type="bibr" target="#b18">[19]</ref> text encoder takes paragraph context into account and therefore the model inputs differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query:</head><p>Query:</p><p>Rank Score</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieved Text Rank Score</head><p>Retrieved Text   MART: A man is seen speaking to the camera and leads into him holding up various objects and presenting them to. He then cuts the knife and cuts the sandwich while still speaking to the camera. He then puts the sandwich into the pan and cuts it in half. He then puts the sandwich into the sandwich and puts it in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COOT (Ours):</head><p>A chef demonstrates how to make a sandwich using bread , then he puts a knife in a kitchen and. Then , the man puts the bread on a bread and cuts it in half. After , the man puts the sandwich in the bread and put it in a plate. Next , the man cuts the bread and put on top of the sandwich .</p><p>GT: A man shows ingredients for a mortadella sandwich. The man cuts the bred in four pieces and puts mustard and then brown on the stove. Then, the man fries an egg and puts it on the bread as well the mortadella, green leaves, cheese and ketchup. After, the man cuts the sandwich in two and eat one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MART:</head><p>A person is seen sitting in front of a large pile of grass and holding a stick. The person then puts the tire on the machine and begins putting the tire on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COOT (Ours):</head><p>A person is seen using a tool on a machine and piecing together with the camera. The man continues to use the machine on the machine and ends by taking out more out of the machine.</p><p>GT: A person is seen walking in with a tire on a plank and painting the tire. The person then un does the tire and places the rubber tightly around the side.</p><p>MART: A small group of people are seen swimming around a pool throwing a ball around to one another. The people continue playing with one another and end by throwing the ball back and fourth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COOT (Ours):</head><p>A large group of people are seen swimming around a pool throwing a ball around to one another. The people continue playing with one another and ends with a large group of people watching on the sides.</p><p>GT: A video of water polo is shown in the gym. A few people watch and the ball goes back and forth.</p><p>MART: A person is seen sitting in front of a large pile of grass and holding a stick. The person then puts the tire on the machine and begins putting the tire on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COOT (Ours):</head><p>A person is seen using a tool on a machine and piecing together with the camera. The man continues to use the machine on the machine and ends by taking out more out of the machine.</p><p>GT: A person is seen walking in with a tire on a plank and painting the tire. The person then undoes the tire and places the rubber tightly around the side. MART: A woman stands on front a house talking. The woman drives the lawn mower with a mower. The woman drives the lawn mower. The woman pushes the lawn mower along the grass. The woman talks to the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COOT (Ours):</head><p>We see the title on the field , white and white text. We then see a man mowing his lawn. The man stops and talks to the camera. The man stops and turns around. We then see the grass again.</p><p>GT: The video begins with a picture of a lawn along with a company name and website. The video cuts to a man riding a lawnmower, cutting the grass in a nice neighborhood. When he begins, some kids are playing in the road. At one point, a car passes by. The video ends with the picture of the lawn showing the company name and website.</p><p>MART: A group of women are dancing on a stage. They are dancing together in a room. They are dancing together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COOT (Ours):</head><p>A large group of girls are seen standing together followed by a woman dancing and performing a dance routine. The woman continues speaking to the camera while more people are seen dancing around and leads into a group of. The group continues dancing with one another and ends with a woman speaking to the camera.</p><p>GT: Several girls are in a classroom dancing and doing ballet. The instructor then comes to talk briefly before continuing on coaching the girls. After,the exercises continue and the girls do leaps and jumps in the room before the outside of the dance studio is shown.</p><p>MART: People are gathered around a street watching. They are holding flags in their hands. A man in a white shirt is standing next to a fence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COOT (Ours):</head><p>A man plays bagpipes while people watch on the sidewalk. A person in a black shirt plays the bagpipes. A person in a white shirt walks past the person.</p><p>GT: A man on stilts is playing the bag pipes on a street. A bus passes on the street behind the man. A street sign on a pole is shown.</p><p>MART: A group of women are dancing on a stage. They are dancing together in a room. They are dancing together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COOT (Ours):</head><p>A large group of girls are seen standing together followed by a woman dancing and performing a dance routine. The woman continues speaking to the camera while more people are seen dancing around and leads into a group of. The group continues dancing with one another and ends with a woman speaking to the camera.</p><p>GT: Several girls are in a classroom dancing and doing ballet. The instructor then comes to talk briefly before continuing on coaching the girls. After,the exercises continue and the girls do leaps and jumps in the room before the outside of the dance studio is shown. <ref type="figure">Figure 5</ref>: Visualization of the video embedding space with t-SNE on ActivityNet-Captions. We apply t-SNE to reduce the video embedding space to 2 dimensions and visualize videos by one sample frame.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of COOT model (best viewed in color). The model consist of two branches: one for video input (top) and one for text input (bottom). Given a video and a corresponding text, we encode them to frame-level/word-level features. Features belonging to each segment (clip/sentence) are fed to a standard temporal transformer (T-Transformer) followed by the proposed feature aggregation module (Attention-FA) to obtain clip/sentence-level features. Finally, a new contextual transformer produces the final video/paragraph embedding based on interactions between local context (clip/sentence features) and global context (all frames/words features). L align , H align , g align and CM C enforce the model to align the representations at different levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Contextual Transformer (CoT)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Cross-Modality Cycle-Consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>heat the apple in a pan with some oil 7 0.567 heat some oil in a pan8 0.475 pierce the knife inside the potatoes and find if the potatoes are cooked properly 8 add white wine onions a bay leaf and thyme to the pot</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Add &amp; Norm Feed Forward Add &amp; Norm Multi-Head Attention Add &amp; Norm Feed Forward Add &amp; Norm Multi-Head Attention Contextual Transformer Mean Concat Positional Encoding</head><label></label><figDesc>) for video-text transformers.</figDesc><table><row><cell></cell><cell cols="2">A woman is speaking about how to mix</cell><cell></cell></row><row><cell></cell><cell cols="2">ingredients and make chocolate cookies</cell><cell></cell></row><row><cell>A woman speaking about making chocolate chip cookies.</cell><cell>Same woman combines and mixes the ingredients with a hand mixer.</cell><cell>She adds it to the cookies dough and mixes it by hand</cell><cell>Placed in the oven and after they cool you have cookies</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on ActivityNet-captions (val1). We quantify the individual contributions of the attention-aware feature aggregation (AF), the Contextual Transformer (CoT), and the cross-modal cycle-consistency loss (CMC). HSE results are reproduced by us. Disabling CoT means removing the cross-attention layer between local and global context. 6±0.<ref type="bibr" target="#b2">3</ref> 76.1±0.7 96.0±0.3 44.9±0.5 75.8±1.2 95.8±0.4 26.1 HSE Max 46.6±0.4 78.1±0.3 97.3±0.1 46.4±0.3 77.6±0.3 97.1±0.3 26.1 COOT CLS 49.4±1.4 77.7±1.3 95.7±0.2 49.7±1.9 77.8±0.9 95.8±0.3 4.9 COOT AVG 52.6±0.6 80.6±0.4 97.0±0.2 52.1±0.4 80.8±0.2 97.0±0.2 4.9 COOT Max 58.2±0.5 84.9±0.2 98.1±0.1 58.7±0.5 86.0±0.2 98.</figDesc><table><row><cell>Model</cell><cell>Pooling CMC CoT</cell><cell cols="3">Paragraph =⇒ Video</cell><cell cols="3">Video =⇒ Paragraph</cell><cell>Param (M)</cell></row><row><cell></cell><cell>Lowlvl</cell><cell>R@1</cell><cell>R@5</cell><cell>R@50</cell><cell>R@1</cell><cell>R@5</cell><cell>R@50</cell></row><row><cell>HSE</cell><cell>Max</cell><cell cols="6">45.2±0.1</cell><cell>4.9</cell></row><row><cell cols="2">COOT AFA</cell><cell cols="6">59.0±0.5 85.4±0.2 98.2±0.0 59.8±0.6 85.8±0.8 98.2±0.1</cell><cell>5.8</cell></row><row><cell cols="2">COOT Max</cell><cell cols="6">59.4±0.9 86.1±0.6 98.3±0.0 60.5±0.1 87.1±0.2 98.5±0.1</cell><cell>6.7</cell></row><row><cell cols="2">COOT AFA</cell><cell cols="6">59.8±1.1 86.3±0.3 98.5±0.1 60.1±0.1 87.1±0.4 98.5±0.1</cell><cell>7.6</cell></row><row><cell cols="2">COOT AFA</cell><cell cols="6">59.5±0.5 85.5±0.4 98.1±0.0 60.5±0.7 86.2±0.5 98.2±0.1</cell><cell>5.8</cell></row><row><cell cols="2">COOT AFA</cell><cell cols="6">60.8±0.6 86.6±0.4 98.6±0.1 60.9±0.3 87.4±0.5 98.6±0.0</cell><cell>7.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Video-paragraph retrieval results on AcitvityNet-captions dataset (val1). Without HowTo100M pretrained features. We use features (A) explained in Section 4 and train the COOT model on the YouCook2 dataset. Using the same training set, COOT outperforms Miech et al.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Paragraph =⇒ Video</cell><cell></cell><cell></cell><cell cols="2">Video =⇒ Paragraph</cell><cell></cell></row><row><cell>Method</cell><cell>R@1</cell><cell>R@5</cell><cell>R@50</cell><cell>MR</cell><cell>R@1</cell><cell>R@5</cell><cell>R@50</cell><cell>MR</cell></row><row><cell>LSTM-YT [52]</cell><cell>0.0</cell><cell>4.0</cell><cell>24.0</cell><cell>102.0</cell><cell>0.0</cell><cell>7.0</cell><cell>38.0</cell><cell>98.0</cell></row><row><cell>No Context [53]</cell><cell>5.0</cell><cell>14.0</cell><cell>32.0</cell><cell>78.0</cell><cell>7.0</cell><cell>18.0</cell><cell>45.0</cell><cell>56.0</cell></row><row><cell>DENSE [39]</cell><cell>14.0</cell><cell>32.0</cell><cell>65.0</cell><cell>34.0</cell><cell>18.0</cell><cell>36.0</cell><cell>74.0</cell><cell>32.0</cell></row><row><cell>VSE [54]( [5])</cell><cell>11.7</cell><cell>34.7</cell><cell>85.7</cell><cell>10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FSE [21]</cell><cell>18.2</cell><cell>44.8</cell><cell>89.1</cell><cell>7</cell><cell>16.7</cell><cell>43.1</cell><cell>88.4</cell><cell>7</cell></row><row><cell>HSE [21]</cell><cell cols="3">44.4±0.5 76.7±0.3 97.1±0.1</cell><cell>2</cell><cell cols="3">44.2±0.6 76.7±0.3 97.0±0.3</cell><cell>2</cell></row><row><cell>COOT</cell><cell cols="3">60.8±0.6 86.6±0.4 98.6±0.1</cell><cell>1</cell><cell cols="3">60.9±0.3 87.4±0.5 98.6±0.0</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Retrieval results on YouCook2 dataset. Results with * are computed by us. we use features of a video-text model [17] pretrained on the HowTo100m dataset. 4±2.6 79.4±0.6 87.4±0.8 1.3±0.6 5.9±0.7 16.7±0.6 24.8±0.8 49.7±2.9 77.2±1.0 95.8±0.8 97.5±0.3 1.0±0.0 16.7±0.4 40.2±0.3 52.3±0.5 9.0±0.0 YouCook2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Paragraph =⇒ Video</cell><cell></cell><cell cols="2">Sentence =⇒ Clip</cell><cell></cell></row><row><cell>Method</cell><cell>TrainSet</cell><cell>R@1</cell><cell>R@5</cell><cell cols="2">R@10 MR</cell><cell>R@1</cell><cell>R@5</cell><cell cols="2">R@10 MR</cell></row><row><cell>Random</cell><cell>-</cell><cell>0.21</cell><cell>1.09</cell><cell>2.19</cell><cell>229</cell><cell>0.03</cell><cell>0.15</cell><cell>0.3</cell><cell>1675</cell></row><row><cell cols="2">Miech et al. [16] HowTo100M</cell><cell cols="4">43.1* 68.6* 79.1* 2*</cell><cell>6.1</cell><cell>17.3</cell><cell>24.8</cell><cell>46</cell></row><row><cell>ActBERT [8]</cell><cell>HowTo100M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>9.6</cell><cell>26.7</cell><cell>38.0</cell><cell>19</cell></row><row><cell cols="2">MIL-NCE [17] HowTo100M</cell><cell cols="4">61.9* 89.4* 98.9* 1*</cell><cell>15.1</cell><cell>38.0</cell><cell>51.2</cell><cell>10</cell></row><row><cell>HGLMM [44]</cell><cell>YouCook2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.6</cell><cell>14.3</cell><cell>21.6</cell><cell>75</cell></row><row><cell cols="2">Miech et al. [16] YouCook2</cell><cell cols="4">32.3* 59.2* 70.9* 4*</cell><cell>4.2</cell><cell>13.7</cell><cell>21.5</cell><cell>65</cell></row><row><cell cols="6">COOT 50.Miech et al. [16] YouCook2 HowTo100M+ 59.6* 86.0* 93.6* 1* YouCook2</cell><cell>8.2</cell><cell>24.5</cell><cell>35.3</cell><cell>24</cell></row><row><cell>COOT</cell><cell>HowTo100M +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Features</cell><cell>Method</cell><cell>TrainSet</cell><cell>B@3</cell><cell>B@4</cell><cell cols="4">RougeL METEOR CIDEr-D R@4↓</cell></row><row><cell>RGB+Flow</cell><cell>VTransformer [55]</cell><cell>YouCook2</cell><cell cols="2">13.08* 7.62</cell><cell>32.18*</cell><cell>15.65</cell><cell>32.26</cell><cell>7.83</cell></row><row><cell>RGB+Flow</cell><cell cols="2">TransformerXL [56] YouCook2</cell><cell cols="2">11.46* 6.56</cell><cell>30.78*</cell><cell>14.76</cell><cell>26.35</cell><cell>6.30</cell></row><row><cell>RGB+Flow</cell><cell>MART [45]</cell><cell>YouCook2</cell><cell cols="2">12.83* 8.00</cell><cell>31.97*</cell><cell>15.90</cell><cell>35.74</cell><cell>4.39</cell></row><row><cell>COOT clip</cell><cell>MART</cell><cell>YouCook2</cell><cell>14.17</cell><cell>8.69</cell><cell>33.01</cell><cell>16.11</cell><cell>38.28</cell><cell>8.07</cell></row><row><cell cols="2">COOT video+clip MART</cell><cell>YouCook2</cell><cell>15.75</cell><cell>9.44</cell><cell>34.32</cell><cell>18.17</cell><cell>46.06</cell><cell>6.30</cell></row><row><cell>COOT clip</cell><cell>MART</cell><cell cols="2">H100M +YC2 17.12</cell><cell cols="2">10.91 37.59</cell><cell>18.85</cell><cell>54.07</cell><cell>5.11</cell></row><row><cell>COOT clip</cell><cell>MART w/o re.</cell><cell cols="2">H100M +YC2 17.16</cell><cell cols="2">10.69 37.43</cell><cell>19.18</cell><cell>54.85</cell><cell>5.45</cell></row><row><cell>COOT clip</cell><cell>VTransformer</cell><cell cols="2">H100M +YC2 17.62</cell><cell cols="2">11.09 37.63</cell><cell>19.34</cell><cell>54.67</cell><cell>4.57</cell></row><row><cell cols="2">COOT video+clip VTransformer [55]</cell><cell cols="2">H100M +YC2 17.79</cell><cell cols="2">11.05 37.51</cell><cell>19.79</cell><cell>55.57</cell><cell>5.69</cell></row><row><cell cols="2">COOT video+clip MART</cell><cell cols="2">H100M +YC2 17.97</cell><cell cols="2">11.30 37.94</cell><cell>19.85</cell><cell>57.24</cell><cell>6.69</cell></row></table><note>Captioning results on the YouCook2 dataset (val split). Results with * are computed by us. we use features of a video-text model [17] pretrained on the HowTo100m dataset. "MART w/o re" denotes a MART variant without recurrence.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Captioning results on the ActivityNet-Captions dataset (ae-test split of MART<ref type="bibr" target="#b44">[45]</ref>). Results with * are computed by us. "MART w/o re" denotes a MART variant without recurrence.</figDesc><table><row><cell>Features</cell><cell>Method</cell><cell>TrainSet</cell><cell>B@3</cell><cell>B@4</cell><cell cols="4">RougeL METEOR CIDEr-D R@4↓</cell></row><row><cell>RGB+Flow</cell><cell>VTransformer [55]</cell><cell cols="3">ActivityNet 16.27* 9.31</cell><cell>29.18*</cell><cell>15.54</cell><cell>21.33</cell><cell>7.45</cell></row><row><cell>RGB+Flow</cell><cell cols="5">TransformerXL [56] ActivityNet 16.71* 10.25 30.53*</cell><cell>14.91</cell><cell>21.71</cell><cell>8.79</cell></row><row><cell>RGB+Flow</cell><cell>MART</cell><cell cols="3">ActivityNet 16.43* 9.78</cell><cell>30.63*</cell><cell>15.57</cell><cell>22.16</cell><cell>5.44</cell></row><row><cell cols="4">COOT video+clip TransformerXL [56] ActivityNet 16.94</cell><cell cols="2">10.57 30.93</cell><cell>14.76</cell><cell>22.04</cell><cell>15.85</cell></row><row><cell cols="2">COOT video+clip VTransformer [55]</cell><cell cols="2">ActivityNet 16.80</cell><cell cols="2">10.47 30.37</cell><cell>15.76</cell><cell>25.90</cell><cell>19.14</cell></row><row><cell>COOT clip</cell><cell>MART w/o re.</cell><cell cols="2">ActivityNet 15.41</cell><cell>9.37</cell><cell>28.66</cell><cell>15.61</cell><cell>22.05</cell><cell>12.03</cell></row><row><cell cols="2">COOT video+clip MART w/o re.</cell><cell cols="2">ActivityNet 16.59</cell><cell cols="2">10.33 29.93</cell><cell>15.64</cell><cell>25.41</cell><cell>17.03</cell></row><row><cell>COOT clip</cell><cell>MART</cell><cell cols="2">ActivityNet 16.53</cell><cell cols="2">10.22 30.68</cell><cell>15.91</cell><cell>23.98</cell><cell>5.35</cell></row><row><cell cols="2">COOT video+clip MART</cell><cell cols="2">ActivityNet 17.43</cell><cell cols="2">10.85 31.45</cell><cell>15.99</cell><cell>28.19</cell><cell>6.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Captioning samples, more accurate (left) and less accurate (right) cases.</figDesc><table><row><cell>First row:</cell></row></table><note>MART: A person is driving the car. A boy is holding a bottle of wood. COOT (Ours): A woman is seen kneeling down next to a car while others stand around her. The woman then pushes the tire back and fourth.GT: A girl is shown trying to change a tire. She successfully removes the tire, then replaces it with a spare, showing off their dirty hands afterward.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters. This table shows the hyperparameter ranges we considered and the final choices for our three best models (ActivityNet-captions, YouCook2-Resnet/Resnext features, Youcook2-Howto100m features.). ROP denotes the Reduce on Plateau Scheduler we used. Dimensions given in multiples (1x, 2x) refer to multiples of the Attention Dimension parameter. FF denotes Feed-Forward. AF is our Attention-aware Feature Aggregation module.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="2">Considered Range</cell><cell>ActivityNet</cell><cell cols="2">Youcook2 Resnet/ResneXt Howto100m</cell></row><row><cell>Optimizer</cell><cell cols="2">Adam, RAdam, SGD</cell><cell>Adam</cell><cell>RAdam</cell><cell>RAdam</cell></row><row><cell>Learning rate</cell><cell>1e-5</cell><cell>1e-2</cell><cell>1e-3</cell><cell>3.6e-4</cell><cell>9e-4</cell></row><row><cell>Weight Decay</cell><cell>0</cell><cell>1e-2</cell><cell>2e-5</cell><cell>2e-5</cell><cell>0</cell></row><row><cell>Momentum</cell><cell>0.5</cell><cell>0.99</cell><cell>0.9</cell><cell>0.56</cell><cell>0.56</cell></row><row><cell>Adam Beta2</cell><cell>0.9</cell><cell>0.9999</cell><cell>0.999</cell><cell>0.98</cell><cell>0.98</cell></row><row><cell>Adam Epsilon</cell><cell>1e-10</cell><cell>1e-7</cell><cell>1e-8</cell><cell>1.5e-9</cell><cell>1.5e-9</cell></row><row><cell>Warmup Epochs</cell><cell>0</cell><cell>8</cell><cell>3</cell><cell>0</cell><cell>0</cell></row><row><cell>ROP Patience</cell><cell>2</cell><cell>10</cell><cell>2</cell><cell>5</cell><cell>5</cell></row><row><cell>ROP Cooldown</cell><cell>0</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>Attention Layers</cell><cell>1</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Attention Dimension</cell><cell>256</cell><cell>1024</cell><cell>384</cell><cell>384</cell><cell>384</cell></row><row><cell>Attention Heads</cell><cell>1</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell></row><row><cell>Attention FF Dimension</cell><cell>1x</cell><cell>2x</cell><cell>1x</cell><cell>1x</cell><cell>1x</cell></row><row><cell>AF Dimension</cell><cell>1x</cell><cell>2x</cell><cell>2x</cell><cell>2x</cell><cell>2x</cell></row><row><cell>AF Heads</cell><cell>1</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Number of AF modules</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Dropout</cell><cell>0%</cell><cell>10%</cell><cell>2.5%</cell><cell>1%</cell><cell>5%</cell></row><row><cell>Gaussian Noise on Frame Features</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0.01</cell><cell>0</cell></row></table><note>Optimization. We tried several optimizers such as Adam, RAdam [89] and SGD. If carefully configured, RAdam can improve over Adam.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Text feature ablation study on ActivityNet-captions (val1). We evaluate our choice of text encoding and show that Bert<ref type="bibr" target="#b18">[19]</ref> outperforms GloVe<ref type="bibr" target="#b89">[90]</ref> on both models and all metrics.7±0.3 76.1±0.7 96.0±0.3 44.9±0.5 75.8±1.2 95.8±0.4 HSE Bert 47.0±1.1 77.0±1.5 96.1±0.4 46.9±0.8 77.2±1.1 95.9±0.6 COOT GloVe 56.5±1.1 84.1±1.3 98.0±0.3 57.3±1.8 84.5±1.4 98.2±0.2 COOT Bert 60.8±0.6 86.6±0.4 98.6±0.1 60.9±0.3 87.4±0.5 98.6±0.0</figDesc><table><row><cell>Model</cell><cell>Text</cell><cell cols="3">Paragraph =⇒ Video</cell><cell cols="2">Video =⇒ Paragraph</cell></row><row><cell></cell><cell cols="2">R@1</cell><cell>R@5</cell><cell>R@50</cell><cell>R@1</cell><cell>R@5</cell><cell>R@50</cell></row><row><cell>HSE</cell><cell>GloVe 45.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Loss function ablation study on ActivityNet-captions (val1). We analyse performance of the COOT model while removing loss components with different base models. CoT denotes using global attention in the contextual transformer. AF is our Attention-aware Feature Aggregation module.</figDesc><table><row><cell>#</cell><cell>Pooling CMC CoT</cell><cell>Alignment</cell><cell>Clustering</cell><cell cols="2">Par. =⇒ Video</cell><cell>Video =⇒ Par.</cell></row><row><cell></cell><cell>Lowlvl</cell><cell cols="3">High Low Ctx High Low R@1</cell><cell>R@5</cell><cell>R@1</cell><cell>R@5</cell></row><row><cell>1</cell><cell>Avg</cell><cell></cell><cell></cell><cell cols="3">30.4±3.2 58.4±4.5 29.9±3.3 58.7±4.5</cell></row><row><cell>2</cell><cell>Avg</cell><cell></cell><cell></cell><cell cols="3">49.7±0.7 79.0±0.6 48.6±0.5 79.1±0.9</cell></row><row><cell>3</cell><cell>Avg</cell><cell></cell><cell></cell><cell cols="3">49.2±0.7 78.9±0.2 48.6±0.6 78.9±0.6</cell></row><row><cell>4</cell><cell>Avg</cell><cell></cell><cell></cell><cell cols="3">50.6±1.1 79.8±0.8 50.8±1.0 79.8±0.8</cell></row><row><cell>5</cell><cell>Avg</cell><cell></cell><cell></cell><cell cols="3">51.5±0.7 80.2±0.4 52.0±0.8 80.5±0.3</cell></row><row><cell>6</cell><cell>Avg</cell><cell></cell><cell></cell><cell cols="3">52.6±0.6 80.6±0.4 52.1±0.4 80.8±0.2</cell></row><row><cell>7</cell><cell>Avg</cell><cell></cell><cell></cell><cell cols="3">27.4±2.1 55.3±2.4 27.3±1.6 56.0±2.3</cell></row><row><cell>8</cell><cell>Avg</cell><cell></cell><cell></cell><cell cols="3">54.1±0.8 82.0±0.1 54.7±0.2 82.1±0.1</cell></row><row><cell>9</cell><cell>Avg</cell><cell></cell><cell></cell><cell cols="3">53.6±0.1 81.7±0.0 53.5±0.5 81.7±0.7</cell></row><row><cell>10</cell><cell>Max</cell><cell></cell><cell></cell><cell cols="3">47.9±0.7 76.9±0.1 48.3±0.2 77.5±0.6</cell></row><row><cell>11</cell><cell>Max</cell><cell></cell><cell></cell><cell cols="3">56.5±0.3 84.5±0.2 56.6±0.4 85.2±0.1</cell></row><row><cell>12</cell><cell>Max</cell><cell></cell><cell></cell><cell cols="3">54.4±0.9 83.3±0.9 55.4±1.4 84.0±0.8</cell></row><row><cell>13</cell><cell>Max</cell><cell></cell><cell></cell><cell cols="3">55.3±0.8 83.0±0.8 56.4±1.1 83.7±1.2</cell></row><row><cell>14</cell><cell>Max</cell><cell></cell><cell></cell><cell cols="3">56.1±0.2 83.3±0.2 57.0±0.3 83.9±0.5</cell></row><row><cell>15</cell><cell>Max</cell><cell></cell><cell></cell><cell cols="3">58.2±0.5 84.9±0.2 58.7±0.5 86.0±0.2</cell></row><row><cell>16</cell><cell>Max</cell><cell></cell><cell></cell><cell cols="3">46.3±1.0 76.2±0.9 47.7±0.9 77.2±0.7</cell></row><row><cell>17</cell><cell>Max</cell><cell></cell><cell></cell><cell cols="3">57.5±0.5 84.8±0.2 58.1±1.0 85.3±0.4</cell></row><row><cell>18</cell><cell>Max</cell><cell></cell><cell></cell><cell cols="3">59.4±0.9 86.1±0.6 60.5±1.0 87.1±0.2</cell></row><row><cell>19</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">47.1±0.7 76.7±0.6 47.6±0.2 77.4±0.2</cell></row><row><cell>20</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">56.3±0.3 84.0±0.2 56.8±0.7 84.7±0.3</cell></row><row><cell>21</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">55.2±0.2 83.3±0.1 55.8±0.5 83.6±0.2</cell></row><row><cell>22</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">57.8±0.3 84.6±0.2 58.1±0.3 85.1±0.2</cell></row><row><cell>23</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">58.8±0.4 85.3±0.4 59.1±0.6 85.8±0.4</cell></row><row><cell>24</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">59.0±0.5 85.4±0.2 59.8±0.6 85.8±0.8</cell></row><row><cell>25</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">47.8±0.5 76.4±0.4 47.8±0.2 77.5±0.5</cell></row><row><cell>26</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">59.5±0.5 85.5±0.4 60.5±0.7 86.2±0.5</cell></row><row><cell>27</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">53.9±0.7 82.6±0.6 53.8±0.6 83.0±0.5</cell></row><row><cell>28</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">55.1±5.3 83.4±3.6 55.5±4.7 83.8±3.2</cell></row><row><cell>29</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">58.5±1.1 85.2±0.5 58.5±0.7 85.5±0.7</cell></row><row><cell>30</cell><cell>AF</cell><cell></cell><cell></cell><cell cols="3">60.8±0.6 86.6±0.4 60.9±0.3 87.4±0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Evaluation of different sequence pooling methods on ActivityNet-captions (val1). We switch both the low level (frames, words) and high level (clips, sentences) pooling methods and observe the changes in performance. In experiments denoted with *, we used a different optimizer setting.</figDesc><table><row><cell></cell><cell>Pooling</cell><cell>CMC</cell><cell>CoT</cell><cell cols="2">Par. =⇒ Video</cell><cell>Video =⇒ Par.</cell></row><row><cell>Low</cell><cell>High</cell><cell></cell><cell></cell><cell>R@1</cell><cell>R@5</cell><cell>R@1</cell><cell>R@5</cell></row><row><cell>AF</cell><cell>AF x1</cell><cell></cell><cell></cell><cell>42.6±0.4</cell><cell>76.5±0.3</cell><cell>42.1±0.8</cell><cell>76.9±0.7</cell></row><row><cell>AF</cell><cell>AF x2</cell><cell></cell><cell></cell><cell>42.8±0.1</cell><cell>76.0±0.7</cell><cell>42.8±0.1</cell><cell>76.4±0.6</cell></row><row><cell>AF*</cell><cell>AF x1</cell><cell></cell><cell></cell><cell>48.7±1.0</cell><cell>82.2±0.6</cell><cell>50.1±0.4</cell><cell>82.6±0.4</cell></row><row><cell>AF*</cell><cell>AF x2</cell><cell></cell><cell></cell><cell>50.5±0.4</cell><cell>82.3±0.4</cell><cell>51.4±1.4</cell><cell>82.9±0.6</cell></row><row><cell>Max</cell><cell>Max</cell><cell></cell><cell></cell><cell>40.9±0.7</cell><cell>75.3±0.1</cell><cell>42.2±0.5</cell><cell>76.2±0.6</cell></row><row><cell>AF</cell><cell>Max</cell><cell></cell><cell></cell><cell>43.3±0.9</cell><cell>76.3±1.0</cell><cell>42.5±0.6</cell><cell>77.2±1.2</cell></row><row><cell>CLS</cell><cell>Avg</cell><cell></cell><cell></cell><cell>49.4±1.4</cell><cell>77.7±1.3</cell><cell>49.7±1.9</cell><cell>77.8±0.9</cell></row><row><cell>CLS</cell><cell>Avg</cell><cell></cell><cell></cell><cell>49.7±0.5</cell><cell>79.4±0.2</cell><cell>51.2±0.1</cell><cell>79.6±0.1</cell></row><row><cell>Avg</cell><cell>Avg</cell><cell></cell><cell></cell><cell>52.6±0.6</cell><cell>80.6±0.4</cell><cell>52.1±0.4</cell><cell>80.8±0.2</cell></row><row><cell>Avg</cell><cell>Avg</cell><cell></cell><cell></cell><cell>53.6±0.1</cell><cell>81.7±0.0</cell><cell>53.5±0.5</cell><cell>81.7±0.7</cell></row><row><cell>Max</cell><cell>Avg</cell><cell></cell><cell></cell><cell>58.2±0.5</cell><cell>84.9±0.2</cell><cell>58.7±0.5</cell><cell>86.0±0.2</cell></row><row><cell>Max</cell><cell>Avg</cell><cell></cell><cell></cell><cell>59.4±0.9</cell><cell>86.1±0.6</cell><cell>60.5±1.0</cell><cell>87.1±0.2</cell></row><row><cell>AF</cell><cell>Avg</cell><cell></cell><cell></cell><cell>60.8±0.6</cell><cell>86.6±0.4</cell><cell>60.9±0.3</cell><cell>87.4±0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Video-paragraph retrieval results on AcitvityNet-captions dataset (val2).</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">Par. =⇒ Video</cell><cell cols="2">Video =⇒ Par.</cell></row><row><cell></cell><cell></cell><cell>R@1</cell><cell>R@5</cell><cell>R@1</cell><cell>R@5</cell></row><row><cell>FSE</cell><cell></cell><cell>11.5</cell><cell>31.0</cell><cell>11.0</cell><cell>30.6</cell></row><row><cell>HSE</cell><cell></cell><cell>32.9</cell><cell>62.7</cell><cell>32.6</cell><cell>63.0</cell></row><row><cell>COOT</cell><cell></cell><cell>48.5</cell><cell>78.9</cell><cell>48.9</cell><cell>79.5</cell></row><row><cell cols="6">Figure 4: Noise vs Performance study on ActivityNet-captions dataset (val1)</cell></row><row><cell></cell><cell>85</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Retrieval %</cell><cell>60 65 70</cell><cell></cell><cell></cell><cell>Video R@1 Video R@5 Text R@1 Text R@5</cell></row><row><cell></cell><cell>55</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>45</cell><cell cols="3">Noise % 0 10 20 30 40 50 60 70 80 90 100 200 Full</cell></row></table><note>GloVe features with features obtained from a pretrained Bert [19] model. Note that we feed an entire paragraph consisting of several sentences into Bert, leveraging high-level context. Our results show that replacing fixed word embeddings with the context-aware Bert features can significantly improve model performance over different architectures. Both models are relatively shallow (1 layer of attention / GRU respectively), which may be the reason why the deeper Bert model (13 layers) can help understand the text better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Qualitative Results on Activitynet for Paragraph-to-Video Retrieval. For each text query, we show some frames from the top three ranked videos together with the correct video. For clarification, we show video results with text. Left: The correct video has a high rank and all top results are very relevant to the query. Right: Even though the correct video is ranked low, the top videos are semantically similar to the text query.</figDesc><table><row><cell cols="2">Query: A man is standing inside a workshop. He leans</cell><cell cols="2">Query: A person is kneeling down painting something</cell></row><row><cell cols="2">over, welding a piece of metal. Sparks fly as he welds.</cell><cell cols="2">on the ground. They smooth out the paint. They con-</cell></row><row><cell></cell><cell></cell><cell cols="2">tinue painting layers on top of the paint.</cell></row><row><cell>Rank</cell><cell>Retrieved Video</cell><cell>Rank</cell><cell>Retrieved Video</cell></row><row><cell>Score</cell><cell></cell><cell>Score</cell><cell></cell></row><row><cell>1</cell><cell></cell><cell>1</cell><cell></cell></row><row><cell>0.827</cell><cell></cell><cell>0.654</cell><cell></cell></row><row><cell>2</cell><cell></cell><cell>2</cell><cell></cell></row><row><cell>0.821</cell><cell></cell><cell>0.643</cell><cell></cell></row><row><cell>3</cell><cell></cell><cell>3</cell><cell></cell></row><row><cell>0.816</cell><cell></cell><cell>0.640</cell><cell></cell></row><row><cell>4</cell><cell></cell><cell>48</cell><cell></cell></row><row><cell>0.783</cell><cell></cell><cell>0.438</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Retrieval Video to Paragraph on Activitynet. Long paragraphs have been shortened, as indicated by "[...]". Left: The correct paragraph is identified with a considerable score margin to the 2nd place. Right: The top results are from the same activity as the input video (dancing).</figDesc><table><row><cell>Query:</cell><cell></cell><cell>Query:</cell><cell></cell></row><row><cell>Rank</cell><cell>Retrieved Text</cell><cell>Rank</cell><cell>Retrieved Text</cell></row><row><cell>Score</cell><cell></cell><cell>Score</cell><cell></cell></row><row><cell>1</cell><cell>A woman is resting next to crashing water.</cell><cell>1</cell><cell>A woman stands in front of a crowd of people</cell></row><row><cell>0.813</cell><cell>She is smoking a pipe. She blows out a</cell><cell>0.717</cell><cell>on a public sidewalk and dances with a male</cell></row><row><cell></cell><cell>plume of smoke.</cell><cell></cell><cell>dance partner in ballroom style dance. [. . . ]</cell></row><row><cell>2</cell><cell>A close up of a man's chin is shown followed</cell><cell>2</cell><cell>A woman in a leather dress and hat dances</cell></row><row><cell>0.654</cell><cell>by him smoking a hookah pipe. He takes the</cell><cell>0.678</cell><cell>in a public station. A man joins her, dancing</cell></row><row><cell></cell><cell>pipe out of his mouth and blows the smoke into</cell><cell></cell><cell>side to side in a flamenco style dance. They</cell></row><row><cell></cell><cell>the camera.</cell><cell></cell><cell>continue dancing as a small crowd gathers to</cell></row><row><cell></cell><cell></cell><cell></cell><cell>watch. [. . . ]</cell></row><row><cell>3</cell><cell>A close up of tin foil is shown leading a woman</cell><cell>3</cell><cell>A large group of people are seen standing</cell></row><row><cell>0.641</cell><cell>taking a large hit out of a hookah hose. She</cell><cell>0.608</cell><cell>around a city center waiting for people to ar-</cell></row><row><cell></cell><cell>continues smoking out of the hookah [. . . ]</cell><cell></cell><cell>rive. Girls dancing are seen walking through</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the parade as other people watch on the side.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[. . . ]</cell></row><row><cell>4</cell><cell>A woman is laying back in a chair getting her</cell><cell>16</cell><cell>People are dancing in a street. People are</cell></row><row><cell>0.601</cell><cell>lip pierced. The piercer removes the tool and</cell><cell>0.496</cell><cell>standing on the sidelines watching them.</cell></row><row><cell></cell><cell>pulls on her lip.</cell><cell></cell><cell>They continue dancing on a street.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Sentence-to-Clip Retrieval on Youcook2. For clarification, we show clip results with corresponding text. Left: The model ranks the correct video at the top and even distinguishes it from other videos about the same activity. Right: The slicing task is correctly recognized, but the model is not able to understand which object is being chopped (bamboo shots).</figDesc><table><row><cell cols="2">Query: melt butter in the pan</cell><cell cols="2">Query: slice the bamboo shoots into strips</cell></row><row><cell>Rank</cell><cell>Retrieved Clip</cell><cell>Rank</cell><cell>Retrieved Clip</cell></row><row><cell>Score</cell><cell></cell><cell>Score</cell><cell></cell></row><row><cell>1</cell><cell></cell><cell>1</cell><cell></cell></row><row><cell>0.642</cell><cell></cell><cell>0.621</cell><cell></cell></row><row><cell>2</cell><cell></cell><cell>2</cell><cell></cell></row><row><cell>0.583</cell><cell></cell><cell>0.610</cell><cell></cell></row><row><cell>3</cell><cell></cell><cell>3</cell><cell></cell></row><row><cell>0.561</cell><cell></cell><cell>0.609</cell><cell></cell></row><row><cell>4</cell><cell></cell><cell>168</cell><cell></cell></row><row><cell>0.553</cell><cell></cell><cell>0.326</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 17 :</head><label>17</label><figDesc>Random Captioning samples on YouCook2 (val split).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 18 :</head><label>18</label><figDesc>Random Captioning samples on ActivityNet (ae-val split).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 19 :</head><label>19</label><figDesc>Random Captioning samples on ActivityNet (ae-test split).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Ehsan Adeli for helpful comments, Antoine Miech for providing details on their retrieval evaluation, and Facebook for providing us a GPU server with Tesla P100 processors for this research work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Importance of low-level supervision. In <ref type="figure">Fig. 4</ref>, we study the effect of adding uniform noise to the start and end frame index of each clip in ActivityNet-captions from the interval [−N f * P, +N f * P ]. N f is the total number of video frames and P is the noise percentage. We also perform a "full" noise experiment where we drop the temporal alignment labels of clips and sentences completely. We observe that increasing the noise from 0% to 40% consistently decreases the performance as labels get less reliable. For noise more than 40%, we do not observe significant changes in performance anymore. This is probably because at this noise level the labels become useless and are ignored. Still a good performance is obtained.</p><p>The study shows that the model is robust to noisy and missing low-level supervision and COOT is still able to capture useful dynamics between low-level and high-level semantics.</p><p>Impact of Text Encoding. We conduct ablation experiments to evaluate the importance of the text encoder for representation learning task. The ablation study results are shown in <ref type="table">Table 8</ref>. We first evaluate the COOT model and the HSE <ref type="bibr" target="#b20">[21]</ref> model with GloVe <ref type="bibr" target="#b89">[90]</ref> features. We then replace </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT:</head><p>Mix bread crumbs and parmesan cheese. Pound the chicken. Rub salt and pepper onto the chicken. Rub flour onto the chicken dip it in egg and coat with breadcrumbs. Fry the chicken in a pan. Spread sauce over the chicken.</p><p>Top the chicken with mozzarella cheese. Bake the chicken in the oven.</p><p>MART: Add tomatoes and beef to a pot. Add water to the pan. Add tomato puree and salt. Add the beef and parsley to the soup. Add the beef to the pot. Add water to the soup and let it simmer. Add the soup to the soup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COOT (Ours):</head><p>Add the tomatoes and onions to a food processor and blend them. Add the tomatoes and a bay leaf to the pot. Add the tomatoes and simmer. Remove the tomatoes from the pot and let it cook. Remove the tomatoes from the pot and let it cook. Strain the soup to a boil and let it boil. Turn on the heat and heat to a boil.</p><p>GT: Add tomato onion green chili and rice to a pan. Add water to the pan. Boil the ingredients and then turn down the heat. Strain the ingredients. Blend the ingredients. Add the water to the mixture and strain. Boil the soup. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT:</head><p>Mix bread crumbs and parmesan cheese. Pound the chicken. Rub salt and pepper onto the chicken. Rub flour onto the chicken dip it in egg and coat with breadcrumbs. Fry the chicken in a pan. Spread sauce over the chicken.</p><p>Top the chicken with mozzarella cheese. Bake the chicken in the oven.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1511.07067</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Look closer to ground better: Weakly-supervised temporal grounding of sentence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
		<idno>abs/2001.09308</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="817" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Grounding action descriptions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominikus</forename><surname>Wetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Bernt Schiele, and Manfred Pinkal</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Find and focus: Retrieve and localize video events with natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><forename type="middle">K</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, ICMR &apos;18</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval, ICMR &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhancing video summarization via vision-language embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1052" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv preprint arxiv:1907.13487</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="385" to="401" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XIII</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1106" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mule: Multimodal universal language embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2006)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<idno>abs/1703.07464</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Near-duplicate video retrieval with deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kordopatis-Zilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<idno>abs/1810.06951</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno>abs/1706.06905</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learnable pooling regions for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Enhancing sentence embedding with generalized pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1806.09828</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Alpha-pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayoung</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoul</forename><surname>Choi</surname></persName>
		</author>
		<idno>abs/1811.03436</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>L. K. Saul, Y. Weiss, and L. Bottou</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal cycle-consistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1801" to="1810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4437" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Move forward and tell: A progressive generator of video descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4729</idno>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos; Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li F Fei-Fei</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Joint visual-textual embedding for multimodal style search. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabi</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Oks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno>abs/1511.06078</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive image-language cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Sacheti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11059</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takato</forename><surname>Horii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07093</idno>
		<title level="m">Language and action learning using multimodal bert</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<idno>abs/1505.01861</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">Torralba</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Read</surname></persName>
		</author>
		<idno>abs/1706.00932</idno>
		<title level="m">Deep aligned representations. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Local-global video-text interactions for temporal grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">TALL: temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Wslln:weakly supervised natural language localization networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Jointly modeling deep video and compositional text to bridge vision and language in a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2346" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Weakly-supervised alignment of video with text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV -IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="4462" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Book2movie: Aligning video scenes with book chapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bäuml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1827" to="1835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Cycle-consistency for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6649" to="6658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Crdoco: Pixel-level domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning to collaborate for question answering and asking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1564" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Bohb: Robust and efficient hyperparameter optimization at scale. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1807.01774</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>abs/1706.02515</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Learning Representations (ICLR 2020)</title>
		<meeting>the Eighth International Conference on Learning Representations (ICLR 2020)</meeting>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
