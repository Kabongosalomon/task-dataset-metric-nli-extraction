<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapted Center and Scale Prediction: More Stable and More Accurate</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wang</surname></persName>
							<email>*correspondingauthor:wangwenhao@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematical Sciences(SMS)</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adapted Center and Scale Prediction: More Stable and More Accurate</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pedestrian Detection</term>
					<term>Anchor-free</term>
					<term>Switchable Normalization</term>
					<term>Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pedestrian detection benefits from deep learning technology and gains rapid development in recent years. Most of detectors follow general object detection frame, i.e. default boxes and two-stage process. Recently, anchor-free and one-stage detectors have been introduced into this area. However, their accuracies are unsatisfactory. Therefore, in order to enjoy the simplicity of anchor-free detectors and the accuracy of two-stage ones simultaneously, we propose some adaptations based on a detector, Center and Scale Prediction(CSP). The main contributions of our paper are: (1) We improve the robustness of CSP and make it easier to train. (2) We propose a novel method to predict width, namely compressing width. (3) We achieve the second best performance on CityPersons benchmark, i.e. 9.3% logaverage miss rate(MR) on reasonable set, 8.7% MR on partial set and 5.6% MR on bare set, which shows an anchor-free and one-stage detector can still have high accuracy. (4) We explore some capabilities of Switchable Normalization which are not mentioned in its original paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the prevalence of artificial intelligence technique, autonomous vehicles have gained more and more attention. Although automatic driving needs integration of a lot of technologies, pedestrian detection is one of the most important. That's because missing pedestrian detection could threaten pedestrians' lives. As a result, the performance of pedestrian detection algorithms is of great importance.</p><p>With the development of generic object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>, the detection performance on benchmark datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49]</ref> is significant improved. Also, some detectors, such as <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47]</ref>, are specially designed for pedestrian detection.</p><p>However, though detection performance is improved on benchmark datasets all the time, there is still a huge gap between current pedestrian detector and a careful people <ref type="bibr" target="#b47">[48]</ref>. Therefore, further performance improvement is necessary. Take pedestrian detection dataset, CityPersons <ref type="bibr" target="#b48">[49]</ref>, for instance. For a fair comparison, the following logaverage miss rates(denoted as MR)(lower is better) are reported on the reasonable validation set with the same input scale (1x).</p><p>From all of the state-of-the-arts literature available(including preprint ones), we summarize as follows: Repulsion Loss <ref type="bibr" target="#b42">[43]</ref>(13.2%), OR-CNN <ref type="bibr" target="#b51">[52]</ref>(12.8%), HBAN <ref type="bibr" target="#b24">[25]</ref>(12.5%), ALF <ref type="bibr" target="#b22">[23]</ref>(12.0%), Adaptive NMS <ref type="bibr" target="#b20">[21]</ref>(11.9%), CSP <ref type="bibr" target="#b23">[24]</ref>(11.0%), MGAN <ref type="bibr" target="#b27">[28]</ref>(10.5%), PSC-Net <ref type="bibr" target="#b44">[45]</ref>(10.4%), APD <ref type="bibr" target="#b46">[47]</ref>(8.8%). In the aforementioned state-of-the-arts methods, most of them have special occlusion/crowd handling process(7/9):</p><p>Repulsion Loss <ref type="bibr" target="#b42">[43]</ref>, OR-CNN <ref type="bibr" target="#b51">[52]</ref>, HBAN <ref type="bibr" target="#b24">[25]</ref>, Adaptive NMS <ref type="bibr" target="#b20">[21]</ref>, MGAN <ref type="bibr" target="#b27">[28]</ref>, PSC-Net <ref type="bibr" target="#b44">[45]</ref>, APD <ref type="bibr" target="#b46">[47]</ref>. In addition, APD <ref type="bibr" target="#b46">[47]</ref> uses more powerful backbone, i.e. DLA-34 <ref type="bibr" target="#b45">[46]</ref>, to improve MR from 10.6%(ResNet-50 <ref type="bibr" target="#b9">[10]</ref>) to 8.8%. APD <ref type="bibr" target="#b46">[47]</ref> also takes advantage of post process like Adaptive NMS <ref type="bibr" target="#b20">[21]</ref>.</p><p>For CSP <ref type="bibr" target="#b23">[24]</ref>, there is no special occlusion/crowd handling process or more powerful backbone. And it achieves competitive MR with other methods. However, there are also some unsolved problems existing in CSP <ref type="bibr" target="#b23">[24]</ref>. First, it is sensitive to the batch size. More specifically, in the case of a small batch size, such as (1, 1)(The bracket (, ) denotes (#GPUs,#samples per GPU)), or a big batch size, such as <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b3">4)</ref>, MR will not converge, i.e. MR will approach 1 after several iterations. Second, when training CSP <ref type="bibr" target="#b23">[24]</ref>, different input scales bring significantly different performance. Finally, when compared to algorithms with occlusion/crowd handling process, there is still much room for improvement.</p><p>To address the above limitations, we propose Adapted Center and Scale Prediction (ACSP), which has slight difference with original CSP <ref type="bibr" target="#b23">[24]</ref> but brings significant improvement on performance. Detection examples using ACSP are shown in <ref type="figure">Fig. 1</ref>. In summary, the main contributions of this paper are as follows: <ref type="bibr" target="#b0">(1)</ref> We make original CSP <ref type="bibr" target="#b23">[24]</ref> more robust, i.e. less sensitive to batch size and input scale. (2)We propose compressing width, a novel method to determine the width of a bounding box. (3)We improve the performance of CSP <ref type="bibr" target="#b23">[24]</ref>. (4) We explore the power of Switchable Normalization when the batch size is big.</p><p>Experiments are conducted on the CityPersons <ref type="bibr" target="#b48">[49]</ref> database. We achieve the second best performance, i.e. 9.3% MR on reasonable set, 8.7% MR on partial set, 5.6% MR on bare set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generic Object Detection</head><p>Early object detection approaches, such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42]</ref>, mainly utilize region proposal classification and sliding window paradigm. Since August 2018, more and more works focus on anchor-free approaches. As a result, modern object detectors can be divided into two classes: anchor-based and anchor-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Anchor-based</head><p>Anchor-based methods includes two-stage detectors and one-stage detectors. The most famous series of two-stage detectors are RCNN <ref type="bibr" target="#b8">[9]</ref> and its descendants, i.e. Fast-RCNN <ref type="bibr" target="#b7">[8]</ref>, Faster-RCNN <ref type="bibr" target="#b31">[32]</ref>. They build two-stage framework, which contains object proposals and classification. For one-stage detectors, YOLOv2 <ref type="bibr" target="#b30">[31]</ref> and SSD <ref type="bibr" target="#b21">[22]</ref> successfully accomplish detection and classification tasks on feature maps at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Anchor-free</head><p>The earliest exploitation of anchor-free mode comes from DenseBox <ref type="bibr" target="#b11">[12]</ref> and YOLOv1 <ref type="bibr" target="#b29">[30]</ref>. The main difference between them is that DenseBox is designed for face detection while YOLOv1 is a generic object detection. The introduction of CornerNet <ref type="bibr" target="#b15">[16]</ref> brings anchor-free detection into key point era. Its followers include ExtremeNet <ref type="bibr" target="#b55">[56]</ref>, CenterNet <ref type="bibr" target="#b54">[55]</ref>, etc. In addition, FoveaBox <ref type="bibr" target="#b14">[15]</ref> and FSAF <ref type="bibr" target="#b56">[57]</ref> use dense object detection strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pedestrian Detection</head><p>Before the dominance of deep learning techniques, traditional pedestrian detectors, such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b49">50]</ref>, focus on integral channel features with sliding window strategy. Recently, with the introduction of Faster RCNN <ref type="bibr" target="#b31">[32]</ref>, some two-stage pedestrian detection approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref> achieve state-of-the-arts on standard benchmarks. Also, some pedestrian detectors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>, which base on single-stage backbone, gain a balance between speed and accuracy.</p><p>Zhou et al. <ref type="bibr" target="#b52">[53]</ref> propose a discriminative feature transformation which enforces feature separability of pedestrian and non-pedestrian examples to handle occlusions for pedestrian detection. In <ref type="bibr" target="#b51">[52]</ref>, a new occlusion-aware R-CNN is proposed to improve the detection accuracy in the crowd. Wang et al. <ref type="bibr" target="#b42">[43]</ref> develop a novel loss, repulsion loss, to address crowd <ref type="figure">Figure 1</ref>: We use CityPersons test set to illustrate our ACSP detection ability. It is worthy to mention that, without any occlusion handling method, small and occlusion pedestrian can still be detected. The detections are shown in green rectangle boxes. occlusion problem. The work of <ref type="bibr" target="#b20">[21]</ref> focuses on Non-Maximum Suppression and proposes a dynamic suppression threshold to refine the bounding boxes given by detectors. HBAN <ref type="bibr" target="#b24">[25]</ref> is introduced to enhance pedestrian detection by fully utilizing the human head prior. ALFNet is proposed in <ref type="bibr" target="#b22">[23]</ref> to use asymptotic localization fitting strategy to evolve the default anchor boxes step by step into precise detection results. MGAN <ref type="bibr" target="#b27">[28]</ref> emphasizes on visible pedestrian regions while suppressing the occluded ones by modulating full body features. PSC-Net <ref type="bibr" target="#b44">[45]</ref> is designed for occluded pedestrian detection. CSP <ref type="bibr" target="#b23">[24]</ref> utilizes an anchor-free method, i.e. directly predicting pedestrian center and scale through convolutions. Based on CSP <ref type="bibr" target="#b23">[24]</ref>, Zhang et al. propose APD <ref type="bibr" target="#b46">[47]</ref> to differentiate individuals in crowds. All of the aforementioned methods achieve state-of-the-arts on CityPersons benchmark <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Normalization</head><p>Batch Normalization(BN) <ref type="bibr" target="#b12">[13]</ref> is proposed to accelerate training process and improve the performance of CNNs. <ref type="bibr" target="#b33">[34]</ref> points out that batch normalization makes the loss surface smoother while the original paper <ref type="bibr" target="#b12">[13]</ref> believes the improvement comes from "internal covariate shift". Although, even today, it is still unknown that why batch normalization works so well, the utilization of batch normalization improves the performance of object detection, image classification, etc.</p><p>After batch normalization, weight normalization(WN) <ref type="bibr" target="#b32">[33]</ref> is introduced to normalize the weights of layers. Layer normalization(LN) <ref type="bibr" target="#b0">[1]</ref> normalizes the inputs across the features instead of the batch dimension. In this way, the performance will not be influenced by batch size and layer normalization is used in RNN at first. Originally designed for style transfer, instance normalization(IN) <ref type="bibr" target="#b40">[41]</ref>   <ref type="figure">Figure 2</ref>: It is the architecture of original CSP <ref type="bibr" target="#b23">[24]</ref>. The frame includes two parts: feature extraction and detection head. malization(GN) <ref type="bibr" target="#b43">[44]</ref> divides the channels into groups and computes the mean and variance for normalization within each group. As a result, it addresses the problem that, when the batch size becomes smaller, the performance of batch normalization goes down. It is a combination of layer normalization and instance normalization to some degree.</p><p>Recently, Luo et al. propose switchable normalization(SN) <ref type="bibr" target="#b25">[26]</ref>, which uses a weighted average of different mean and variance statistics from batch normalization, instance normalization, and layer normalization.</p><p>3 Proposed Adaptation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CSP Revisit</head><p>CSP <ref type="bibr" target="#b23">[24]</ref> was proposed by Wei Liu and Shengcai Liao in 2019. They first introduced anchor-free method into pedestrian detection area. More specifically, CSP <ref type="bibr" target="#b23">[24]</ref> includes two parts: feature extraction and detection head. In feature extraction part, a backbone, such as ResNet-50 <ref type="bibr" target="#b9">[10]</ref>, MobileNet <ref type="bibr" target="#b10">[11]</ref>, is used to extract different levels of features. Shallower feature maps can provide more precise localization information while deeper feature maps can provide highlevel semantic information. In detection head part, convolutions are used to predict center, scale, and offset respectively. In <ref type="figure">Fig. 2</ref>, we summarize the architecture of CSP <ref type="bibr" target="#b23">[24]</ref>.</p><p>A more detailed architecture of CSP <ref type="bibr" target="#b23">[24]</ref> will be revisited in this paragraph. However, it will be slightly different with original paper <ref type="bibr" target="#b23">[24]</ref>. We take ResNet-50 <ref type="bibr" target="#b9">[10]</ref> and a picture with original shape, i.e. 1024×2048 for instance. The difference between keeping original shape and resizing picture to 640 × 1280 as showed in <ref type="bibr" target="#b23">[24]</ref> will be discussed in ablation study. First, CSP <ref type="bibr" target="#b23">[24]</ref> enlarges a picture with 3 channels into 64 channels through a 7x7 Conv layer. Certainly, BN layer, ReLU layer and Maxpool layer follow the Conv layer. In this way, a (3, 1024, 2048)(The bracket (, , ) denotes (#channels, height, width)) picture will be turned into a (64, 256, 512) one. Second, CSP <ref type="bibr" target="#b23">[24]</ref> take 4 layers from ResNet-50 <ref type="bibr">[</ref>  <ref type="bibr" target="#b23">[24]</ref> chooses to use a deconvolution layer to fuse the last 3 multi-scale feature maps into a single one. As a result, a (768, 256, 512) final feature map is made. Third, a 3x3 Conv layer is used on the final feature map to reduce its channel dimensions to 256. Finally, three convolutions: 1x1, 1x1 and 2x2 are appended for the prediction of center, scale and offset respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SN Layer</head><p>According to the aforementioned revisit, we conclude that there are totally 50 BN layers in CSP <ref type="bibr" target="#b23">[24]</ref>. Although BN layer brings performance improvement to CSP <ref type="bibr" target="#b23">[24]</ref> as it brings to other tasks, CSP <ref type="bibr" target="#b23">[24]</ref> also suffers from the drawback of BN layer. On one hand, BN layer is unsuitable when the batch size is small. That is because small batch size will make the training process noisy, i.e. the amplitude of training loss  is relatively huge. However, ablation study will show a bigger batch size, even <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b7">8)</ref>, brings more harm to CSP <ref type="bibr" target="#b23">[24]</ref>. More specifically, MR of validation set will decrease to near 16% and then increase to 1. It is likely that CSP <ref type="bibr" target="#b23">[24]</ref> falls into local optimum and loses generalization ability.</p><p>To address this limitation, we replace all BN layers with SN layers. The effectiveness of this change will be shown in the ablation part, we try to explain the reason of it now.</p><p>To illustrate more specifically, we take <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b7">8)</ref> for instance and the backbone is ResNet-50. The architecture of network can be divided into 6 parts: The first 5 come from backbone and the last one(denoted as After) is detection head. The first part(denoted as Before) is the operations before 4 layers in ResNet-50. The next four is the four layers. There is only 1 BN layer in the first part while there are 9, 12, 18, and 9 BN layers in the next four parts respectively. Finally, the detection head has only 1 BN layer. As suggested in 2.3, Switchable normalization is the combination of batch normalization, instance normalization, and layer normalization with different weights. Therefore, exploring the proportion of different weights in each part of network will show what makes a difference on earth. For each part, we calculate the weights of each normalization method in SN layers. Then the average of these weights are shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. Although different normalization methods have different weights in each part, we figure out two main differences with original BN layers. For one hand, in Layer4, the weight of BN variance is very small while the weight of LN variance is very big. For the other hand, in 'After' Part, IN, LN and BN share similar weights. The conclusions are as follows: (i) The low BN variance in Layer4 decreases the influence of noise when estimating variance. In this way, high-level semantic information can be utilized fully during inference process. (ii) The similar weights in 'After' part enable these three normalization methods to play same important roles. (iii) Different normalization methods in all parts complement one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Backbone</head><p>The feature extraction ability of backbone is of great importance in object detection. Some networks, such as, ResNet-50 <ref type="bibr" target="#b25">[26]</ref>, ResNet-101 <ref type="bibr" target="#b25">[26]</ref>, VGG <ref type="bibr" target="#b35">[36]</ref> and MobileNet <ref type="bibr" target="#b10">[11]</ref>, which are original designed for image classification, are widely used in pedestrian detectors. In addition, some other networks, such as DetNet <ref type="bibr" target="#b17">[18]</ref>, are specially designed for object detection.</p><p>In the original paper <ref type="bibr" target="#b23">[24]</ref>, ResNet-50 <ref type="bibr" target="#b25">[26]</ref> and Mo-bileNet <ref type="bibr" target="#b10">[11]</ref> are used as backbone. However, because of the nature of CSP <ref type="bibr" target="#b23">[24]</ref>, i.e. it fuses different level of feature maps, it is suitable to use deeper backbone network. In this way, the location information will still be stored in shallow feature maps and higherlevel semantic information will be extracted at the same time.</p><p>Inspiring by the aforementioned idea, we select two new backbones, expecting to obtain better performance. First, we use ResNet-101 <ref type="bibr" target="#b25">[26]</ref> as our ACSP backbone. Compared to ResNet-50 <ref type="bibr" target="#b25">[26]</ref>, the only difference of ResNet-101 <ref type="bibr" target="#b25">[26]</ref> is its third layer: there are 23 Bottleneck blocks rather than 6 Bottleneck blocks. As a result, in our ACSP, the last two feature maps presents higher level semantic information than CSP <ref type="bibr" target="#b23">[24]</ref>. Meanwhile, localization information will not be changed. In theory, the fusion in our ACSP is more efficient than original CSP <ref type="bibr" target="#b23">[24]</ref>. We will conduct ablation study to prove it. Second, in <ref type="bibr" target="#b17">[18]</ref>, authors point out that using DetNet <ref type="bibr" target="#b17">[18]</ref> as backbone, they achieve state-of-the-art on the MSCOCO benchmark <ref type="bibr" target="#b19">[20]</ref>. Therefore, it is likely that DetNet <ref type="bibr" target="#b17">[18]</ref> will improve the performance of original CSP <ref type="bibr" target="#b23">[24]</ref>. However, after fine tuning learning rate and so on, we find it is unpromising. We conclude the reason is that: one of the design concept of DetNet <ref type="bibr" target="#b17">[18]</ref> is to address poor location problem, however, in CSP <ref type="bibr" target="#b23">[24]</ref>, this problem is solved by the fusion of different level layers and efficient center prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Input Size</head><p>In the original paper <ref type="bibr" target="#b23">[24]</ref>, Liu et al. do not justify the resizing process, i.e. why in training part, the authors resize the original picture shape(1024 × 2048) into 640 × 1280. After comparison, we find that: For one hand, resizing shape is beneficial to time-saving and memory-saving. More importantly, keeping original shape will worsen the performance of CSP <ref type="bibr" target="#b23">[24]</ref> and bring non-convergent results. However, most of its counterparts take advantage of original resolution and achieve state-of-the-arts. Inspiring by this, we believe some adjustment will make a difference. Based on the improvement in 3.2, we compare the performance between keeping and resizing. In ablation part, we will show that the performance also becomes worse, however. We conclude the reasons: First, some noise may exist in the original pictures. With the ResNet-50 backbone, the semantic features are not extracted adequately. Therefore, parameters of the network may be influenced by noise. Second, the quanlity of parameter is not sufficient to fit the useful part of so high resolution pictures. Finally, resizing process will omit some detail features, and focusing on them excessively will influence the ability of generalization.</p><p>To address the aforementioned problems, we replace ResNet-50 with ResNet-101 as suggested in 3.3. In this way, the performance is improved and we achieve the lowest MR of our ACSP. The reasons are as follows: For one hand, the increase of parameters enhances fitting ability of our ACSP. For the other hand, the increase of layers enables our ACSP to extract more high-level semantic feature and decrease the focus on details. Therefore, ACSP is immune to noise and has more generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Compressing Width</head><p>From the original paper <ref type="bibr" target="#b23">[24]</ref>, we can see that the width of a box is obtained by multiplying the height by 0.41. It concurs with pedestrian aspect ratio in CityPersons Dataset <ref type="bibr" target="#b48">[49]</ref>. However, it is not suitable in the reference process. That is because, in crowded scene, relatively wide boxes will increase the chance of overlapping and the NMS process will eliminate some of boxes. In this way, we will lose some detections.</p><p>As a result, we try to design a novel method to determine the width. On one hand, as we mentioned before, a wide box is not appropriate. On the other hand, a too narrow box is also not suitable. That is because, in this way, IoU between detections and ground truths will be small and detections will not be regarded as correct. Inspiring by the aforementioned analysis, we give our formula for calculating width:</p><formula xml:id="formula_0">w = r · h,</formula><p>where r is the aspect ratio(r &lt; 0.41) and h is the predicted height of a bounding box.</p><p>It should be mentioned that the exact form of our compressing width is not crucial and we choose the most basic one. What matters most is the design concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Vanilla L1 Loss</head><p>As pointed in <ref type="bibr" target="#b23">[24]</ref>, total loss consists of classification loss, scale loss and offset loss. The weights are 0.01, 1 and 0.1, respectively. And for scale regression loss, <ref type="bibr" target="#b23">[24]</ref> utilizes Smooth L1 to accelerate convergence. However, <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">55]</ref> show that vanilla L1 is better than Smooth L1. Therefore, we try to replace Smooth L1 with vanilla L1. We experimentally set the weights as 0.01, 0.05 and 0.1, respectively. The effectiveness of this improvement will be shown in ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Dataset</head><p>To prove the efficacy of our adaptation, we conduct our experiments on CityPersons Dataset <ref type="bibr" target="#b48">[49]</ref>. CityPersons is introduced recently and with high resolution. And the dataset is based on CityScapes benchmark <ref type="bibr" target="#b2">[3]</ref>. It includes 5, 000 images with various occlusion levels. We train our model on official training set with 2, 975 images and test on the validation set with 500 images. In our test, the input scale is 1x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Training details</head><p>The ACSP is realized in Pytorch <ref type="bibr" target="#b28">[29]</ref>. Adam <ref type="bibr" target="#b13">[14]</ref> optimizer is utilized to optimize our network. Same as CSP <ref type="bibr" target="#b23">[24]</ref> and APD <ref type="bibr" target="#b46">[47]</ref>, moving average weights <ref type="bibr" target="#b39">[40]</ref> is adopted. Experiments show it helps achieve better performance. The backbone is fixed ResNet-101 <ref type="bibr" target="#b25">[26]</ref> unless otherwise stated, i.e. replacing all BN layers with SN layers. It is pretrained on ImageNet <ref type="bibr" target="#b3">[4]</ref>. We optimize the network on 2 GPUs (Tesla V100) with 2 images per GPU. The learning rate is 2 × 10 −4 and training process is stopped after 150 epochs with 744 iterations per epoch. In the training process, we keep the original shape of pictures, i.e. 1024 × 2048.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>In this section, we conduct an ablative analysis on the CityPersons Dataset <ref type="bibr" target="#b48">[49]</ref>. We use the most common and standard pedestrian detection criterion, logaverage miss rates(denoted as MR), as evaluation metric. In addition, the following MRs are all reported on reasonable set.</p><p>What is the influence of SN layer on stable training?</p><p>The stability of training process is of great importance. It comes from two aspects: whether the network is sensitive to the batch size and whether the performance will become poor after many iterations. To answer these two questions, we compare our ACSP with original CSP <ref type="bibr" target="#b23">[24]</ref>. It should be mentioned that learning rate is appropriate in the following experiments, i.e. the training loss decreases and converges.</p><p>For the first one, comparisons are shown in <ref type="table" target="#tab_3">Table  1</ref>. To conduct a fair comparison, the only difference is we replace all BN layers with SN layers, i.e. the backbone is still Resnet-50, the training input scale is still 640×1280 and so on. In the table, the bracket (, ) denotes (#GPUs,#samples per GPU). For instance, (4, 4) means 4 GPUs with 4 images per GPU. 'Con' means the training is convergent, i.e. MR is still low no matter how many iterations are used. 'Exp' means the training is not convergent, i.e. MR increases to 1 after several iterations. The improvement line shows the percentage of decrease in MR from CSP <ref type="bibr" target="#b23">[24]</ref> to ACSP. It is shown that when we choose GPU number and image number per GPU carefully, such as (4, 2), (2, 2), although ACSP outperforms CSP <ref type="bibr" target="#b23">[24]</ref> to some degree, the improvement is not significant. However, when the batch size is bigger or smaller, such as (4, 4), (1, 1), ACSP brings conspicuous improvement. It is noteworthy that, though batch size is 8, there is a huge difference in MR between (4, 2) and <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b7">8)</ref> for CSP <ref type="bibr" target="#b23">[24]</ref>. That difference does not come from BN layer because BN layer will only be invalid when it is (8, 1) rather than <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b7">8)</ref>. Therefore, it is impossible to reproduce the reported result in <ref type="bibr" target="#b23">[24]</ref> for someone who only has single GPU resource.</p><p>For the second one, we can come to a conclusion from <ref type="figure" target="#fig_1">Figure 4</ref> and <ref type="table" target="#tab_3">Table 1</ref>. For CSP <ref type="bibr" target="#b23">[24]</ref>, only (4, 2) and (2, 2) bring convergence result. However, for ACSP, all of the results are convergent.</p><p>How important is the backbone?</p><p>In this part, we compare three different backbones, i.e. ResNet-50 <ref type="bibr" target="#b25">[26]</ref>, ResNet-101 <ref type="bibr" target="#b25">[26]</ref>, DetNet <ref type="bibr" target="#b17">[18]</ref>. The experiments are conducted based on BN layer  and SN layer respectively. The experiments setting is <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b1">2)</ref>. And the input size is 640×1280. The results are reported in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>We can conclude that: (i) As suggested in the theory part, ResNet-101 <ref type="bibr" target="#b25">[26]</ref> outperforms ResNet-50 <ref type="bibr" target="#b25">[26]</ref> no matter which normalization method is choosen.</p><p>(ii) DetNet <ref type="bibr" target="#b17">[18]</ref> underperforms ResNet-50 <ref type="bibr" target="#b25">[26]</ref> and ResNet-101 <ref type="bibr" target="#b25">[26]</ref> slightly. (iii) As discussed before, replacing BN layers with SN layers bring performance improvement on ResNet-50 <ref type="bibr" target="#b25">[26]</ref> and ResNet-101 <ref type="bibr" target="#b25">[26]</ref>. However, on DetNet <ref type="bibr" target="#b17">[18]</ref>, the MR increases. That is partly because we cannot find pretrained parameters of SN layers in DetNet <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How important is the input resolution?</head><p>To prove the discussion in 3.4, we conduct some experiments with different resolutions under different circumstances. From <ref type="table" target="#tab_5">Table 3</ref>, we can find that: For original CSP <ref type="bibr" target="#b23">[24]</ref>, the MR is not convergent when we do not resize pictures to 640×1280. When we use SN, as expected, the MR is convergent and performance is improved. But keeping original resolution is still not a better choice.</p><p>In <ref type="table" target="#tab_6">Table 4</ref>, the experiments are conducted using SN as normalization method and ResNet-101 as backbone. As analysed in 3.4, the performance gets better no matter which batch size is chosen. What is the contribution of SN layer to the MR?</p><p>As stated in the before part, SN layer brings significant improvement when batch size is not carefully selected. In addition, from <ref type="table" target="#tab_4">Table 2</ref>, we conclude SN     <ref type="table" target="#tab_5">Table 3</ref> shows no matter which solution we select, SN layer always contributes to performance improvement. Finally, as displayed in <ref type="table" target="#tab_6">Table  4</ref>, we obtain our best performance under the help of SN layer. In conclusion, SN layer can substitute BN layer totally in our ACSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How important is the compressing width and vanilla L1 loss?</head><p>We talk about the contribution of the compressing width and vanilla L1 loss together in this part. Experiments show that, for Smooth L1, setting r in compressing width formula as 0.40 yields relatively good performance. And for vanilla L1 loss, r = 0.36 is suitable. It should be mentioned that other settings may yield better results, but we choose to keep these settings in the following paragraphs(except where noted).</p><p>First, we only replace r = 0.41 with r = 0.40, and the results are shown in <ref type="table" target="#tab_7">Table 5</ref>. It can be seen that MR decreases about 0.3%.</p><p>Second, we compare the performance of Smooth L1 with vanilla L1 under respective optimized r. As displayed in <ref type="table" target="#tab_8">Table 6</ref>, MR decreases to varying degrees on reasonable set, partial set, and bare set. However, MR increases about 0.2% on heavy set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State of the Arts</head><p>We compare our ACSP with all existing state-of-theart detectors(including preprint ones) on the validation set of CityPersons. The results are shown in <ref type="table">Table 7</ref>. The evaluation metric is MR. To conduct a fair comparison, all methods are trained on the training set without any extra data(except ImageNet). When testing, the input scale is 1x. The top three results are highlighted in red, green and blue, respectively. Because the difference in training and test environment, i.e. most of other methods use Nvidia GTX 1080Ti GPU while we use Nvidia Tesla V100 GPU, time comparing is meaningless. As a result, it will not be reported in our table.</p><p>From the table, we can figure out that our ACSP achieves state-of-the-art on bare set and the second best performance on reasonable set, heavy set and <ref type="table">Table 7</ref>: Comparisons with state-of-the-arts on validation set: The evaluation metric is MR and the input scale is 1x. The top three results are highlighted in red, green and blue, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Reasonable Heavy Partial Bare FRCNN <ref type="bibr" target="#b48">[49]</ref> VGG-16 15.4% ---FRCNN+Seg <ref type="bibr" target="#b48">[49]</ref> VGG-16 14.8% ---TLL <ref type="bibr" target="#b36">[37]</ref> ResNet-50 15.5% 53.6% 17.2% 10.0% TLL+MRF <ref type="bibr" target="#b36">[37]</ref> ResNet-50 14.4% 52.0% 15.9% 9.2% Repulsion Loss <ref type="bibr" target="#b42">[43]</ref> ResNet-50 13.2% 56.9% 16.8% 7.6% OR-CNN <ref type="bibr" target="#b51">[52]</ref> VGG-16 12.8% 55.7% 15.3% 6.7% HBAN <ref type="bibr" target="#b24">[25]</ref> VGG-16 12.5% 48.1% --ALF <ref type="bibr" target="#b22">[23]</ref> ResNet-50 12.0% 51.9% 11.4% 8.4% Adaptive NMS <ref type="bibr" target="#b20">[21]</ref> ResNet-50 11.9% 54.0% 11.4% 6.2% CSP <ref type="bibr" target="#b23">[24]</ref> ResNet-50 11.0% 49.3% 10.4% 7.3% MGAN <ref type="bibr" target="#b27">[28]</ref> VGG-16 10.5% 47.2% --PSC-Net <ref type="bibr" target="#b44">[45]</ref> VGG-16 10.4% 39.7% --APD <ref type="bibr" target="#b46">[47]</ref> ResNet-50 10.6% 49.8% 9.5% 7.1% APD <ref type="bibr" target="#b46">[47]</ref> DLA partial set. On reasonable set, the best one, APD <ref type="bibr" target="#b46">[47]</ref>, uses more powerful backbone and other post process method. Without DLA-34, its MR will increase to 10.6% instead. On heavy set, without any special occlusion handling process, we outperform other special designed methods except for PSC-Net <ref type="bibr" target="#b44">[45]</ref>. Also, we only lags behind APD <ref type="bibr" target="#b46">[47]</ref> on partial set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose several improvements on original pedestrian detector CSP <ref type="bibr" target="#b23">[24]</ref>. In this way, the training process of our ACSP is more robust. And we try to explain why we make these adaptations and why they make a difference. What's more, we propose a novel method to estimate the width of a bounding box. In addition, we explore some functions of Switchable Normalization which are not mentioned in its original paper <ref type="bibr" target="#b25">[26]</ref>. Experiments are conducted on the CityPersons <ref type="bibr" target="#b48">[49]</ref> and we achieve state-of-theart on bare set and the second best performance on reasonable set, heavy set and partial set. In the future, it is interesting to explore the "representative point" rather than the "center point" of pedestrian.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The proportion of the weight of each normalization method in different parts is shown in the histogram. The weights of mean and variance are displayed separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Comparisons of different batch size. It is shown that: For CSP<ref type="bibr" target="#b23">[24]</ref>, 4 experiment settings are not convergent; for ACSP, all experiments are convergent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Feature Map 256 channels Detections Center Heatmap Scale Map Offset Map Conv 1x1 Conv 1x1 Conv 2x2</head><label></label><figDesc>normalizes across each channel in each training example. Group nor-</figDesc><table><row><cell></cell><cell></cell><cell>H/4</cell><cell></cell><cell>H/4</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Feature</cell><cell></cell><cell>Conv</cell><cell></cell></row><row><cell>H</cell><cell>W</cell><cell>Extraction</cell><cell>W/4</cell><cell>3x3x256</cell><cell>W/4</cell></row><row><cell></cell><cell>Input</cell><cell></cell><cell>Feature Map C channels</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>10] with dilated convolutions. The 4 layers downsample the input image by 4, 8, 16, 16 respectively. At that time, we get 4 feature maps: (256, 256, 512), (512, 128, 256), (1024, 64, 128), (2048, 64, 128). CSP</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of different batch sizes and different methods. The bracket (, ) denotes (#GPUs,#samples per GPU). 'Con' means the training is convergent while 'Exp' means the training is not convergent. Bold number indicates the best result.</figDesc><table><row><cell></cell><cell cols="2">MR batch</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(4, 2)</cell><cell></cell><cell>(4, 4)</cell><cell>(2, 2)</cell><cell>(1, 1)</cell><cell>(1, 8)</cell><cell>(8, 1)</cell></row><row><cell></cell><cell cols="2">method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>CSP</cell><cell cols="5">11.56% Con 27.75% Exp 11.34% Con 16.35% Exp 16.10% Exp 14.51% Exp</cell></row><row><cell></cell><cell cols="2">ACSP</cell><cell cols="5">11.16% Con 11.89% Con 10.80% Con 13.42% Con 11.66% Con 12.88% Con</cell></row><row><cell></cell><cell cols="2">Improvement</cell><cell></cell><cell>+3.46%</cell><cell></cell><cell>+57.15%</cell><cell>+4.76%</cell><cell>+17.92%</cell><cell>+27.58% +11.23% Con</cell></row><row><cell></cell><cell></cell><cell cols="5">Comparison between Different Batch Size</cell></row><row><cell>MR</cell><cell>20% 40% 60% 80% 100%</cell><cell>ACSP (1,1) CSP (1,1) ACSP (2,2) CSP (2,2) ACSP (4,2) CSP (4,2) ACSP (4,4) CSP (4,4) ACSP (8,1) CSP (8,1) ACSP (1,8) CSP (1,8)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40 epoch</cell><cell>50</cell><cell>60</cell><cell>70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Comparisons of different backbones and</cell></row><row><cell cols="3">different normalization methods. Bold number</cell></row><row><cell cols="3">indicates the best result. The experiments setting is</cell></row><row><cell>(4, 2).</cell><cell></cell></row><row><cell>MR Method</cell><cell></cell></row><row><cell></cell><cell>BN</cell><cell>SN</cell></row><row><cell>Backbone</cell><cell></cell></row><row><cell>ResNet-50</cell><cell cols="2">11.56% 11.16%</cell></row><row><cell cols="3">ResNet-101 11.29% 10.91%</cell></row><row><cell>DetNet</cell><cell cols="2">12.66% 12.91%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison between different resolutions under different normalization methods. Resolution part means the input picture scale. The experiments setting is (2, 2) and the backbone is ResNet-50.</figDesc><table><row><cell>MR</cell><cell>Resolution</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">1024 × 2048 640 × 1280</cell></row><row><cell>Method</cell><cell></cell><cell></cell></row><row><cell></cell><cell>BN</cell><cell>30.08%</cell><cell>11.34%</cell></row><row><cell></cell><cell>SN</cell><cell>11.41%</cell><cell>10.80%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison between different resolutions under different batch sizes. Resolution part means the input picture scale. The normalization method is SN and the backbone is ResNet-101.</figDesc><table><row><cell>Batch MR</cell><cell>Resolution</cell><cell cols="2">1024 × 2048 640 × 1280</cell></row><row><cell></cell><cell>(2, 2)</cell><cell>10.30%</cell><cell>10.81%</cell></row><row><cell></cell><cell>(4, 2)</cell><cell>10.69%</cell><cell>10.91%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparisons between different aspect ratio under different sets.</figDesc><table><row><cell>MR Set</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Reasonable Heavy Partial Bare</cell></row><row><cell>Ratio</cell><cell></cell><cell></cell></row><row><cell>r = 0.41</cell><cell>10.30%</cell><cell>46.12% 9.15% 6.79%</cell></row><row><cell>r = 0.40</cell><cell>10.00%</cell><cell>46.11% 8.80% 6.65%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparisons between different L1 loss under different sets.</figDesc><table><row><cell>L1 MR Set</cell><cell cols="2">Reasonable Heavy Partial Bare</cell></row><row><cell>Smooth</cell><cell>10.00%</cell><cell>46.11% 8.80% 6.65%</cell></row><row><cell>Vanilla</cell><cell>9.27%</cell><cell>46.34% 8.66% 5.62%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgment</head><p>We thank Informatization Office of Beihang University for the supply of High Performance Computing Platform, which have 32 Nvidia Tesla V100 GPUs. This work is also supported by School of Mathematical Sciences, Beihang University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eurocity persons: A novel benchmark for person detection in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dariu M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1844" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1091" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detnet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An extended set of haar-like features for rapid object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Maydt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. international conference on image processing</title>
		<meeting>international conference on image processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive nms: Refining pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning efficient singlestage pedestrian detectors by asymptotic localization fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High-level semantic feature detection: A new perspective for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5187" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semantic head enhanced pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11985</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Differentiable learning-tonormalize via switchable normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanglin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10779</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Local decorrelation for improved detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonhyun</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Hee</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1134</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mask-guided attention network for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Haris</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4967" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2483" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Small-scale pedestrian detection based on topological line localization and temporal feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leiyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust realtime object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7774" to="7783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Psc-net: Learning part spatial co-occurence for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09252</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">CSID: center, scale, identity and density-aware pedestrian detection in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1910.09188</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards reaching human performance in pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="973" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3213" to="3221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6995" to="7003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Occlusion-aware r-cnn: detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Discriminative feature transformation for occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9557" to="9566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bi-box regression for pedestrian detection and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for singleshot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
