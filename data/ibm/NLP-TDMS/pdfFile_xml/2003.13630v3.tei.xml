<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TResNet: High Performance GPU-Dedicated Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
							<email>tal.ridnik@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
							<email>hussam.lawen@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
							<email>asaf.noy@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben</surname></persName>
							<email>emanuel.benbaruch@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baruch</forename><surname>Gilad</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharir</forename><forename type="middle">Itamar</forename><surname>Friedman</surname></persName>
							<email>itamar.friedman@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DAMO Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TResNet: High Performance GPU-Dedicated Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many deep learning models, developed in recent years, reach higher ImageNet accuracy than ResNet50, with fewer or comparable FLOPs count. While FLOPs are often seen as a proxy for network efficiency, when measuring actual GPU training and inference throughput, vanilla ResNet50 is usually significantly faster than its recent competitors, offering better throughput-accuracy trade-off.</p><p>In this work, we introduce a series of architecture modifications that aim to boost neural networks' accuracy, while retaining their GPU training and inference efficiency. We first demonstrate and discuss the bottlenecks induced by FLOPs oriented optimizations. We then suggest alternative designs that better utilize GPU structure and assets. Finally, we introduce a new family of GPU-dedicated models, called TResNet, which achieves better accuracy and efficiency than previous ConvNets Using a TResNet model, with similar GPU throughput to ResNet50, we reach 80.8% top-1 accuracy on Ima-geNet. Our TResNet models also transfer well and achieve state-of-the-art accuracy on competitive single-label classification datasets such as Stanford Cars (96.0%), CIFAR-10 (99.0%), CIFAR-100 (91.5%) and Oxford-Flowers (99.1%). TResNet models also achieve state-of-the-art results on a multi-label classification task, and perform well on object detection. Implementation is available at: https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The seminal ResNet models <ref type="bibr" target="#b9">[10]</ref>, introduced in 2015, revolutionized the world of deep learning. ResNet models use repeated well-designed residual blocks, allowing training of very deep networks to high accuracy while maintaining high GPU utilization. ResNet models are also easy to train, and converge fast and consistent even with plain SGD optimizer <ref type="bibr" target="#b44">[45]</ref>. NVIDIA Volta tensor cores <ref type="bibr" target="#b26">[27]</ref> fur-ther improved ResNet models GPU utilization, up to quadrupling their GPU throughput on mixed-precision training and inference <ref type="bibr" target="#b43">[44]</ref>. Among the ResNet models, ResNet50 established itself as a prominent model in terms of speedaccuracy trade-off, and became a leading backbone model for many computer vision tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Since ResNet50, many modern deep learning models were developed, which achieve better ImageNet accuracy with fewer or comparable FLOPs. Surprisingly, even though most deep learning models are trained, and sometimes deployed, on GPUs, few models try explicitly to find an optimal design in terms of GPU throughput. Since FLOPs are not an accurate proxy for GPU speed <ref type="bibr" target="#b1">[2]</ref>, suboptimal design for GPUs might occur. This is especially true for GPU training speed, which is rarely measured and documented in academic literature, and can be severely hindered by some modern architecture design tricks <ref type="bibr" target="#b25">[26]</ref>. <ref type="table">Table 1</ref> compares ResNet50 to popular newer architectures, with similar ImageNet top-1 accuracy -ResNet50-D <ref type="bibr" target="#b10">[11]</ref>, ResNeXt50 <ref type="bibr" target="#b42">[43]</ref>, SEResNeXt50 <ref type="bibr" target="#b12">[13]</ref>, EfficientNet-B1 <ref type="bibr" target="#b35">[36]</ref> and MixNet-L <ref type="bibr" target="#b36">[37]</ref>. We see from <ref type="table">Table 1</ref> that the reduction of FLOPs and the usage of new tricks in modern networks, compared to ResNet50, is not translated to improvement in GPU throughput. This is especially evident for GPU training speed, where ResNet50 gives by a large margin better speed-accuracy trade-off. We identify two main reasons for this throughput gap: 1. Modern networks like EfficientNet, ResNeXt and MixNet do extensive usage of depthwise and 1x1 convolutions, that provide significantly fewer FLOPs than 3x3 convolutions. However, GPUs are usually limited by memory access cost and not by number of computations, especially for low-FLOPs layers. Hence, the reduction in FLOPs is not translated well to an equivalent increase in GPU throughput <ref type="bibr" target="#b25">[26]</ref>. 2. Modern networks like ResNeXt and MixNet do extensive usage of multi-path. For training, this creates lots of activation maps that need to be stored for backward propagation, which reduces the maximal possible batch size, thus  <ref type="table">Table 1</ref>. Comparison of ResNet50 to top modern networks, with similar top-1 ImageNet accuracy. All measurements were done on Nvidia V100 GPU with mixed precision. For gaining optimal speeds, training and inference were measured on 90% of maximal possible batch size. Except TResNet-M, all the models' ImageNet scores were taken from the public repository <ref type="bibr" target="#b40">[41]</ref>, which specialized in providing top implementations for modern networks. Except EfficientNet-B1, which has input resolution of 240, all other models have input resolution of 224.</p><p>hurting the GPU throughput. Multi-path also limits the ability to use inplace operations <ref type="bibr" target="#b31">[32]</ref>, and can lead to network fragmentation <ref type="bibr" target="#b25">[26]</ref>. Following our analysis of <ref type="table">Table 1</ref>, we want to design a new family of networks, TResNet, aimed at high accuracy while maintaining high GPU utilization. TResNet models will contain the latest published design tricks available, along with our own novelties and optimizations. Unlike previous works, which measure only the FLOPS proxy or just GPU inference speed, we will directly focus on both GPU inference and training speed. For a proper comparison to previous models, one network variant (TResNet-M) is designed to match ResNet50 GPU throughput, while the rest match modern larger architectures.</p><p>We will show that for all tested datasets, TResNet models offer an improved speed-accuracy trade-off. Specifically, they reach ImageNet top-1 accuracy of 80.8% with GPU throughput similar to ResNet50 (79.0%), and top-1 accuracy of 84.3% with better GPU throughput than EfficientNet-B5 (83.7%). Besides ImageNet, TResNet models also achieve state-of-the-art accuracy on 3 out of 4 widely used downstream single-label datasets, with x8-15 faster GPU inference speed. They also excel on multi-label classification and object detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TResNet Design</head><p>TResNet design is based on the ResNet50 architecture, with dedicated refinements, modifications and optimizations. It contains three variants, TResNet-M, TResNet-L and TResNet-XL, that vary only in their depth and the number of channels. TResNet architecture contains the following refinements and changes compared to plain ResNet50 design:</p><p>• SpaceToDepth Stem Previous works usually offer refinements to ResNet50 which increase the accuracy at the cost of reducing the GPU throughput <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46]</ref>. Differently, in our design some refinements increase the models' throughput and some decrease it. All-in-all, for TResNet-M we chose a mixture of refinements that provide a similar GPU throughput to ResNet50, for a fair comparison of the models' accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Refinements</head><p>SpaceToDepth Stem -Neural networks usually start with a stem unit -a component whose goal is to quickly reduce the input resolution. ResNet50 stem is comprised of a stride-2 conv7x7 followed by a max pooling layer <ref type="bibr" target="#b9">[10]</ref>, which reduces the input resolution by a factor of 4 (224 → 56). ResNet50-D stem design <ref type="bibr" target="#b10">[11]</ref>, for comparison, is more elaborate -the conv7x7 is replaced by three conv3x3 layers. The new ResNet50-D stem design did improve accuracy, but at a cost of lowering the training throughput -see <ref type="table">Table 1</ref>, where the new stem design is responsible for almost all the decline in the throughput.</p><p>We wanted to replace the traditional convolution-based downscaling unit by a fast and seamless layer, with little information loss as possible, and let the well-designed residual blocks do all the actual processing work. The new stem layer sole functionality should be to downscale the input resolution to match the rest of the architecture, e.g., by a factor of 4. We met these goals by using a dedicated Space-ToDepth transformation layer <ref type="bibr" target="#b32">[33]</ref>, that rearranges blocks of spatial data into depth. Notice that in contrast to <ref type="bibr" target="#b32">[33]</ref>, which mainly used SpaceToDepth in the context of isometric (single-resolution) networks, in our novel design Space-ToDepth is used as a drop-in replacement for the tradition stem unit. The SpaceToDepth layer is followed by simple convolution, to match the number of wanted channels, as can be seen in <ref type="figure" target="#fig_1">Figure 1</ref>. Anti-Alias Downsampling (AA) - <ref type="bibr" target="#b45">[46]</ref> proposed to replace all downscaling layers in a network by an equivalent AA component, to improve the shift-equivariance of deep networks and give better accuracy and robustness.</p><p>We implemented an economic variant of AA, similar to <ref type="bibr" target="#b19">[20]</ref>, that provides an improved speed-accuracy trade-offonly our stride-2 convolutions are replaced by stride-1 convolutions followed by a 3x3 blur kernel filter with stride 2, as described in <ref type="figure">Figure 2</ref>. <ref type="figure">Figure 2</ref>. The AA downsampling scheme of TResNet architecture. All stride-2 convolutions are replaced by stride-1 convolutions, followed by a fixed downsampling blur filter <ref type="bibr" target="#b45">[46]</ref>.</p><p>In-Place Activated BatchNorm (Inplace-ABN) -Along the architecture, we replaced all BatchNorm+ReLU layers by Inplace-ABN <ref type="bibr" target="#b31">[32]</ref> layers, which implements BatchNorm with activation as a single inplace operation, allowing to reduce significantly the memory required for training deep networks, with only a small increase in the computational cost. As an activation function for the Inplace-ABN, we chose to use Leaky-ReLU instead of ResNet50's plain ReLU.</p><p>Using Inplace-ABN in TResNet models offers the following advantages:</p><p>• BatchNorm layers are major consumers of GPU memory. Replacing BatchNorm layers with Inplace-ABN enables to significantly increase the maximal possible batch size, which can improve the GPU utilization.</p><p>• For TResNet models, Leaky-ReLU provides better accuracy than plain ReLU. While some modern activation, like Swish and Mish <ref type="bibr" target="#b27">[28]</ref>, might also give better accuracy than ReLU, their GPU memory consumption is higher, as well as their computational cost. In contrast, Leaky-ReLU has exactly the same GPU memory consumption and computational cost as plain ReLU.</p><p>• The increased batch size can also improve the effectiveness of popular algorithms like triplet loss <ref type="bibr" target="#b18">[19]</ref> and momentum-contrastive learning. <ref type="bibr" target="#b8">[9]</ref> Novel Block-Type Selection -ResNet34 and ResNet50 share the same architecture, with one difference: ResNet34 uses solely 'BasicBlock' layers, which comprise of two conv3x3 as the basic building block, while ResNet50 uses 'Bottleneck' layers, which comprise of two conv1x1 and one conv3x3 as the basic building block <ref type="bibr" target="#b9">[10]</ref>. Bottleneck layers have higher GPU usage than BasicBlock layers, but usually give better accuracy. However, BasicBlock layers have larger receptive field, so they might be more suited to be placed at the early stages of a network.</p><p>We found that the uniform block selection of ResNet models is not optimal, and a better speed-accuracy trade-off can be obtained using a novel design, which uses a mixture of BasicBlock and Bottleneck layers. Since BasicBlock layers have a larger receptive field, we placed them at the first two stages of the network, and Bottleneck layers at the last two stages.</p><p>Compared to ResNet50, we also modified the number of channels and the number of residual blocks in the 3rd stage for the different TResNet models. Full specification of TResNet networks, including block type, width and number of residual blocks in each stage, appears in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Optimized SE Layers -We added dedicated squeezeand-excitation <ref type="bibr" target="#b12">[13]</ref> layers (SE) to TResNet architecture. In order to reduce the computational cost of the SE blocks, and gain the maximal speed-accuracy benefit, we placed SE layers only in the first three stages of the network. The last stage, which works on low-resolution maps, does not get a large accuracy benefit from the global average pooling operation that SE provides.</p><p>Compared to standard SE design <ref type="bibr" target="#b12">[13]</ref>, TResNet SE placement and hyper-parameters are also optimized: For Bottleneck units, we added the SE module after the conv3x3 operation, with a reduction factor of 8, and for BasicBlock units, we added SE module just before the residual sum, with a reduction factor of 4. This change aims to reduce the number of parameters and computational cost of SE layers: since BasicBlock units are placed on the first two stages of the network, they contain relatively low number of input channels, so only a small reduction factor (4) is needed. The Bottleneck units are placed in later stages of the network, with more input channels, so a higher reduction factor (8) is needed. Placing the SE layers after the reduction phase of the Bottleneck layer further reduces the computational cost. The complete blocks design, with SE layers and Inplace-ABN, is presented in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Code Optimizations</head><p>In this section we will describe code optimizations we did to enhance the GPU throughput and reduce the memory footprint of TResNet models. While code optimizations are sometimes overlooked and seen as 'implementation details', we claim that they are crucial for designing a modern  <ref type="figure">Figure 3</ref>. TResNet BasicBlock and Bottleneck design (stride 1). IBN = Inplace-BatchNorm, r = reduction factor, * -Only for 3rd stage.</p><p>network with top GPU performance. We designed TResNet using the PyTorch library <ref type="bibr" target="#b29">[30]</ref>, due to its popularity and ability for easy code prototyping. However, all the optimization described below are also applicable to other deep learning libraries, such as TensorFlow <ref type="bibr" target="#b0">[1]</ref> and MXNet <ref type="bibr" target="#b3">[4]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">JIT Compilation</head><p>JIT compilation enables, at execution time, to dynamically compile a high-level code into highly-efficient, optimized machine code. This is in contrast to the default Pythonic option of running a code dynamically, via an interpreter. We used JIT compilations for network modules that don't contain learnable parameters -the AA blur filter and the Space-ToDepth modules. For modules without learnable parameters, JIT compilation is a seamless process that accelerates the network GPU throughput without imposing limitations on the actual training and inference -for example, the input size does not need to be fixed and pre-determined, flow control statements are still possible.</p><p>For the AA and SpaceToDepth modules, we found that JIT compilation reduces the GPU cost by almost a factor of two. The module's JIT code appears in appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Inplace Operations</head><p>Inplace operations change directly the content of a given tensor, without making a copy. They reduce the memory access cost of an operation, and also prevent creation of unneeded activation maps for backward propagation, hence increasing the maximal possible batch size. In TResNet code, inplace operations are used as as much as possible. In addition of using Inplace-ABN, there are also inplace operations for the residual connection, SE layers, blocks' final activation and more. This is a key factor in enabling large batch size -TResNet-M maximal batch size is almost twice of ResNet50 -512, as can be seen in <ref type="table">Table 1</ref>. For full review of TResNet inplace operations, see our public code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Fast Global Average Pooling</head><p>Global average pooling (GAP) is used heavily in TResNet architecture -both in the SE layers, and before the final fully connected. GAP can be implemented via a general boilerplate method, AvgPool2d, that does average pooling of a tensor to a given spatial dimension, which in the GAP is (1,1). However, usually using this boilerplate methodology is usually not optimal, and leading to sub-optimal performances, mainly due to bad memory utilization.</p><p>We found that a simple dedicated implementation of GAP, with optimized code for the specific case of (1,1) spatial output, can be up to 5 times faster than the boilerplate implementation on GPU, since it can bring data from memory more efficiently. Our TResNet implementation for Fast GAP appears in the appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Deployment Optimizations</head><p>There are dedicated optimizations for enhancing (frozen) model inference speed during deployment. For example, BatchNorm layers can be fully absorbed into the convolution layers before them, significantly accelerating the model. In addition, there are dedicated libraries for GPU deployment, such as TensorRT <ref type="bibr" target="#b38">[39]</ref>. However, we wanted to provide a fair comparison of TResNet to other architectures, while focusing on all aspects of GPU efficiency (training speed, inference speed and maximal batch size), so we avoided doing inference-tailored optimizations. In practice, the inference speed reported for TResNet can be improved via such optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ImageNet Results</head><p>In this section, we will evaluate TResNet models on standard ImageNet training (input resolution 224), and compare their top-1 accuracy and GPU throughput to other known models. We will also perform an ablation study to better understand the effect of different refinements, show results for fine-tuning TResNet to higher input resolution, and do a thorough comparison to EfficientNet models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic Training</head><p>Our main benchmark for evaluating TResNet models is the popular ImageNet dataset <ref type="bibr" target="#b17">[18]</ref>. We trained the models on input resolution 224, for 300 epochs, using a SGD optimizer and 1-cycle policy <ref type="bibr" target="#b33">[34]</ref>. For regularization, we used AutoAugment <ref type="bibr" target="#b5">[6]</ref>, Cutout <ref type="bibr" target="#b6">[7]</ref>, Label-smoothing <ref type="bibr" target="#b34">[35]</ref> and True-weight-decay <ref type="bibr" target="#b24">[25]</ref>. We found that the common ImageNet statistics normalization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b35">36]</ref> does not improve the training accuracy, and instead normalized all the RGB channels to be between 0 and 1. For comparison, we repeated the same training procedure for ResNet50. Results appear in <ref type="table" target="#tab_2">Table 3</ref>. We can see from <ref type="table" target="#tab_2">Table 3</ref> that TResNet-M, which has similar GPU throughput to ResNet50, has significantly higher validation accuracy on ImageNet (+1.8%). It also outperforms all the other models that appear in <ref type="table">Table 1</ref>, both in terms of GPU throughput and ImageNet top-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Note that our ResNet50 ImageNet top-1 accuracy, 79.0%, is significantly higher than the accuracy stated in previous articles <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>, demonstrating the effectiveness of our training procedure. In addition, training TResNet-M and ResNet50 models takes less than 24 hours on an 8xV100 GPU machine, showing that our training scheme is also efficient and economical.</p><p>Another strength of the TResNet models, as reflected by <ref type="table" target="#tab_2">Table 3</ref>, is the ability to work with significantly larger batch sizes than other models. In general, large batch size leads to better GPU utilization, and allows easier scaling to large inputs. For distributed learning, it also reduces the number of synchronization needed in an epoch between the different GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Network Refinements</head><p>We performed an ablation study to evaluate the impact of the different refinements and modifications in TResNet-M model on the validation accuracy, inference speed, training speed and maximal batch size. Results appear in <ref type="table" target="#tab_4">Table 4</ref>. We can better understand from <ref type="table" target="#tab_4">Table 4</ref> the contribution of each refinement:</p><p>SpaceToDepth: The SpaceToDepth module provides improvements to all the indices. Notice that while we are not the first to use this innovative module (see <ref type="bibr" target="#b32">[33]</ref>), we are the first to integrate it into a high-performance network as drop-in replacement for the traditional convolution-based stem, and get a meaningful improvement. While the GPU throughput improvement is expected, the fact that also the accuracy improves (marginally) when replacing the ResNet stem cell by a "cheaper" SpaceToDepth unit is somewhat surprising. This result supports our intuition that there could be "information loss" within convolution-based stem unitdetails from the original image are not propagated well due to the aggressive downscaling process. Although simpler, a SpaceToDepth module minimizes this loss, and enables to process the data via the residual blocks, which are protected from information loss by the skip connections. We will further investigate this issue in future works.</p><p>Block-Type selection: Our novel block-type selection scheme, which uses both 'BasicBlock' and 'Bottleneck' blocks in the network, provides significant improvements to all indices, and shows that the uniform block-type design of the ResNet models is not optimal. Notice that this refinement also includes changing the number of residual blocks in the 3rd stage, from 6 to 11. In practice, the actual number <ref type="bibr" target="#b10">(11)</ref> was chosen after all the other refinements were finalized. Its goal was to bring TResNet-M to a similar GPU  throughput as ResNet50. Setting the number of blocks in the 3rd stage to 11 gave a slightly lower training speed and a slightly higher inference speed than ResNet50. However, using fewer blocks at the 3rd stage could have given relative improvement also for the training speed, at the cost of reducing the accuracy. Inplace-ABN: As expected, Inplace-ABN enables us to significantly increase the possible batch size, by 200 images. In addition, using Leaky-ReLU as activation function, instead of plain ReLU, marginally improved the accuracy. However, in terms of contribution to the actual GPU throughput, the impact of Inplace-ABN is mixed: while the inference speed improved, the training speed was somewhat reduced. While Inplace-ABN enables us to increase the batch size (which should be translated to better GPU throughput), it is also a more complicated module than a simple BatchNorm layer, so there is an inherent trade-off for using it. Since the ability to work with larger batch is highly beneficial for multi-GPU training and some dedicated loss functions, we chose to include this refinement.</p><p>Optimized SE + Anti-Aliasing layers: As expected, these layers significantly improve the ImageNet top-1 accuracy, with a price of reducing the model GPU throughput. We were able to compensate for this decrease with the previous refinements, and all-in-all get a better speed-accuracy trade-off than ResNet50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Code Optimizations</head><p>In <ref type="table" target="#tab_6">Table 5</ref> we evaluate the contribution of the different code optimizations. We can see from <ref type="table" target="#tab_6">Table 5</ref> that among the optimizations, dedicated inplace operations give the greatest boost -not only it improves the GPU throughput, but it also significantly increases the maximal possible batch size, since it avoids the creation of unneeded activation maps for backward propagation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refinement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">High-Resolution Fine-Tuning</head><p>We tested the scaling of TResNet models to higher input resolutions on ImageNet. We used the pre-trained TResNet models that appear in <ref type="table" target="#tab_2">Table 3</ref> as a starting point, and did a short 10 epochs fine-tuning to input resolution of 448. The results appear in <ref type="table">Table 6</ref>.</p><p>We see from <ref type="table">Table 6</ref> that TResNet models scale well to high resolutions. Even TResNet-M, which is a relatively small and compact model, can achieve top-1 accuracy of 83.2% on ImageNet with high-resolution input. TResNet largest variant, TResNet-XL, achieves 84.3% top-1 accuracy on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison to EfficientNet Models</head><p>EfficientNet models, which are based on MobilenetV3 architecture <ref type="bibr" target="#b11">[12]</ref>, propose to balance the resolution, height, and width of a base network for generating a series of larger networks. They are considered state-of-the-art architec-  <ref type="table">Table 6</ref>. Impact of the input resolution on the top1 ImageNet accuracy for TResNet models. All TResNet 448 input-resolution accuracies are obtained with 10 epochs of fine-tuning.</p><p>tures, that provide efficient networks for all ImageNet top-1 accuracy spectrum <ref type="bibr" target="#b35">[36]</ref>. In <ref type="figure" target="#fig_2">Figure 4</ref> and <ref type="figure" target="#fig_3">Figure 5</ref>, we compare the inference and training speed of TResNet models to the different EfficientNet models respectively.  We can see from <ref type="figure" target="#fig_2">Figure 4</ref> and <ref type="figure" target="#fig_3">Figure 5</ref> that all along the top-1 accuracy curve, TResNet models give better inference-speed-accuracy and training-speed-accuracy tradeoff than EfficientNet models. Note that each Efficient-Net model was bundled and optimized to a specific resolution, while TResNet models were trained and tested on multi-resolutions, which makes this comparison biased toward EfficientNet models; Yet, TResNet models show superior results. Also note that EfficientNet models were trained for 450 epochs and not for 300 epochs like TRes-Net models, and that EfficientNet training procedure included more GPU intensive tricks (RMSProp optimizer, drop-block) <ref type="bibr" target="#b35">[36]</ref>, so the actual gap in training times is even higher than stated in <ref type="figure" target="#fig_3">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Transfer Learning Results</head><p>In this section, we will present transfer learning results of TResNet models on four well-known single-label classification downstream datasets. We will also present transfer learning results on multi-label classification and object detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Single-Label Classification</head><p>We evaluated TResNet on four commonly used, competitive transfer learning datasets: Stanford-Cars <ref type="bibr" target="#b15">[16]</ref>, CIFAR-10 <ref type="bibr" target="#b16">[17]</ref>, CIFAR-100 <ref type="bibr" target="#b16">[17]</ref> and Oxford-Flowers <ref type="bibr" target="#b28">[29]</ref>. For each dataset, we used ImageNet pre-trained checkpoints, and fine-tuned the models for 80 epochs using 1-cycle policy <ref type="bibr" target="#b33">[34]</ref> . For the fine-grained classification tasks (Stanford-Cars and Oxford-Flowers), in addition to cross-entropy loss we used weighted triplet loss with soft-margin <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b18">19]</ref>, which emphasizes hard examples by focusing of the most difficult positives and negatives samples in the batch. <ref type="table">Table 7</ref> shows the transfer learning performance of TResNet, compared to the known state-of-the-art models.</p><p>We can see from <ref type="table">Table 7</ref> that TResNet surpasses or matches the state-of-the-art accuracy on 3 of the 4 datasets, with x8-15 faster GPU inference speed. Note that all TRes-Net's results are from single-crop single-model evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Model</head><p>Top-1 Acc.  <ref type="table">Table 7</ref>. Comparison of TResNet to state-of-the-art models on transfer learning datasets (only ImageNet-based transfer learning results). Models inference speed is measured on a mixed precision V100 GPU. Since no official implementation of Gpipe was provided, its inference speed is unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-Label Classification</head><p>For multi-label classification tests, we chose to work with MS-COCO dataset <ref type="bibr" target="#b22">[23]</ref> (multi-label recognition task). We used the 2014 split, which contains about 82K images for training and 41K for validation. In total, images are involved with 80 object labels, with an average of 2.9 labels per image.</p><p>Our training scheme is similar to the one used for singlelabel training. The main difference is the loss function, which is adapted for a multi-label setting -we implemented a variant of the well known focal-loss <ref type="bibr" target="#b21">[22]</ref>, where two different gamma values are used for positive and negative sample. This enables to better tackle the highly imbalanced nature of a multi-label dataset.</p><p>Following the conventional settings <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">40]</ref>, we report the main performance evaluation metric, mean average precision (mAP), but in addition state average per-class precision (CP), recall (CR), F1 (CF1) and the average overall precision (OP), recall (OR), F1 (OF1).</p><p>In <ref type="table" target="#tab_10">Table 9</ref>, we present the transfer learning results of TResNet model and compare it to the known state-of-theart model. We can see from <ref type="table" target="#tab_10">Table 9</ref> that the TResNet-based solution significantly outperforms previous top solution for MS-COCO multi-label dataset, increasing the known SOTA by a large margin, from 83.7 mAP to 86.4 mAP. All additional evaluation metrics also show improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object Detection</head><p>While our main focus was on various classification tasks, we wanted to further test TResNet on another popular computer vision task -object detection.</p><p>We used the known MS-COCO <ref type="bibr" target="#b22">[23]</ref> dataset (object detection task), with a training set with 118k images, and an evaluation set (minival) of 5k images. For training, we used the popular mm-detection <ref type="bibr" target="#b2">[3]</ref> package, with FCOS <ref type="bibr" target="#b37">[38]</ref> as the object detection method and the enhancements discussed in ATSS <ref type="bibr" target="#b46">[47]</ref>.</p><p>We trained with SGD optimizer for 70 epochs with 0.9 momentum, weight decay of 0.0001 and batch size of 24. We used learning rate warm up, initial learning rate of 0.01 and 10x reduction at epochs 40, 60. We also implemented the data augmentations techniques described in <ref type="bibr" target="#b23">[24]</ref>.</p><p>For a fair comparison, we used first ResNet50 as backbone, and then replace it by TResNet-M (both give similar GPU throughput). Comparison results appear in <ref type="table">Table 10</ref>.</p><p>Method Babkbone mAP % FCOS ResNet50 42.8 FCOS TResNet-M 44.0 <ref type="table">Table 10</ref>. Comparison of TResNet-M to ResNet50 on MS-COCO object detection task. Results were obtained using mmdetection package, with FCOS as the object detection method .</p><p>We can see from <ref type="table">Table 10</ref> that TResNet-M outperform ResNet50 on this object-detection task, increasing COCO mAP score from 42.8 to 44.0. This is consistent with the improvement we saw in the single-label ImageNet classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we point out a possible blind-spot of latest developments in neural network design patterns. They tend not to consider actual GPU utilization as one of the measurements for a network quality. While GPU inference speed is sometimes measured, GPU training speed and maximal possible batch size are widely overlooked. For many real-world deep learning applications, training speed, inference speed and maximal batch size are all critical factors.</p><p>To address this issue, we propose a carefully selected set of design refinements, which are highly effective in utilizing typical GPU resources -SpaceToDepth stem cell, economical AA downsampling, Inplace-ABN operations, blocktype selection redesign and optimized SE layers. We combine these refinements with a series of code optimizations and enhancements to suggest a family of new models, dedicated for GPU high-performance, which we call TResNet.</p><p>We demonstrate that on ImageNet, all along the top-1 accuracy curve TResNet gives better GPU throughput than existing models. In addition, on three commonly used downstream single-label classification datasets it reaches new state-of-the-art accuracies. We also show that TResNet generalizes well to other computer vision tasks, reaching top scores on multi-label classification and object detection datasets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Anti-Alias Downsampling • In-Place Activated BatchNorm • Novel Block-type Selection • Optimized SE Layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>TResNet-M SpaceToDepth stem design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>TResNet Vs EfficientNet models inference speed comparison. Y label is the accuracy[%]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>TResNet Vs EfficientNet models training speed comparison. Y label is the accuracy[%]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fast</head><label></label><figDesc>implementation of global average pooling class FastGlobalAvgPool2d (): def i n i t (self , flatten =False ): self. flatten = flatten def c a l l (self , x): if self. flatten : in size = x.size () return x.view (( in size [0], in size [1], −1)). mean(dim =2) else: return x.view(x.size (0), x.size (1), −1). mean(−1).view( x.size (0), x.size (1), 1, 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Model Training Speed [img/sec] Inference Speed [img/sec] Top1 Accuracy [%] Flops [G]</figDesc><table><row><cell>ResNet50 [10]</cell><cell>805</cell><cell>2830</cell><cell>79.0</cell><cell>4.1</cell></row><row><cell>ResNet50-D [11]</cell><cell>600</cell><cell>2670</cell><cell>79.3</cell><cell>4.4</cell></row><row><cell>ResNeXt50 [43]</cell><cell>490</cell><cell>1940</cell><cell>79.4</cell><cell>4.3</cell></row><row><cell>EfficientNetB1 [36]</cell><cell>480</cell><cell>2740</cell><cell>79.2</cell><cell>0.6</cell></row><row><cell>SEResNeXt50 [36]</cell><cell>400</cell><cell>1770</cell><cell>79.9</cell><cell>4.3</cell></row><row><cell>MixNet-L [37]</cell><cell>400</cell><cell>1400</cell><cell>79.0</cell><cell>0.5</cell></row><row><cell>TResNet-M</cell><cell>730</cell><cell>2930</cell><cell>80.8</cell><cell>5.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Overall architecture of the three TResNet models.</figDesc><table><row><cell>Layer</cell><cell>Block Type</cell><cell cols="2">Output Stride</cell><cell></cell><cell></cell><cell cols="2">TResNet</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>M</cell><cell></cell><cell>L</cell><cell></cell><cell>XL</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">Repeats Channels Repeats Channels Repeats Channels</cell></row><row><cell>Stem</cell><cell>SpaceToDepth Conv1x1</cell><cell>56×56</cell><cell>-1</cell><cell>1 1</cell><cell>48 64</cell><cell>1 1</cell><cell>48 76</cell><cell>1 1</cell><cell>48 84</cell></row><row><cell>Stage1</cell><cell cols="2">BasicBlock+SE 56×56</cell><cell>1</cell><cell>3</cell><cell>64</cell><cell>4</cell><cell>76</cell><cell>4</cell><cell>84</cell></row><row><cell>Stage2</cell><cell cols="2">BasicBlock+SE 28×28</cell><cell>2</cell><cell>4</cell><cell>128</cell><cell>5</cell><cell>152</cell><cell>5</cell><cell>168</cell></row><row><cell>Stage3</cell><cell cols="2">Bottleneck+SE 14×14</cell><cell>2</cell><cell>11</cell><cell>1024</cell><cell>18</cell><cell>1216</cell><cell>24</cell><cell>1344</cell></row><row><cell>Stage4</cell><cell>Bottleneck</cell><cell>7×7</cell><cell>2</cell><cell>3</cell><cell>2048</cell><cell>3</cell><cell>2432</cell><cell>3</cell><cell>2688</cell></row><row><cell>Pooling</cell><cell>GlobalAvgPool</cell><cell>1×1</cell><cell>1</cell><cell>1</cell><cell>2048</cell><cell>1</cell><cell>2432</cell><cell>1</cell><cell>2688</cell></row><row><cell>#Params.</cell><cell></cell><cell></cell><cell></cell><cell cols="2">29.4M</cell><cell cols="2">54.7M</cell><cell cols="2">77.1M</cell></row><row><cell cols="2">Basic Block</cell><cell cols="2">Bottleneck Block</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3x3 Conv</cell><cell></cell><cell>1x1 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>IBN (leaky relu)</cell><cell></cell><cell>IBN (leaky relu)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>3x3 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3x3 Conv</cell><cell></cell><cell>IBN (leaky relu)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>IBN</cell><cell cols="2">SE layer* (r=8)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SE layer (r=4)</cell><cell></cell><cell>1x1 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>IBN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+</cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ReLU</cell><cell></cell><cell>ReLU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>TResNet models accuracy and GPU throughput on ImageNet, compared to ResNet50. All measurements were done on Nvidia V100 GPU, with mixed precision. All models are trained on input resolution of 224.</figDesc><table><row><cell></cell><cell>Top Training Speed (img/sec)</cell><cell>Top Inference Speed (img/sec)</cell><cell>Max Train Batch Size</cell><cell>Top-1 Acc. [%]</cell></row><row><cell>ResNet50</cell><cell>805</cell><cell>2830</cell><cell>288</cell><cell>79.0</cell></row><row><cell>TResNet-M</cell><cell>730</cell><cell>2930</cell><cell>512</cell><cell>80.8</cell></row><row><cell>TResNet-L</cell><cell>345</cell><cell>1390</cell><cell>316</cell><cell>81.5</cell></row><row><cell>TResNet-XL</cell><cell>250</cell><cell>1060</cell><cell>240</cell><cell>82.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study -The impact of different refinements in TResNet-M model on ImageNet top-1 accuracy, inference speed, training speed and maximal batch size. All measurements are done on Nvidia V100 GPU, with mixed precision.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation study -The impact of code optimizations in TResNet-M model on inference speed, training speed and maximal batch size.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Backbone mAP CP CR CF1 OP OR OF1 KSSNet[40] 83.7 84.6 73.2 77.2 87.8 76.2 81.5 TResNet-L 86.4 87.6 76.0 81.4 88.4 78.9 83.4 Comparison of TResNet to state-of-the-art model on multi-label classification on MS-COCO dataset. KSSNet [40], is the known SOTA, based on ResNet101 backbone.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">(( channels , 1, 1, 1)</ref><p>) self.filt=self.filt.cuda () if mixed precision :</p><p>self.filt = self.filt.half () def c a l l (self , input : torch. Tensor ): input pad = F.pad(input , <ref type="figure">(1, 1, 1, 1)</ref>, 'reflect ') return F. conv2d ( input pad , self.filt , stride =2, padding =0, groups =input.shape <ref type="bibr" target="#b0">[1]</ref>)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Res2net: A new multi-scale backbone architecture. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adapting mask-rcnn for automatic nucleus segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah W Johnson</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00500</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Large scale learning of general visual representations for transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Collecting a large-scale dataset of fine-grained cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compact network training for person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Hussam Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval</title>
		<meeting>the 2020 International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeryun</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiho</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06268</idno>
		<title level="m">Compounding the performance improvements of assembled techniques in a convolutional neural network</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Detnet: A backbone network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06215</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollr. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="21" to="37" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nvidia tensor core programmability, performance &amp; precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Markidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Wei Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivy</forename><forename type="middle">Bo</forename><surname>Laure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mish: A self regularized non-monotonic neural activation function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diganta</forename><surname>Misra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Features for multi-target multi-camera tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6036" to="6046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Baccash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03205</idno>
		<title level="m">Non-discriminative data or weak model? on the relative importance of data and model resolution</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno>abs/1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09595</idno>
		<title level="m">Mixed depthwise convolutional kernels</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Efficient inference with tensorrt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Vanholder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-label classification with label graph superimposing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<idno>abs/1911.09243</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">pytorch-image-models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Benchmarking and Simulation of High Performance Computer Systems (PMBS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rengan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quy</forename><surname>Ta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Performance Modeling</title>
		<imprint>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>Deep learning at scale on nvidia v100 accelerators</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masafumi</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Kasagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tabuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takumi</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Fukumoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12650</idno>
		<title level="m">Tsuguchika Tabaru, Atsushi Ike, and Kohta Nakashima. Yet another accelerated sgd: Resnet-50 training on imagenet in 74.7 seconds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
