<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Training for Free!</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
							<email>ashafahi@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
							<email>najibi@cs.umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ghiasi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
							<email>studer@cornell.edu</email>
							<affiliation key="aff5">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
							<email>taylor@usna.edu</email>
							<affiliation key="aff7">
								<orgName type="institution">Naval Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
							<email>tomg@cs.umd.edu</email>
							<affiliation key="aff8">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Training for Free!</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our "free" adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40% accuracy against PGD attacks. The code is available at https://github.com/ashafahi/free_adv_train.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The high cost of adversarial training has motivated a number of alternatives. Some recent works replace the perturbation generation in adversarial training with a parameterized generator network <ref type="bibr" target="#b7">[Baluja and Fischer, 2018</ref><ref type="bibr" target="#b8">, Poursaeed et al., 2018</ref><ref type="bibr" target="#b9">, Xiao et al., 2018</ref>.</p><p>This approach is slower than standard training, and problematic on complex datasets, such as ImageNet, for which it is hard to produce highly expressive GANs that cover the entire image space. Another popular defense strategy is to regularize the training loss using label smoothing, logit squeezing, or a Jacobian regularization <ref type="bibr" target="#b10">[Shafahi et al., 2019a</ref><ref type="bibr" target="#b11">, Mosbach et al., 2018</ref><ref type="bibr" target="#b12">, Ross and Doshi-Velez, 2018</ref><ref type="bibr" target="#b13">, Hein and Andriushchenko, 2017</ref><ref type="bibr" target="#b14">, Jakubovitz and Giryes, 2018</ref><ref type="bibr" target="#b15">, Yu et al., 2018</ref>. These methods have not been applied to large-scale problems, such as ImageNet, and can be applied in parallel to adversarial training.</p><p>Recently, there has been a surge of certified defenses <ref type="bibr" target="#b16">[Wong and Kolter, 2017</ref><ref type="bibr" target="#b17">, Wong et al., 2018</ref><ref type="bibr" target="#b18">, Raghunathan et al., 2018a</ref><ref type="bibr">,b, Wang et al., 2018</ref>. These methods were mostly demonstrated for small networks, low-res datasets, and relatively small perturbation budgets ( ). <ref type="bibr" target="#b21">Lecuyer et al. [2018]</ref> propose randomized smoothing as a certified defense and which was later improved by <ref type="bibr" target="#b22">Li et al. [2018a]</ref>. <ref type="bibr" target="#b23">Cohen et al. [2019]</ref> prove a tight robustness guarantee under the 2 norm for smoothing with Gaussian noise. Their study was the first certifiable defense for the ImageNet dataset <ref type="bibr" target="#b24">[Deng et al., 2009]</ref>.They claim to achieve 12% robustness against non-targeted attacks that are within an 2 radius of 3 (for images with pixels in [0, 1]). This is roughly equivalent to an ∞ radius of = 2 when pixels lie in <ref type="bibr">[0,</ref><ref type="bibr">255]</ref>.</p><p>Adversarial training remains among the most trusted defenses, but it is nearly intractable on largescale problems. Adversarial training on high-resolution datasets, including ImageNet, has only been within reach for research labs having hundreds of GPUs 1 . Even on reasonably-sized datasets, such as CIFAR-10 and CIFAR-100, adversarial training is time consuming and can take multiple days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>We propose a fast adversarial training algorithm that produces robust models with almost no extra cost relative to natural training. The key idea is to update both the model parameters and image perturbations using one simultaneous backward pass, rather than using separate gradient computations for each update step. Our proposed method has the same computational cost as conventional natural training, and can be 3-30 times faster than previous adversarial training methods <ref type="bibr" target="#b6">[Madry et al., 2017</ref><ref type="bibr" target="#b25">, Xie et al., 2019</ref>. Our robust models trained on CIFAR-10 and CIFAR-100 achieve accuracies comparable and even slightly exceeding models trained with conventional adversarial training when defending against strong PGD attacks.</p><p>We can apply our algorithm to the large-scale ImageNet classification task on a single workstation with four P100 GPUs in about two days, achieving 40% accuracy against non-targeted PGD attacks.</p><p>To the best of our knowledge, our method is the first to successfully train a robust model for ImageNet based on the non-targeted formulation and achieves results competitive with previous (significantly more complex) methods <ref type="bibr" target="#b26">[Kannan et al., 2018</ref><ref type="bibr" target="#b25">, Xie et al., 2019</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Non-targeted adversarial examples</head><p>Adversarial examples come in two flavors: non-targeted and targeted. Given a fixed classifier with parameters θ, an image x with true label y, and classification proxy loss l, a bounded non-targeted attack sneaks an example out of its natural class and into another. This is done by solving</p><formula xml:id="formula_0">max δ l(x + δ, y, θ), subject to ||δ|| p ≤ ,<label>(1)</label></formula><p>where δ is the adversarial perturbation, ||.|| p is some p -norm distance metric, and is the adversarial manipulation budget. In contrast to non-targeted attacks, a targeted attack scooches an image into a specific class of the attacker's choice.</p><p>In what follows, we will use non-targeted adversarial examples both for evaluating the robustness of our models and also for adversarial training. We briefly review some of the closely related methods for generating adversarial examples. In the context of ∞ -bounded attacks, the Fast Gradient Sign Method (FGSM) by <ref type="bibr" target="#b27">Goodfellow et al. [2015]</ref> is one of the most popular non-targeted methods that uses the sign of the gradients to construct an adversarial example in one iteration:</p><p>x adv = x + · sign(∇ x l(x, y, θ)).</p><p>( <ref type="formula">2)</ref> The Basic Iterative Method (BIM) by <ref type="bibr" target="#b28">Kurakin et al. [2016a]</ref> is an iterative version of FGSM. The PGD attack is a variant of BIM with uniform random noise as initialization, which is recognized by  to be one of the most powerful first-order attacks. The initial random noise was first studied by <ref type="bibr" target="#b29">Tramèr et al. [2017]</ref> to enable FGSM to attack models that rely on "gradient masking." In the PGD attack algorithm, the number of iterations K plays an important role in the strength of attacks, and also the computation time for generating adversarial examples. In each iteration, a complete forward and backward pass is needed to compute the gradient of the loss with respect to the image. Throughout this paper we will refer to a K-step PGD attack as PGD-K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adversarial training</head><p>Adversarial training can be traced back to <ref type="bibr" target="#b27">[Goodfellow et al., 2015]</ref>, in which models were hardened by producing adversarial examples and injecting them into training data. The robustness achieved by adversarial training depends on the strength of the adversarial examples used. Training on fast non-iterative attacks such as FGSM and Rand+FGSM only results in robustness against non-iterative attacks, and not against PGD attacks <ref type="bibr" target="#b30">[Kurakin et al., 2016b</ref><ref type="bibr" target="#b6">, Madry et al., 2017</ref>. Consequently, <ref type="bibr" target="#b6">Madry et al. [2017]</ref> propose training on multi-step PGD adversaries, achieving state-of-the-art robustness levels against ∞ attacks on MNIST and CIFAR-10 datasets.</p><p>While many defenses were broken by , PGD-based adversarial training was among the few that withstood strong attacks. Many other defenses build on PGD adversarial training or leverage PGD adversarial generation during training. Examples include Adversarial Logit Pairing (ALP) <ref type="bibr" target="#b26">[Kannan et al., 2018]</ref>, Feature Denoising <ref type="bibr" target="#b25">[Xie et al., 2019]</ref>, Defensive Quantization <ref type="bibr" target="#b31">[Lin et al., 2019]</ref>, Thermometer Encoding <ref type="bibr" target="#b32">[Buckman et al., 2018]</ref>, PixelDefend <ref type="bibr" target="#b33">[Song et al., 2017]</ref>, Robust Manifold Defense <ref type="bibr" target="#b34">[Ilyas et al., 2017]</ref>, L2-nonexpansive nets <ref type="bibr" target="#b35">[Qian and Wegman, 2018]</ref>, Jacobian Regularization <ref type="bibr" target="#b14">[Jakubovitz and Giryes, 2018]</ref>, Universal Perturbation <ref type="bibr" target="#b36">[Shafahi et al., 2018]</ref>, and Stochastic Activation Pruning <ref type="bibr" target="#b37">[Dhillon et al., 2018]</ref>.</p><p>We focus on the min-max formulation of adversarial training <ref type="bibr" target="#b6">[Madry et al., 2017]</ref>, which has been theoretically and empirically justified. This widely used K-PGD adversarial training algorithm has an inner loop that constructs adversarial examples by PGD-K, while the outer loop updates the model using minibatch SGD on the generated examples. In the inner loop, the gradient ∇ x l(x adv , y, θ) for updating adversarial examples requires a forward-backward pass of the entire network, which has similar computation cost as calculating the gradient ∇ θ l(x adv , y, θ) for updating network parameters. Compared to natural training, which only requires ∇ θ l(x, y, θ) and does not have an inner loop, K-PGD adversarial training needs roughly K + 1 times more computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">"Free" adversarial training</head><p>K-PGD adversarial training <ref type="bibr" target="#b6">[Madry et al., 2017]</ref> is generally slow. For example, the 7-PGD training of a WideResNet <ref type="bibr" target="#b38">[Zagoruyko and Komodakis, 2016]</ref> on CIFAR-10 in <ref type="bibr" target="#b6">Madry et al. [2017]</ref> takes about four days on a Titan X GPU. To scale the algorithm to ImageNet, <ref type="bibr" target="#b25">Xie et al. [2019]</ref> and Kannan et al.</p><p>[2018] had to deploy large GPU clusters at a scale far beyond the reach of most organizations.</p><p>Here, we propose free adversarial training, which has a negligible complexity overhead compared to natural training. Our free adversarial training algorithm (alg. 1) computes the ascent step by re-using the backward pass needed for the descent step. To update the network parameters, the current training minibatch is passed forward through the network. Then, the gradient with respect to the network parameters is computed on the backward pass. When the "free" method is used, the gradient of the loss with respect to the input image is also computed on this same backward pass.</p><p>Unfortunately, this approach does not allow for multiple adversarial updates to be made to the same image without performing multiple backward passes. To overcome this restriction, we propose a minor yet nontrivial modification to training: train on the same minibatch m times in a row. Note that we divide the number of epochs by m such that the overall number of training iterations remains constant. This strategy provides multiple adversarial updates to each training image, thus providing for minibatch B ⊂ X do 5:</p><formula xml:id="formula_1">for i = 1 . . . m do 6:</formula><p>Update θ with stochastic gradient descent 7:</p><formula xml:id="formula_2">g θ ← E (x,y)∈B [∇ θ l(x + δ, y, θ)] 8: g adv ← ∇ x l(x + δ, y, θ)] 9: θ ← θ − τ g θ 10:</formula><p>Use gradients calculated for the minimization step to update δ 11:</p><formula xml:id="formula_3">δ ← δ + · sign(g adv ) 12: δ ← clip(δ, − ,<label>) 13:</label></formula><p>end for 14:</p><p>end for 15: end for strong/iterative adversarial examples. Finally, when a new minibatch is formed, the perturbation generated on the previous minibatch is used to warm-start the perturbation for the new minibatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The effect of mini-batch replay on natural training</head><p>While the hope for alg. 1 is to build robust models, we still want models to perform well on natural examples. As we increase m in alg. 1, there is risk of increasing generalization error. Furthermore, it may be possible that catastrophic forgetting happens. Consider the worst case where all the "informative" images of one class are in the first few mini-batches. In this extreme case, we do not see useful examples for most of the epoch, and forgetting may occur. Consequently, a natural question is: how much does mini-batch replay hurt generalization?</p><p>To answer this question, we naturally train wide-resnet 32-10 models on CIFAR-10 and CIFAR-100 using different levels of replay. <ref type="figure" target="#fig_0">Fig. 1</ref> plots clean validation accuracy as a function of the replay parameter m. We see some dropoff in accuracy for small values of m. Note that a small compromise in accuracy is acceptable given a large increase in robustness due to the fundamental tradeoffs between robustness and generalization <ref type="bibr" target="#b39">[Tsipras et al., 2018</ref><ref type="bibr" target="#b40">, Zhang et al., 2019a</ref><ref type="bibr" target="#b41">, Shafahi et al., 2019b</ref>. As a reference, CIFAR-10 and CIFAR-100 models that are 7-PGD adversarially trained have natural accuracies of 87.25% and 59.87%, respectively. These same accuracies are exceeded by natural training with m = 16. We see in section 5 that good robustness can be achieved using "free" adversarial training with just m ≤ 10. 5 Robust models on CIFAR-10 and 100</p><p>In this section, we train robust models on CIFAR-10 and CIFAR-100 using our "free" adversarial training ( alg. 1) and compare them to K-PGD adversarial training 23 . We find that free training is able to achieve state-of-the-art robustness on the CIFARs without the overhead of standard PGD training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>We train various CIFAR-10 models using the Wide-Resnet 32-10 model and standard hyperparameters used by <ref type="bibr" target="#b6">Madry et al. [2017]</ref>. In the proposed method (alg. 1), we repeat (i.e. replay) each minibatch m times before switching to the next minibatch. We present the experimental results for various choices of m in table 1. Training each of these models costs roughly the same as natural training since we preserve the same number of iterations. We compare with the 7-PGD adversarially trained model from <ref type="bibr" target="#b6">Madry et al. [2017]</ref> 4 , whose training requires roughly 7× more time than all of our free training variations. We attack all models using PGD attacks with K iterations on both the cross-entropy loss (PGD-K) and the Carlini-Wagner loss (CW-K) <ref type="bibr" target="#b42">[Carlini and Wagner, 2017]</ref>. We test using the PGD-20 attack following <ref type="bibr" target="#b6">Madry et al. [2017]</ref>, and also increase the number of attack iterations and employ random restarts to verify robustness under stronger attacks. To measure the sensitivity of our method to initialization, we perform five trials for the Free-m = 8 case and find that our results are insensitive. The natural accuracy is 85.95±0.14 and robustness against a 20-random restart PGD-20 attack is 46.49±0.19. Note that gradient free-attacks such as SPSA will result in inferior results for adversarially trained models in comparison to optimization based attacks such as PGD as noted by <ref type="bibr" target="#b43">Uesato et al. [2018]</ref>. Gradient-free attacks are superior in settings where the defense works by masking or obfuscating the gradients.  <ref type="figure">Figure 2</ref>: Attack images built for adversarially trained models look like the class into which they get misclassified. We display the last 9 CIFAR-10 clean validation images (top row) and their adversarial examples built for a 7-PGD adversarially trained (middle) and our "free" trained (bottom) models.</p><p>Our "free training" algorithm successfully reaches robustness levels comparable to a 7-PGD adversarially trained model. As we increase m, the robustness is increased at the cost of validation accuracy on natural images. Additionally note that we achieve reasonable robustness over a wide range of choices of the main hyper-parameter of our model, 10 ≥ m &gt; 2, and the proposed method is significantly faster than 7-PGD adversarial training. Recently, a new method called YOPO <ref type="bibr" target="#b44">[Zhang et al., 2019b]</ref> has been proposed for speeding up adversarial training, in their CIFAR-10 results they use a wider networks (WRN-34-10) with larger batch-sizes (256). As shown in our supplementary, both of these factors increase robustness. To do a direct comparison, we a train WRN-34-10 using m = 10 and batch-size=256. We match their best reported result (48.03% against PGD-20 attacks for "Free" training v.s. 47.98% for YOPO 5-3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-100</head><p>We also study the robustness results of "free training" on CIFAR-100 which is a more difficult dataset with more classes. As we will see in sec. 4, training with large m values on this dataset hurts the natural validation accuracy more in comparison to CIFAR-10. This dataset is less studied in the adversarial machine learning community and therefore for comparison purposes, we adversarially train our own Wide ResNet 32-10 models for CIFAR-100. We train two robust models by varying K in the K-PGD adversarial training algorithm. One is trained on PGD-2 with a computational cost almost 3× that of free training, and the other is trained on PGD-7 with a computation time roughly 7× that of free training. We adopt the code for adversarial training from <ref type="bibr" target="#b6">Madry et al. [2017]</ref>, which produces state-of-the-art robust models on CIFAR-10. We summarize the results in table. 2.</p><p>We see that "free training" exceeds the accuracy on both natural images and adversarial images when compared to traditional adversarial training. Similar to the effect of increasing m, increasing K in K-PGD adversarial training results in increased robustness at the cost of clean validation accuracy. However, unlike the proposed "free training" where increasing m has no extra cost, increasing K for standard K-PGD substantially increases training time.</p><p>6 Does "free" training behave like standard adversarial training?</p><p>Here, we analyze two properties that are associated with PGD adversarially trained models: The interpretability of their gradients and the flattness of their loss surface. We find that "free" training enjoys these benefits as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative behavior for largely perturbed examples</head><p>Tsipras et al. <ref type="bibr">[2018]</ref> observed that hardened classifiers have interpretable gradients; adversarial examples built for PGD trained models often look like the class into which they get misclassified.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smooth and flattened loss surface</head><p>Another property of PGD adversarial training is that it flattens and smoothens the loss landscape. In contrast, some defenses work by "masking" the gradients, i.e., making it difficult to identify adversarial examples using gradient methods, even though adversarial examples remain present. Reference  argues that gradient masking adds little security. We show in <ref type="figure" target="#fig_2">fig. 3a</ref> that free training does not operate by masking gradients using a rough loss surface. In <ref type="figure" target="#fig_2">fig. 3</ref> we plot the cross-entropy loss projected along two directions in image space for the first few validation examples of CIFAR-10 <ref type="bibr" target="#b46">[Li et al., 2018b]</ref>. In addition to the loss of the free m = 8 model, we plot the loss of the 7-PGD adversarially trained model for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Robust ImageNet classifiers</head><p>ImageNet is a large image classification dataset of over 1 million high-res images and 1000 classes <ref type="bibr" target="#b47">(Russakovsky et al. [2015]</ref>). Due to the high computational cost of ImageNet training, only a few research teams have been able to afford building robust models for this problem. <ref type="bibr" target="#b30">Kurakin et al. [2016b]</ref> first hardened ImageNet classifiers by adversarial training with non-iterative attacks. 5 Adversarial training was done using a targeted FGSM attack. They found that while their model became robust against targeted non-iterative attacks, the targeted BIM attack completely broke it.</p><p>Later, <ref type="bibr" target="#b26">Kannan et al. [2018]</ref> attempted to train a robust model that withstands targeted PGD attacks. They trained against 10 step PGD targeted attacks (a process that costs 11 times more than natural training) to build a benchmark model. They also generated PGD targeted attacks to train their adversarial logit paired (ALP) ImageNet model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Free training results</head><p>Our alg. 1 is designed for non-targeted adversarial training. As  state, defending on this task is important and more challenging than defending against targeted attacks, and for this reason smaller values are typically used. Even for = 2 (the smallest we consider defensing against), a PGD-50 non-targeted attack on a natural model achieves roughly 0.05% top-1 accuracy.</p><p>To put things further in perspective, <ref type="bibr" target="#b43">Uesato et al. [2018]</ref> broke three defenses for = 2 non-targeted attacks on ImageNet <ref type="bibr" target="#b48">[Guo et al., 2017</ref><ref type="bibr" target="#b49">, Liao et al., 2018</ref><ref type="bibr" target="#b50">, Xie et al., 2017</ref>, degrading their performance below 1%. Our free training algorithm is able to achieve 43% robustness against PGD attacks bounded by = 2. Furthermore, we ran each experiment on a single workstation with four P100 GPUs. Even with this modest setup, training time for each ResNet-50 experiment is below 50 hours.</p><p>We summarize our results for various 's and m's in table 3 and <ref type="figure" target="#fig_3">fig. 4</ref>. To craft attacks, we used a step-size of 1 and the corresponding used during training. In all experiments, the training batch size was 256. <ref type="table" target="#tab_4">Table 3</ref> shows the robustness of Resnet-50 on ImageNet with = 2. The validation accuracy for natural images decreases when we increase the minibatch replay m, just like it did for CIFAR in section 5.   Comparison with PGD-trained models</p><p>We compare "free" training to a more costly method using 2-PGD adversarial examples = 4. We run the conventional adversarial training algorithm and set s = 2, = 4, and K = 2. All other hyper-parameters were identical to those used for training our "free" models. Note that in our experiments, we do not use any label-smoothing or other common tricks for improving robustness since we want to do a fair comparison between PGD training and our "free" training. These extra regularizations can likely improve results for both approaches.</p><p>We compare our "free trained" m = 4 ResNet-50 model and the 2-PGD trained ResNet-50 model in table 4. 2-PGD adversarial training takes roughly 3.4× longer than "free training" and only achieves slightly better results (≈4.5%). This gap is less than 0.5% if we free train a higher capacity model (i.e. ResNet-152, see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Free training on models with more capacity</head><p>It is believed that increased network capacity leads to greater robustness from adversarial training <ref type="bibr" target="#b6">[Madry et al., 2017</ref><ref type="bibr" target="#b30">, Kurakin et al., 2016b</ref>. We verify that this is the case by "free training" ResNet-101 and ResNet-152 with = 4. The comparison between ResNet-152, ResNet-101, and ResNet-50 is summarized in table 5. Free training on ResNet-101 and ResNet-152 each take roughly 1.7× and 2.4× more time than ResNet-50 on the same machine, respectively. The higher capacity model enjoys a roughly 4% boost to accuracy and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>Adversarial training is a well-studied method that boosts the robustness and interpretability of neural networks. While it remains one of the few effective ways to harden a network to attacks, few can afford to adopt it because of its high computation cost. We present a "free" version of adversarial training with cost nearly equal to natural training. Free training can be further combined with other defenses to produce robust models without a slowdown. We hope that this approach can put adversarial training within reach for organizations with modest compute resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) CIFAR-10 sensitivity to m (b) CIFAR-100 sensitivity to m Natural validation accuracy of Wide Resnet 32-10 models using varied mini-batch replay parameters m. Here m = 1 corresponds to natural training. For large m's, validation accuracy drops drastically. However, small m's have little effect. For reference, CIFAR-10 and CIFAR-100 models that are 7-PGD adversarially trained have natural accuracies of 87.25% and 59.87%, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2plots "weakly bounded" adversarial examples for the CIFAR-10 7-PGD adversarially trained model<ref type="bibr" target="#b6">[Madry et al., 2017]</ref> and our free m = 8 trained model. Both models were trained to resist ∞ attacks with = 8. The examples are made using a 50 iteration BIM attack with = 30 and s = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The loss surface of a 7-PGD adversarially trained model and our "free" trained model for CIFAR-10 on the first 2 validation images. In (a) and (b) we display the cross-entropy loss projected on one random (Rademacher) and one adversarial direction. In (c) and (d) we display the the cross entropy loss projected along two random directions. Both training methods behave similarly and do not operate by masking the gradients as the adversarial direction is indeed the direction where the cross-entropy loss changes the most."Free training" maintains generative properties, as our model's adversarial examples resemble the target class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The effect of the perturbation bound and the mini-batch replay hyper-parameter m on the robustness achieved by free training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 "Free" Adversarial Training (Free-m) Require: Training samples X, perturbation bound , learning rate τ , hop steps m 1: Initialize θ 2: δ ← 0 3: for epoch = 1 . . . N ep /m do</figDesc><table><row><cell>4:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :Table 2 :</head><label>12</label><figDesc>Validation accuracy and robustness of CIFAR-10 models trained with various methods. Validation accuracy and robustness of CIFAR-100 models trained with various methods.</figDesc><table><row><cell>Training</cell><cell cols="5">Evaluated Against Nat. Images PGD-20 PGD-100 CW-100</cell><cell cols="2">10 restart PGD-20</cell><cell>Train Time (min)</cell></row><row><cell>Natural</cell><cell>95.01%</cell><cell>0.00%</cell><cell>0.00%</cell><cell></cell><cell>0.00%</cell><cell cols="2">0.00%</cell><cell>780</cell></row><row><cell>Free m = 2</cell><cell>91.45%</cell><cell>33.92%</cell><cell>33.20%</cell><cell cols="2">34.57%</cell><cell cols="2">33.41%</cell><cell>816</cell></row><row><cell>Free m = 4</cell><cell>87.83%</cell><cell>41.15%</cell><cell>40.35%</cell><cell cols="2">41.96%</cell><cell cols="2">40.73%</cell><cell>800</cell></row><row><cell>Free m = 8</cell><cell>85.96%</cell><cell>46.82%</cell><cell>46.19%</cell><cell cols="2">46.60%</cell><cell cols="2">46.33%</cell><cell>785</cell></row><row><cell>Free m = 10</cell><cell>83.94%</cell><cell>46.31%</cell><cell>45.79%</cell><cell cols="2">45.86%</cell><cell cols="2">45.94%</cell><cell>785</cell></row><row><cell>7-PGD trained</cell><cell>87.25%</cell><cell>45.84%</cell><cell>45.29%</cell><cell cols="2">46.52%</cell><cell cols="2">45.53%</cell><cell>5418</cell></row><row><cell>Training</cell><cell></cell><cell cols="5">Evaluated Against Natural Images PGD-20 PGD-100</cell><cell>Training Time (minutes)</cell></row><row><cell>Natural</cell><cell></cell><cell>78.84%</cell><cell>0.00%</cell><cell></cell><cell cols="2">0.00%</cell><cell>811</cell></row><row><cell cols="2">Free m = 2</cell><cell>69.20%</cell><cell cols="2">15.37%</cell><cell cols="2">14.86%</cell><cell>816</cell></row><row><cell cols="2">Free m = 4</cell><cell>65.28%</cell><cell cols="2">20.64%</cell><cell cols="2">20.15%</cell><cell>767</cell></row><row><cell cols="2">Free m = 6</cell><cell>64.87%</cell><cell cols="2">23.68%</cell><cell cols="2">23.18%</cell><cell>791</cell></row><row><cell cols="2">Free m = 8</cell><cell>62.13%</cell><cell cols="2">25.88%</cell><cell cols="2">25.58%</cell><cell>780</cell></row><row><cell cols="2">Free m = 10</cell><cell>59.27%</cell><cell cols="2">25.15%</cell><cell cols="2">24.88%</cell><cell>776</cell></row><row><cell cols="2">Madry et al. (2-PGD trained)</cell><cell>67.94%</cell><cell cols="2">17.08%</cell><cell cols="2">16.50%</cell><cell>2053</cell></row><row><cell cols="2">Madry et al. (7-PGD trained)</cell><cell>59.87%</cell><cell cols="2">22.76%</cell><cell cols="2">22.52%</cell><cell>5157</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>ImageNet validation accuracy and robustness of ResNet-50 models trained with various replay parameters and = 2.</figDesc><table><row><cell>Training</cell><cell cols="4">Evaluated Against Natural Images PGD-10 PGD-50 PGD-100</cell></row><row><cell>Natural</cell><cell>76.038%</cell><cell>0.166%</cell><cell>0.052%</cell><cell>0.036%</cell></row><row><cell>Free m = 2</cell><cell>71.210%</cell><cell cols="3">37.012% 36.340% 36.250%</cell></row><row><cell>Free m = 4</cell><cell>64.446%</cell><cell cols="3">43.522% 43.392% 43.404%</cell></row><row><cell>Free m = 6</cell><cell>60.642%</cell><cell cols="3">41.996% 41.900% 41.892%</cell></row><row><cell>Free m = 8</cell><cell>58.116%</cell><cell cols="3">40.044% 40.008% 39.996%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The naturally trained model is vulnerable to PGD attacks (first row of table 3), while free training produces robust models that achieve over 40% accuracy vs PGD attacks (m = 4, 6, 8 in table 3). Attacking the models using PGD-100 does not result in a meaningful drop in accuracy compared to PGD-50. Therefore, we did not experiment with increasing the number of PGD iterations further.Fig. 4summarizes experimental results for robust models trained and tested under different perturbation bounds . Each curve represents one training method (natural training or free training) with hyperparameter choice m. Each point on the curve represents the validation accuracy for an -bounded robust model. These results are also provided as tables in the appendix. The proposed method consistently improves the robust accuracy under PGD attacks for = 2 − 7, and m = 4 performs the best. It is difficult to train robust models when is large, which is consistent with previous studies showing that PGD-based adversarial training has limited robustness for ImageNet<ref type="bibr" target="#b26">[Kannan et al., 2018]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Validation accuracy and robustness of "free" and 2-PGD trained ResNet-50 models -both trained to resist ∞ = 4 attacks. Note that 2-PGD training time is 3.46× that of "free" training.</figDesc><table><row><cell>Model &amp; Training</cell><cell cols="2">Evaluated Against Natural Images PGD-10 PGD-50 PGD-100</cell><cell>Train time (minutes)</cell></row><row><cell>RN50 -Free m = 4</cell><cell>60.206%</cell><cell>32.768% 31.878% 31.816%</cell><cell>3016</cell></row><row><cell>RN50 -2-PGD trained</cell><cell>64.134%</cell><cell>37.172% 36.352% 36.316%</cell><cell>10,435</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Validation accuracy and robustness of free-m = 4 trained ResNets with various capacities.</figDesc><table><row><cell>Architecture</cell><cell cols="2">Evaluated Against Natural Images PGD-10 PGD-50 PGD-100</cell></row><row><cell>ResNet-50</cell><cell>60.206%</cell><cell>32.768% 31.878% 31.816%</cell></row><row><cell>ResNet-101</cell><cell>63.340%</cell><cell>35.388% 34.402% 34.328%</cell></row><row><cell>ResNet-152</cell><cell>64.446%</cell><cell>36.992% 36.044% 35.994%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"><ref type="bibr" target="#b25">Xie et al. [2019]</ref> use 128 V100s and Kannan et al. [2018] use 53 P100s for targeted adv training ImageNet.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Adversarial Training for Free code for CIFAR-10 in Tensorflow can be found here: https://github. com/ashafahi/free_adv_train/ 3 ImageNet Adversarial Training for Free code in Pytorch can be found here: https://github.com/ mahyarnajibi/FreeAdversarialTraining 4 Results based on the "adv_trained" model in Madry's CIFAR-10 challenge repo.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Training using a non-iterative attack such as FGSM only doubles the training cost.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: Goldstein and his students were supported by DARPA GARD, DARPA QED for RML, DARPA L2M, and the YFA program. Additional support was provided by the AFOSR MURI program. Davis and his students were supported by the Office of the Director of National Intelligence (ODNI), and IARPA (2014-14071600012). Studer was supported by Xilinx, Inc. and the US NSF under grants ECCS-1408006, CCF-1535897, CCF-1652065, CNS-1717559, and ECCS-1824379. Taylor was supported by the Office of Naval Research (N0001418WX01582) and the Department of Defense High Performance Computing Modernization Program. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igino</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nedim</forename><surname>Šrndić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Characterizing adversarial subspaces using local intrinsic dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02613</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Magnet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01155</idno>
		<title level="m">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial transformation networks: Learning to generate adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumeet</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generative adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isay</forename><surname>Katsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bicheng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating adversarial examples with adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Label smoothing and logit squeezing: A replacement for adversarial training?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Logit pairing methods can fool gradient-based attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Trost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12042</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Slavin Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Formal guarantees on the robustness of a classifier against adversarial manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2266" to="2276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving dnn robustness to adversarial attacks using jacobian regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jakubovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="514" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Interpreting adversarial robustness: A view from decision surface in input space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00144</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scaling provable adversarial defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8400" to="8409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semidefinite relaxations for certifying robustness to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10877" to="10887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09344</idno>
		<title level="m">Certified defenses against adversarial examples</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixtrain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02625</idno>
		<title level="m">Scalable training of formally robust neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Certified robustness to adversarial examples with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lecuyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaggelis</forename><surname>Atlidakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Geambasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Jana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03471</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Second-order adversarial attack and certifiable robustness. CoRR, abs/1809.03113</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1809.03113" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elan</forename><surname>Jeremy M Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02918</idno>
		<title level="m">Certified adversarial robustness via randomized smoothing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Laurens van der Maaten, Alan Yuille, and Kaiming He. Feature denoising for improving adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Harini Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06373</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial logit pairing. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mc-Daniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<title level="m">Ensemble adversarial training: Attacks and defenses</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Defensive quantization: When efficiency meets robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><forename type="middle">Kushman</forename><surname>Pixeldefend</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10766</idno>
		<title level="m">Leveraging generative models to understand and defend against adversarial examples</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirini</forename><surname>Asteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros G</forename><surname>Dimakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09196</idno>
		<title level="m">The robust manifold defense: Adversarial training using generative models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark N Wegman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07896</idno>
		<title level="m">L2-nonexpansive neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Universal adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11304</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01442</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<title level="m">Robustness may be at odds with accuracy. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<title level="m">Theoretically principled trade-off between robustness and accuracy. ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Are adversarial examples inevitable? ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adversarial risk and the dangers of evaluating against weak attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05666</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">You only propagate once: Painless adversarial training using maximal principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00877</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Evaluating and understanding the robustness of adversarial logit pairing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10272</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6389" to="6399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Rana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00117</idno>
		<title level="m">Moustapha Cisse, and Laurens van der Maaten. Countering adversarial images using input transformations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1778" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01991</idno>
		<title level="m">Mitigating adversarial effects through randomization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
