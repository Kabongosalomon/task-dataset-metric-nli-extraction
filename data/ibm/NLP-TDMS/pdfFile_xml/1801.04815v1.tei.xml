<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
						</author>
						<title level="a" type="main">Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>TPAMI SUBMISSION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Metric Learning</term>
					<term>Deep Learning</term>
					<term>Convolutional Neural Network !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning similarity functions between image pairs with deep neural networks yields highly correlated activations of embeddings. In this work, we show how to improve the robustness of such embeddings by exploiting the independence within ensembles. To this end, we divide the last embedding layer of a deep network into an embedding ensemble and formulate training this ensemble as an online gradient boosting problem. Each learner receives a reweighted training sample from the previous learners. Further, we propose two loss functions which increase the diversity in our ensemble. These loss functions can be applied either for weight initialization or during training. Together, our contributions leverage large embedding sizes more effectively by significantly reducing correlation of the embedding and consequently increase retrieval accuracy of the embedding. Our method works with any differentiable loss function and does not introduce any additional parameters during test time. We evaluate our metric learning method on image retrieval tasks and show that it improves over state-of-the-art methods on the CUB-200-2011, Cars-196, Stanford  Online Products, In-Shop Clothes Retrieval and VehicleID datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D EEP Convolutional Neural Network (CNN) based metric learning methods map images to a high dimensional feature space. In this space semantically similar images should be close to each other, whereas semantically dissimilar images should be far apart from each other. To learn such metrics, several approaches based on image pairs (e.g. <ref type="bibr">[1]</ref>, <ref type="bibr">[2]</ref>), triplets (e.g. <ref type="bibr">[3]</ref>, <ref type="bibr">[4]</ref>) or quadruples (e.g. <ref type="bibr">[5]</ref>, <ref type="bibr">[6]</ref>) have been proposed in the past. Metric learning has a variety of applications, such as image or object retrieval (e.g. <ref type="bibr">[7]</ref>, <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), single-shot object classification (e.g. <ref type="bibr">[7]</ref>, <ref type="bibr">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>), keypoint descriptor learning (e.g. <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>), face verification (e.g. <ref type="bibr">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>), person re-identification (e.g. <ref type="bibr">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>), object tracking (e.g. <ref type="bibr" target="#b14">[15]</ref>), etc.</p><p>In this work, we focus on learning simple similarity functions based on the dot product, since they can be computed rapidly and thus, facilitate approximate search methods (e.g. <ref type="bibr" target="#b15">[16]</ref>) for largescale image retrieval. Typically, however, the accuracy of these methods saturates or declines due to over-fitting, especially when large embeddings are used <ref type="bibr">[7]</ref>.</p><p>To address this issue, we present a learning approach, called Boosting Independent Embeddings Robustly (BIER), which leverages large embedding sizes more effectively. The main idea is to divide the last embedding layer of a CNN into multiple nonoverlapping groups (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Each group is a separate metric learning network on top of a shared feature representation. The accuracy of an ensemble depends on the accuracy of individual learners as well as the correlation between them <ref type="bibr" target="#b16">[17]</ref>. Ideally, individual learners are highly accurate and have low correlation with each other, so that they complement each other during test time.</p><p>Manuscript received January 15, 2018. Naïvely optimizing a global loss function for the whole ensemble shows no benefits since all learners have access to the same feature representation and the same training samples. All groups will end up learning highly correlated embeddings, which results in no performance improvements at all, which is especially true for metric learning. To overcome this problem, we formulate the ensemble training as an online gradient boosting problem. In online gradient boosting, each learner reweights a training sample for successive learners according to the gradient of the loss function. Consequently, successive learners will focus on different samples than the previous learners, resulting in a more diverse feature representation (Section 3.1). To encourage the individual embeddings to have low correlation with each other already at the beginning of the training, we propose a novel initialization method for our embedding matrix (Section 3.2 and Section 3.3). The matrix is initialized from a solution of an optimization problem which implicitly minimizes the correlation between groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1801.04815v1 [cs.CV] 15 Jan 2018</head><p>In comparison to our earlier version of this work <ref type="bibr" target="#b17">[18]</ref>, we extend BIER by integrating our weight initialization method as auxiliary loss function directly into the training objective (Section 3.3.2). As we show in our evaluation (Section 4.6), this allows training BIER at higher learning rates which significantly reduces training time. By jointly training our network with this loss function, we can further reduce the correlation between learners and improve the accuracy of our method (Section 4.6).</p><p>Additionally, we improve our the performance by introducing a novel Adversarial Loss, which learns adversarial regressors between pairs of embeddings (Section 3.2.2). These regressors learn a non-linear transformation between embeddings. Their objective is to maximize similarity between embeddings. Between our embeddings and the regressors, we insert a gradient reversal layer <ref type="bibr" target="#b18">[19]</ref>. This layer changes the sign of the gradients during backpropagation and behaves like the identity function during forward propagation. As a consequence, our embeddings are trained to maximize this loss function w.r.t. our adversarial regressors and hence our ensemble becomes even more diverse.</p><p>We demonstrate the effectiveness of our metric on several image retrieval datasets <ref type="bibr">[7]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. In our evaluation we show that BIER significantly reduces the correlation of large embeddings (Section 4.1) and works with several loss functions (Section 4.2) while increasing retrieval accuracy by a large margin. BIER does not introduce any additional parameters into a CNN and has only negligible additional cost during training time and runtime. We show that BIER achieves state-ofthe-art performance on the CUB-200-2011 <ref type="bibr" target="#b22">[23]</ref>, Cars-196 <ref type="bibr" target="#b19">[20]</ref>, Stanford Online Products <ref type="bibr">[7]</ref>, In-Shop Clothes Retrieval <ref type="bibr" target="#b21">[22]</ref> and VehicleID <ref type="bibr" target="#b20">[21]</ref> datasets <ref type="bibr">(Section 4.8)</ref>. Further, by employing our novel Adversarial Loss during training time as auxiliary loss, we can significantly outperform the state-of-the-art on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is related to metric learning (Section 2.1) and boosting in combination with CNNs (Section 2.2). Additionally, since we propose a novel initialization method, we discuss related data dependent initialization methods for CNNs (Section 2.3). Next, we discuss techniques to increase the diversity of ensembles related to our auxiliary function (Section 2.4). Finally, we summarize adversarial loss functions for CNNs (Section 2.5), as we use an adversarial loss to encourage diversity of our learners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Metric Learning</head><p>The main objective of metric learning in Computer Vision is to learn a distance function d(·, ·) : R k × R k → R + mapping two k-dimensional input vectors, which are typically an input image or a feature representation of an image, to a distance between images. Typically, these distance functions have the form d(x, y) 2 = (x − y) M (x − y), where M is a positive semidefinite matrix. M can be factorized as (x − y) LL (x − y) = x L − y L 2 , where L ∈ R k×d projects an image, or a feature representation of an image into a d-dimensional vector space. In this vector space, semantically similar images should be close to each other, whereas semantically dissimilar images should be far apart from each other. For a complete review of metric learning approaches we refer the interested reader to <ref type="bibr" target="#b23">[24]</ref>. In this work we focus our discussion on boosting based metric learning approaches and deep CNN based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Boosting Based Metric Learning</head><p>In boosting based approaches, weak learners are typically rank one matrices. The ensemble then combines several of these matrices to form a positive semidefinite matrix M , e.g. <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. <ref type="bibr">Kedem et al. [29]</ref> propose gradient boosted trees for metric learning. They learn the non-linear mapping f (·) with an ensemble of regression trees, by minimizing a Large Margin Nearest Neighbor (LMNN) loss function <ref type="bibr">[4]</ref> with the gradient boosting framework. Further, they initialize their first learner as the solution of the linear LMNN optimization problem. In contrast to these offline boosting based works, our method is an online boosting method, which directly integrates into deep CNN training. Our weak learners are fully connected layers on top of a shared CNN feature representation and, compared to these methods, typically have a higher rank. Further, we use auxiliary loss functions to explicitly encourage diversity in our metric ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">CNN Based Metric Learning</head><p>CNN based methods learn a non-linear transformation of an input image of the form φ(·) : R k → R h . This CNN based feature extractor, i.e. φ(·), can be pre-trained on other tasks, such as large scale image classification, e.g. <ref type="bibr" target="#b29">[30]</ref>, and is then fine-tuned on metric learning datasets. To map the feature representation into the d-dimensional vector space, an additional linear embedding layer is typically added at the end of a CNN feature extractor as f (x) = φ(x) W , W ∈ R h×d . Hence, metric learning CNNs learn the distance function</p><formula xml:id="formula_0">d(x, y) 2 = (φ(x) − φ(y)) W W (φ(x) − φ(y)), which is equivalent to (φ(x) − φ(y)) M (φ(x) − φ(y)).</formula><p>To jointly learn all parameters of the CNN and the embedding, special loss functions operating on image pairs, triplets or quadruples are used. One of the most widely used pairwise loss functions for metric learning is the contrastive loss function, e.g. <ref type="bibr">[1]</ref>, <ref type="bibr">[2]</ref>, <ref type="bibr">[7]</ref>. This loss function minimizes the squared Euclidean distance between positive feature vectors while encouraging a margin between positive and negative pairs. To train networks with this loss function, a Siamese architecture, i.e. two copies of a network with shared weights, is commonly used, e.g. <ref type="bibr">[1]</ref>, <ref type="bibr">[2]</ref>.</p><p>Other approaches adopt the LMNN formulation <ref type="bibr">[4]</ref> and sample triplets consisting of a positive image pair and a negative image pair, e.g. <ref type="bibr">[3]</ref>, <ref type="bibr">[7]</ref>, <ref type="bibr">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The loss function encourages a margin between distances of positive and negative pairs. Hence, positive image pairs are mapped closer to each other in the feature space compared to negative image pairs.</p><p>Recently, several new loss functions for metric learning have been proposed. Song et al. <ref type="bibr">[7]</ref> propose to lift a mini-batch to a matrix of pairwise distances between samples. They use a structural loss function on this distance matrix to train the neural network. Ustinova et al. <ref type="bibr">[8]</ref> propose a novel histogram loss. They also lift a mini-batch to a distance matrix and compute a histogram of positive and negative distances. Their loss operates on this histogram and minimizes the overlap between the distribution of positive and negative distances. Huang et al. <ref type="bibr" target="#b30">[31]</ref> introduce a position dependent deep metric unit which is capable of learning a similarity metric adaptive to the local feature space. Sohn <ref type="bibr" target="#b31">[32]</ref> generalizes the triplet loss to n-tuples and propose a more efficient batch construction scheme. Song et al. <ref type="bibr" target="#b32">[33]</ref> propose a structured clustering loss to train embedding networks. Wang et al. <ref type="bibr" target="#b33">[34]</ref> propose a novel angular loss, which improves the traditional triplet loss by imposing geometric constraints for triplets. Movshovitz-Attias et al. <ref type="bibr" target="#b34">[35]</ref> propose a proxy-loss where they introduce a set of proxies which approximate the dataset. Their Proxy-Neighborhood Component Analysis (NCA) loss function optimizes distances to these proxies. <ref type="bibr">Rippel et al. [36]</ref> propose a "magnet" loss function which models multimodal data distributions and minimizes the overlap between distributions of different classes.</p><p>Our work is complementary to these approaches. We show in our evaluation that combining existing loss functions with our method yields significant improvements (Section 4.2).</p><p>Another line of work aims at improving the sample mining strategy used for embedding learning. Schroff et al. <ref type="bibr">[3]</ref> propose a semi-hard mining strategy for the triplet loss. Within a minibatch, they only use samples for training where the negative image pair has a larger distance than the positive pair. This avoids getting stuck in a local minima early in training <ref type="bibr">[3]</ref>. Harwood et al. <ref type="bibr" target="#b36">[37]</ref> use offline sampling of training samples. To avoid the large computational cost, they use approximate nearest neighbor search methods to accelerate distance computation. Wu et al. <ref type="bibr" target="#b37">[38]</ref> propose a distance weighted sampling method in combination with a margin based loss function to improve metric learning.</p><p>Although the main objective of our method is to reduce correlation in a large embedding, we apply a form of hard negative mining. We reweight samples for successive learners according to the gradient of the loss function. More difficult samples are typically assigned a higher gradient than easier samples. Hence, successive learners focus on harder examples than previous learners. However, we do not use any sample mining strategy for our first learner and hypothesize that our method can benefit from the above approaches, e.g. by selecting better samples from the training-set or mini-batch.</p><p>Most closely related to our method is the concurrent work of Yuan et al. <ref type="bibr" target="#b38">[39]</ref>. They propose a hard-aware deeply cascaded embedding. This method leverages the benefits of deeply supervised networks <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> by employing a contrastive loss function and train lower layers of the network to handle easier examples, and higher layers in a network to handle harder examples. In contrast to this multi-layer approach, we focus on reducing the correlation on just a single layer. Further, our method allows continuous weights for samples depending on the loss function. Finally, we show that employing auxiliary loss functions during initialization or training decreases correlation of learners and consequently improves the accuracy of the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Boosting for CNNs</head><p>Boosting is a greedy ensemble learning method, which iteratively trains an ensemble from several weak learners <ref type="bibr" target="#b41">[42]</ref>. The original boosting algorithm, AdaBoost <ref type="bibr" target="#b41">[42]</ref>, minimizes an exponential loss function. Friedman <ref type="bibr" target="#b42">[43]</ref> extends the boosting framework to allow minimizing arbitrary differentiable loss functions. They show that one interpretation of boosting is that it performs gradient descent in function space and propose a novel method leveraging this insight called gradient boosting. Successive learners in gradient boosting are trained to have high correlation with the negative gradient of the loss function. There are several algorithms which extend gradient boosting for the online learning setting, e.g. <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. In contrast to offline boosting, which has access to the full dataset, online boosting relies on online weak learners and updates the boosting model and their weak learners one sample at a time.</p><p>In the context of CNNs these methods are rarely used. Several works, e.g. <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> use CNN features in an offline boosting framework. These approaches, however, do not train the network and the weak learners end-to-end, i.e. the CNN is typically only used as a fixed feature extractor. In contrast to these approaches, we train our system end-to-end. We directly incorporate an online boosting algorithm into training a CNN.</p><p>Similarly, Walach et al. <ref type="bibr" target="#b49">[50]</ref> leverage gradient boosting to train several CNNs within an offline gradient boosting framework for person counting. The ensemble is then fine-tuned with a global loss function. In contrast to their work, which trains several copies of full CNN models, our method trains a single CNN with an online boosting method. Similar to dropout <ref type="bibr" target="#b50">[51]</ref>, all our learners share a common feature representation. Hence, our method does not introduce any additional parameters.</p><p>Very recently, Han et al. <ref type="bibr" target="#b51">[52]</ref> propose to use boosting to select discriminative neurons for facial action unit classification. They employ decision stumps on top of single neurons as weak learners, and learn weighting factors for each of these neurons by offline AdaBoost <ref type="bibr" target="#b41">[42]</ref> applied to each mini-batch separately. Weights are then exponentially averaged over several mini-batches. They combine the weak learner loss functions with a global loss function over all learners to train their network. In contrast to this work, we use weak learners consisting of several neurons (i.e. linear classifiers). Further, our method is more tightly integrated in an online boosting framework. We reweight the training set according to the negative gradient of the loss function for successive weak learners. This encourages them to focus on different parts of the training set. Finally, our method does not rely on optimizing an explicit discriminative global loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Initialization Methods</head><p>Most initialization methods for CNNs initialize weights randomly, either with carefully chosen variance parameters, e.g. <ref type="bibr" target="#b52">[53]</ref>, or depending on the fan-in and fan-out of a weight matrix, e.g. <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, with the goal of having an initialization which provides a large gradient during learning. Rather than focusing on determining the variance of the weight matrix, Saxe et al. <ref type="bibr" target="#b55">[56]</ref> propose to initialize the weight matrix as orthogonal matrix.</p><p>Recently, several approaches which initialize weights depending on the input data were proposed, e.g. <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>. These methods typically scale a random weight matrix such that the activations on the training set have unit variance.</p><p>Another line of work, e.g. <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, greedily initializes a network layer-by-layer, by applying unsupervised feature learning, such as Autoencoders or Restricted Bolzman Machines (RBMs). These methods seek for a weight matrix which minimizes the reconstruction error or a matrix which learns a generative model of the data.</p><p>Our initialization method is also a form of unsupervised pretraining of a single layer, as we use unsupervised loss functions for initializing the weights of our embedding layer. However, as opposed to minimizing a reconstruction loss or learning a generative model of the data, we initialize the weight matrix from a solution of an optimization problem which implicitly minimizes correlation between groups of features. With this initialization our weak learners already have low correlation at the beginning of the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Diversity in Ensembles</head><p>Previous approaches which exploit diversity in ensembles are based on Negative Correlation Learning (NCL) <ref type="bibr" target="#b60">[61]</ref>, e.g. <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>. These methods train neural networks in an ensemble to be negatively correlated to each other by penalizing the crosscorrelation of their predictions. As a consequence, they complement each other better during test time. These approaches are typically focused on training regressor ensembles, as opposed to classification or metric ensembles and do not use boosting. Further, they train several full regressor networks from scratch as opposed to using a single shared feature extractor CNN.</p><p>More closely related is AdaBoost.NC <ref type="bibr" target="#b62">[63]</ref>, which extends NCL to AdaBoost for classification. AdaBoost.NC defines an ambiguity penalty term based on the deviation of the predictions of the weak learners to the ensemble prediction. Intuitively, if many learners deviate from the ensemble prediction for a sample, the ambiguity is high. This ambiguity measure is used to update the weights for the samples for successive learners in the ensemble. In contrast to this work, we encourage diversity in our ensemble by directly using a differentiable loss function for our learners.</p><p>Finally, in an earlier work we applied auxiliary loss functions for a deep CNN based classification ensemble with a shared feature representation <ref type="bibr" target="#b63">[64]</ref>. Similar to this work, for computational efficiency, we share all low level CNN features and divide the network at the end into several non-overlapping groups. In contrast to our earlier work, we use online boosting to build our metric ensemble and different loss functions which are compatible with metric learning to encourage diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Adversarial Loss Functions</head><p>Adversarial networks, such as Generative Adversarial Networks (GANs) <ref type="bibr" target="#b64">[65]</ref>, have several applications such as image generation (e.g. <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>), style transfer (e.g. <ref type="bibr" target="#b67">[68]</ref>), domain adaptation (e.g. <ref type="bibr" target="#b68">[69]</ref>), etc. These approaches typically consist of two neural networks, a discriminator and a generator. During training, discriminator and generator are playing a two-player minimax game. The discriminator minimizes a loss function to distinguish real-world images from fake images, which are generated by the generator. On the other hand, the generator tries to confuse the discriminator by generating plausible fake images. To achieve this, it maximizes the loss function the discriminator tries to minimize. The problem has a unique solution where the generator recovers the training data distribution and the discriminator assigns an equal probability of 1 2 to real-world and fake samples <ref type="bibr" target="#b64">[65]</ref>. During training, GANs use alternating Stochastic Gradient Descent (SGD) to optimize the two networks. In the first step the parameters of the generator are updated, keeping the parameters of the discriminator fixed. Then, in the second step the discriminator is updated, while keeping the generator fixed e.g. <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>.</p><p>Most closely related to our work are methods which apply GANs and adversarial loss functions for domain adaptation. Tzeng et al. <ref type="bibr" target="#b69">[70]</ref> propose an adversarial loss at feature level for domain adaptation. They train a linear classifier on top of a hidden feature representation to categorize the domain of a sample. The feature generator (i.e. the hidden representation of the neural network) is trained to maximize the loss function of this classifier. Consequently, the hidden representation of samples from different domains will be aligned and hence undistinguishable for the linear classifier.</p><p>Similar to the GAN setup, Ganin et al. <ref type="bibr" target="#b18">[19]</ref> propose Domain Adversarial Neural Networks (DANNs). This method uses a gradient reversal layer for domain adaptation. They insert a discriminator on top of a neural network feature generator. The discriminator  </p><formula xml:id="formula_1">Binomial Deviance log(1 + e −(2y−1)β 1 (s−β 2 )Cy ) Contrastive (1 − y) max(0, s − m) + y(s − 1) 2 Triplet max(0, s − − s + + m)</formula><p>minimizes a loss function to distinguish samples of two different domains. Between the discriminator and feature extractor they insert a gradient reversal layer which flips the sign of the gradients during backpropagation. As a consequence, the feature extractor maximizes the loss function of the discriminator, making the hidden layer representation of different domains undistinguishable for the discriminator. Compared to GAN based approaches, DANNs do not need alternating updates of the generator and discriminator. At each step, the method updates the parameters of both, the generator and the discriminator. As opposed to aligning two domains with each other, our method makes embeddings more diverse. To this end, we adopt the gradient reversal layer of DANNs to make different learners more diverse from each other. We train a regressor, as opposed to a discriminator, which projects features from one learner to the other with a non-linear neural network. We optimize the regressor to maximize the similarity between embeddings. By inserting the gradient reversal layer between the regressor and our embeddings, we force our embeddings to be more diverse to each other. To the best of our knowledge, domain adaptation approaches have not been applied to increase diversity among classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BOOSTING A METRIC NETWORK</head><p>Our method builds upon metric CNNs, e.g. <ref type="bibr">[7]</ref>, <ref type="bibr">[8]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The main objective of these networks is to learn a high-dimensional non-linear embedding f (x), which maps an image x to a feature space R d . In this space, similar image pairs should be close to each other and dissimilar image pairs should be far apart from each other. To achieve this, instead of relying on a softmax output layer, these methods use a final linear layer consisting of an embedding matrix W ∈ R h×d , which maps samples from the last hidden layer of size h into the feature space R d . To learn this embedding matrix W and the parameters of the underlying network, these networks are typically trained on pairs or triplets of images and use loss functions to encourage separation of positive and negative pairs, e.g. <ref type="bibr">[7]</ref>.</p><p>As opposed to learning a distance metric, in our work we learn a cosine similarity score s(·, ·), which we define as dot product between two embeddings</p><formula xml:id="formula_2">s(f (x (1) ), f (x (2) )) = f (x (1) ) f (x (2) ) f (x (1) ) · f (x (2) )</formula><p>.</p><p>(1)</p><p>This has the advantage that the similarity score is bounded between [−1, +1].</p><p>In our framework, we do not use a Siamese architecture, e.g. as <ref type="bibr">[1]</ref>, <ref type="bibr">[2]</ref>. Instead, we follow recent work, e.g. <ref type="bibr">[3]</ref>, <ref type="bibr">[7]</ref>, <ref type="bibr">[8]</ref>, and sample a mini-batch of several images, forward propagate them through the network and sample pairs or triplets in the last loss layer of the network. The loss is then backpropagated through all layers of the network. This has the advantage that we do not need to keep several separate copies of the network in memory and that we can improve the computational efficiency.</p><p>We consider three different loss functions (defined in <ref type="table" target="#tab_0">Table 1</ref> and illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>), which are commonly used to train metric networks, e.g. <ref type="bibr">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. To avoid cluttering the notations in <ref type="table" target="#tab_0">Table 1</ref>, let s = s(f (x <ref type="bibr">(1)</ref> ), f (x (2) )) be the similarity score between image x (1) and x <ref type="bibr">(2)</ref> . Let y ∈ {1, 0} denote the label of the image pair (i.e. 1 for similar pairs, and 0 for dissimilar pairs). Let s − denote the similarity score for a negative image pair and s + denote the similarity score for a positive image pair. Further, m denotes the margin for the contrastive and triplet loss, which is set to 0.5 and 0.01, respectively. β 1 and β 2 are scaling and translation parameters and are set to 2 and 0.5, similar to <ref type="bibr">[8]</ref>. Finally, we follow <ref type="bibr">[8]</ref> and set the cost C y to balance positive and negative pairs for the binomial deviance loss as</p><formula xml:id="formula_3">C y = 1 if y = 1 25 otherwise.<label>(2)</label></formula><p>The binomial deviance loss is similar to the contrastive loss, but has a smooth gradient (see <ref type="figure" target="#fig_1">Fig. 2</ref>). In contrast, the contrastive and triplet loss have a gradient of either 0 or 1. As we show in our evaluation (Section 4.2) the binomial deviance loss benefits more from our method compared to the triplet and contrastive loss. We hypothesize that the main reason for that is that the gradient of the binomial deviance loss is smooth compared to the triplet loss or the contrastive loss. As a consequence, our method assigns smooth weights to training samples which conveys more information for successive learners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Online Gradient Boosting CNNs for Metric Learning</head><p>To encourage diverse learners we borrow ideas from online gradient boosting. Online gradient boosting iteratively minimizes a loss function using a fixed number of M weak learners, e.g. <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Learners are trained on reweighted samples according to the gradient of the loss function. Correctly classified samples typically receive a lower weight while misclassified samples are assigned a higher weight for successive learners. Hence, successive learners focus on different samples than previous learners, which consequently encourages higher diversity among weak learners. More formally, for a loss (·), we want to find a set of weak learners {f 1 (x), f 2 (x), . . . , f M (x)} and their corresponding boosting model where F (x (1) , x <ref type="bibr">(2)</ref> ) denotes the ensemble output and α m is the weighting factor of the m-th learner. The m-th learner of the ensemble is trained on a reweighted training batch according to the negative gradient − (·) of the loss function at the ensemble prediction until stage m − 1.</p><formula xml:id="formula_4">F (x (1) , x (2) ) = M m=1 α m s(f m (x (1) ), f m (x (2) )),<label>(3)</label></formula><p>To train the weak learners f m (·) in an online fashion, we adapt an online gradient boosting learning algorithm <ref type="bibr" target="#b43">[44]</ref> with fixed weights α m and integrate it within a CNN. Naïvely training multiple CNNs within the boosting framework is, however, computationally too expensive. To avoid this additional computational cost, we divide the embedding layer of our CNN into several non-overlapping groups, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. A single group represents a weak learner. All our weak learners share the same underlying feature representation, which is a pre-trained ImageNet CNN in all our experiments.</p><p>Our network is trained end-to-end on mini-batches with SGD and momentum. We illustrate the training procedure for loss functions operating on pairs and a single example per batch in Algorithm 1. Our algorithm also works with triplets, but for the sake of clarity we omit a detailed explanation here and refer the interested reader to the supplementary material. The training procedure can be easily integrated into the standard backpropagation algorithm, introducing only negligible additional cost, since most time during training is spent on computing convolutions. First, in the forward pass we compute similarity scores s m n for each input sample n and each group m. In the backward pass we backpropagate the reweighted losses for each group iteratively. The weight w m n for the n-th sample and the m-th learner is computed from the negative gradient − (·) of the ensemble prediction until stage m − 1. Hence, successive learners focus on examples which have large gradients (i.e. are misclassified) by previous learners.</p><p>This online gradient boosting algorithm yields a convex combination of weak learners f m (·), 1 ≤ m ≤ M . Successive learners in the ensemble typically have to focus on more complex training samples compared to previous learners and therefore, should have a larger embedding size. We exploit this prior knowledge and set the group size of learner m to be proportional to its Let η m = 2 m+1 , for m = 1, 2, . . . , M , M = number of learners, I = number of iterations for n = 1 to I do / * Forward pass * / Sample pair (x <ref type="bibr">(1)</ref> n , x <ref type="bibr">(2)</ref> n ) and corresponding label y n s 0 n := 0 for m = 1 to M do s m n :</p><formula xml:id="formula_5">= (1 − η m )s m−1 n + η m s(f m (x (1) n ), f m (x (2) n )) end Predict s n = s M n / * Backward pass * / w 1 n := 1 for m = 1 to M do Backprop w m n (s(f m (x (1) n ), f m (x (2)</formula><p>n )), y n ) w m+1 n := − (s m n , y n ) end end Algorithm 1: Online gradient boosting algorithm for our CNN.</p><formula xml:id="formula_6">weight α m = η m · M n=m+1 (1 − η n ) in the boosting algorithm, where η m = 2 m+1</formula><p>. We experimentally verify this design choice in Section 4.1.</p><p>During test time our method predicts a single feature vector for an input image x. We simply compute the embeddings from all weak learners f 1 (·), f 2 (·), . . . f M (·), L 2 -normalize each of them individually and weight each of them according to the boosting weights α m . Finally, we concatenate all vectors to a single feature vector, which is the embedding f (x) of the input image x. As a consequence, distances between our vectors can be efficiently computed via dot products and hence, our vectors can be used by approximate search methods, e.g. <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Diversity Loss Functions</head><p>Rather than relying on boosting alone to increase the diversity in our ensemble, we propose additional loss functions which make learners more diverse from each other. We present two different loss functions to encourage the diversity of learners. These can either be used for weight initialization or as auxiliary loss function during training (see Section 3.3). Our first loss function, which we denote as Activation Loss, optimizes the embeddings such that for a given sample, only a single embedding is active and all other embeddings are close to zero (see Section 3.2.1). As second loss function, we propose an Adversarial Loss. We train a regressor on top of our embeddings which maps one embedding to a different embedding, maximizing their similarity. By inserting a gradient reversal layer <ref type="bibr" target="#b18">[19]</ref> between the regressors and our embeddings, we update our embeddings so that they minimize the similarity between each other with respect to these regressors which results in more diverse embeddings (see Section 3.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Activation Loss</head><p>Our Activation Loss directly operates on the activations of our embeddings, making our learners more diverse by suppressing all activations except those of a single embedding (see <ref type="figure" target="#fig_3">Fig. 4</ref>). As a consequence, for a given sample, only a single embedding is active and all other embeddings are close to zero. More formally, let M denote the number of groups (i.e. weak learners) and G i denote the index set of neurons of group i, 1 ≤ i ≤ M . We want to increase the diversity of the embedding matrix W ∈ R h×d , where d denotes the embedding size and h the input feature dimensionality, i.e. the size of the last hidden layer in a CNN. Finally, let X = x (1) , x <ref type="bibr">(2)</ref> , . . . , x (N ) denote the training set. For our initialization experiments, which we will discuss in Section 3.3.1, we use feature vectors extracted from the last hidden layer of a pre-trained CNN, which we denote as φ(x) : R k → R h , where k denotes the input image dimensionality and h the dimensionality of the last hidden layer of our feature extractor. When we apply our loss function as auxiliary loss during end-to-end training, we jointly optimize this loss function with the metric loss, as will be shown in Section 3.3.2. Intuitively, we want to ensure that activations are not correlated between groups. For a sample x (n) , we encourage this with the following suppression loss function</p><formula xml:id="formula_7">L sup (i,j) (x (n) ) = k∈Gi, l∈Gj (f i (x (n) ) k · f j (x (n) ) l ) 2 ,<label>(4)</label></formula><p>where f i (x (n) ) = φ(x (n) ) W i denotes the i-th embedding (1 ≤ i ≤ M ) of input image x (n) , W i denotes the sub-matrix of W corresponding to the i-th embedding and f i (x (n) ) k the k-th dimension of f i (x (n) ). Naïvely solving this problem, however, leads to the trivial solution W = 0. To prevent this trivial solution, we add the regularization term</p><formula xml:id="formula_8">L weight = d i=1 (w i w i − 1) 2 ,<label>(5)</label></formula><p>where w i (with 1 ≤ i ≤ d) are the row vectors of W . This term forces the squared row vector norms of W to be close to 1 and hence avoids a trivial solution. Our final Activation Loss combines both L sup and L weight</p><formula xml:id="formula_9">L act = 1 N N n=1 M i=1, j=i+1 L sup (i,j) (x (n) ) + λ w · L weight ,<label>(6)</label></formula><p>where λ w is a regularization parameter, which we set high enough such that all row-vectors have a squared norm close to 1 ± 1e −3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Adversarial Loss</head><p>The previous Activation Loss imposes a rather strong constraint on the embeddings, i.e. for a given sample only a single embedding should be active and all other embeddings should be close to zero. While this improves the results, our objective is to maximize diversity between two feature vectors f i (x) ∈ R di and f j (x) ∈ R dj extracted from embedding i and j, where in general d i = d j .</p><p>Rather than imposing the constraint that only a single embedding <ref type="figure">Fig. 5</ref>. Illustration of our adversarial regressors (blue) between learner i and learner j of our embedding (red). We learn a regressor which maps the vector of learner j to learner i, maximizing the similarity of feature vectors. The gradient reversal layer flips the sign of the gradients which are backpropagated to our embeddings, therefore minimizing the similarity of feature vectors. We apply these regressors during training between all pairs of our learners.</p><p>is active for a given training sample, we could also aim for a weaker constraint. We want the two vectors f i (x) and f j (x) to be different, nonetheless discriminative. Therefore, the distance between the two vectors should be large. Unfortunately, there is no straightforward way to measure distances between two different vector spaces, since they can e.g. be of different dimensionality or be permuted.</p><p>To overcome this problem, we introduce an adversarial loss function, which we illustrate in <ref type="figure">Fig. 5</ref>. We learn regressors between pairs of embeddings which project f j (x) into the feature space f i (x), maximizing the similarity between embeddings f i (x) and f j (x), by maximizing a loss function. On the other hand, our learners try to minimize this loss function w.r.t. these adversarial regressors and therefore maximize their diversity. To achieve this, we use a reverse gradient layer <ref type="bibr" target="#b18">[19]</ref> between regressors and embeddings. During the forward pass this layer behaves like the identity function. However, during the backward pass, this layer flips the sign of the gradients. As a consequence, our embedding learners minimize this loss function with respect to the regressors, i.e. increasing their diversity.</p><p>More formally, let f m (x) ∈ R dm denote the d m dimensional embedding of the m-th learner. The objective of our adversarial regressor is to learn a function g (j,i) (·) : R dj → R di from the d j -dimensional embedding j to the d i -dimensional embedding i, maximizing similarity between vectors from embedding j and i via the loss</p><formula xml:id="formula_10">L sim(i,j) (x (n) ) = 1 d j (f i (x (n) ) g (j,i) (f j (x (n) ))) 2 ,<label>(7)</label></formula><p>where denotes the Hadamard (i.e. elementwise) product. This loss function can be made arbitrary large by scaling the weights of the regressors g (j,i) as well as the weights W of the embedding. Hence, we penalize large weights W and biases b of g (j,i) , and the weights W of our embedding as</p><formula xml:id="formula_11">L weight = max(0, b b − 1)+ i ( w i w i − 1) 2 + i (w i w i − 1) 2 ,<label>(8)</label></formula><p>where w i denotes the i-th row of the weight matrix W and w i denotes the i-th row of the weight matrix W . We combine both terms to train the regressor with our adversarial loss</p><formula xml:id="formula_12">L adv = 1 N N n=1 M i=1 j=i+1 −L sim(i,j) (x (n) ) + λ w · L weight ,<label>(9)</label></formula><p>where M is the number of learners in our ensemble. λ w is a regularization parameter, which we set high enough so that our weight vectors have a squared norm close to 1 ± 1e −3 .</p><p>Backpropagating the errors of this loss function to our learners increases their correlation and reduces their diversity. However, since we use a gradient reversal layer between our learners and the regressors, we actually force our learners to minimize L sim(i,j) , consequently increasing their diversity. In our experiments, we use two-layer neural networks with a Rectified Linear Unit (ReLU) as non-linearity in the hidden layer for the regressor g (j,i) . Further, we choose a hidden layer size of 512.</p><p>We use the loss function in Eq. (9) as auxiliary loss function during training as shown in the following section. At test time, we simply discard the regressors. Hence, we do not introduce any additional parameters during test time with this adversarial loss function. During training time, computational cost is dominated by calculating the forward and backward pass of the convolution layers. Further, since we are only using a gradient reversal layer as opposed to alternating updates of our adversarial network and our base network, we can update the parameters of both networks in a single forward and backward pass. Hence, we do not introduce significant additional computational cost during training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimizing Diversity Loss Functions</head><p>We present two ways to apply the previously defined loss functions to improve our boosting based method. In our first approach, we use one of our diversity loss functions, i.e. either our Activation Loss or our Adversarial Loss, for initializing the embedding matrix W . We fix all lower level CNN parameters and solve an optimization problem for the embedding matrix W . Then, we perform end-to-end training of the CNN with this initialization and our boosting based method (Section 3.3.1). Our second method applies the diversity loss during training time as auxiliary loss together with our boosting based method (Section 3.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Initialization Method</head><p>During initialization we want to find an initial estimate of the embedding matrix W , so that our learners already have low correlation with each other at the beginning of the training. Therefore, we omit end-to-end training and instead fix all the CNN parameters except the embedding matrix W . We minimize a loss function which encourages diversity of learners by solving the following optimization problem with SGD and momentum arg min</p><formula xml:id="formula_13">W L div ,<label>(10)</label></formula><p>where L div is either L act (c.f . Eq. <ref type="formula" target="#formula_9">(6)</ref>) if we use our Activation Loss or L adv (c.f . Eq. <ref type="formula" target="#formula_12">(9)</ref>) if we use our Adversarial Loss.</p><p>Compared to training a full CNN, solving this problem takes only seconds to a few minutes depending on the size of the dataset. The main reason for this is that we can pre-compute all lower level CNN features and just optimize with respect to the last layer (i.e. the embedding matrix W ). As a consequence, the number of parameters for which we are optimizing is smaller and the computational load is lower, hence convergence is quicker.</p><p>We show the benefits of both, our Adversarial Loss and Activation Loss as initialization method in Section 4.5. Both loss functions significantly improve the accuracy of our boosting based method, as they reduce the correlation between embeddings already from the beginning of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Auxiliary Loss Function</head><p>When we apply the loss functions as auxiliary loss during training, we sample the matrix W uniformly random <ref type="bibr" target="#b53">[54]</ref> and introduce an additional weighting parameter λ div , which controls the strength of our diversity regularizer. More formally, during training time we optimize the following loss</p><formula xml:id="formula_14">L = L metric + λ div · L div ,<label>(11)</label></formula><p>where L metric is the discriminative metric loss (e.g. binomial deviance, contrastive, triplet), which is minimized by our boosting based algorithm and L div is our loss function which encourages diversity in our ensemble. We either use L act (Eq. (6)) or L adv (Eq. (9)) for L div , depending on whether we use our Activation Loss or our Adversarial Loss, respectively. The weighting parameter λ div controls the strength of the diversity and can be set via cross-validation. We found it necessary to backpropagate the gradients of this auxiliary loss function only to the last layer of the CNN, i.e. the embedding layer. The main reason for this is that setting parameters of a CNN to 0 allows a trivial optimal solution for all our loss functions. To prevent this collapse, we add a constraint on the weights of our network (see Eq. (5) and Eq. <ref type="formula" target="#formula_11">(8)</ref>). For the embedding layer, we typically constrain the weights to have a squared L 2 norm of 1 for all row vectors. Adding this constraint to the hidden layers of a CNN, however, corrupts the learned ImageNet features. Hence, we only backpropagate this loss to the embedding layer, which we add on top of the last hidden layer of our feature extractor. During training time this has only a small computational overhead compared to standard backpropagation, as only the last layer is affected. We show the benefits of using our Activation Loss and our Adversarial Loss as auxiliary loss function in Section 4.6. When applied as auxiliary loss, our Adversarial Loss is more effective than our Activation Loss, i.e. it reduces the correlation between embeddings more without impairing their accuracy and as a result achieves higher ensemble accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>We first conduct a detailed ablation study on the CUB-200-2011 <ref type="bibr" target="#b22">[23]</ref> dataset. We follow the evaluation protocol proposed in <ref type="bibr">[7]</ref> and use the first 100 classes (5, 864 images) for training and the remaining 100 classes (5, 924 images) for testing.</p><p>For evaluation we use the Recall@K metric <ref type="bibr">[7]</ref>. For each image in the test set, we compute the feature vectors from our CNN and then retrieve the K most similar images from the remaining test set. If one of the K retrieved images has the same label as the query image, it is a match and increases the recall score by 1. The final Recall@K score is the average over all test images.</p><p>We implement our method with Tensorflow <ref type="bibr" target="#b70">[71]</ref>. As network architecture, we follow previous works (e.g. <ref type="bibr">[7]</ref>, <ref type="bibr">[8]</ref>) and use a GoogLeNet * <ref type="bibr" target="#b40">[41]</ref> which is pre-trained on the ImageNet dataset <ref type="bibr" target="#b29">[30]</ref>. As optimization method we use ADAM <ref type="bibr" target="#b72">[73]</ref> with a learning rate of 1e −6 . When we use auxiliary loss functions, we can increase the learning rate by an order of magnitude to 1e −5 (see Section 4.6). We construct a mini-batch by first sampling a fixed number of categories from the dataset and then sampling several images for each of these categories. Each mini-batch consists of approximately 5-10 images per category.</p><p>For preprocessing, we follow previous work, e.g. <ref type="bibr">[7]</ref>, <ref type="bibr">[8]</ref> and resize the longest axis of our images to 256 pixels and pad the shorter axis with white pixels such that images have a size of 256×256 pixels. We subtract the mean from the ImageNet dataset channel-wise from the image. During training time, we crop random 224 × 224 pixel patches from the images and randomly mirror them. During test time, we use the 224 × 224 pixel center crop from an image to predict the final feature vector used for retrieval.</p><p>In the following section, we show the impact of an ensemble trained with BIER on the strength (i.e. accuracy) and correlation of an embedding (Section 4.1). Next, we show that BIER works with several widely used loss functions (Section 4.2), we analyse the impact of the number of groups in an embedding (Section 4.3) and the embedding size (Section 4.4). Then, we demonstrate the effectiveness of our diversity loss functions during initialization (Section 4.5) and as auxiliary loss function during training (Section 4.6). We show the influence of our weighting parameter λ div (Section 4.7). Finally, we show that our method outperforms stateof-the-art methods on several datasets <ref type="bibr">[7]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> (Section 4.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Strength and Correlation</head><p>The performance of an ensemble depends on two elements: the strength (i.e. accuracy) of individual learners and the correlation between the learners <ref type="bibr" target="#b16">[17]</ref>. Ideally, learners of an ensemble are highly accurate and lowly correlated, so that they can complement each other well.</p><p>To evaluate the impact of our contributions on strength and correlation, we compare several models. First, we train a model with a regular loss function with an embedding size of 512 (Baseline). Next, we use a simple model averaging approach, where we split the last embedding layer into three non-overlapping groups of size 170, 171 and 171 respectively, initialize them with our Activation Loss initialization method and optimize a discriminative metric loss function on each of these groups separately (Init-170-171-171). Finally, we apply our boosting based reweighting scheme on the three groups (BIER-170-171-171).</p><p>As discussed in Section 3.1, we propose to use groups of different sizes proportional to the weighting of the online boosting algorithm, as subsequent learners have to deal with harder samples. To this end, we divide the embedding into differently sized groups. We assign the first learner a size of 96 neurons, the second learner 160 neurons and the last learner 256 neurons. Finally, we train a model with our Activation Loss initialization method (Init-96-160-256) and add our boosting method (BIER-96-160-256) on top of these learners. * We dump the weights of the network from the Caffe [72] model. <ref type="table" target="#tab_1">Table 2</ref>, initializing the weight matrix such that activations are independent already achieves a notable improvement over our baseline model. Additionally, our boosting method significantly increases the accuracy of the ensemble. Without boosting, the individual classifiers are highly correlated. By training successive classifiers on reweighted samples, the classifiers focus on different training examples leading to less correlated classifiers. Interestingly, the individual weak learners trained with just our Activation Loss initialization method achieve similar accuracy compared to our boosted learners (e.g. 51.94 vs 51.47 of Learner-1-170), but the combination achieves a significant improvement since each group focuses on a different part of the dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss Functions</head><p>To show that BIER works with several loss functions such as triplet loss or contrastive loss, we train a baseline CNN with embedding size of 512 and then with our boosting based method. For our method, we set the group size to 96, 160 and 256 respectively. In <ref type="table" target="#tab_2">Table 3</ref> we see that binomial deviance, triplet loss and contrastive loss can benefit from our method. Further, we see that our method performs best for loss functions with smooth (i.e. continuous) gradient. We hypothesize that this is due to the fact that non-smooth loss functions convey less information in their gradient. The gradient of the triplet and contrastive loss (for negative samples) is either 0 or 1, whereas the gradient of binomial deviance has continuous values between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Number of Groups</head><p>We demonstrate the influence of the number of groups on our method. To this end, we fix the embedding size to 512 and run our method with M = {2, 3, 4, 5} groups. The group size is proportional to the final weights of our boosting algorithm (see Section 3.1). In <ref type="table" target="#tab_0">Table 14</ref> we report the correlation of the feature embedding, the R@1 score of the ensemble and the average of the R@1 score of each individual learner. We see that with a fixed embedding size of 512, the optimal number of learners for our method is 3-4. For a larger number of groups the strength of individual learners declines and hence performance decreases. For a smaller number of groups the individual embeddings are larger. They achieve higher individual accuracy, but are more correlated with each other, since they benefit less from the gradient boosting algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Embedding Sizes</head><p>Next, we show the effect of different embedding sizes. We train a CNN with embedding sizes of 384, 512, 1024 with BIER and compare it to a regular CNN. For our method, we split the embeddings into several groups according to the weights of the learners (see Section 3.1). We divide the 384 sized embedding into groups of size 64, 128 and 192, respectively. For the embedding of size 512 we use groups of size 96, 160 and 256. Finally, for the largest embedding we use groups of size 50, 96, 148, 196, 242 and 292. We use the binomial deviance loss function, as it consistently achieves best results compared to triplet loss or contrastive loss (recall <ref type="table" target="#tab_2">Table 3</ref>). In <ref type="table" target="#tab_4">Table 5</ref> we see that our method yields a consistent gain for a variety of different embedding sizes. For larger embedding sizes a larger number of groups is more beneficial. We found that the main reason for this is that larger embeddings are more likely to over-fit. Hence, it is more beneficial to train several smaller learners which complement each other better.</p><p>Further, we illustrate the effect of the number of learners and the number of groups in <ref type="figure" target="#fig_4">Fig. 6</ref>. We observe that with larger embedding sizes our method can use a larger number of groups. The main reason for that is that larger embedding sizes have typically more redundancy (hence higher correlation) compared to smaller embedding sizes. Therefore, it is more beneficial to split a larger embedding into a larger number of groups. We set the group sizes proportional to the weight of our boosting algorithm (Section 3.1). For the interested reader, we also list the corresponding group sizes in our supplementary.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Impact of Initialization</head><p>To show the effectiveness of both, our Activation Loss and Adversarial Loss for weight initialization, we compare it with random initialization, as proposed by Glorot et al. <ref type="bibr" target="#b53">[54]</ref> and an orthogonal initialization method <ref type="bibr" target="#b55">[56]</ref>. All networks are trained with binomial deviance as loss function with our proposed boosting based reweighting scheme. We report mean R@1 of the three methods.</p><p>In <ref type="table" target="#tab_5">Table 6</ref> we see that BIER with both our initialization methods achieves better accuracy compared to orthogonal or random initialization. This is due to the fact that with our initialization method learners are already less correlated at the beginning of the training. This makes it easier for the boosting algorithm to maintain diversity of our learners during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Impact of Auxiliary Loss Functions</head><p>To show the benefits of adding our diversity loss functions during training as auxiliary loss function, we run several experiments on the CUB-200-2011 <ref type="bibr" target="#b22">[23]</ref> dataset. We compare our Adversarial Loss function to our Activation Loss function and a network which does not use an auxiliary loss function during training. All networks are trained with the boosting based reweighting scheme and use an embedding size of 512 with 3 groups (i.e. 96, 160 and 256 learners). Further, we observe that we can train our networks with auxiliary loss function with an order of magnitude higher learning rate (i.e. 1e −5 instead of 1e −6 ), which results in significantly faster convergence times. We report the R@1 accuracy of all our methods.</p><p>As we can see in <ref type="table">Table 7</ref>, by including an auxiliary loss during training we significantly improve over our previous baseline BIER <ref type="bibr" target="#b17">[18]</ref>, which used our boosting based training but the Activation Loss only during initialization. By including the auxiliary loss function during training, we can improve the stability of training, allowing our models to be trained with larger learning rates and therefore faster convergence. Training BIER <ref type="bibr" target="#b17">[18]</ref> without auxiliary loss functions and with such high learning rates yields a significant drop in performance, since training becomes too unstable.</p><p>Finally, the Adversarial Loss function outperforms the Activation Loss function by a significant margin. We hypothesize this is due to the fact that the Activation Loss function constrains the individual learners too much. The Activation Loss encourages the ensemble that for a given training sample, only a single learner should be active and all other learners should be close to zero. In contrast to that, our Adversarial Loss minimizes similarity between embeddings w.r.t. an adversarial regressor, which tries to make two vector spaces as similar as possible under a nonlinear mapping. According to our results, minimizing similarity is more effective for reducing correlation than suppressing entire vector spaces.</p><p>We also analyze the impact on strength and correlation of our auxiliary loss functions on our ensemble. We show these results in <ref type="table">Table 8</ref>. Notably, by including an auxiliary loss function we can significantly reduce correlation of the feature vectors as well as the correlation between classifiers. This suggests that our auxiliary loss functions further reduce redundancies in our embedding and therefore improve results. Compared to the Activation Loss, our Adversarial Loss can reduce the correlation between classifiers more effectively and achieves a higher accuracy in terms of R@1.</p><p>The individual learners of the Adversarial Loss achieve comparable accuracy to the learners of the Activation Loss (i.e. 51.1% vs 51.3%, 53.8% vs 53.5% and 55.3% vs 55.2%). The Adversarial Loss, however, can significantly reduce the correlation between classifiers (i.e. 0.6031 vs 0.7310) and features (i.e. 0.0731 vs 0.0882). As a consequence, the individual learners are more diverse from each other and complement each other better. Therefore, our Adversarial Loss achieves a significantly better ensemble accuracy of 57.5% vs 56.5%.</p><p>When we use our Adversarial Loss as auxiliary loss during training, in contrast to the work of Ganin et al. <ref type="bibr" target="#b18">[19]</ref>, which uses the gradient reversal layer for domain adaptation, we do not require a dynamic schedule for the regularization parameter λ div (see Section 3.3.2). Instead, we keep this weighting parameter fixed. Rather than scaling back the gradients inside the gradient reversal layer, we weight the loss function of our adversarial network with λ div . As a consequence, our adversarial auxiliary network trains slower compared to our base network, which turns out to be beneficial for the training process. We hypothesize that the main reason for this is that the adversarial network gets too strong if we update it too fast, which in turn degrades the performance of the base network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Evaluation of the Regularization Parameter</head><p>When we add our diversity loss functions during training time we introduce an additional parameter λ div (recall Section 3.3.2). To <ref type="bibr">TABLE 7</ref> Comparison of several auxiliary loss functions on CUB-200-2011 <ref type="bibr" target="#b22">[23]</ref>. Our adversarial loss function significantly improves accuracy over our baseline (BIER <ref type="bibr" target="#b17">[18]</ref>) and enables higher learning rates and faster convergence. demonstrate its effect, we train several models on the CUB-200-2011 dataset <ref type="bibr" target="#b22">[23]</ref> with a learning rate of 1e −5 and vary λ div . In <ref type="figure" target="#fig_5">Fig. 7</ref> we see that for our Adversarial Loss λ div peaks around 1e −3 , whereas for our Activation Loss λ div peaks around 1e −2 . Further, our Adversarial Loss significantly outperforms our Activation Loss by about 1% R@1. Finally, applying any of our loss functions as auxiliary loss function with a learning rate of 1e −5 significantly improves R@1 compared to networks without an auxiliary loss function trained with the same learning rate. Therefore, by integrating any of the two auxiliary loss function, we can improve the training stability of BIER at higher learning rates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Comparison with the State-of-the-Art</head><p>We show the robustness of our method by comparing it with the state-of-the-art on the CUB-200-2011 <ref type="bibr" target="#b22">[23]</ref>, Cars-196 <ref type="bibr" target="#b19">[20]</ref>, Stanford Online Product <ref type="bibr">[7]</ref>, In-Shop Clothes Retrieval <ref type="bibr" target="#b21">[22]</ref> and VehicleID <ref type="bibr" target="#b20">[21]</ref>  For training on CUB-200-2011, Cars-196 and Stanford Online Products, we follow the evaluation protocol proposed in <ref type="bibr">[7]</ref>. For the CUB-200-2011 dataset, we use the first 100 classes (5, 864 images) for training and the remaining 100 classes (5, 924 images) for testing. We further use the first 98 classes of the Cars-196 dataset for training (8, 054 images) and the remaining 98 classes for testing <ref type="bibr">(8, 131 images)</ref>. For the Stanford Online Products dataset we use the same train/test split as <ref type="bibr">[7]</ref>, i.e. we use 59, 551 images of 11, 318 classes for training and 60, 502 images of 11, 316 classes for testing. For the In-Shop Clothes Retrieval dataset, we use the predefined 25, 882 training images of 3, 997 classes for training. The test set is partitioned into a query set (14, 218 images of 3, 985 classes) and a gallery set (12, 612 images of 3, 985 classes). When evaluating on VehicleID, we use the predefined 110, 178 images of 13, 134 vehicles for training and the predefined test sets (Small, Medium, Large) for testing <ref type="bibr" target="#b20">[21]</ref>.</p><p>We fix all our parameters and train BIER with the binomial deviance loss function and an embedding size of 512 and group size of 3 (i.e. we use groups of size 96, 160, 256). For the CUB-200-2011 and Cars-196 dataset we follow previous work, e.g. <ref type="bibr">[7]</ref>, and report our results in terms of Recall@K, K ∈ {1, 2, 4, 8, 16, 32}. For Stanford Online Products we also stick to previous evaluation protocols <ref type="bibr">[7]</ref> and report Recall@K, K ∈ {1, 10, 100, 1000}, for the In-Shop Clothes Retrieval dataset we compare with K ∈ {1, 10, 20, 30, 40, 50} and for VehicleID we evaluate with K ∈ {1, 5}. We also report the results for the last learner in our ensemble (BIER Learner-3), as it was trained on the most difficult examples. Further, we also show the benefits of using our adversarial loss function during training time in combination with BIER (A-BIER) on all datasets and also report the last learner in this ensemble (A-BIER Learner-3).</p><p>Results and baselines are shown in Tables 9, 10, 11, 12 and 13. Our method in combination with a simple loss function operating on pairs is able to outperform or achieve comparable performance to state-of-the-art methods relying on higher order tuples <ref type="bibr">[7]</ref>, <ref type="bibr" target="#b31">[32]</ref>, histograms <ref type="bibr">[8]</ref>, novel loss functions <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> or hard sample mining strategies <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. We consistently improve our strong baseline method by a large margin at R@1 on all datasets, which demonstrates the robustness of our approach. Further, by using our adversarial loss function during training (A-BIER), we significantly improve over BIER <ref type="bibr" target="#b17">[18]</ref> and outperform state-of-theart methods. On CUB-200-2011 and Cars-196 we can improve over the state-of-the-art significantly by about 2-4% at R@1. The Stanford Online Products, the In-Shop Clothes Retrieval and VehicleID datasets are more challenging since there are only few (≈ 5) images per class. On these datasets our auxiliary <ref type="bibr">TABLE 9</ref> Comparison with the state-of-the-art on the CUB-200-2011 <ref type="bibr" target="#b22">[23]</ref> and Cars-196 <ref type="bibr" target="#b19">[20]</ref> dataset. Best results are highlighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB-200-2011</head><p>Cars <ref type="table" target="#tab_0">-196   R@K  1  2  4  8  16  32  1  2  4  8  16  32</ref> Contrastive adversarial loss achieves a notable improvement over BIER of 1.5%, 6.1% and 3-6%, respectively. A-BIER outperforms state-ofthe-art methods on all datasets. Notably, even the last learner in our adversarial ensemble (A-BIER Learner-3), evaluated on its own, already outperforms the state-of-the-art on most of the datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work we cast training an ensemble of metric CNNs with a shared feature representation as online gradient boosting problem. We further introduce two loss functions which encourage diversity in our ensemble. We apply these loss functions either during initialization or as auxiliary loss function during training. In our experiments we show that our loss functions increase diversity among our learners and, as a consequence, significantly increase accuracy of our ensemble. Further, we show that our novel Adversarial Loss function outperforms our previous Activation Loss function. This is because our Adversarial Loss increases the diversity in our ensemble more effectively. Consequently, the ensemble accuracy is higher for networks trained with our Adversarial Loss. Horst Bischof is member of the European academy of sciences and has received several awards <ref type="bibr" target="#b19">(20)</ref>, among them the 29th Pattern Recognition award in 2002, the main price of the German Association for Pattern Recognition (DAGM) in 2007 and 2012, the best scientific paper award at the BMCV 2007, the BMVC best demo award 2012 and the best scientific paper awards at the ICPR 2008, ICPR 2010, PCV 2010, AAPR 2010 and ACCV 2012.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A OVERVIEW</head><p>In this appendix to our main manuscript we provide further insights into BIER. First, in Appendix B we describe our method for loss functions operating on triplets. Next, in Appendix C we list the group sizes we used in our experiments (Section 4.4 of the main manuscript). Further, in Appendix D we summarize the effect of our boosting based training approach, our initialization approaches and our auxiliary loss functions. We provide an experiment evaluating the impact of end-to-end training in Appendix E. Further, in Appendix F we demonstrate that our method is also applicable to generic image classification problems. Finally, we show a qualitative comparison of the different embeddings in our ensemble in Appendix G and conclude with qualitative results in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B BIER FOR TRIPLETS</head><p>For loss functions operating on triplets of samples, we illustrate our training method in Algorithm 2. In contrast to our tuple based algorithm, we sample triplets x (1) , x <ref type="bibr">(2)</ref> and x <ref type="bibr">(3)</ref> which satisfy the constraint that the first pair (x (1) , x <ref type="bibr">(2)</ref> ) is a positive pair (i.e. y (1),(2) = 1) and the second pair (x (1) , x <ref type="bibr">(3)</ref> ) is a negative pair (i.e. y (1),(3) = 0). We accumulate the positive and negative similarity scores separately in the forward pass. In the backward pass we reweight the training set for each learner m according to the negative gradient at the ensemble predictions of both image pairs up to stage m − 1.</p><p>Let η m = 2 m+1 , for m = 1, 2, . . . , M , M = number of learners, I = number of iterations for n = 1 to I do / * Forward pass * / Sample triplet (x <ref type="bibr">(1)</ref> n , x <ref type="bibr">(2)</ref> n , x     </p><formula xml:id="formula_15">n ), f m (x (2) n )) s m − n := (1 − η m )s m−1 − n + η m s(f m (x (1) n ), f m (x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C LIST OF GROUP SIZES</head><p>In our second experiment in Section 4.4 of the main manuscript we evaluate the impact of the number of groups with embeddings of dimensionality 512 and 1024. For the sake of clarity, we list in <ref type="table" target="#tab_0">Table 14</ref> the dimensionality of all groups. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D SUMMARY OF IMPROVEMENTS</head><p>For the sake of clarity, we summarize our contributions on the CUB-200-2011 dataset <ref type="bibr">[1]</ref> in <ref type="table" target="#tab_0">Table 15</ref>. Our initialization method, our boosting based training method and our auxiliary loss functions improve the final R@1 score of the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E EVALUATION OF END-TO-END TRAINING</head><p>To show the benefits of end-to-end training with our method we apply our online boosting approach to a finetuned network and fix all hidden layers in the network (denoted as Stagewise training). We compare the results against end-to-end training with BIER with no auxiliary loss function during training time and summarize the results in <ref type="table" target="#tab_0">Table 16</ref>. End-to-end training significantly improves the final R@1 score, since weights of lower layers benefit from the increased diversity of the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX F GENERAL APPLICABILITY</head><p>Ideally, our idea of boosting several independent classifiers with a shared feature representation should be applicable beyond the task of metric learning. To analyse the generalization capabilities of our method on regular image classification tasks, we run an experiment on the CIFAR-10 [2] dataset. CIFAR-10 consists of 60, 000 color images grouped into 10 categories. Images are of size 32 × 32 pixel. The dataset is divided into 10, 000 test images and 50, 000 training images. In our experiments we split the training set into 10, 000 validation images and 40, 000 training images. We select the number of groups for BIER based on the performance on the validation set.</p><p>The main objective of this experiment is not to show that we can achieve state-of-the-art accuracy on CIFAR-10 <ref type="bibr">[2]</ref>, but rather to demonstrate that it is generally possible to improve a CNN with our method. To this end, we run experiments on the CIFAR-10-Quick <ref type="bibr">[3]</ref> and an enlarged version of the CIFAR-10-Quick architecture <ref type="bibr">[4]</ref> (see <ref type="table" target="#tab_0">Table 17</ref>). In the enlarged version, denoted as CIFAR-10-Quick-Wider, the number of convolution channels and the number of neurons in the fully connected layer is doubled. Further, an additional fully connected layer is inserted into the network. In both architectures, each convolution layer is followed by ReLU nonlinearity and a pooling layer of size 3 × 3 with stride 2. The last fully connected layer in both architectures has no nonlinearity.</p><p>To apply our method, we divide the last fully connected layer into 2 and 4 non-overlapping groups for the CIFAR-10-Quick and CIFAR-10-Quick-Wider architecture, respectively, and append a classifier to each group (see <ref type="table" target="#tab_0">Table 17</ref>). As loss function we use crossentropy. During training time, we apply either our Activation Loss, or Adversarial loss as auxiliary loss function to the last hidden layer of the network. This encourages the groups to be independent of each other. The main reason we have to add the loss function during training time is that weights change too drastically in networks trained from scratch compared to finetuning a network from a pre-trained ImageNet model. Hence, for this type of problems it is more effective to additionally encourage diversity of the learners with a separate loss function. Further, as opposed to our metric learning version of the algorithm, we can backpropagate the error of our auxiliary loss functions to all CNN layers without ending up with a trivial solution, where all weights are 0.</p><p>We compare our method to dropout <ref type="bibr">[5]</ref> applied to the last hidden layer of the network. As we see in <ref type="table" target="#tab_0">Tables 18 and 19</ref>, BIER with any of our two proposed loss functions improves on the CIFAR-10-Quick architecture over a baseline with just weight decay by 3.00% and over dropout by 1.10%. On the larger network which is more prone to overfitting, BIER improves over the baseline by 2.54% and over dropout by 1.52%.</p><p>These preliminary results indicate that BIER generalizes well for other tasks beyond metric learning. Thus, we will further investigate the benefits of BIER for other computer vision tasks in our future work. <ref type="bibr">TABLE 17</ref> We use the CIFAR-10-Quick <ref type="bibr">[3]</ref> and an enlarged version of CIFAR-10-Quick <ref type="bibr">[4]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX G QUALITATIVE COMPARISON OF EMBEDDINGS</head><p>To illustrate the differences between the learned embeddings we show several qualitative examples in <ref type="figure">Figure 8</ref>. Successive learners typically perform better at harder examples compared to previous learners, which have a smaller embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX H QUALITATIVE RESULTS</head><p>To illustrate the effectiveness of BIER we show some qualitative examples in Figures 9, 10, 11, 12 and 13.  <ref type="figure" target="#fig_0">Fig. 13</ref>. Qualitative results on the VehicleID <ref type="bibr">[9]</ref> dataset. We retrieve the 5 most similar images to the query image. Correct results are highlighted green and incorrect results are highlighted red.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>BIER divides a large embedding into an ensemble of several smaller embeddings. During training we reweight the training set for successive learners in the ensemble with the negative gradient of the loss function. During test time we concatenate the individual embeddings of all learners into a single embedding vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of triplet loss, contrastive loss (for negative samples) and binomial deviance loss (for negative samples) and their gradients. Triplet and contrastive loss have a non-continuous gradient, whereas binomial deviance has a continuous gradient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>We divide the embedding (shown as dashed layer) of a metric CNN into several weak learners and cast training them as online gradient boosting problem. Each learner iteratively reweights samples according to the gradient of the loss function. Training a metric CNN this way encourages successive learners to focus on different samples than previous learners and consequently reduces correlation between learners and their feature representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of our Activation Loss. Neurons (green) of different embeddings (red) suppress each other. We apply this loss during training time between all pairs of our learners.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Evaluation of different embedding sizes and group sizes on the CUB-200-2011 [23] dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Evaluation of λdiv on CUB-200-2011<ref type="bibr" target="#b22">[23]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>s.t. y (1),(2) = 1 and y (1),(3) = 0. s 0 + n := 0 s 0 − n := 0 for m = 1 to M do s m + n := (1 − η m )s m−1 + n + η m s(f m (x (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 1 )</head><label>1</label><figDesc>,(3) m := s(f m (x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>( 1 )</head><label>1</label><figDesc>n ), f m (x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>− (s m + n , s m − n ) end end Algorithm 2: Online gradient boosting algorithm for our CNN using triplet based loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Definition of loss functions used in our work.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Evaluation</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">of classifier (Clf.) and feature correlation on</cell></row><row><cell cols="4">CUB-200-2011 [23]. Best results are highlighted.</cell></row><row><cell cols="2">Method Clf. Corr. Baseline-512 -</cell><cell>0.1530</cell><cell>51.76</cell></row><row><cell>Init-170-171-171</cell><cell>0.8362</cell><cell>0.1005</cell><cell>53.73</cell></row><row><cell>Learner-1-170</cell><cell></cell><cell></cell><cell>51.94</cell></row><row><cell>Learner-2-171</cell><cell></cell><cell></cell><cell>51.99</cell></row><row><cell>Learner-3-171</cell><cell></cell><cell></cell><cell>52.26</cell></row><row><cell>Init-96-160-256</cell><cell>0.9008</cell><cell>0.1197</cell><cell>53.93</cell></row><row><cell>Learner-1-96</cell><cell></cell><cell></cell><cell>50.35</cell></row><row><cell>Learner-2-160</cell><cell></cell><cell></cell><cell>52.60</cell></row><row><cell>Learner-3-256</cell><cell></cell><cell></cell><cell>53.36</cell></row><row><cell cols="2">BIER-170-171-171 0.7882</cell><cell>0.0988</cell><cell>54.76</cell></row><row><cell>Learner-1-170</cell><cell></cell><cell></cell><cell>51.47</cell></row><row><cell>Learner-2-171</cell><cell></cell><cell></cell><cell>52.28</cell></row><row><cell>Learner-3-171</cell><cell></cell><cell></cell><cell>52.38</cell></row><row><cell>BIER-96-160-256</cell><cell>0.7768</cell><cell>0.0934</cell><cell>55.33</cell></row><row><cell>Learner-1-96</cell><cell></cell><cell></cell><cell>49.95</cell></row><row><cell>Learner-2-160</cell><cell></cell><cell></cell><cell>52.82</cell></row><row><cell>Learner-3-256</cell><cell></cell><cell></cell><cell>54.09</cell></row></table><note>↓ Feature Corr.↓ R@1↑</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Evaluation of loss functions on CUB-200-2011<ref type="bibr" target="#b22">[23]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Feature Corr. ↓ R@1 ↑</cell></row><row><cell>Triplet-512</cell><cell>0.2122</cell><cell>50.12</cell></row><row><cell>Triplet-96-160-256</cell><cell>0.1158</cell><cell>53.31</cell></row><row><cell>Contrastive-512</cell><cell>0.1639</cell><cell>50.62</cell></row><row><cell>Contrastive-96-160-256</cell><cell>0.1246</cell><cell>53.8</cell></row><row><cell>Binomial-Deviance-512</cell><cell>0.1530</cell><cell>51.76</cell></row><row><cell cols="2">Binomial-Deviance-96-160-256 0.0934</cell><cell>55.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Evaluation of group sizes on CUB-200-2011<ref type="bibr" target="#b22">[23]</ref>.</figDesc><table><row><cell>Group Sizes</cell><cell cols="3">Clf. Corr. ↓ Avg R@1 ↑ R@1 ↑</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>51.76</cell></row><row><cell>170-342</cell><cell>0.8252</cell><cell>53.06</cell><cell>54.66</cell></row><row><cell>96-160-256</cell><cell>0.7768</cell><cell>52.29</cell><cell>55.33</cell></row><row><cell>52-102-152-204</cell><cell>0.7091</cell><cell>50.67</cell><cell>55.62</cell></row><row><cell cols="2">34-68-102-138-170 0.6250</cell><cell>48.5</cell><cell>54.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc>Evaluation of embedding size on CUB-200-2011<ref type="bibr" target="#b22">[23]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell cols="3">Feature Corr. ↓ R@1 ↑</cell></row><row><cell></cell><cell></cell><cell cols="2">Baseline-384</cell><cell>0.1453</cell><cell></cell><cell>51.57</cell></row><row><cell></cell><cell></cell><cell cols="2">BIER-64-128-192</cell><cell>0.0939</cell><cell></cell><cell>54.66</cell></row><row><cell></cell><cell></cell><cell cols="2">Baseline-512</cell><cell>0.1530</cell><cell></cell><cell>51.76</cell></row><row><cell></cell><cell></cell><cell cols="2">BIER-96-160-256</cell><cell>0.0934</cell><cell></cell><cell>55.33</cell></row><row><cell></cell><cell></cell><cell cols="2">Baseline-1024</cell><cell>0.1480</cell><cell></cell><cell>52.89</cell></row><row><cell></cell><cell></cell><cell cols="3">BIER-50-96-148-196-242-292 0.0951</cell><cell></cell><cell>55.99</cell></row><row><cell></cell><cell>56.2</cell><cell cols="5">Evaluation of Embedding Size and Group Size</cell></row><row><cell></cell><cell>56.0</cell><cell></cell><cell>512</cell><cell></cell><cell></cell><cell></cell></row><row><cell>R@1</cell><cell>55.2 55.4 55.6 55.8</cell><cell></cell><cell>1024</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>55.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>54.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>54.6</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Number of Groups</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>Evaluation of Glorot, orthogonal and our Activation Loss and Adversarial Loss initialization method on CUB-200-2011<ref type="bibr" target="#b22">[23]</ref>.</figDesc><table><row><cell>Method</cell><cell>R@1</cell></row><row><cell>Glorot</cell><cell>54.41</cell></row><row><cell>Orthogonal</cell><cell>54.58</cell></row><row><cell>Activation Loss</cell><cell>55.33</cell></row><row><cell cols="2">Adversarial Loss 55.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>datasets. CUB-200-2011 consists of 11, 788 images of 200 bird categories. The Cars-196 dataset contains 16, 185 images of 196 cars classes. The Stanford Online Product dataset consists of 120, 053 images with 22, 634 classes crawled from Ebay. Classes are hierarchically grouped into 12 coarse categories (e.g. cup, bicycle, etc.). The In-Shop Clothes Retrieval dataset consists of 54, 642 images with 11, 735 clothing classes. VehicleID consists of 221, 763 images with 26, 267 vehicles.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>[7] 26.4 37.7 49.8 62.3 76.4 85.3 21.7 32.3 46.1 58.9 72.2 83.4 Triplet [7] 36.1 48.6 59.3 70.0 80.2 88.4 39.1 50.4 63.3 74.5 84.1 89.8 LiftedStruct [7] 47.2 58.9 70.2 80.2 89.3 93.2 49.0 60.3 72.1 81.5 89.2 92.8</figDesc><table><row><cell cols="8">Binomial Deviance [8] 52.8 64.4 74.7 83.9 90.4 94.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Histogram Loss [8]</cell><cell cols="6">50.3 61.9 72.6 82.4 88.8 93.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>N-Pair-Loss [32]</cell><cell></cell><cell cols="4">51.0 63.3 74.3 83.2</cell><cell>-</cell><cell>-</cell><cell cols="4">71.1 79.7 86.5 91.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Clustering [33]</cell><cell></cell><cell cols="4">48.2 61.4 71.8 81.9</cell><cell>-</cell><cell>-</cell><cell cols="4">58.1 70.6 80.3 87.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Proxy NCA [35]</cell><cell></cell><cell cols="4">49.2 61.9 67.9 72.4</cell><cell>-</cell><cell>-</cell><cell cols="4">73.2 82.4 86.4 87.8</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Smart Mining [37]</cell><cell cols="4">49.8 62.3 74.1 83.3</cell><cell>-</cell><cell>-</cell><cell cols="4">64.7 76.2 84.2 90.2</cell><cell>-</cell><cell>-</cell></row><row><cell>HDC [39]</cell><cell></cell><cell cols="12">53.6 65.7 77.0 85.6 91.5 95.5 73.7 83.2 89.5 93.8 96.7 98.4</cell></row><row><cell>Angular Loss [34]</cell><cell></cell><cell cols="4">54.7 66.3 76.0 83.9</cell><cell>-</cell><cell>-</cell><cell cols="4">71.4 81.4 87.5 92.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours Baseline</cell><cell></cell><cell cols="12">51.8 63.8 74.1 83.1 90.0 94.8 73.6 82.6 89.0 93.5 96.4 98.2</cell></row><row><cell cols="2">BIER Learner-3 [18]</cell><cell cols="12">54.1 66.1 76.5 84.7 91.2 95.3 76.5 84.9 90.9 94.9 97.6 98.7</cell></row><row><cell>BIER [18]</cell><cell></cell><cell cols="12">55.3 67.2 76.9 85.1 91.7 95.5 78.0 85.8 91.1 95.1 97.3 98.7</cell></row><row><cell cols="2">A-BIER Learner-3</cell><cell cols="12">55.3 67.0 76.8 86.0 91.1 95.3 80.6 88.2 92.3 95.8 97.6 98.6</cell></row><row><cell>A-BIER</cell><cell></cell><cell cols="12">57.5 68.7 78.3 86.2 91.9 95.5 82.0 89.0 93.2 96.1 97.8 98.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">Comparison with the state-of-the-art on the cropped versions of the CUB-200-2011 [23] and Cars-196 [20] dataset.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CUB-200-2011</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cars-196</cell><cell></cell></row><row><cell>R@K</cell><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell cols="2">PDDM + Triplet [31]</cell><cell cols="12">50.9 62.1 73.2 82.5 91.1 94.4 46.4 58.2 70.3 80.1 88.6 92.6</cell></row><row><cell cols="14">PDDM + Quadruplet [31] 58.3 69.2 79.0 88.4 93.1 95.7 57.4 68.6 80.1 89.4 92.3 94.9</cell></row><row><cell>HDC [39]</cell><cell></cell><cell cols="12">60.7 72.4 81.9 89.2 93.7 96.8 83.8 89.8 93.6 96.2 97.8 98.9</cell></row><row><cell>Margin [38]</cell><cell></cell><cell cols="5">63.9 75.3 84.4 90.6 94.8</cell><cell>-</cell><cell cols="5">86.9 92.7 95.6 97.6 98.7</cell><cell>-</cell></row><row><cell>Ours Baseline</cell><cell></cell><cell cols="12">58.9 70.1 79.8 87.6 92.6 96.0 82.6 88.8 93.1 96.1 97.5 98.7</cell></row><row><cell cols="2">BIER Learner-3 [18]</cell><cell cols="12">62.8 73.5 81.9 89.0 93.7 96.7 85.8 91.7 94.8 97.2 98.4 99.2</cell></row><row><cell>BIER [18]</cell><cell></cell><cell cols="12">63.7 74.0 82.5 89.3 93.8 96.8 87.2 92.2 95.3 97.4 98.5 99.3</cell></row><row><cell>A-BIER Learner-3</cell><cell></cell><cell cols="12">64.0 74.3 83.1 89.2 94.1 96.9 88.5 93.2 98.9 97.7 98.5 99.2</cell></row><row><cell>A-BIER</cell><cell></cell><cell cols="12">65.5 75.8 83.9 90.2 94.2 97.1 90.3 94.1 96.8 97.9 98.9 99.4</cell></row><row><cell cols="2">TABLE 11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE 12</cell><cell></cell></row><row><cell cols="6">Comparison with the state-of-the-art on the Stanford Online Products [7]</cell><cell></cell><cell cols="7">Comparison with the state-of-the-art on the In-Shop Clothes</cell></row><row><cell cols="2">dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Retrieval [22] dataset.</cell></row><row><cell>R@K</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>1000</cell><cell></cell><cell cols="2">R@K</cell><cell></cell><cell></cell><cell>1</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell>Contrastive [7]</cell><cell cols="3">42.0 58.2 73.8</cell><cell>89.1</cell><cell></cell><cell cols="4">FasionNet + Joints [22]</cell><cell cols="4">41.0 64.0 68.0 71.0 73.0 73.5</cell></row><row><cell>Triplet [7]</cell><cell cols="3">42.1 63.5 82.5</cell><cell>94.8</cell><cell></cell><cell cols="8">FasionNet + Poselets [22] 42.0 65.0 70.0 72.0 72.0 75.0</cell></row><row><cell>LiftedStruct [7]</cell><cell cols="3">62.1 79.8 91.3</cell><cell>97.4</cell><cell></cell><cell cols="3">FasionNet [22]</cell><cell></cell><cell cols="4">53.0 73.0 76.0 77.0 79.0 80.0</cell></row><row><cell cols="4">Binomial Deviance [8] 65.5 82.3 92.3</cell><cell>97.6</cell><cell></cell><cell cols="2">HDC [39]</cell><cell></cell><cell></cell><cell cols="4">62.1 84.9 89.0 91.2 92.3 93.1</cell></row><row><cell>Histogram Loss [8]</cell><cell cols="3">63.9 81.7 92.2</cell><cell>97.7</cell><cell></cell><cell cols="3">Ours Baseline</cell><cell></cell><cell cols="4">70.6 90.5 93.4 94.7 95.5 96.1</cell></row><row><cell>N-Pair-Loss [32]</cell><cell cols="3">67.7 83.8 93.0</cell><cell>97.8</cell><cell></cell><cell cols="3">BIER Learner-3 [18]</cell><cell></cell><cell cols="4">76.4 92.7 95.0 96.1 96.6 97.0</cell></row><row><cell>Clustering [33]</cell><cell cols="3">67.0 83.7 93.2</cell><cell>-</cell><cell></cell><cell cols="2">BIER [18]</cell><cell></cell><cell></cell><cell cols="4">76.9 92.8 95.2 96.2 96.7 97.1</cell></row><row><cell>HDC [39]</cell><cell cols="3">69.5 84.4 92.8</cell><cell>97.7</cell><cell></cell><cell cols="3">A-BIER Learner-3</cell><cell></cell><cell cols="4">82.8 95.0 96.8 97.4 97.7 98.0</cell></row><row><cell>Angular Loss [34]</cell><cell cols="3">70.9 85.0 93.5</cell><cell>98.0</cell><cell></cell><cell cols="2">A-BIER</cell><cell></cell><cell></cell><cell cols="4">83.1 95.1 96.9 97.5 97.8 98.0</cell></row><row><cell>Margin [38]</cell><cell cols="3">72.7 86.2 93.8</cell><cell>98.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Proxy NCA [35]</cell><cell>73.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours Baseline</cell><cell cols="3">66.2 82.3 91.9</cell><cell>97.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BIER Learner-3 [18]</cell><cell cols="3">72.5 86.3 93.9</cell><cell>97.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BIER [18]</cell><cell cols="3">72.7 86.5 94.0</cell><cell>98.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A-BIER Learner-3</cell><cell cols="3">74.0 86.8 93.9</cell><cell>97.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A-BIER</cell><cell cols="3">74.2 86.9 94.0</cell><cell>97.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 13</head><label>13</label><figDesc>Comparison with the state-of-the-art on VehicleID<ref type="bibr" target="#b20">[21]</ref>.</figDesc><table><row><cell></cell><cell>Small</cell><cell></cell><cell cols="2">Medium</cell><cell>Large</cell><cell></cell></row><row><cell>R@K</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>5</cell></row><row><cell cols="7">Mixed Diff+CCL [21] 49.0 73.5 42.8 66.8 38.2 61.6</cell></row><row><cell>GS-TRS loss [74]</cell><cell cols="6">75.0 83.0 74.1 82.6 73.2 81.9</cell></row><row><cell>Ours Baseline</cell><cell cols="6">78.0 87.5 73.0 84.7 67.9 82.4</cell></row><row><cell>BIER Learner-3 [18]</cell><cell cols="6">82.6 90.5 79.3 88.0 75.5 86.0</cell></row><row><cell>BIER [18]</cell><cell cols="6">82.6 90.6 79.3 88.3 76.0 86.4</cell></row><row><cell>A-BIER Learner-3</cell><cell cols="6">86.0 92.7 83.2 88.6 81.5 88.6</cell></row><row><cell>A-BIER</cell><cell cols="6">86.3 92.7 83.3 88.7 81.9 88.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 14</head><label>14</label><figDesc>Group sizes used in our experiments.</figDesc><table><row><cell cols="3">Embedding Number of Groups Groups</cell></row><row><cell>512</cell><cell>2</cell><cell>170-342</cell></row><row><cell>512</cell><cell>3</cell><cell>96-160-256</cell></row><row><cell>512</cell><cell>4</cell><cell>52-102-152-204</cell></row><row><cell>512</cell><cell>5</cell><cell>34-68-102-138-170</cell></row><row><cell>1024</cell><cell>3</cell><cell>170-342-512</cell></row><row><cell>1024</cell><cell>4</cell><cell>102-204-308-410</cell></row><row><cell>1024</cell><cell>5</cell><cell>68-136-204-274-342</cell></row><row><cell>1024</cell><cell>6</cell><cell>50-96-148-196-242-292</cell></row><row><cell>1024</cell><cell>7</cell><cell>36-74-110-148-182-218-256</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 15</head><label>15</label><figDesc>Summary of the impact of our initialization method and boosting on the CUB-200-2011 dataset.</figDesc><table><row><cell>Method</cell><cell>R@1</cell></row><row><cell>Baseline</cell><cell>51.76</cell></row><row><cell>Activation Loss initialization</cell><cell>53.73</cell></row><row><cell>Boosting with random initialization</cell><cell>54.41</cell></row><row><cell cols="2">Boosting with Activation Loss initialization 55.33</cell></row><row><cell>Boosting with auxiliary Activation Loss</cell><cell>56.5</cell></row><row><cell>Boosting with auxiliary Adversarial Loss</cell><cell>57.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 16</head><label>16</label><figDesc>Influence of end-to-end training on the CUB-200-2011 dataset. End-to-End training (BIER) 55.3</figDesc><table><row><cell>Method</cell><cell>R@1</cell></row><row><cell>Stagewise training</cell><cell>52.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>architecture.CIFAR-10-QuickCIFAR-10-Quick-Widerconv 5 × 5 × 32 conv 5 × 5 × 64 max-pool 3 × 3/2 max-pool 3 × 3/2 conv 5 × 5 × 32 conv 5 × 5 × 64 avg-pool 3 × 3/2 avg-pool 3 × 3/2 conv 5 × 5 × 64 conv 5 × 5 × 128 avg-pool 3 × 3/2 avg-pool 3 × 3/2</figDesc><table><row><cell>fc 64</cell><cell>fc 128</cell></row><row><cell>clf 10 × 2</cell><cell>fc 128</cell></row><row><cell></cell><cell>clf 10 × 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 18</head><label>18</label><figDesc>Results on CIFAR-10 [2] with the CIFAR-10-Quick architecture.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Baseline</cell><cell>78.72</cell></row><row><cell>Dropout</cell><cell>80.62</cell></row><row><cell>Activation BIER</cell><cell>81.40</cell></row><row><cell cols="2">Adversarial BIER 81.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 19</head><label>19</label><figDesc>Results on CIFAR-10[2]  with the CIFAR-10-Quick-Wider architecture.Fig. 9. Qualitative results on the CUB-200-2011[1]  dataset. We retrieve the 5 most similar images to the query image. Correct results are highlighted green and incorrect results are highlighted red.Fig. 10. Qualitative results on the Cars-196[6]  dataset. We retrieve the 5 most similar images to the query image. Correct results are highlighted green and incorrect results are highlighted red.Fig. 11. Qualitative results on the Stanford Online Products[7]  dataset. We retrieve the 5 most similar images to the query image. Correct results are highlighted green and incorrect results are highlighted red.Fig. 12. Qualitative results on the In-Shop Clothes Retrieval[8]  dataset. We retrieve the 5 most similar images to the query image. Correct results are highlighted green and incorrect results are highlighted red.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Baseline</cell><cell>80.67</cell></row><row><cell>Dropout</cell><cell>81.69</cell></row><row><cell>Activation BIER</cell><cell>83.10</cell></row><row><cell cols="2">Adversarial BIER 83.21</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This project was supported by the Austrian Research Promotion Agency (FFG) projects MANGO (836488) and Darknet (858591). We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This project was supported by the Austrian Research Promotion Agency (FFG) projects MANGO (836488) and Darknet (858591). We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a Similarity Metric Discriminatively, with Application to Face Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FaceNet: A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distance Metric Learning for Large Margin Nearest Neighbor Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quadruplet-wise Image Similarity Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reidentification by Relative Distance Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="653" to="668" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Metric Learning via Lifted Structured Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Deep Embeddings with Histogram Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Descriptors for Object Recognition and 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BaCoN: Building a Classifier from only N Samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVWW</title>
		<meeting>CVWW</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative Learning of Deep Convolutional Feature Point Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Embedding Deep Metric for Person Re-identification: A Study Against Large Variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Siamese Instance Search for Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable Nearest Neighbor Algorithms for High Dimensional Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2227" to="2240" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Random Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ML</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BIER: Boosting Independent Embeddings Robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain-Adversarial Training of Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D Object Representations for Fine-Grained Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Workshops</title>
		<meeting>ICCV Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
	</analytic>
	<monogr>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A survey on metric learning for feature vectors and structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<idno>abs/1306.6709</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaboost on Low-Rank PSD Matrices for Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Robust and Efficient Doubly Regularized Metric Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Vemuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MLBoost Revisited: A Faster Metric Learning Algorithm for Identity-Based Face Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Negrel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lechervy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Positive Semidefinite Metric Learning using Boosting-Like Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1007" to="1036" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-linear Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kedem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Local Similarity-Aware Deep Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved Deep Metric Learning with Multi-Class n-Pair Loss Objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Metric Learning via Facility Location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep Metric Learning With Angular Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">No Fuss Distance Metric Learning Using Proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Metric Learning with Adaptive Density Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Smart Mining for Deep Metric Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar B G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sampling Matters in Deep Embedding Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hard-Aware Deeply Cascaded Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeply-Supervised Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCSS</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Greedy Function Approximation: a Gradient Boosting Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AoS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online Gradient Boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Optimal and Adaptive Algorithms for Online Boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An Online Boosting Algorithm with Theoretical Justifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On Robustness of On-line Boosting -a Competitive Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Workshops</title>
		<meeting>ICCV Workshops</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Boosting Convolutional Features for Robust Object Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karianakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno>abs/1503.06350</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional Channel Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to Count with CNN Boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Understanding the Difficulty of Training Deep FeedForward Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Data-dependent Initializations of Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">All you need is a good init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Greedy Layer-Wise Training of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A Fast Learning Algorithm for Deep Belief Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NECO</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ensemble Learning via Negative Correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1399" to="1404" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multiobjective Neural Network Ensembles Based on Regularized Negative Correlation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1738" to="1751" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Negative Correlation Learning for Classification Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Efficient Model Averaging for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Least Squares Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Adversarial Discriminative Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Simultaneous Deep Transfer Across Domains and Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Incorporating Intra-Class Variance to Fine-Grained Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<idno>abs/1703.00196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Qualitative results on the CUB-200-2011 [1] dataset of the different learners in our ensemble. We retrieve the most similar image to the query image for learner 1, 2 and 3, respectively. Correct results are highlighted green and</title>
		<imprint/>
	</monogr>
	<note>incorrect results are highlighted red</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Reducing Overfitting in Deep Networks by Decorrelating Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">3D Object Representations for Fine-Grained Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Workshops</title>
		<meeting>ICCV Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deep Metric Learning via Lifted Structured Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
