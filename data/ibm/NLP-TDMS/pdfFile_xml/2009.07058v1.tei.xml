<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MLMLM: Link Prediction with Mean Likelihood Masked Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Clouatre</surname></persName>
							<email>louis.clouatre@polymtl.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Trempe</surname></persName>
							<email>philippe.trempe@polymtl.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><surname>Zouaq</surname></persName>
							<email>amal.zouaq@polymtl.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
							<email>sarath.chandar@polymtl.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polytechnique</forename><surname>Montral</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mila</forename></persName>
						</author>
						<title level="a" type="main">MLMLM: Link Prediction with Mean Likelihood Masked Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge Bases (KBs) are easy to query, verifiable, and interpretable. They however scale with man-hours and high-quality data. Masked Language Models (MLMs), such as BERT, scale with computing power as well as unstructured raw text data. The knowledge contained within those models is however not directly interpretable. We propose to perform link prediction with MLMs to address both the KBs scalability issues and the MLMs interpretability issues. To do that we introduce MLMLM, Mean Likelihood Masked Language Model, an approach comparing the mean likelihood of generating the different entities to perform link prediction in a tractable manner. We obtain State of the Art (SotA) results on the WN18RR dataset and the best nonentity-embedding based results on the FB15k-237 dataset. We also obtain convincing results on link prediction on previously unseen entities, making MLMLM a suitable approach to introducing new entities to a KB.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction 1.1 Context KBs have many desirable properties. They are easy to query, verifiable, and perhaps most importantly human interpretable. They however have one critical shortcoming, they are expensive to build making them harder to scale. Indeed, modern KBs scale with high-quality data, manual labor, or a mix of both. Approaches that scale with available computation and the massive amounts of unstructured data that are being created and accumulated have proven invaluable in the recent deep learning boom.</p><p>Large pretrained MLMs have been shown to scale well with large amounts of unstructured text data as well as with computing power. They also have shown some interesting emergent abilities, such as the ability to perform zero-shot question answering <ref type="bibr" target="#b13">(Radford et al., 2019)</ref>. This ability implies that the model parameters contain a large amount of factual knowledge that it can leverage to answer a wide array of questions. That knowledge is, however, hardly interpretable by humans, as it is hidden within the hundreds of millions or even tens of billions of parameters of the language model.</p><p>In this paper, we are interested in exploiting MLMs for link prediction. Many attempts at leveraging language models to complete KBs already exist. They, however, either rely on handcrafted templates to query the model <ref type="bibr" target="#b12">(Petroni et al., 2019)</ref>, limiting the generalizability of the solution, or are intractable for any decently sized KB <ref type="bibr" target="#b23">(Yao et al., 2019)</ref>. They also generally cannot introduce new, previously unseen, entities to KB and therefore require human intervention to keep a KB up to date. 1.2 Motivation By using MLMs to completes KBs, we can address both the issue of scalability of KBs and the issue of the interpretability of MLMs by committing knowledge of the latter to an interpretable format in the former. The MLM can learn new knowledge from the large amount of unstructured textual data that keeps being added to the World Wide Web and then be used to continually complete and update the KB. This will have the very desirable effect of making the link prediction approach scale with both computational power and a large quantity of unstructured data, both of which show no sign of slowing down.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Problem Definition</head><p>Simply put, we want to train an MLM to, given an entity and a relation, generate all entities completing the KB triplet.</p><p>Several technical blockades had to be broken to achieve proper link prediction with pretrained MLMs. The first one is tractability. The models being extremely large and expensive to perform inference on, it was necessary to enable link prediction with as little inference to the model as possible.</p><p>The second one has to do with the format of the MLMs inference outputs. The length of the output needs to be known at inference time, making it hard to sample entities of varying lengths from it. Work like <ref type="bibr" target="#b12">Petroni et al. (2019)</ref> is limited to single token outputs, which serves well to probe the model for the presence of embedded knowledge, but is not usable in practice for tasks such as link prediction. Any approach has to be able to sample an MLM for entities of varying lengths to have practical applications. Finally, the usage of MLMs opens the door to performing link prediction on unseen entities. Some capability of such an approach with MLMs was previously demonstrated <ref type="bibr" target="#b12">(Petroni et al., 2019)</ref>. We show that our approach yields strong results with unseen entities of arbitrary lengths in this task and should be explored further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Contribution</head><p>Our main contributions are summarized here:</p><p>• We propose MLMLM, a mean likelihood method to compare the likelihood of different text of different token lengths sampled from an MLM. • We demonstrate the tractability of our approach, which was not previously done by an MLM based model on the link prediction task. • We achieve SotA results on the WN18RR benchmark and the best non entity-embedding based mean reciprocal rank on the FB15k-237 benchmark. • We demonstrate that our approach can generalize reasonably well to previously unseen entities on both benchmarks.  <ref type="bibr">(Vaswani et al., 2017)</ref> to reconstruct the original text from the noisy inputs. Those models incorporate enormous amounts of language knowledge and world knowledge within their weights. This lets them be further tuned on challenging NLU tasks with great success.</p><p>Following on the footprints of BERT, several second-generation MLMs have been released. These models <ref type="bibr" target="#b8">(Liu et al., 2019;</ref><ref type="bibr" target="#b7">Lan et al., 2020)</ref> have seen great improvements when compared to BERT on downstream tasks. Among other improvements to the original training process, these models were trained for much longer with much larger text corpus to achieve those results.</p><p>Being based on the transformer encoder architecture, the output length of the model is equal to the input length. This makes it challenging to sample text of arbitrary length when using MLMs without knowing the length of the desired sample in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Link Prediction</head><p>Link prediction is the task of finding all potential entities that are in a specific relation with another entity. A knowledge graph (KG) is composed of a set of entities E, a set of relations R, and a set of valid triplets (h, r,t) representing the head entity h, the relation r and the tail entity t. By assigning a score to all possible triplets completing (h, r, ?) and (?, r,t), it is possible to rank all possible entities and thus complete the missing links within a KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">A Re-evaluation of Knowledge Graph</head><p>Completion Methods Recently, <ref type="bibr">Sun et al. (2020)</ref> has found that many of the SotA approaches to link prediction have used an inappropriate evaluation protocol. They have shown that the evaluation protocol typically used in the link prediction approaches assigns a perfect score to a constant output, by putting the correct entities on top during a tiebreaker. In essence, under this evaluation protocol, assigning a likelihood of 0 to all entities would yield a perfect reranking score, since the tiebreaker would put the target entity as the first prediction. This was shown to yield very inflated scores for many neural network based link prediction approaches <ref type="bibr" target="#b9">(Nathani et al., 2019;</ref><ref type="bibr" target="#b19">Vu et al., 2019;</ref><ref type="bibr" target="#b10">Nguyen et al., 2017)</ref>, as several of them output a large number of tied scores for the various entities. Entity embedding-based approaches <ref type="bibr" target="#b0">(Balažević et al., 2019;</ref><ref type="bibr" target="#b15">Sun et al., 2019;</ref><ref type="bibr" target="#b4">Dettmers et al., 2018)</ref> do not suffer from this issue. While we have found that our approach does not suffer from this issue despite not being an embedding approach, we will use the random evaluation protocol proposed by <ref type="bibr">Sun et al. (2020)</ref> for all evaluations and compare against approaches that used a similar protocol to ensure the validity of the comparisons. This protocol is similar to the filtered setting <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, with the difference that the rank among entities with tied scores is randomly assigned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">KG-BERT</head><p>KG-BERT <ref type="bibr" target="#b23">(Yao et al., 2019)</ref> is an approach to KB tasks based on MLM. It successfully demonstrates the potential of leveraging those models' internal knowledge on KB tasks. They train a BERT model to classify whether an individual triplet fed to the model is correct or not. In essence, they feed every single possible (h, r, ?) and (?, r, t) triplet to the model to obtain all scores to be reranked. This can result in millions of inference steps on the MLM for a single triplet completion. In contrast, our approach requires only one inference step through the MLM model for every triplet completion, by generating all logits required to obtain the likelihood of any potential entity. A comparison of the evaluation time is pictured in <ref type="figure" target="#fig_1">Figure 2</ref>  3 Methodology 3.1 Overview Our system performs link prediction. It uses MLM to generate all possible logits of all tokens required to rebuild all entities, and mean likelihood sampling to rerank all possible entities and perform the task. It can also be used to sample likelihoods for previously unseen entities. The system overview is as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. These inputs are then passed to the trained language model to generate a lookup table. This lookup table is then used by the ranking system to assign a score to entity tokens based on their likelihood. These scores are then finally used to rank the entities, the highest-scoring ones being the best candidates to complete the link.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Pre-processing</head><p>The data pre-processing pipeline takes a link prediction dataset and transforms it into a generic format usable by the model. It is required that both the entity and relations have string representations. For every entity in the dataset, we extract an entity string, which uniquely identifies the entity, and a definition string, which is a textual description of the given entity. For every relation, we extract a relation string, which uniquely identifies and describes the relation.</p><p>We tokenize all strings through the pretrained RoBERTa tokenizer <ref type="bibr" target="#b14">(Sennrich et al., 2016)</ref> and further transform the entity string by adding padding to match the longest tokenized entity within the dataset. Concretely, in a dataset where the longest entity has a length of 4 token ids, the entity string "dog" would be padded to have the representation "dog " and the entity string "cat and dog" would have the representation "cat and dog " where " " is the padding token. The purpose of this padding is to make the masked representation of all entities the same for the model, therefore letting the model treat all entities in the same manner.  <ref type="figure">Figure 1</ref>: Ranking System. The figure details the inner workings of the ranking system which uses the lookup table generated by the masked language model to compute the score associated with each possible entity. The scored entities are then ranked by highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model</head><p>Head Our approach uses the RoBERTa-Large model <ref type="bibr" target="#b8">(Liu et al., 2019)</ref> for all experiments. It finetunes the pretrained model on the link prediction datasets to generate the logits of the unknown entities. As our approach does a single call to the model to rerank all possible entities, it is acceptable to use the larger model for better performance. <ref type="figure">Figure 4</ref> shows the inference process for tail entity prediction. The head entity prediction would take as input the head entity mask, the relation, the tail entity and the tail entity definition. We use the relation string, the known entity string and the entity definition of the known entity string to make the model generate the logits representing the unknown entity string.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ranking System</head><p>The ranking system pictured in <ref type="figure">Figure 1</ref> performs link prediction on a given triplet. The MLM outputs logits for all possible token ids and positions for the missing entity to complete the triplet. This forms the lookup table T . The link prediction dataset contains a list of all possible entities. The token ids forming those entities make up E. We obtain the entity token logits L by matching all token ids in E with their corresponding values in T . L represents how likely every token of the entity was to be generated by the MLM at that specific position. The mean likelihood 1 of each entity is computed by averaging L over non-padded token logits 2 . This value is used to determine the ranking of the entity. It provides a proper comparison between entities of different lengths. Concretely, in our previous "cat and dog " ex- ample, we average the outputted logits for the "cat and dog" token ids and positions while ignoring the final padded logit. This averaging is done on all entities in the dataset completing the triplet, yielding the average likelihood assigned by the model to all entities. Entities are then sorted by highest rank using the randomized setting <ref type="bibr">(Sun et al., 2020)</ref>, meaning that for equal scores the tie-breaking is done randomly, to produce the ordered list of ranked entities R. We use the filtered setting <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> for evaluation and remove corrupted triplets from the list of ranked entities, corrupted triplets being all other known correct triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The two datasets used are WN18RR and FB15K-237 <ref type="bibr" target="#b2">(Bordes et al., 2013;</ref><ref type="bibr" target="#b17">Toutanova and Chen, 2015;</ref><ref type="bibr" target="#b3">Dettmers et al., 2017;</ref><ref type="bibr">Fellbaum, 1998;</ref><ref type="bibr" target="#b1">Bollacker et al., 2008)</ref>, two commonly used link prediction benchmarks. Summary stats for both are shown in <ref type="table" target="#tab_2">Table 1.</ref> WN18RR is a dataset composed of WordNet synsets. We use the cleaned synset as the entity string. The synset "dog.n.01" would have a string representation of "dog noun 1" which should be more interpretable by the model while remaining a unique identifier. The entity definition is the definition of the entity given by WordNet. The relation string is the cleaned relation. The relation " member of domain usage" would be represented with the string "member of domain usage". Full examples of inputs and outputs are shown in Listing 1 and Listing 2.</p><p>FB15k-237 is composed of triplets found in the now-defunct FreeBase KB, limiting itself to entities appearing in at least 100 triplets. We use the entity string and definitions as defined in <ref type="bibr" target="#b21">Xie et al. (2016)</ref>. We clean the relations to only include the words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>We use the Mean Reciprocal Rank (MRR) metric to validate our model and select the best model. For all experiments, we also report the Mean Rank (MR), the Mean Precision at 1 (MP@1), the Mean Precision at 3 (MP@3), and the Mean Precision at 10 (MP@10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training</head><p>The training setup is a modified MLM training, where we let the model generate the missing entity. The previously mentioned padding lets us deal with the generation of entities of varying sizes. The input fed to the model for tail entity prediction, pictured in <ref type="figure">Figure 4</ref>, consists of the concatenated token ids of the head entity, the head entity definition, the relation and the tail entity mask. The model will then generate, in the place of the mask, the missing entity. The input fed to the model for head entity prediction is similar. An example of the input for head entity prediction is found in Listing 1 and an example for tail entity prediction is found in Listing 2.</p><p>We use the categorical cross-entropy loss to train the language model. The loss only depends on the non-padded token of the generated entity, ignoring all other outputs. The target is the actual entity completing the triplet, aligned with the mask in the input. We retain the model with the best validation MRR. All experiments are run for 5 random seeds and the mean and standard deviation of the results are reported.</p><p>For all experiments, we use the hyperparameters and training setup described in <ref type="bibr" target="#b8">Liu et al. (2019)</ref> and shown in <ref type="table" target="#tab_3">Table 3</ref>, with a total of 10 epochs for the FB15k-237 dataset and 25 epochs for the WN18RR dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Unseen Entities</head><p>A secondary version of the dataset is made to test the generalization capacity of our methodology to  <ref type="figure">?, r,t)</ref>. The reported results are therefore only on the performance of previously unseen entities in the KB. The validation and test set are rebuilt for every random seed, to evaluate our approach on a wider array of unseen entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">WN18RR</head><p>We achieve SotA results on the WN18RR dataset on all tested metrics with the exception of MR, shown in <ref type="table" target="#tab_4">Table 2</ref>. The WN18RR dataset is sparse in terms of relations, see <ref type="table" target="#tab_2">Table 1</ref>. This sparseness lends itself naturally to leveraging a pretrained model, since the amount of information that can be extracted from the dataset on any given entity is limited, which makes outside information all the more valuable. We can observe a large discrepancy between the MP@1 and MP@3 metrics, implying that the model will have the correct answer in its top 3 much more often than within its top 1. This could be explained by an issue of disambiguation in the name of the entity. While approaches using entity em-beddings <ref type="bibr" target="#b0">(Balažević et al., 2019;</ref><ref type="bibr" target="#b15">Sun et al., 2019;</ref><ref type="bibr" target="#b4">Dettmers et al., 2018)</ref> will have no issue separating the synsets dog.n.01 and dog.n.03 as meaning respectively "a member of the genus Canis [...]" and "informal term for a man", our model will have to discern between those two meanings only by the digit appended to the name. It is probable that the model is often confused about whether it should generate dog noun 1 or dog noun 3, having only the final digit to differentiate both of them. An example of such an error is shown in Listing 2, where the model confuses aid.n.01 and aid.n.03. Follow up work on better representations for entity names could yield stronger results.</p><p>We performed some quantitative and qualitative error analysis to understand some of the remaining shortcomings of our approach. It seems like our model generally has a much easier time predicting the tail entity than the head entity, having an MRR of 0.6015 on tail entities and an MRR of 0.4009 on head entities. By observing the instances where our model gives the worst rank to the correct answer, we can understand why. A large number of those cases are hypernyms on the head entity. The definition of a hypernym is as follows: "A hypernym of something is its superordinate term: if X is a hypernym of Y, then all Y are X." <ref type="bibr">(Fellbaum, 1998</ref>). An example of a hypernym relationship would be: "animal is an hypernym of dog, since all dogs are animals." Correctly ranking all possibilities for "X is an hypernym of dog." seems easier for the model to do than correctly ranking all possibilities for "Animal is an hypernym of Y.". An example of such failure is shown in Listing 1, where we look for the hypernym of the term mediator. It is clear Listing 1: Example of an error of the model on WN18RR. Shown are the top 5 ranked entities by the model with the score assigned to them. The correct answer, matchmaker noun 1, was ranked 14,108 by the system.  The results are reported as &lt;mean&gt; ± &lt;standard deviation&gt;. Results for other models are taken from <ref type="bibr">Sun et al. (2020)</ref>.</p><p>that the model understands the concept and outputs plausible answers in its top 5. A large amount of the model's severe failure cases are similar to this one, where the model will output a plausible hypernym of the tail entity, while completely missing the targeted hypernym.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">FB15K-237</head><p>The results on FB15k-237 shown in <ref type="table" target="#tab_6">Table 4</ref> are, comparatively to the results obtained on WN18RR, fairly weak. FB15k-237 is very dense and contains a lot more training examples than the WN18RR dataset for a smaller amount of entities. Thus, nonpretrained models have way more examples to learn from in the dataset, which makes the learned information of pretrained models comparatively less impactful. This implies, non-surprisingly, that our approach heavily relies on the pre-training of the model and that it is less adept than other specialized approaches at learning from dense link prediction datasets.</p><p>However, FB15k-237 is an especially dense section of the FreeBase dataset, being composed of only entities containing a minimum of 100 relations, and is thus not representative of the KB as a whole. In practice, KB completion will often be used on entities rarely or never seen within the KB. While our FB15k-237 results are not SotA when compared to all approaches, the MRR however compares favorably to all other non entityembedding approaches on the randomized setting. The results are reported as &lt;mean&gt; ± &lt;standard deviation&gt;. The results are reported as &lt;mean&gt; ± &lt;standard deviation&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Unknown Entities Experiments</head><p>We demonstrate the capacity of our approach to generalize to unknown entities. Results for the WN18RR and the FB15k-237 datasets are shown in <ref type="table" target="#tab_7">Table 5</ref> and <ref type="table" target="#tab_8">Table 6</ref>. For baselines, we use a random baseline, reranking the entities randomly, as well as a non-finetuned RoBERTa-large model, that simply generates the entity tokens without being finetuned on the dataset first. We can notice that while our approach outperforms a non-finetuned benchmark, the nonfinetuned RoBERTa model still far outperforms the random baseline, supporting some of the findings of <ref type="bibr" target="#b12">Petroni et al. (2019)</ref> in the capacity of MLM to perform unsupervised link prediction.</p><p>It is to be noted that the high standard deviation of the results in this set of experiments comes from the fact that the validation and test entities are resampled with a different random seed on every run, yielding more variability in the results.</p><p>We are unaware of other approaches that can generalize to unknown entities of arbitrary size in the task of link prediction. We believe that leveraging MLMs could eventually lead to automatically populating KBs with new entities, as new knowledge and new facts are created and added to the web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Limitations</head><p>MLMLM comes with several limitations. Our approach to padding limits the size of an unknown entity to the size of the longest known entity. While it is likely to not be limiting in practice, it is still a weakness of our approach to sampling. The model size can be very prohibitive and specialized hardware such as GPUs is required to run it in a timely fashion. The approach however remains tractable as it can provide likelihoods for all possible entities in a single inference call. Compared to entityembedding based methods, our approach needs additional information in the form of meaningful string representations for both entities and relations. Entity disambiguation is also a limiting factor that does not affect other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have developed a methodology for training masked language models to perform link prediction. By leveraging the natural language understanding abilities of these models as well as the factual knowledge embedded within their weights, we have achieved a tractable approach to link prediction that yields state of the art results on a standard benchmark and the best non entity-embedding based results on another. We have also demonstrated the ability of our model to perform link prediction of previously unseen entities, making our approach suitable to introduce new entities to knowledge bases. More generally, we have introduced an approach to sampling text from a masked language model of varying lengths, which can have a wider use case.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. Modern KBs can contain millions of entities. Approaches like KG-BERT cannot scale to hundreds of thousands of entities at evaluation time, having an MLM inference complexity of O(N) where we boast a constant complexity with relation to the number of entities within the KB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Approach Inference Time. This figure shows the per-entity inference time based on the total number of entities to be re-ranked, of MLMLM and KG-BERT, the most comparable approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>System Overview. This figure presents the system overview. The inputs are the string representation of the (h, r, ?) or (?, r,t) triplets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Lookup Table GenerationFor Tail Entity Prediction. The figure shows how the lookup table for tail entity prediction is generated. A string representation of the head entity and the relation are fed to the masked language model which outputs logits that represent the likelihood of finding each token at each possible position of the tail entity.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Lookup</cell></row><row><cell></cell><cell></cell><cell></cell><cell>table</cell></row><row><cell>Logits</cell><cell>Logits</cell><cell>Logits</cell><cell>Logits</cell></row><row><cell></cell><cell cols="2">Language model</cell><cell></cell></row><row><cell>entity</cell><cell>Definition of head entity</cell><cell>Relation</cell><cell>? (Tail entity)</cell></row><row><cell>Figure 4:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>: Datasets</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">Entities Relations Mean in-degree Median in-degree</cell></row><row><cell>WN18RR</cell><cell>40943</cell><cell>11</cell><cell>2.12</cell><cell>1</cell></row><row><cell cols="2">FB15k-237 14541</cell><cell>237</cell><cell>18.71</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>All experiments hyperparameters.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Max sequence length</cell><cell>512</cell></row><row><cell>Batch size</cell><cell>32</cell></row><row><cell>Learning rate</cell><cell>2 × 10 −5</cell></row><row><cell>Weight decay</cell><cell>0.1</cell></row><row><cell>Gradient Norm</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>The results are reported as &lt;mean&gt; ± &lt;standard deviation&gt;. Results for other models are taken fromSun et al. (2020).    unseen entities. For both datasets, we start by randomly sampling 5% of the entities for the validation entities and 5% of the entities for the testing entities. Our training set consists of all triplets not containing any of the validation or testing entities. Our validation set consists of all triplets containing the validation entities. Finally, our test set consists of all triplets containing the test entities, but not containing any of the validation entities. The training is done in the same fashion. The validation and testing are only done on entities present in the validation or test entity list. If the tail entity is the one present in the test entity list, we will complete the link (h, r, ?) and not the link (</figDesc><table><row><cell></cell><cell></cell><cell cols="2">: WN18RR Results</cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell>MRR ↑</cell><cell>MR ↓</cell><cell>MP@1 ↑</cell><cell>MP@3 ↑</cell><cell>MP@10 ↑</cell></row><row><cell>ConvE</cell><cell>0.444</cell><cell>4950</cell><cell>-</cell><cell>-</cell><cell>0.503</cell></row><row><cell>RotatE</cell><cell>0.473</cell><cell>3343</cell><cell>-</cell><cell>-</cell><cell>0.571</cell></row><row><cell>TuckER</cell><cell>0.461</cell><cell>6324</cell><cell>-</cell><cell>-</cell><cell>0.516</cell></row><row><cell>ConvKB</cell><cell>0.249</cell><cell>3433</cell><cell>-</cell><cell>-</cell><cell>0.524</cell></row><row><cell>CapsE</cell><cell>0.415</cell><cell>718</cell><cell>-</cell><cell>-</cell><cell>0.559</cell></row><row><cell>KBAT</cell><cell>0.412</cell><cell>1921</cell><cell>-</cell><cell>-</cell><cell>0.554</cell></row><row><cell>MLMLM</cell><cell>0.5017 ± 0.0018</cell><cell>1603 ± 26.8184</cell><cell>0.4391 ± 0.0020</cell><cell>0.5418 ± 0.0028</cell><cell>0.6110 ± 0.0020</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">: FB15k-237 Results</cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell>MRR ↑</cell><cell>MR ↓</cell><cell>MP@1 ↑</cell><cell>MP@3 ↑</cell><cell>MP@10 ↑</cell></row><row><cell>ConvE</cell><cell>0.324</cell><cell>285</cell><cell>-</cell><cell>-</cell><cell>0.501</cell></row><row><cell>RotatE</cell><cell>0.336</cell><cell>178</cell><cell>-</cell><cell>-</cell><cell>0.530</cell></row><row><cell>TuckER</cell><cell>0.353</cell><cell>162</cell><cell>-</cell><cell>-</cell><cell>0.536</cell></row><row><cell>ConvKB</cell><cell>0.243</cell><cell>309</cell><cell>-</cell><cell>-</cell><cell>0.421</cell></row><row><cell>CapsE</cell><cell>0.150</cell><cell>403</cell><cell>-</cell><cell>-</cell><cell>0.356</cell></row><row><cell>KBAT</cell><cell>0.157</cell><cell>270</cell><cell>-</cell><cell>-</cell><cell>0.331</cell></row><row><cell>MLMLM</cell><cell>0.2591 ± 0.0017</cell><cell>411.23 ± 0.0014</cell><cell>0.1871 ± 0.0028</cell><cell>0.2820 ± 0.0017</cell><cell>0.4026 ± 2.9313</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">: WN18RR Unseen Entities Result</cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell cols="2">MRR ↑ MR ↓</cell><cell>MP@1 ↑</cell><cell>MP@3 ↑</cell><cell>MP@10 ↑</cell></row><row><cell>Random baseline</cell><cell>0.0003 ± 0.00007</cell><cell>20541.91 ± 87.88</cell><cell>0.00002 ± 0.00004</cell><cell>0.00002 ± 0.00004</cell><cell>0.00026 ± 0.00008</cell></row><row><cell>Non-finetuned RoBERTa</cell><cell>0.0273 ± 0.0005</cell><cell>10130.35 ± 187.61</cell><cell>0.0154 ± 0.0007</cell><cell>0.0295 ± 0.0011</cell><cell>0.0492 ± 0.0019</cell></row><row><cell>MLMLM</cell><cell>0.1842 ± 0.0266</cell><cell>3761.50 ± 255.4437</cell><cell>0.1416 ± 0.0081</cell><cell>0.2175 ± 0.0119</cell><cell>0.2939 ± 0.0088</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">: FB15k-237 Unseen Entities Result</cell><cell></cell><cell></cell></row><row><cell>Approach</cell><cell cols="2">MRR ↑ MR ↓</cell><cell>MP@1 ↑</cell><cell>MP@3 ↑</cell><cell>MP@10 ↑</cell></row><row><cell>Random baseline</cell><cell>0.0007 ± 0.00011</cell><cell>7065.95 ± 12.29</cell><cell>0.00006 ± 0.00012</cell><cell>0.00026 ± 0.00008</cell><cell>0.00074 ± 0.00013</cell></row><row><cell>Non-finetuned RoBERTa</cell><cell>0.0115 ± 0.0028</cell><cell>4870.56 ± 437.03</cell><cell>0.0060 ± 0.0013</cell><cell>0.0101 ± 0.0016</cell><cell>0.0190 ± 0.0069</cell></row><row><cell>MLMLM</cell><cell>0.0694 ± 0.01823</cell><cell>2057.61 ± 293.94</cell><cell>0.0258 ± 0.0019</cell><cell>0.0768 ± 0.0400</cell><cell>0.1499 ± 0.0410</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Because the length of non-padded tokens is variable, using the mean of the logits is the correct comparison metric for re-ranking.2 By far, the token the model sees most is the padding token. Counting it would most likely yield a heavy skew towards shorter entities with more padding.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tucker: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09590</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1145/1376616.1376746</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD 08</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD 08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">12471250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">WordNet: an electronic lexical database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for selfsupervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Zhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno>abs/1909.11942</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01195</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02121</idno>
		<title level="m">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10197</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03903</idno>
		<title level="m">Soumya Sanyal, Partha Talukdar, and Yiming Yang. 2020. A Reevaluation of Knowledge Graph Completion Methods. arXiv e-prints, accepted at ACL 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Illia Polosukhin. 2017. Attention is all you need. CoRR, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A capsule network-based embedding model for knowledge graph completion and search personalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Tu Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2180" to="2189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R&amp;apos;emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kgbert: Bert for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno>abs/1909.03193</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
