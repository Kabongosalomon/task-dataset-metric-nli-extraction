<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Estimation of 3D Human Poses from a Single Image by Robust Estimation of 3D Human Poses from a Single Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20149-06-10">June 10, 2014 9 Jun 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Nat&apos;l Engineering Lab for Video Technology, Sch&apos;l of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
							<email>zlin@pku.edu.cn@yuille@stat.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Nat&apos;l Engineering Lab for Video Technology, Sch&apos;l of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Nat&apos;l Engineering Lab for Video Technology, Sch&apos;l of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Lab. of Machine Perception (MOE), Sch&apos;l of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<email>wgao@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Nat&apos;l Engineering Lab for Video Technology, Sch&apos;l of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Lab. of Machine Perception (MOE), Sch&apos;l of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Nat&apos;l Engineering Lab for Video Technology, Sch&apos;l of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Lab. of Machine Perception (MOE), Sch&apos;l of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Nat&apos;l Engineering Lab for Video Technology, Sch&apos;l of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Lab. of Machine Perception (MOE), Sch&apos;l of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>UCLA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Nat&apos;l Engineering Lab for Video Technology, Sch&apos;l of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Estimation of 3D Human Poses from a Single Image by Robust Estimation of 3D Human Poses from a Single Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="20149-06-10">June 10, 2014 9 Jun 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human pose estimation is a key step to action recognition. We propose a method of estimating 3D human poses from a single image, which works in conjunction with an existing 2D pose/joint detector. 3D pose estimation is challenging because multiple 3D poses may correspond to the same 2D pose after projection due to the lack of depth information. Moreover, current 2D pose estimators are usually inaccurate which may cause errors in the 3D estimation. We address the challenges in three ways: (i) We represent a 3D pose as a linear combination of a sparse set of bases learned from 3D human skeletons. (ii) We enforce limb length constraints to eliminate anthropomorphically implausible skeletons. (iii) We estimate a 3D pose by minimizing the L 1 -norm error between the projection of the 3D pose and the corresponding 2D detection. The L 1 -norm loss term is robust to inaccurate 2D joint estimations. We use the alternating direction method (ADM) to solve the optimization problem efficiently. Our approach outperforms the state-of-the-arts on three benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Human pose estimation is a key step to action recognition. We propose a method of estimating 3D human poses from a single image, which works in conjunction with an existing 2D pose/joint detector. 3D pose estimation is challenging because multiple 3D poses may correspond to the same 2D pose after projection due to the lack of depth information. Moreover, current 2D pose estimators are usually inaccurate which may cause errors in the 3D estimation. We address the challenges in three ways: (i) We represent a 3D pose as a linear combination of a sparse set of bases learned from 3D human skeletons. (ii) We enforce limb length constraints to eliminate anthropomorphically implausible skeletons. (iii) We estimate a 3D pose by minimizing the L 1 -norm error between the projection of the 3D pose and the corresponding 2D detection. The L 1 -norm loss term is robust to inaccurate 2D joint estimations. We use the alternating direction method (ADM) to solve the optimization problem efficiently. Our approach outperforms the state-of-the-arts on three benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition is a key problem in computer vision <ref type="bibr" target="#b18">[19]</ref> and has many applications such as human-computer interaction and video surveillance. Since an action is naturally represented by human poses <ref type="bibr" target="#b17">[18]</ref>, 2D and 3D pose estimation has attracted a lot of attention. A 2D pose is usually represented by a set of joint locations <ref type="bibr" target="#b20">[21]</ref> whose estimation remains challenging because of the huge human appearance variation, viewpoint change, etc.</p><p>A 3D pose is typically represented by a skeleton model parameterized by joint locations <ref type="bibr" target="#b15">[16]</ref> or by rotation angles <ref type="bibr" target="#b7">[8]</ref>. The representation is intrinsic as it is invariant to viewpoint changes. However, estimating 3D poses from a single image remains a difficult problem. First, a 3D pose is usu- ally inferred from 2D joint locations. So, the accuracy of 2D joint estimation can greatly affect the 3D estimation performance. Second, multiple 3D poses may correspond to the same 2D pose after projection. This introduces severe ambiguities in 3D pose estimation. Third, the problem is further complicated when camera parameters are unknown. We propose a novel method, which alternately updates the 3D pose and camera parameters. <ref type="figure" target="#fig_0">Figure 1</ref> shows the overview of the method. On an input image, we first employ a 2D pose estimator (e.g. <ref type="bibr" target="#b20">[21]</ref>) to detect the 2D joints. Then we initialize a 3D pose (e.g. the mean pose). Using both the poses, we estimate the camera parameters (step 2). Next, we re-estimate the 3D pose with the current camera parameters (step 3). Step 2 and 3 are iterated until convergence.</p><p>We represent a 3D human pose by a linear combination of a set of overcomplete bases. Since human poses lie in a low dimensional space <ref type="bibr" target="#b2">[3]</ref>, in the basis pursuit optimization, we enforce an L 1 -norm regularization on the basis coefficients so that only a few of them are activated. Such holistic representation is able to reduce the ambiguities in the 3D pose estimation and is robust to occlusions (e.g. missing joints), because it encodes the structural prior of the human skeleton manifold.</p><p>We estimate a 3D pose (i.e. basis coefficients) by minimizing an L 1 -norm penalty between the projection of the 3D joints and the 2D detections. The commonly used L 2norm tends to distribute errors evenly over all variables. When some joints of the estimated 2D pose are inaccurate, the inferred 3D pose may be biased to a completely wrong configuration. In contrast, L 1 -norm is more tolerant to the inaccurate 2D joints. However, even if the L 1 -norm error is adopted, the inferred 3D skeleton may still violate the anthropometric quantities such as limb proportions. Hence, we enforce eight limb length constraints on the estimated 3D pose to eliminate the incorrect ones.</p><p>We use an efficient alternating direction method (ADM) to solve our optimization problem. Although global optimality is not guaranteed, we obtain reasonably good solutions. Our method outperforms the state-of-the-arts on three benchmark datasets.</p><p>The paper is organized as follows. Section 2 reviews the related work. Section 3 introduces the proposed approach. Section 4 shows implementation details and experiment results. Conclusion is in section 5. Section 6 (Appendix) presents the optimization method in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Existing work on 3D pose estimation can be classified into four categories according to their inputs to the system, e.g. the image, image features, camera parameters, etc. The first class <ref type="bibr" target="#b6">[7]</ref> [15] takes camera parameters as inputs. For example, Lee et al. <ref type="bibr" target="#b6">[7]</ref> represent a 3D pose by a skeleton model and parameterize the body parts by truncated cones. They estimate the rotation angles of body parts by minimizing the silhouette discrepancy between the model projections and the input image by applying Markov Chain Monte Carlo (MCMC). Simo-Serra et al. <ref type="bibr" target="#b14">[15]</ref> represent a 3D pose by a set of joint locations. They automatically estimate the 2D pose, model each joint by a Gaussian distribution, and propagate the uncertainty to 3D pose space. They sample a set of 3D skeletons from the space and learn a SVM to determine the most feasible one.</p><p>The second class <ref type="bibr" target="#b16">[17]</ref> [20] requires manually labeled 2D joint locations in a video as input. Valmadre et al. <ref type="bibr" target="#b16">[17]</ref> first apply structure from motion to estimate the camera parameters and the 3D pose of the rigid torso, and then require human input to resolve the depth ambiguities for non-torso joints. Wei et al. <ref type="bibr" target="#b19">[20]</ref> propose the "rigid body constraints", e.g. the pelvis, left and right hip joints form a rigid structure, and require that the distance between any two joints on the rigid structure remains unchanged across time. They estimate the 3D poses by minimizing the discrepancy between the projection of the 3D poses and the 2D joint detections without violating the "rigid body constraints".</p><p>The third class <ref type="bibr" target="#b15">[16]</ref> [12] requires manually labeled 2D joints in one image. Taylor <ref type="bibr" target="#b15">[16]</ref> assumes that the limb lengths are known and calculates the relative depths of the limbs by considering foreshortening. It requires human input to resolve the depth ambiguities at each joint. Ramakrishna et al. <ref type="bibr" target="#b11">[12]</ref> represent a 3D pose by a linear combination of a set of bases. They split the training data into classes, apply PCA to each class, and combine the principal components as bases. They greedily add the most correlated basis into the model and estimate the coefficients by minimizing an L 2 -norm error between the projection of 3D pose and the 2D pose. They enforce a constraint on the sum of the limb lengths, which is just a weak constraint. This work <ref type="bibr" target="#b11">[12]</ref> achieves the state-of-the-art performance but relies on manually labeled 2D joint locations.</p><p>The fourth class <ref type="bibr" target="#b10">[11]</ref> [3] requires only a single image or image features (e.g. silhouettes). For example, Mori et al. <ref type="bibr" target="#b10">[11]</ref> match a test image to the stored exemplars using shape context descriptors, and transfer the matched 2D pose to the test image. They lift the 2D pose to 3D using the method proposed in <ref type="bibr" target="#b15">[16]</ref>. Elgammal et al. <ref type="bibr" target="#b2">[3]</ref> propose to learn viewbased silhouettes manifolds and the mapping function from the manifold to 3D poses. These approaches do not explicitly estimate camera parameters, but require a lot of training data from various viewpoints.</p><p>Our method requires only a single image to infer 3D human poses. It is similar to <ref type="bibr" target="#b11">[12]</ref> but there are five distinctive differences. (i) We obtain 2D joint locations by running a detector <ref type="bibr" target="#b20">[21]</ref> rather than by manual labeling. (ii) We use L 1 -norm penalty instead of the L 2 -norm one as it is more robust to inaccurate 2D joint locations. (iii) They <ref type="bibr" target="#b11">[12]</ref> enforce a weak anthropomorphic constraint (i.e. sum of limb length) for the sake of computational simplicity, which is insufficient to eliminate incorrect poses; while we enforce eight limb length constraints, which is much more effective. (iv) We enforce an L 1 -norm constraint on the basis coefficients rather than greedily adding bases into the model to encourage sparsity. They need to re-estimate the coefficients every time a new basis is introduced, which is inefficient. (v) We use an efficient alternating direction method to solve our optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Robust 3D Pose Estimation</head><p>We represent a 3D pose y as a linear combination of a set of bases</p><formula xml:id="formula_0">B = {b 1 , · · · , b k }, i.e. y = k i=1 α i · b i + µ,</formula><p>where α are the basis coefficients and µ is the mean pose. Given a 2D pose x and camera parameter M 0 , we estimate the coefficients α by minimizing an L 1 -norm error between the projection of the estimated 3D pose and the 2D pose: M (Bα + µ) − x 1 . We also enforce L 1 -norm regularization on the basis coefficients α and eight limb length constraints on the inferred 3D pose. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">L 1 -norm Objective Function</head><p>L 2 -norm is the most widely used error metric in the literature. However, it is sensitive to inaccuracies in 2D pose estimation, which are usually caused by failures in feature detections and other factors, because it tends to distribute errors uniformly. In this work, we propose to minimize an L 1 -norm error, i.e. x − M (Bα + µ) <ref type="bibr" target="#b0">1</ref> . As a widely used robust regularizer in statistics, the L 1 penalty is robust to inaccurate 2D joint outliers. For example, in <ref type="figure" target="#fig_1">Figure 2</ref> the 2D location of the right foot is inaccurate. The estimated 3D pose using L 2 -norm error is drastically biased to a wrong configuration. The camera parameter estimation is also incorrect. However, using L 1 -norm returns a reasonable 3D pose. Extensive experiments on benchmark datasets justify that using the L 1 -norm can improve the performance, especially when 2D pose estimation is inaccurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Sparsity Constraint on the Basis Coefficients</head><p>Although human poses are highly variant, they lie in a low dimensional space <ref type="bibr" target="#b12">[13]</ref>. Hence, we enforce sparsity on the basis coefficients α so that each 3D pose is represented by only a few bases. The sparsity can be induced by minimizing the L 1 -norm of α. This is an important structural prior to remove incorrect or anthropomorphically implausible 3D poses. In addition, the sparsity constraint can also prevent overfitting to (inaccurate) 2D pose estimations. If there is no sparsity constraint, given a large number of bases we can always decrease the projection error to zero for an arbitrary 2D pose; however, there is no guarantee that the resulted 3D pose is correct. In experiments, we observe that the sparsity constraint is quite important. In summary, the resulted objective function is:</p><formula xml:id="formula_1">min α x − M (Bα + µ) 1 + θ α 1<label>(1)</label></formula><p>where θ &gt; 0 is a parameter that balances the loss term and the regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Anthropomorphic Constraints</head><p>We require that the eight limb lengths of a 3D pose comply with certain proportions <ref type="bibr">[6]</ref>. The eight limbs are left/rightupper/lower-arm/leg. We define a joint selection matrix E j = [0, · · · , I, · · · , 0] ∈ R 3×3n , where the j th block is the identity matrix. The product of E j and y is the 3D location of the j th joint in pose y.</p><formula xml:id="formula_2">Let C i = E i1 − E i2 . Then C i y 2 2</formula><p>is the squared length of the i th limb whose ends are the i 1 -th and i 2 -th joints.</p><p>We normalize the length of the right lower leg to one and compute the squared lengths of other limbs (say L i ) according to the limb proportions used in <ref type="bibr">[6]</ref>. The proportions are kept the same for all people. Now we have constraints</p><formula xml:id="formula_3">C i (Bα + µ) 2 2 = L i .</formula><p>Given the camera parameters we can formulate the 3D pose estimation problem as follows:</p><formula xml:id="formula_4">min α x − M (Bα + µ) 1 + θ α 1 s.t. C i (Bα + µ) 2 2 = L i , i = 1, · · · , t<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Robust Camera Parameter Estimation</head><p>Given a 3D pose, we estimate the camera parameters by minimizing the L 1 -norm projection error. We reshape the 2D and 3D poses, x and y, as X ∈ R 2×n and Y ∈ R 3×n , respectively. Then ideally</p><formula xml:id="formula_5">X = M 0 Y should hold, where M 0 = m T 1 m T 2</formula><p>is the projection matrix of a weak projective camera, i.e. m T 1 m 2 = 0. Due to errors, we estimate the camera parameters m 1 and m 2 by solving the following problem:</p><formula xml:id="formula_6">min m1,m2 X − m T 1 m T 2 Y 1 , s.t. m T 1 m 2 = 0. (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>We alternately update the 3D pose and the camera parameters. We first initialize the 3D pose X by the mean pose of the training data, and estimate camera parameters m 1 and m 2 by solving problem <ref type="bibr" target="#b2">(3)</ref>. With the updated camera parameters, we then re-estimate the 3D pose by solving problem <ref type="bibr" target="#b1">(2)</ref>. We repeat the above process until convergence or the maximum number of iterations is reached. We use the alternating direction method to solve the two optimization problems efficiently. Please see Appendix for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Experimental Results</head><p>We conduct two types of experiments to evaluate our approach. The first type is controlled. We assume that the 2D joint locations are known and evaluate the influence: (i) of the three factors (i.e. the sparsity term, the anthropomorphic constraints and the L 1 -norm penalty), (ii) of the inaccurate 2D pose estimations and (iii) of the human-camera angles, on the 3D pose estimation performance. The second type is real. We estimate the 2D pose in an image by a detector <ref type="bibr" target="#b20">[21]</ref> and then estimate the 3D skeletons. We compare our method with the state-of-the-art ones <ref type="bibr" target="#b11">[12]</ref> [15] <ref type="bibr" target="#b1">[2]</ref>. Our approach can also refine the 2D pose estimation by projecting the estimated 3D pose to 2D image.</p><p>We use 12 body joints, i.e. the left and right shoulders, elbows, hands, hips, knees and feet, being consistent with the 2D pose detector <ref type="bibr" target="#b20">[21]</ref>. 200 bases are used for all experiments and about 6 of them are activated for representing a 3D pose. In optimization, we terminate the algorithm if the number of iterations exceeds 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Datasets</head><p>We evaluate our approach on three datasets: the CMU motion dataset <ref type="bibr" target="#b0">[1]</ref>, the HumanEva dataset <ref type="bibr" target="#b13">[14]</ref> and the UvA 3D pose dataset <ref type="bibr" target="#b4">[5]</ref>. For the CMU dataset, we learn the bases on actions of "climb", "swing", "sit" and "jump", and test on different actions of "walk", "run", "golf" and "punch" to justify the generalization ability of our method. For the HumanEva dataset, we use the walking and jogging actions of three subjects for evaluation as in <ref type="bibr" target="#b14">[15]</ref>. For the UvA dataset, we use the first four sequences for training and the remaining eight for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Basis Learning</head><p>Our approach pursues a set of sparse bases by enforcing an L 1 -norm regularization on the basis coefficients (as in <ref type="bibr" target="#b9">[10]</ref>). But we also compare with other two basis learn- ing methods. The first method applies principle component analysis (PCA) on the training set and uses the first k principal components as the bases. The second splits the training set into different classes by action labels, then applies PCA on each class, and finally collect the principal components as bases (which we call classwise PCA) as in <ref type="bibr" target="#b11">[12]</ref>. We learn the bases on the training data (of four action classes) of the CMU dataset, and reconstruct each test 3D pose by solving an L 1 -norm regularized least square problem. The reconstruction errors are shown in <ref type="figure" target="#fig_2">Figure 3</ref> (a). The sparse bases consistently achieve the lowest errors among the three methods. Note that the maximum number of bases for PCA and classwise PCA is 36 (i.e. the dimension of a 3D pose) and 144, respectively. In addition, fewer bases are activated using the L 1 -norm induced bases (see <ref type="figure" target="#fig_2">Figure 3</ref> (b)). This justifies the bases' representative power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Controlled Experiments</head><p>We assume the ground-truth 2D pose x is known and recover the 3D pose y from x. The residual error between the estimated 3D poseŷ and the ground truth y, i.e. ||y − y|| 2 , is used as the evaluation criterion as in <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Influence of the Three Factors</head><p>We design seven baselines to evaluate the influence of the three factors, i.e. the sparsity term, the anthropomorphic constraints and the L 1 -norm penalty. The first baseline is symbolized as L2NAWS, i.e. the approach uses an L 2norm objective function, No Anthropomorphic constraints and With the Sparsity constraint. The remaining baselines are L2NANS, L2WANS, L2WAWS, L1NANS, L1NAWS and L1WANS, which can be similarly understood by their names. We solve the optimization problem in L2WANS and L2WAWS by the alternating direction method. The optimization problems in other baselines can be solved trivially. <ref type="figure" target="#fig_3">Figure 4</ref> shows the results on the CMU dataset. First, the baselines without the sparsity term perform worse than those with the sparsity term. Second, enforcing limb length <ref type="table">Table 1</ref>. Real experiment on the HumanEva dataset: comparison with the state-of-the-art methods <ref type="bibr" target="#b14">[15]</ref>  <ref type="bibr" target="#b1">[2]</ref>. We present results for both walking and jogging actions of all three subjects and camera C1. The numbers in each cell are the root mean square error (RMS) and standard deviation, respectively. We use the unit of millimeter as in <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b1">[2]</ref>. constraints improves the performance (e.g. L2WAWS outperforms L2NAWS). Third, L 1 -norm outperforms L 2 -norm (e.g. L1NAWS is better than L2NAWS). Finally, our approach performs best among the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Influence of Inaccurate 2D Poses</head><p>We evaluate the robustness of our approach against inaccurate 2D pose estimations. We synthesize noisy 2D poses by generating ten levels of random Gaussian noises and adding them to the original 2D poses. The magnitude of the tenth (largest) level noise is one, which is equal to the normalized length of the right lower leg. We estimate the 3D poses from those corrupted 2D joints. <ref type="figure" target="#fig_4">Figure 5</ref> shows the results. First, L1NANS outperforms L2NANS, which demonstrates that L 1 -norm is more robust to 2D pose errors. Second, L2NANS and L2WANS get larger errors than L2NAWS and L2WAWS, respectively, which shows the importance of sparsity in handling inaccurate 2D poses. Our approach achieves a better performance than all baselines and Ramakrishna et al's method <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Influence of Human-Camera Angles</head><p>We explore the influence of human-camera angles on 3D pose estimation. We first transform the 3D poses into a local coordinate system, where the x-axis is defined by the line passing the two hips, the y-axis is defined by the line of spine and the z-axis is the cross product of the x-axis and y-axis. Then we rotate the 3D poses around y-axis by a particular angle, ranging from 0 to 180, and project them to 2D by a weak perspective camera. We estimate the 3D poses from their 2D projections. <ref type="figure" target="#fig_5">Figure 6</ref> shows that the estimation errors using <ref type="bibr" target="#b11">[12]</ref> increase drastically as human moves from profile (90 degrees) towards frontal pose (0 degree). This may be due to the fact that frontal view has more severe foreshortenings than the profile view, hence introduces more ambiguities into 3D pose estimation. Our approach is  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Real Experiments</head><p>Given an image, we first detect the 2D joint locations by a detector <ref type="bibr" target="#b20">[21]</ref>, from which we estimate the corresponding 3D pose using the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Comparisons to the State-of-the-arts</head><p>We compare our 3D pose estimator against a state-of-the-art method <ref type="bibr" target="#b11">[12]</ref> on the UvA dataset. <ref type="figure" target="#fig_6">Figure 7</ref> shows the estimation errors on each joint. Our approach achieves smaller estimation errors on all joints, especially for the left and the right hands. This proves that our approach is robust to inaccurate 2D joint locations. We also compare our approach with the state-of-the-arts <ref type="bibr" target="#b14">[15]</ref> [2] on the HumanEva dataset. <ref type="table">Table 1</ref> shows the root mean square errors adopted in <ref type="bibr" target="#b14">[15]</ref>. Our approach outperforms both <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Evaluation on Camera Parameter Estimation</head><p>Our camera parameter estimation usually converges within nine iterations. <ref type="figure" target="#fig_8">Figure 8</ref> shows the 3D pose estimation results using the estimated cameras and groundtruth cameras, respectively. We can see that the difference is subtle for 70% of cases. We discover that the initialization of the 3D pose can influence the estimation accuracy. So we cluster the training poses into 30 clusters and initialize the 3D pose <ref type="table">Table 2</ref>. Real experiment on the UvA dataset: Comparison of 2D pose estimation results. We report: (1) the Probability of Correct Pose (PCP) for the eight body parts (i.e. left upper arm (LUA), left lower arm (LLA), right upper arm (RUA), right lower arm (RLA), left upper leg (LUL), left lower leg (LLL), right upper leg (RUL) and right lower leg (RLL)), (2) PCP for the whole pose, (3) and the Euclidean distance between the estimated 2D pose and the groundtruth in pixels.  with each of the cluster centers for parallel optimization. We keep the one with the smallest error. We see that the performance can be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PCP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Evaluation on 2D Pose Estimation</head><p>We project the estimated 3D pose to 2D and compare with the original 2D estimation <ref type="bibr" target="#b20">[21]</ref>. We report the results using two criteria. The first is the probability of correct pose (PCP) <ref type="bibr" target="#b20">[21]</ref> -an estimated body part is considered correct if its segment endpoints lie within 50% of the length of the ground-truth segment from their annotated location. The second criterion is the Euclidean distance between the estimated 2D pose and the groundtruth in pixels as in <ref type="bibr" target="#b14">[15]</ref>. <ref type="table">Table 2</ref> shows that our approach performs the best on six body parts. In particular, we improve over the original 2D pose estimators by about 0.03 (0.741 vs. 0.714) using the first PCP criteria. Our approach also performs the best using the second criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We address the problem of estimating 3D human poses from a single image. The approach is used in conjunction with an existing 2D pose detector. It is robust to inaccurate 2D pose estimations by using a sparse basis representation, anthropometric constraints and an L 1 -norm projection error metric. We use an efficient alternating direction method to  solve the optimization problem. Our approach outperforms the state-of-the art ones on three benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">3D Pose Estimation</head><p>We introduce two auxiliary variables β and γ and rewrite Eq. (2) as: min α,β,γ</p><formula xml:id="formula_7">γ 1 + θ β 1 s.t. γ = x − M (Bα + µ) , α = β, C i (Bα + µ) 2 2 = L i , i = 1, · · · , m.<label>(4)</label></formula><p>The augmented Lagrangian function of Eq. (4) is:</p><formula xml:id="formula_8">L 1 (α, β, γ, λ 1 , λ 2 , η) = ||γ|| 1 + θ||β|| 1 + λ T 1 [γ − x + M (Bα + µ)] + λ T 2 (α − β)+ η 2 ||γ − x + M (Bα + µ)|| 2 + ||α − β|| 2</formula><p>where λ 1 and λ 2 are the Lagrange multipliers and η &gt; 0 is the penalty parameter. ADM is to update the variables by minimizing the augmented Lagrangian function w.r.t. the variables alternately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Update γ</head><p>We discard the terms in L 1 which are independent of γ and update γ by:</p><formula xml:id="formula_9">γ k+1 = argmin γ γ 1 + η k 2 γ − x − M (Bα k + µ) − λ k 1 η k 2</formula><p>which has a closed form solution <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Update β</head><p>We drop the terms in L 1 which are independent of β and update β by:</p><formula xml:id="formula_10">β k+1 = argmin β β 1 + η k 2θ β − λ k 2 η k + α k 2</formula><p>which also has a closed form solution <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Update α</head><p>We dismiss the terms in L 1 which are independent of α and update α by:</p><formula xml:id="formula_11">α k+1 = arg min α z T W z s.t. z T Ω i z = 0, i = 1, · · · , m<label>(5)</label></formula><p>where </p><formula xml:id="formula_12">z = [α T 1] T , W=   B T M T M B + I 0 2 γ k+1 − x + M µ + λ k 1 η k T M B − β k+1 + λ k 2 η k 0   and Ω i = B T C T i C i B B T C T i C i µ µ T C T i C i B µ T C T i C i µ − L i . Let Q = zz T .</formula><p>We still solve problem (6) by the alternating direction method <ref type="bibr" target="#b8">[9]</ref>. We introduce an auxiliary variable P and rewrite the problem as: min Q,P tr(W Q) s.t. tr(Ω i Q) = 0, i = 1, · · · , m, P = Q, rank(P ) ≤ 1, P 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(7)</head><p>Its augmented Lagrangian function is:</p><formula xml:id="formula_14">L 2 (Q, P, G, δ) = tr(W Q)+tr(G T (Q−P ))+ δ 2 Q − P 2 F</formula><p>where G is the Lagrange Multiplier and δ &gt; 0 is the penalty parameter. We update Q and P alternately.</p><p>• Update Q:</p><p>Q l+1 = argmin tr(Ω i Q) = 0, i = 1, · · · , m L 2 (Q, P l , G l , δ l ).</p><p>This is convex and solved using CVX <ref type="bibr" target="#b3">[4]</ref>, a package for specifying and solving convex programs.</p><p>• Update P : We discard the terms in L 2 which are independent of P and update P by: . Then (9) has a closed form solution by the following lemma. </p><formula xml:id="formula_16">P l+1 = argmin P 0, rank(P ) ≤ 1 P −Q</formula><p>is P = max(ζ 1 , 0)ν 1 ν T 1 , where S is a symmetric matrix and ζ 1 and ν 1 are the largest eigenvalue and eigenvector of S, respectively.</p><p>Proof Since P is a symmetric semi-positive definite matrix and its rank is one, we can write P as: P = ζνν T , where ζ ≥ 0. Let the largest eigenvalue of S be ζ 1 , then we have ν T Sν ≤ ζ 1 , ∀ν. Then we have:</p><formula xml:id="formula_18">||P − S|| 2 F = ||P || 2 F + ||S|| 2 F − 2tr(P T S) ≥ ζ 2 + n i=1 ζ 2 i − 2ζζ 1 = (ζ − ζ 1 ) 2 + n i=2 ζ 2 i ≥ n i=2 ζ 2 i + min(ζ 1 , 0) 2<label>(11)</label></formula><p>The minimum value can be achieved when ζ = max(ζ 1 , 0) and ν = ν 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Camera Parameter Estimation</head><p>We introduce an auxiliary variable R and rewrite Eq. <ref type="table">(3):</ref> min R,m1,m2</p><formula xml:id="formula_19">R 1 s.t. R = X − m T 1 m T 2 Y, m T 1 m 2 = 0.<label>(12)</label></formula><p>We still use ADM to solve problem <ref type="bibr" target="#b11">(12)</ref>. Its augmented Lagrangian function is:</p><formula xml:id="formula_20">L 3 (R, m 1 , m 2 , H, ζ, τ ) = R 1 + tr H T m T 1 m T 2 Y + R − X + ζ(m T 1 m 2 ) + τ 2 m T 1 m T 2 Y + R − X 2 F + m T 1 m 2 2</formula><p>where H and ζ are Lagrange multipliers and τ &gt; 0 is the penalty parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Update R</head><p>We discard the terms in L 3 which are independent of R and update R by:</p><formula xml:id="formula_21">R k+1 = argmin R R 1 + τ k 2 R + m k 1 T m k 2 T Y − X + H k τ k 2 F</formula><p>which has a closed form solution <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Update m 1</head><p>We discard the terms in L 3 which are independent of m 1 and update m 1 by:</p><formula xml:id="formula_22">m k+1 1 = argmin m1 m T 1 m k 2 T Y + R k+1 − X + H k τ k 2 F + m T 1 m k 2 + ζ k τ k 2</formula><p>This has a closed form solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Update m 2</head><p>We discard the terms in L 3 which are independent of m 2 and update m 2 by:</p><formula xml:id="formula_23">m k+1 2 = argmin m2 m k+1 1 T m T 2 Y + R k+1 − X + H k τ k 2 F + m k+1 1 T m 2 + ζ k τ k 2</formula><p>This has a closed form solution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Method overview. (1) On a test image, we first estimate the 2D joint locations and initialize a 3D pose. (2) Then camera parameters are estimated from the 2D and 3D poses. (3) Next we update the 3D pose with the current camera parameters and the 2D pose. We repeat steps (2) and (3) until convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Comparison of 3D pose estimation by minimizing L1norm vs L2-norm penalty. (a) estimated 2D joint locations where the right foot location is inaccurate. (b-c) are the estimated 3D poses using the L1-norm and L2-norm, respectively. The L2-norm penalty biases the estimation to a wrong pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of the three basis learning methods on the CMU dataset. (a) 3D pose reconstruction errors using different number of bases. (b) Cumulative distribution of the number of activated bases in represent the 3D poses. The y-axis is the percentage of the cases whose activated basis number is less than or equal to the corresponding x-axis value on the curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Controlled experiment: Cumulative distribution of 3D pose estimation errors on the CMU dataset. The y-axis is the percentage of the cases whose estimation error is less than or equal to the corresponding x-axis value on the curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Controlled experiment on the CMU dataset: 3D pose estimation errors when different levels of noises are added to 2D poses. See Section 4.3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Controlled experiment on the CMU dataset: 3D pose estimation error when the human-camera angle varies from 0 to 180. See Section 4.3.3. more robust against viewpoint changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Real experiment on the UvA dataset: comparison with a state-of-the-art [12]. Both average estimation errors and standard deviations are shown for each joint (i.e. left shoulder, left elbow, left hand, right shoulder, right elbow, right hand, left hip, left knee, left foot, right hip, right knee and right foot). See Section 4.4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Real experiment on the CMU dataset: cumulative distribution of 3D pose estimation errors when camera parameters are (1) assigned by groundtruth, estimated by initializing the 3D pose with (2) mean pose, or (3) 30 cluster centers. The y-axis is the percentage of the cases whose estimation error is less than or equal to the corresponding x-axis value on the curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Then the objective function becomes z T W z = tr(W Q) and Eq. (5) is transformed to: min Q tr(W Q) s.t. tr(Ω i Q) = 0, i = 1, · · · , m, Q 0, rank(Q) ≤ 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 F ( 9 ) 2 F</head><label>292</label><figDesc>whereQ = Q l+1 + 2 δ l G l . Note that P −Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Lemma 6 . 1 ||P − S|| 2 F</head><label>612</label><figDesc>The solution to min P s.t. P 0, rank(P ) ≤ 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The length of the right lower leg is about 380 mm. See Section 4.4.1.</figDesc><table><row><cell>Walking</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell></row><row><cell>Ours</cell><cell>71.9 (19.0)</cell><cell>75.7 (15.9)</cell><cell>85.3 (10.3)</cell></row><row><cell>[15]</cell><cell cols="3">99.6 (42.6) 108.3 (42.3) 127.4 (24.0)</cell></row><row><cell>[2]</cell><cell>89.3</cell><cell>108.7</cell><cell>113.5</cell></row><row><cell>Jogging</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell></row><row><cell>Ours</cell><cell>62.6 (10.2)</cell><cell>77.7 (12.1)</cell><cell>54.4 (9.0)</cell></row><row><cell>[15]</cell><cell cols="3">109.2 (41.5) 93.1 (41.1) 115.8 (40.6)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Our ApproachWe represent 2D and 3D poses by n joint locations x ∈ R 2n and y ∈ R 3n , respectively. By assuming a weak perspective camera model, the 2D projectionx of a 3D pose y in an image are related as: x = M y, where M = I n ⊗M 0 , in which I is the identity matrix, ⊗ is the Kronecker product, and M 0 = m T 1 m T 2∈ R 2×3 is the camera projection matrix. Given the estimated x, we alternately estimate the camera parameter M 0 and the 3D pose y. We describe the details for 3D pose estimation in section 3.1 and for camera parameter estimation in section 3.2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements:We'd like to thank for the support from the following research grants NSFC-61272027, NSFC-61231010, NSFC-61121002, NSFC-61210005 and USA ARO Proposal 62250-CS. And, this material is based upon work supported by the Center for Minds, Brains and Machines (CBMM), funded by NSF STC award CCF-1231216. Z. Lin is supported by NSF China (grant nos. 61272341, 61231002, 61121002) and MSRA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">CMU human motion capture database</title>
		<ptr target="http://mocap.cs.cmu.edu/search.html" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tracking 3D human pose with large root node uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Daubney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1321" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inferring 3d body pose from silhouettes using activity manifold learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">681</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cvx: Matlab software for disciplined convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d human pose estimation combining single-frame recovery, temporal integration and model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2214" to="2221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Determination of 3d human body postures from a single view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="168" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Proposal maps driven mcmc for estimating human body pose in static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">334</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Human pose tracking in monocular sequence using multilevel structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online dictionary learning for sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Recovering 3d human body configurations using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1052" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synthesizing physically realistic human motion in low-dimensional, behavior-specific spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Safonova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="514" to="521" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Humaneva: Synchronized video and motion capture dataset for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">120</biblScope>
		</imprint>
		<respStmt>
			<orgName>Brown Univertsity TR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single Image 3D Human Pose Estimation from Noisy Observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alenyà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reconstruction of articulated objects from point correspondences in a single uncalibrated image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="677" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deterministic 3d human pose estimation using rigid structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="467" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining layered grammar rules for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="162" to="182" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling 3d human poses from uncalibrated monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1873" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Appendix: Optimization by ADM Due to space limit, we only sketch the major steps of ADM for our optimization problems</title>
		<imprint/>
	</monogr>
	<note>In the following, k and l are the number of iterations</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
