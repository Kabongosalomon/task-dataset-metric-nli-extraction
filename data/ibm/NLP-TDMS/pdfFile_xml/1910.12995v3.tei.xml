<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A SIMPLE BUT EFFECTIVE BERT MODEL FOR DIALOG STATE TRACKING ON RESOURCE-LIMITED SYSTEMS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Manh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IN † Adobe Research</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette, San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai</forename><forename type="middle">†</forename><surname>Quan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IN † Adobe Research</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette, San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IN † Adobe Research</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette, San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Bui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IN † Adobe Research</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette, San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kihara</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IN † Adobe Research</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette, San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A SIMPLE BUT EFFECTIVE BERT MODEL FOR DIALOG STATE TRACKING ON RESOURCE-LIMITED SYSTEMS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Task-Oriented Dialog Systems</term>
					<term>Dialog State Tracking</term>
					<term>BERT</term>
					<term>Knowledge Distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In a task-oriented dialog system, the goal of dialog state tracking (DST) is to monitor the state of the conversation from the dialog history. Recently, many deep learning based methods have been proposed for the task. Despite their impressive performance, current neural architectures for DST are typically heavily-engineered and conceptually complex, making it difficult to implement, debug, and maintain them in a production setting. In this work, we propose a simple but effective DST model based on BERT. In addition to its simplicity, our approach also has a number of other advantages: (a) the number of parameters does not grow with the ontology size (b) the model can operate in situations where the domain ontology may change dynamically. Experimental results demonstrate that our BERT-based model outperforms previous methods by a large margin, achieving new state-of-the-art results on the standard WoZ 2.0 dataset 1 . Finally, to make the model small and fast enough for resource-restricted systems, we apply the knowledge distillation method to compress our model. The final compressed model achieves comparable results with the original model while being 8x smaller and 7x faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Task-oriented dialog systems have attracted more and more attention in recent years, because they allow for natural interactions with users to help them achieve simple tasks such as flight booking or restaurant reservation. Dialog state tracking (DST) is an important component of task-oriented dialog systems <ref type="bibr" target="#b0">[1]</ref>. Its purpose is to keep track of the state of the conversation from past user inputs and system outputs. Based on this estimated dialog state, the dialog system then plans the next action and responds to the user. In a slot-based dialog system, a state in DST is often expressed as a set of slot-value pairs. The set of slots and their possible values are typically domain-specific, defined in a domain ontology.</p><p>With the renaissance of deep learning, many neural network based approaches have been proposed for the task of DST <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. These methods achieve highly competitive performance on standard DST datasets such as DSTC-2 <ref type="bibr" target="#b9">[10]</ref> or WoZ 2.0 <ref type="bibr" target="#b10">[11]</ref>. However, most of these methods still have some limitations. First, many approaches require training a separate model for each slot type in the domain ontology <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7]</ref>. Therefore, the number of parameters is proportional to the number of slot types, making the scalability of these approaches a significant issue. Second, some methods only operate on a fixed domain ontology <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. The slot types and possible values need to be defined in advance and must not change dynamically. Finally, state-of-the-art neural architectures for DST are typically heavily-engineered and conceptually complex <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Each of these models consists of a number of different kinds of sub-components. In general, complicated deep learning models are difficult to implement, debug, and maintain in a production setting.</p><p>Recently, several pretrained language models, such as ELMo <ref type="bibr" target="#b11">[12]</ref> and BERT <ref type="bibr" target="#b12">[13]</ref>, were used to achieve state-ofthe-art results on many NLP tasks. In this paper, we show that by finetuning a pretrained BERT model, we can build a conceptually simple but effective model for DST. Given a dialog context and a candidate slot-value pair, the model outputs a score indicating the relevance of the candidate. Because the model shares parameters across all slot types, the number of parameters does not grow with the ontology size. Furthermore, because each candidate slot-value pair is simply treated as a sequence of words, the model can be directly applied to new types of slot-value pairs not seen during training. We do not need to retrain the model every time the domain ontology changes. Empirical results show that our proposed model outperforms prior work on the standard WoZ 2.0 dataset. However, a drawback of the model is that it is too large for resource-limited systems such as mobile devices.</p><p>To make the model less computationally demanding, we propose a compression strategy based on the knowledge distillation framework <ref type="bibr" target="#b13">[14]</ref>. Our final compressed model achieves results comparable to that of the full model, but it is around 8 times smaller and performs inference about 7 times faster on a single CPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">BERT</head><p>BERT is a powerful language representation model pretrained on vast amounts of unlabeled text corpora <ref type="bibr" target="#b12">[13]</ref>. It consists of multiple layers of Transformer <ref type="bibr" target="#b14">[15]</ref>, a self-attention based architecture. The base version of BERT consists of 12 Transformer layers, each with a hidden size of 768 units and 12 self-attention heads. The input to BERT is a sequence of tokens (words or pieces of words). The output is a sequence of vectors, one for each input token. The input representation of BERT is flexible enough that it can unambiguously represent both a single text sentence and a pair of text sentences in one token sequence. The first token of every input sequence is always a special classification token -[CLS]. The output vector corresponding to this token is typically used as the aggregate representation of the original input. For single text sentence tasks (e.g., sentiment classification), this [CLS] token is followed by the actual tokens of the input text and a special separator token -[SEP]. For sentence pair tasks (e.g., entailment classification), the tokens of the two input texts are separated by another [SEP] token. This input sequence also ends with the [SEP] token. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates the input representation of BERT.</p><p>During pretraining, BERT was trained using two selfsupervised tasks: masked language modeling (masked LM) and next sentence prediction (NSP). In masked LM, some of the tokens in the input sequence are randomly selected and replaced with a special token [MASK], and then the objective is to predict the original vocabulary ids of the masked tokens. In NSP, BERT needs to predict whether two input segments follow each other in the original text. Positive examples are created by taking consecutive sentences from the text corpus, whereas negative examples are created by picking segments from different documents. After the pretraining stage, BERT can be applied to various downstream tasks such as question answering and language inference, without substantial task-specific architecture modifications. At a high level, given a dialog context and a candidate slotvalue pair, our model outputs a score indicating the relevance of the candidate. In other words, the approach is similar to a sentence pair classification task. The first input corresponds to the dialog context, and it consists of the system utterance from the previous turn and the user utterance from the current turn. The two utterances are separated by a [SEP] token. The second input is the candidate slot-value pair. We simply represent the candidate pair as a sequence of tokens (words or pieces of words). The two input segments are concatenated into one single token sequence and then simply passed to BERT to get the output vectors (h 1 , h 2 · · · , h M ). Here, M denotes the total number of input tokens (including special tokens such as [CLS] and [SEP]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">BERT for Dialog State Tracking</head><p>Based on the output vector corresponding the first special token -[CLS] (i.e., h 1 ) the probability of the candidate slotvalue pair being relevant is:</p><formula xml:id="formula_0">y = σ(Wh 1 + b) ∈ IR<label>(1)</label></formula><p>where the transformation matrix W and the bias term b are model parameters, and σ denotes the sigmoid function. It squashes the score to a probability between 0 and 1. At each turn, the proposed BERT-based model is used to estimate the probability score of every candidate slot-value pair. After that, only pairs with predicted probability equal to at least 0.5 are chosen as the final prediction for the turn. To obtain the dialog state at the current turn, we use the newly predicted slot-value pairs to update the corresponding values in the state of previous turn. For example, suppose the user specifies a food=chinese restaurant during the current turn. If the dialog state has no existing food specification, then we can add food=chinese to the dialog state. If food=korean had been specified before, we replace it with food=chinese.</p><p>Compared to previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, our model is conceptually simpler. For example, in the GLAD model <ref type="bibr" target="#b3">[4]</ref>, there are two scoring modules: the utterance scorer and the action scorer. Intuitively, the utterance scorer determines whether the current user utterance mentions the candidate slot-value pair or not, while the action scorer determines the degree to which the slot-value pair was expressed by the previous system actions. In our proposed approach, we simply use a single BERT model for examining the information from all sources at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model Compression</head><p>BERT is a powerful language representation model, because it was pretrained on large text corpora (Wikipedia and Book-Corpus). However, the original pretrained BERT models are computationally expensive and have a huge number of parameters. For example, the base version of BERT consists of about 110M parameters. Therefore, if we directly integrate an existing BERT model into our DST model, it will be difficult to deploy the final model in resource-limited systems such as mobile devices. In this part, we describe our approach to compress BERT into a smaller model. Over the years, many model compression methods have been proposed <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14]</ref>. In this work, we propose a strategy for compressing BERT based on the knowledge distillation framework <ref type="bibr" target="#b13">[14]</ref>. Knowledge distillation (KD) aims at transferring knowledge acquired in one model (a teacher) to another model (a student) that is typically smaller. We generally assume that the teacher has previously been trained, and that we are estimating parameters for the student. KD suggests training by matching the student's predictions to the teacher's predictions. In other words, we train the student to mimic output activations of individual data examples represented by the teacher.</p><p>We choose the pretrained base version of BERT as the teacher model. Our student model has the same general architecture as BERT but it is much smaller than the teacher model ( <ref type="table">Table 1</ref>). In the student model, the number of Transformer layers is 8, each with a hidden size of 256 units and 8 selfattention heads. The feedforward/filter size is 1024. Overall, our student model has 14M parameters in total and it is 8x smaller and 7x faster on inference than our original teacher model.</p><p>We first extract sentences from the BooksCorpus <ref type="bibr" target="#b18">[19]</ref>, a large-scale corpus consisting of about 11,000 books and nearly 1 billion words. For each sentence, we use the Word-Piece tokenizer <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b12">13]</ref> to tokenize the sentence into a sequence of tokens. Similar to the pretraining phase of BERT, we mask 15% of the tokens in the sequence at random. After that, we define the cross-entropy loss for each token as follow:</p><formula xml:id="formula_1">L(a T , a S ) = H softmax a T τ , softmax a S τ<label>(2)</label></formula><p>where H refers to the cross-entropy function, a T is the teacher model's pre-softmax logit for the current token, a S is the student model's pre-softmax logit. Finally, τ is the temperature hyperparameter <ref type="bibr" target="#b13">[14]</ref>. In this work, we set τ to be 10. Intuitively, L(a T , a S ) will be small if the student's prediction for the current token is similar to that of the teacher model. The distillation loss for the entire sentence is simply defined as the sum of all the cross-entropy losses of all the tokens. To summarize, during the KD process, we use this distillation loss to train our student model from scratch using the teacher models logits on unlabeled examples extracted from the BooksCorpus. After that, we can integrate our distilled student BERT model into our DST model ( <ref type="figure" target="#fig_1">Figure 2)</ref> and use the final model for monitoring the state of the conversation.</p><p>Different from the very first work on exploring knowledge distillation for BERT <ref type="bibr" target="#b20">[21]</ref>, our approach does not use any data augmentation heuristic. We only extract unlabeled sentences from the BooksCorpus to build training examples for distillation. Our work is in spirit similar to DistilBERT <ref type="bibr" target="#b21">[22]</ref>, which also uses the original BERT as the teacher and a large-scale unlabeled text corpus as the basic learning material. However, as shown in <ref type="table">Table 1</ref>, the DistilBERT model is about 5 times larger than our student model. Recently, at WWDC 2019, Apple presented a BERT-based on-device model for question answering <ref type="bibr" target="#b22">[23]</ref>. Instead of using knowledge distillation, Apple used the mixed precision training technique <ref type="bibr" target="#b23">[24]</ref> to build their model. From the <ref type="table">Table 1</ref>, we see that the model of Apple is much larger than our student model, as it has 8x more parameters and requires 4x more storage space. This implies that our student model is small enough to be deployed on mobile systems. To the best of our knowledge, we are the first to explore the use of knowledge distillation to compress neural networks for DST. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14M 55MB</head><p>Apple Core ML BERT <ref type="bibr" target="#b22">[23]</ref> 110M 220 MB DistilBERT <ref type="bibr" target="#b21">[22]</ref> (N = 6, d1 = 768, d2 = 3072, h = 12)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>66M</head><p>270 MB GLAD <ref type="bibr" target="#b3">[4]</ref> 17M - <ref type="table">Table 1</ref>. Approximated size of different models. For models based on BERT, N is the number of Transformer layers, d 1 is the hidden size, d 2 is the feedforward size, and h is the number of self-attention heads</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data and Evaluation Metrics</head><p>To evaluate the effectiveness of our proposed approach, we use the standard WoZ 2.0 dataset. The dataset consists of user conversations with dialog systems designed to help users find suitable restaurants. The ontology contains three informable slots: food, price, and area. In a typical conversation, a user would first search for restaurants by specifying values for some of these slots. As the dialog progresses, the dialog system may ask the user for more information about these slots, and the user answers these questions. The users goal may also change during the dialog. For example, the user may want an 'expensive' restaurant initially but change to wanting a 'moderately priced' restaurant by the end. Once the system suggests a restaurant that matches the user criteria, the user can also ask about the values of up to eight requestable slots (phone, address, ...). The dataset has 600/200/400 dialogs for train/dev/test split. Similar to previous work, we focus on two key evaluation metrics introduced in <ref type="bibr" target="#b9">[10]</ref>: joint goal accuracy and turn request accuracy.  <ref type="table">Table 2</ref>. Test accuracies on the WoZ 2.0 restaurant reservation datasets. <ref type="table">Table 2</ref> shows the test accuracies of different models on the WoZ 2.0 dataset. Our full BERT-based model uses the base version of BERT (i.e., the teacher model in the knowledge distillation process), whereas the distilled BERT-based model uses the compressed student model. Both our full model and the compressed model outperform previous methods by a considerable margin. Even though our compressed model is 8x smaller and 7x faster than the full model <ref type="table">(Table  1)</ref>, it still achieves almost the same results as the full model. In fact, the smaller model has a slightly higher turn request accuracy. From <ref type="table">Table 1</ref>, we see that our compressed model even has less parameters than GLAD, a DST model that is not based on BERT. This demonstrates the effectiveness of our proposed knowledge distillation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dialog State Tracking Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Note that BERT-DST PS <ref type="bibr" target="#b8">[9]</ref> is a recent work that also utilizes BERT for DST. However, the work focuses only on situation where the target slot value (if any) should be found as word segment in the dialog context. According to <ref type="table">Table 2</ref>  <ref type="bibr" target="#b21">[22]</ref> 0.579 0.043 <ref type="table">Table 3</ref>. Inference time of different models on CPU and GPU. We measured the average time it takes for each model to process one dialog turn (in seconds). <ref type="table">Table 1</ref> shows that our distilled student model is much smaller than many other BERT-based models in previous works. <ref type="table">Table 3</ref> shows the inference speed of our models on CPU (Intel Core i7-8700K @3.70GHz) and on GPU (GeForce GTX 1080). On average, on CPU, our compressed model is 7x faster on inference than our full model and 3x faster than Dis-tilBERT. On GPU, our compressed model is about 5x faster than the full model and 2x faster than DistilBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Size and inference speed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we propose a simple but effective model based on BERT for the task of dialog state tracking. Because the original version of BERT is large, we apply the knowledge distillation method to compress our model. Our compressed model achieves state-of-the-art performance on the WoZ 2.0 dataset while being 8x smaller and 7x faster on inference than the original model. In future work, we will experiment on more large scale datasets such as the MultiWOZ dataset <ref type="bibr" target="#b24">[25]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>BERT input format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>shows our proposed application of BERT to DST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of our BERT-based model for DST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>N = 12, d1 = 768, d2 = 3072, h = 12) 110M 440MB Student Model (N = 8, d1 = 256, d2 = 1024, h = 8)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, our models outperform BERT-DST PS on the WoZ dataset. Furthermore, BERT-DST PS only uses the original version of BERT, making it large and slow.</figDesc><table><row><cell>Model</cell><cell>Inf. Time</cell><cell>Inf. Time</cell></row><row><cell></cell><cell>on CPU</cell><cell>on GPU</cell></row><row><cell></cell><cell>(secs)</cell><cell>(secs)</cell></row><row><cell>Full BERT-based Model</cell><cell>1.465</cell><cell>0.113</cell></row><row><cell>Distilled BERT-based Model</cell><cell>0.205</cell><cell>0.024</cell></row><row><cell>DistilBERT</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We did not use the DSTC-2 dataset because its clean version is no longer accessible (http://mi.eng.cam.ac.uk/˜nm480/dstc2-clean. zip). In addition, it has the exact same ontology as the WoZ 2.0 dataset.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural belief tracker: Data-driven dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuidó</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network model with belief tracking for task-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<idno>abs/1708.05956</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global-locally self-attentive encoder for dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1458" to="1467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards universal dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaige</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elnaz</forename><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00899</idno>
		<title level="m">Toward scalable neural dialogue state tracking model</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully statistical neural belief tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dialogue state tracking with convolutional semantic taggers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Korpusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7220" to="7224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert-dst: Scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Lin</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<idno>abs/1907.03040</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The second dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A network-based end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">Maria</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hao Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>abs/1510.00149</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distilling task-specific knowledge from bert into simple neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Core ml models</title>
		<ptr target="https://developer.apple.com/machine-learning/models/" />
		<imprint>
			<date type="published" when="2019-10-20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">Frederick</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1710.03740</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiwoz -a large-scale multidomain wizard-of-oz dataset for task-oriented dialogue modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
