<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learnable Triangulation of Human Pose</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
							<email>k.iskakov@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
							<email>e.burkov@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
							<email>v.lempitsky@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
							<email>y.malkov@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learnable Triangulation of Human Pose</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The first (baseline) solution is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images. The second solution is based on a novel method of volumetric aggregation from intermediate 2D backbone feature maps. The aggregated volume is then refined via 3D convolutions that produce final 3D joint heatmaps and allow modelling a human pose prior. Crucially, both approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multi-view state of the art on the Human3.6M dataset. Video demonstration, annotations and additional materials will be posted on our project page 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose estimation is one of the fundamental problems in computer vision, with applications in sports, action recognition, computer-assisted living, humancomputer interfaces, special effects, and telepresence. To date, most of the efforts in the community are focused on monocular 3D pose estimation. Despite a lot of recent progress, the problem of in-the-wild monocular 3D human pose estimation is far from being solved. Here, we consider a simpler yet still challenging problem of multi-view 3D human pose estimation.</p><p>There are at least two reasons why multi-view human pose estimation is interesting. First, multi-view pose estimation is arguably the best way to obtain ground truth for monocular 3D pose estimation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref> in-the-wild. This is because the competing techniques such as marker-based motion capture <ref type="bibr" target="#b9">[10]</ref> and visual-inertial methods <ref type="bibr" target="#b18">[19]</ref> have certain limitations such as inability to capture rich pose representations (e.g. to estimate hands pose and face pose alongside limb pose) as well as various clothing limitations. The downside is, previous works that used multi-view triangulation for constructing datasets relied on excessive, almost impractical number of views to get the 3D ground truth of sufficient quality <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>. This makes the collection of new in-the-wild datasets for 3D pose estimation very challenging and calls for the reduction of the number of views needed for accurate triangulation.</p><p>The second motivation to study multi-view human pose estimation is that, in some cases, it can be used directly to track human pose in real-time for the practical end purpose. This is because multi-camera setups are becoming progressively available in the context of various applications, such as sports or computer-assisted living. Such practical multiview setups rarely go beyond having just a few views. At the same time, in such a regime, modern multi-view methods have accuracy comparable to well-developed monocular methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13]</ref>. Thus, improving the accuracy of multi-view pose estimation from few views is an important challenge with direct practical applications.</p><p>In this work, we argue that given its importance, the task of multi-view pose estimation has received disproportionately little attention. We propose and investigate two simple and related methods for multi-view human pose estimation. Behind both of them lies the idea of learnable triangulation, which allows us to dramatically reduce the number of views needed for accurate estimation of 3D pose. During learning, we either use marker based motion capture ground truth or "meta"-ground truth obtained from the excessive number of views. The methods themselves are as follows: (1) a simpler approach based on algebraic triangulation with learnable camera-joint confidence weights, and (2) a more complex volumetric triangulation approach based on dense geometric aggregation of information from different views that allows modelling a human pose prior. Crucially, both of the proposed solutions are fully differentiable, which permits end-to-end training.</p><p>Below, we review related work in monocular and multiview human pose estimation, and then discuss the details of the new learnable triangulation methods. In the experimental section, we perform an evaluation on the popular Human3.6M <ref type="bibr" target="#b2">[3]</ref> and CMU Panoptic <ref type="bibr" target="#b4">[5]</ref> datasets, demonstrating state-of-the-art accuracy of the proposed methods and and their ability of cross-dataset generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Single view 3D pose estimation. Current state-of-the-art solutions for the monocular 3D pose estimation can be divided into two sub-categories. The first category is using high quality 2D pose estimation engines with subsequent separate lifting of the 2D coordinates to 3D via deep neural networks (either fully-connected, convolutional or recurrent). This idea was popularized in <ref type="bibr" target="#b7">[8]</ref> and offers several advantages: it is simple, fast, can be trained on motion capture data (with skeleton/view augmentations) and allows switching 2D backbones after training. Despite known ambiguities inherent to this family of methods (i.e. orientation of arms' joints in current skeleton models), this paradigm is adopted in the current multi-frame state of the art <ref type="bibr" target="#b12">[13]</ref> on the Human3.6M benchmark <ref type="bibr" target="#b2">[3]</ref>.</p><p>The second option is to infer the 3D coordinates directly from the images using convolutional neural networks. The present best solutions use volumetric representations of the pose, with current single-frame state-of-the-art results on Human3.6M <ref type="bibr" target="#b2">[3]</ref>, namely <ref type="bibr" target="#b15">[16]</ref>.</p><p>Multi-view view 3D pose estimation. Studies of multiview 3D human pose estimation are generally aimed at getting the ground-truth annotations for the monocular 3D human pose estimation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref>. The work <ref type="bibr" target="#b5">[6]</ref> proposed concatenating joints' 2D coordinates from all views into a single batch as an input to a fully connected network that is trained to predict the global 3D joint coordinates. This approach can efficiently use the information from different views and can be trained on motion capture data. However, the method is by design unable to transfer the trained models to new camera setups, while the authors show that the approach is prone to strong over-fitting.</p><p>Few works used volumetric pose representation in multiview setups <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5]</ref>. Specifically, <ref type="bibr" target="#b4">[5]</ref> utilized unprojection of 2D keypoint probability heatmaps (obtained from a pretrained 2D keypoint detector) to volume with subsequent non-learnable aggregation. Our work differs in two ways. First, we process information inside the volume in a learnable way. Second, we train the network end-to-end, thus adjusting the 2D backbone and alleviating the need for interpretability of the 2D heatmaps. This allows to transfer several self-consistent pose hypotheses from 2D detectors to the volumetric aggregation stage (which was not possible with previous designs).</p><p>The work <ref type="bibr" target="#b17">[18]</ref> used a multi-stage approach with an external 3D pose prior <ref type="bibr" target="#b16">[17]</ref> to infer the 3D pose from 2D joints' coordinates. During the first stage, images from all views were passed through the backbone convolutional neural network to obtain 2D joints' heatmaps. The positions of maxima in the heatmaps were jointly used to infer the 3D pose via optimizing latent coordinates in 3D pose prior space. In each of the subsequent stages, 3D pose was reprojected back to all camera views and fused with predictions from the previous layer (via a convolutional network). Next, the 3D pose was re-estimated from the positions of heatmap maxima, and the process repeated. Such procedure allowed correcting the predictions of 2D joint heatmaps via indirect holistic reasoning on a human pose. In contrast to our approach, in <ref type="bibr" target="#b17">[18]</ref> there is no gradient flow from the 3D predictions to 2D heatmaps and thus no direct signal to correct the prediction of 3D coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our approach assumes we have synchronized video streams from C cameras with known projection matrices P c capturing performance of a single person in the scene. We aim at estimating the global 3D positions y j,t of a fixed set of human joints with indices j ∈ (1..J) at timestamp t. For each timestamp the frames are processed independently (i.e. without using temporal information), thus we omit the index t for clarity.</p><p>For each frame, we crop the images using the bounding boxes either estimated by available off-the-shelf 2D human detectors or from ground truth (if provided). Then we feed the cropped images I c into a deep convolutional neural network backbone based on the "simple baselines" architecture <ref type="bibr" target="#b20">[21]</ref>.</p><p>The convolutional neural network backbone with learnable weights θ consists of a ResNet-152 network (output denoted by g θ ), followed by a series of transposed convolutions that produce intermediate heatmaps (the output denoted by f θ ) and a 1 × 1 -kernel convolutional neural network that transforms the intermediate heatmaps to interpretable joint heatmaps (output denoted by h θ ; the number of output channels is equal to the number of joints J). In the two following sections we describe two different methods to infer joints' 3D coordinates by aggregating information from multiple views.</p><p>Algebraic triangulation approach. In the algebraic triangulation baseline we process each joint j independently of each other. The approach is built upon triangulating the 2D positions obtained from the j-joint's backbone heatmaps from different views: H c,j = h θ (I c ) j <ref type="figure">(Figure 1</ref>). To estimate the 2D positions we first compute the softmax across the spatial axes: 2D joint keypoints <ref type="bibr">[J, 2]</ref> joints' confidences: <ref type="bibr" target="#b2">3]</ref> 2D joint heatmaps <ref type="bibr">[J, 96, 96]</ref> 2D joint keypoints [J, 2] <ref type="figure">Figure 1</ref>. Outline of the approach based on algebraic triangulation with learned confidences. The input for the method is a set of RGB images with known camera parameters. The 2D backbone produces the joints' heatmaps and camera-joint confidences. The 2D positions of the joints are inferred from 2D joint heatmaps by applying soft-argmax. The 2D positions together with the confidences are passed to the algebraic triangulation module that outputs the triangulated 3D pose. All blocks allow backpropagation of the gradients, so the model can be trained end-to-end.</p><formula xml:id="formula_0">H c,j = exp(αH c,j )/ W rx=1 H ry=1 exp(αH c,j (r)) , (1)</formula><formula xml:id="formula_1">[ 1,1 , …, 1,J ] joints' confidences: [ C,1 , …, C,J ] 3D pose [J,</formula><p>where parameter α is discussed below. Then we calculate the 2D positions of the joints as the center of mass of the corresponding heatmaps (so-called soft-argmax operation):</p><formula xml:id="formula_2">x c,j = W rx=1 H ry=1 r · (H c,j (r))<label>(2)</label></formula><p>An important feature of soft-argmax is that rather than getting the index of the maximum, it allows the gradients to flow back to heatmaps H c from the output 2D position of the joints x. Since the backbone was pretrained using a loss other than soft-argmax (MSE over heatmaps without softmax <ref type="bibr" target="#b15">[16]</ref>), we adjust the heatmaps via multiplying them by an 'inverse temperature' parameter α = 100 in (1), so at the start of the training the soft-argmax gives an output close to the positions of the maximum.</p><p>To infer the 3D positions of the joints from their 2D estimates x c,j we use a linear algebraic triangulation approach <ref type="bibr" target="#b0">[1]</ref>. The method reduces the finding of the 3D coordinates of a joint y j to solving the overdetermined system of equations on homogeneous 3D coordinate vector of the jointỹ:</p><formula xml:id="formula_3">A jỹj = 0,<label>(3)</label></formula><p>where A j ∈ R (2C,4) is a matrix composed of the components from the full projection matrices and x c,j (see <ref type="bibr" target="#b0">[1]</ref> for full details). A naïve triangulation algorithm assumes that the joint coordinates from each view are independent of each other and thus all make comparable contributions to the triangulation. However, on some views the 2D position of the joints cannot be estimated reliably (e.g. due to joint occlusions), leading to unnecessary degradation of the final triangulation result. This greatly exacerbates the tendency of methods that optimize algebraic reprojection error to pay uneven attention to different views. The problem can be dealt with by applying RANSAC together with the Huber loss (used to score reprojection errors corresponding to inliers). However, this has its own drawbacks. E.g. using RANSAC may completely cut off the gradient flow to the excluded cameras.</p><p>To address the aforementioned problems, we add learnable weights w c to the coefficients of the matrix corresponding to different views:</p><formula xml:id="formula_4">(w j • A j )ỹ j = 0,<label>(4)</label></formula><p>where w j = (w 1,j , w 1,j , w 2,j , w 2,j , ..., , w C,j , w C,j ); • denotes the Hadamard product (i.e. i-th row of matrix A is multiplied by i-th element of vector w). The weights w c,j are estimated by a convolutional network q φ with learnable parameters φ (comprised of two convolutional layers, global average pooling and three fully-connected layers), applied to the intermediate output of the backbone:</p><formula xml:id="formula_5">w c,j = q φ (g θ (I c )) j<label>(5)</label></formula><p>This allows the contribution of the each camera view to be controlled by the neural network branch that is learned jointly with the backbone joint detector. The equation <ref type="formula" target="#formula_4">(4)</ref>   <ref type="figure">Figure 2</ref>. Outline of the approach based on volumetric triangulation. The input for the method is a set of RGB images with known camera parameters. The 2D backbone produces intermediate feature maps that are unprojected into volumes with subsequent aggreagation to a fixed size volume. The volume is passed to a 3D convolutional neural network that outputs the interpretable 3D heatmaps. The output 3D positions of the joints are inferred from 3D joint heatmaps by computing soft-argmax. All blocks allow backpropagation of the gradients, so the model can be trained end-to-end.</p><p>Volumetric triangulation approach. The main drawback of the baseline algebraic triangulation approach is that the images I c from different cameras are processed independently from each other, so there is no easy way to add a 3D human pose prior and no way to filter out the cameras with wrong projection matrices.</p><p>To solve this problem we propose to use a more complex and powerful triangulation procedure. We unproject the feature maps produced by the 2D backbone into 3D volumes (see <ref type="figure">Figure 2</ref>). This is done by filling a 3D cube around the person via projecting output of the 2D network along projection rays inside the 3D cube. The cubes obtained from multiple views are then aggregated together and processed. For such volumetric triangulation approach, the 2D output does not have to be interpretable as joint heatmaps, thus, instead of unprojecting H c themselves, we use the output of a trainable single layer convolutional neural network o γ with 1 × 1 kernel and K output channels (the weights of this layer are denoted by γ) applied to the input from the backbone intermediate heatmaps f θ (I c ):</p><formula xml:id="formula_6">M c,k = o γ (f θ (I c )) k<label>(6)</label></formula><p>To create the volumetric grid, we place a L × L × L -sized 3D bound box in the global space around the human pelvis (the position of the pelvis is estimated by the algebraic triangulation baseline described above, L denotes the size of the box in meters) with the Y-axis perpendicular to the ground and a random orientation of the X-axis. We discretize the bounding box by a volumetric cube V coords ∈ R 64,64,64,3 , filling it with the global coordinates of the center of each voxel (in a similar way to <ref type="bibr" target="#b4">[5]</ref>). For each view, we then project the 3D coordinates in V coords to the plane: V proj c = P c V coords (note that V proj c ∈ R 64,64,64,2 ) and fill a cube V view c ∈ R 64,64,64,K by bilinear sampling <ref type="bibr" target="#b3">[4]</ref> from the maps M c,k of the corresponding camera view using 2D coordinates in V proj c :</p><formula xml:id="formula_7">V view c,k = M c,k {V proj c },<label>(7)</label></formula><p>where {·} denotes bilinear sampling. We then aggregate the volumetric maps from all views to form an input to the further processing that does not depend on the number of camera views. We study three diverse methods for the aggregation:</p><p>1. Raw summation of the voxel data:</p><formula xml:id="formula_8">V input k = c V view c,k<label>(8)</label></formula><p>2. Summation of the voxel data with normalized confidence multipliers d c (obtained similarly to w c using a branch attached to backbone):</p><formula xml:id="formula_9">V input k = c d c · V view c,k / c d c<label>(9)</label></formula><p>3. Calculating a relaxed version of maximum. Here, we first compute the softmax for each individual voxel V view c across all cameras, producing the volumetric coefficient distribution V w c,k with the role similar to scalars d c :</p><formula xml:id="formula_10">V w c,k = exp(V view c,k )/ c exp(V view c,k )<label>(10)</label></formula><p>Then, the voxel maps from each view are summed with the volumetric coefficients V w c :</p><formula xml:id="formula_11">V input k = c V w c,k • V view c<label>(11)</label></formula><p>Aggregated volumetric maps are then fed into a learnable volumetric convolutional neural network u ν (with weights denoted by ν), with architecture similar to V2V <ref type="bibr" target="#b10">[11]</ref>, producing the interpretable 3D-heatmaps of the output joints:</p><formula xml:id="formula_12">V output j = (u ν (V input )) j<label>(12)</label></formula><p>Next, we compute softmax of V output j across the spatial axes (similar to (1)):</p><formula xml:id="formula_13">V output j = exp(V output j )/ W rx=1 H ry=1 D rz=1 exp(V output j (r)) ,<label>(13)</label></formula><p>and estimate the center of mass for each of the volumetric joint heatmaps to infer the positions of the joints in 3D:</p><formula xml:id="formula_14">y j = W rx=1 H ry=1 D rz=1 r · V output j (r)<label>(14)</label></formula><p>Lifting to 3D allows getting more robust results, as the wrong predictions are spatially isolated from the correct ones inside the cube, so they can be discarded by convolutional operations. The network also naturally incorporates the camera parameters (uses them as an input), allows modelling the human pose prior and can reasonably handle multimodality in 2D detections.</p><p>Losses. For both of the methods described above, the gradients pass from the output prediction of 3D joints' coordinates y j to the input RGB-images I c making the pipeline trainable end-to-end. For the case of algebraic triangulation, we apply a soft version of per-joint Mean Square Error (MSE) loss to make the training more robust to outliers. This variant leads to better results compared to raw MSE or L1 (mean absolute error):</p><formula xml:id="formula_15">L alg j (θ, φ) = MSE(y j , y gt j ), if MSE(y j , y gt j ) &lt; ε MSE(y j , y gt j ) 0.1 · ε 0.9 , otherwise<label>(15)</label></formula><p>Here, ε denotes the threshold for the loss, which is set to (20 cm) 2 in the experiments. The final loss is the average over all valid joints and all scenes in the batch.</p><p>For the case of volumetric triangulation, we use the L1 loss with a weak heatmap regularizer, which maximizes the prediction for the voxel that has inside of it the ground-truth joint:</p><formula xml:id="formula_16">L vol (θ, γ, ν) = j |y j − y gt j | − β · log(V output j (y gt j ))<label>(16)</label></formula><p>Without the second term, for some of the joints (especially, pelvis) the produced output volumetric heatmaps are not interpretable, probably due to insufficient size of the training datasets <ref type="bibr" target="#b15">[16]</ref>. Setting the β to a small value (β = 0.01) makes them interpretable, as the produced heatmaps always have prominent maxima close to the prediction. At the same time, such small β does not seem to have any effect on the final metrics, so its use can be avoided if interpretability is not needed. We have also tried the loss (15) from the algebraic triangulation instead of L1, but it performed worse in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments on two available large multiview datasets with available ground-truth 3D pose annotations: Human3.6M <ref type="bibr" target="#b2">[3]</ref> and CMU Panoptic <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Human3.6M dataset. The Human3.6M <ref type="bibr" target="#b2">[3]</ref> is currently one of the largest 3D human pose benchmarks with many reported results both for monocular and multi-view setups. The full dataset consist of 3.6 million frames from 4 synchronized 50 Hz digital cameras along with the 3D pose annotations (collected using a marker-based MoCap system comprised of 10 separate IR-cameras). The dataset has 11 human subjects (5 females and 6 males) split into train, validation and test (only train and validation have the groundtruth annotations).</p><p>The 2D backbone for Human3.6M was pretrained on the COCO dataset <ref type="bibr" target="#b6">[7]</ref> and finetuned jointly on MPII and Hu-man3.6M for 10 epochs using the Adam optimizer with 10 −4 learning rate. We use the 3D groundtruth and camera parameters provided by Martinez et al. <ref type="bibr" target="#b7">[8]</ref>. We undistort the images by applying grid-sampling on the video frames. If not mentioned explicitly, all networks are trained using four cameras and evaluated using all available cameras (either three or four, as one of the subjects lacks data from one camera). All algorithms use the 2D bounding boxes annotations provided with the dataset. The networks are trained for 6 epochs with 10 −4 learning rate for the 2D backbone and a separate learning rate 10 −3 for the volumetric backbone.</p><p>Note that the volumetric triangulation approach uses predictions obtained from the algebraic triangulation (the whole system, however, potentially can be fine-tuned endto-end). The size of volumetric cube L was set to 2.5 m, which can enclose all subjects even if there is a few tens of centimeter error in pelvis prediction (which is much higher than the average error by the algebraic triangulation baseline). The number of output channels from the 2D backbone was set to K = 32. We did not apply any augmentations during the training, other than rotating the orientation of the cube in volumetric triangulation around the vertical axis.</p><p>For Human3.6M we follow the most popular protocol with 17-joint subset and testing on the validation. We used the MPJPE (Mean Per Joint Position Error) metric, which  <ref type="table">Table 1</ref>. The results of evaluation on the Human3.6M dataset. The table presents the MPJPE error for the joints (relative to pelvis) for published state-of-the-art monocular and multi-view methods. The methods that are using temporal information during inference are marked by ( * ). Note that our monocular method (labeled by †) is using the approximate position of the pelvis estimated from the multiview. <ref type="bibr">Protocol</ref>   <ref type="table">Table 2</ref>. The results of evaluation on the Human3.6M dataset. The table presents the absolute positions MPJPE error for our algorithms. Note that the validation set has been filtered by removing the scenes with erroneous ground-truth 3D pose annotations.</p><p>is L2 distance between the ground-truth and predicted positions of the joints (in most cases, measured with respect to pelvis). We use every fifth frame for the evaluation. As a baseline, we implemented a simple triangulation method with RANSAC and Huber loss, which is the de-facto standard for solving robust estimation problems. The baseline uses the same pretrained 2D backbone. The results of the standard protocol are summarized in <ref type="table">Table 1</ref>.</p><p>Our implementations surpass the previous art by a large margin, even for the simplest RANSAC baseline. The introduced volumetric methods performs the best, providing about 30% additional reduction in the error to the RANSAC, which is significant.</p><p>While most of the works on monocular 3D human pose evaluate the positions of the joints relative to the pelvis (in order to avoid the estimation of global coordinates, which is problematic for monocular algorithms), evaluating with respect to the global coordinates is more reasonable for multiview setups. This, however, is not straightforward due to 3D pose annotation errors in the Human3.6M -the problem is that some scenes of the 'S9' validation actor (parts of 'Greeting', 'SittingDown' and 'Waiting', see our project page) have the ground truth erroneously shifted in 3D compared to the actual position. Interestingly, the error is nullified when the pelvis is subtracted (as done for monocular methods), however, to make the results for the multi-view setup interpretable we must exclude these scenes from the evaluation. The results for the absolute MPJPE for our methods with these scenes excluded are presented in <ref type="table">Table  2</ref>, giving a better sense of the magnitude of errors. Interestingly, the average MPJPE for volumetric is much smaller than the size of the voxel (3.9 cm), showing the importance of the subpixel resolving soft-argmax.</p><p>Our volumetric multi-view methods can be generalized to the case of a single camera view, naturally taking into account the camera intrinsics. To check the monocular performance we have done a separate experiment with training using a random number of cameras from 1 to 4. For the case of a single camera we used the L1 loss on joint positions relative to the pelvis (as is usually done for the monocular methods). Without any tuning this resulted in 49.9 mm error, which is close to the current state of the art. Note that the position of the cube center was estimated by triangulating the pelvis from all 4 cameras, so the performance of the methods might be somewhat overoptimistic. On the other hand, the average relative positions error of the monocular method lies within 4-6 cm, which corresponds to a negligible shift for the volumetric cube position (less than 2 voxels).</p><p>CMU Panoptic dataset. The CMU panoptic is a new multi-camera dataset maintained by the Carnegie Mellon  University <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15]</ref>. The dataset provides 30 Hz Full-HD videostreams of 40 subjects from up to 31 synchronized cameras.</p><p>The dataset is provided with annotations of the Full-HD cameras acquired via triangulation using all camera views. Since there are no other published results on the quality of multi-view pose estimation on CMU, we use our own protocol. For the tests we use the 17-subset of the 19-joint annotation format in the dataset, which coincides with the popular COCO format <ref type="bibr" target="#b6">[7]</ref>. We used the same train/val split as in <ref type="bibr" target="#b19">[20]</ref>, which only has scenes with a single person at each point of time. Additionally, we split the dataset by camera views (4 cameras in val, up to 27 cameras in train), so there is no overlap between the test and validation both in terms of subjects and camera views. To get the human bounding boxes we use Mask R-CNN 2D detector with ResNet-152 backbone <ref type="bibr" target="#b8">[9]</ref>). Note that we process the frames without taking into account the lens distortion which leads to some loss The networks are trained using the Adam optimizer similarly to Human3.6M as described in the previous section. The number of views during training was selected randomly from 2 to 5.</p><p>The comparison between our multi-view methods is presented in <ref type="table" target="#tab_3">Table 3</ref> with absolute MPJPE used as the main metric. Here, the volumetric approach has a dramatic advantage over the algebraic one, and its superiority is far more evident than on Human3.6M. We believe that the main point in which CMU differs from Human3.6M is that in CMU most cameras do not have the full view of a person the whole time, leading to strong occlusions and missing parts. This suggests the importance of the 3D prior that can be learnt only by volumetric models. It seems that RANSAC performs worse compared to algebraic triangulation without confidences because it is not finetuned on the CMU data. The difference between the methods' prediction is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. We have not observed any significant difference between the volumetric aggregation methods.</p><p>In <ref type="figure" target="#fig_2">Figure 4</ref> we present a plot for the error versus the numbers of used cameras. The plot demonstrates that the proposed volumetric triangulation methods allow drastically reducing the number of cameras in real-life setups: the accuracy on "meta"-groundtruth for RANSAC approach with 28 cameras is surpassed by the volumetric approach with just four cameras.</p><p>We also conducted experiments to demonstrate that the learnt model indeed generalizes to new setups. For that we applied a CMU-trained model to Human3.6M validation scenes. The qualitative results for the learnable triangulation are presented in <ref type="figure" target="#fig_3">Figure 5</ref>. Please see videos for all of the methods on our project page. To provide a quantitative measure of the generalizing ability, we have measured the MPJPE for the set of joints which seem to have the most similar semantics (namely, 'elbows', 'wrists' and 'knees'). The measured MPJPE is 36 mm for learnable triangulation and 34 mm for the volumetric approach, which seems reasonable when compared to the results of the methods trained on Human3.6M (16-18 mm, depending on the triangulation method).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented two novel methods for the multi-view 3D human pose estimation based on learnable triangulation that achieve state-of-the-art performance on the Hu-man3.6M dataset. The proposed solutions drastically reduce the number of views needed to achieve high accuracy, and produce smooth pose sequences on the CMU Panoptic dataset without any temporal processing (see our project page for demonstration), pointing that it can potentially improve the ground truth annotation of the dataset. An ability to transfer the trained method between setups is demonstrated for the CMU Panoptic → Human3.6M pair. The volumetric triangulation strongly outperformed all other approaches both on CMU Panoptic and Human3.6M datasets. We speculate that due to its ability to learn a human pose prior this method is robust to occlusions and partial views of a person. Another important advantage of this method is that it explicitly takes the camera parameters as independent input. Finally, volumetric triangulation also generalizes to monocular images if human's approximate position is known, producing results close to state of the art.</p><p>One of the major limitations of our approach is that it supports only a single person in the scene. This problem can be mitigated by applying available ReID solutions to the 2D detections of humans, however there might be more seamless solutions. Another major limitation of the volumetric triangulation approach is that it relies on the predictions of the algebraic triangulation. This leads to the need for having at least two camera views that observe the pelvis, which might be a problem for some applications. The performance of our method can also potentially be further improved by adding multi-stage refinement in a way similar to <ref type="bibr" target="#b17">[18]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the difference in performance of the approaches on the CMU dataset validation (using 2 cameras) that demonstrates the robustness of the volumetric triangulation approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Estimate of the MPJPE absolute error on the subset of CMU validation versus the numbers of cameras (up to 28, treating the annotations from CMU as ground truth). Each value on the plot is obtained by sampling 50 random subsets of cameras followed by averaging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Demonstration of successful transfer of the solution trained on CMU dataset to Human3.6M scenes. Note that keypoint skeleton models on Human3.6M and CMU are different. of accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is solved via differentiable Singular Value Decomposition of the matrix B = U DV T , from whichỹ is set as the last column of V . The final nonhomogeneous value of y is obtained by dividing the homogeneous 3D coordinate vectorỹ by its fourth coordinate: y =ỹ/(ỹ) 4 .</figDesc><table><row><cell></cell><cell>2D features</cell><cell></cell></row><row><cell>1 st camera</cell><cell>[K, 96, 96]</cell><cell></cell></row><row><cell></cell><cell>P 1</cell><cell></cell></row><row><cell>2D backbone</cell><cell cols="2">volumetric triangulation</cell><cell>3D pose [J, 3]</cell></row><row><cell></cell><cell>V2V</cell><cell>soft-argmax</cell></row><row><cell></cell><cell>aggregated</cell><cell>processed</cell></row><row><cell></cell><cell>volumes</cell><cell>volumes</cell></row><row><cell>2D backbone</cell><cell>[K, 64, 64, 64]</cell><cell>[J, 64, 64, 64]</cell></row><row><cell></cell><cell>P С</cell><cell></cell></row><row><cell>C th camera</cell><cell>2D features</cell><cell></cell></row><row><cell></cell><cell>[K, 96, 96]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Protocol 1 (relative to pelvis) Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg Monocular methods (MPJPE relative to pelvis, mm)</figDesc><table><row><cell>Martinez et al. [8]</cell><cell cols="4">51.8 56.2 58.1 59.0</cell><cell>69.5</cell><cell>78.4</cell><cell>55.2</cell><cell>58.1</cell><cell cols="2">74.0 94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Sun et al. [16]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.6</cell></row><row><cell>Pavllo et al. [13] ( * )</cell><cell cols="4">45.2 46.7 43.3 45.6</cell><cell>48.1</cell><cell>55.1</cell><cell>44.6</cell><cell>44.3</cell><cell cols="2">57.3 65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell>Hossain &amp; Little [2] ( * )</cell><cell cols="4">48.4 50.7 57.2 55.2</cell><cell>63.1</cell><cell>72.6</cell><cell>53.0</cell><cell>51.7</cell><cell cols="2">66.1 80.9</cell><cell>59.0</cell><cell>57.3</cell><cell>62.4</cell><cell>46.6</cell><cell>49.6</cell><cell>58.3</cell></row><row><cell>Ours, volumetric single view ( †)</cell><cell cols="4">41.9 49.2 46.9 47.6</cell><cell>50.7</cell><cell>57.9</cell><cell>41.2</cell><cell>50.9</cell><cell cols="2">57.3 74.9</cell><cell>48.6</cell><cell>44.3</cell><cell>41.3</cell><cell>52.8</cell><cell>42.7</cell><cell>49.9</cell></row><row><cell>Multi-view methods (MPJPE relative to pelvis, mm)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multi-View Martinez [18]</cell><cell cols="4">46.5 48.6 54.0 51.5</cell><cell>67.5</cell><cell>70.7</cell><cell>48.5</cell><cell>49.1</cell><cell cols="2">69.8 79.4</cell><cell>57.8</cell><cell>53.1</cell><cell>56.7</cell><cell>42.2</cell><cell>45.4</cell><cell>57.0</cell></row><row><cell>Pavlakos et al. [12]</cell><cell cols="4">41.2 49.2 42.8 43.4</cell><cell>55.6</cell><cell>46.9</cell><cell>40.3</cell><cell>63.7</cell><cell cols="2">97.6 119.0</cell><cell>52.1</cell><cell>42.7</cell><cell>51.9</cell><cell>41.8</cell><cell>39.4</cell><cell>56.9</cell></row><row><cell>Tome et al. [18]</cell><cell cols="4">43.3 49.6 42.0 48.8</cell><cell>51.1</cell><cell>64.3</cell><cell>40.3</cell><cell>43.3</cell><cell cols="2">66.0 95.2</cell><cell>50.2</cell><cell>52.2</cell><cell>51.1</cell><cell>43.9</cell><cell>45.3</cell><cell>52.8</cell></row><row><cell>Kadkhodamohammadi &amp; Padoy [6]</cell><cell cols="4">39.4 46.9 41.0 42.7</cell><cell>53.6</cell><cell>54.8</cell><cell>41.4</cell><cell>50.0</cell><cell cols="2">59.9 78.8</cell><cell>49.8</cell><cell>46.2</cell><cell>51.1</cell><cell>40.5</cell><cell>41.0</cell><cell>49.1</cell></row><row><cell>RANSAC (our implementation)</cell><cell cols="4">24.1 26.1 24.0 24.6</cell><cell>27.0</cell><cell>25.0</cell><cell>23.3</cell><cell>26.8</cell><cell cols="2">31.4 49.5</cell><cell>27.8</cell><cell>25.4</cell><cell>24.0</cell><cell>27.4</cell><cell>24.1</cell><cell>27.4</cell></row><row><cell>Ours, algebraic (w/o conf)</cell><cell cols="4">22.9 25.3 23.7 23.0</cell><cell>29.2</cell><cell>25.1</cell><cell>21.0</cell><cell>26.2</cell><cell cols="2">34.1 41.9</cell><cell>29.2</cell><cell>23.3</cell><cell>22.3</cell><cell>26.6</cell><cell>23.3</cell><cell>26.9</cell></row><row><cell>Ours, algebraic</cell><cell cols="4">20.4 22.6 20.5 19.7</cell><cell>22.1</cell><cell>20.6</cell><cell>19.5</cell><cell>23.0</cell><cell cols="2">25.8 33.0</cell><cell>23.0</cell><cell>21.6</cell><cell>20.7</cell><cell>23.7</cell><cell>21.3</cell><cell>22.6</cell></row><row><cell>Ours, volumetric (softmax aggregation)</cell><cell cols="4">18.8 20.0 19.3 18.7</cell><cell>20.2</cell><cell>19.3</cell><cell>18.7</cell><cell>22.3</cell><cell cols="2">23.3 29.1</cell><cell>21.2</cell><cell>20.3</cell><cell>19.3</cell><cell>21.6</cell><cell>19.8</cell><cell>20.8</cell></row><row><cell>Ours, volumetric (sum aggregation)</cell><cell cols="4">19.3 20.5 20.1 19.3</cell><cell>20.6</cell><cell>19.8</cell><cell>19.0</cell><cell>22.9</cell><cell cols="2">23.5 29.8</cell><cell>22.0</cell><cell>21.4</cell><cell>19.8</cell><cell>22.1</cell><cell>20.3</cell><cell>21.3</cell></row><row><cell>Ours, volumetric (conf aggregation)</cell><cell cols="4">19.9 20.0 18.9 18.5</cell><cell>20.5</cell><cell>19.4</cell><cell>18.4</cell><cell>22.1</cell><cell>22.5</cell><cell>28.7</cell><cell>21.2</cell><cell>20.8</cell><cell>19.7</cell><cell>22.1</cell><cell>20.2</cell><cell>20.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results of evaluation on the CMU dataset in terms of MPJPE error on the CMU dataset validation (using 4 cameras).</figDesc><table><row><cell>Model</cell><cell>MPJPE, mm</cell></row><row><cell>RANSAC</cell><cell>39.5</cell></row><row><cell>Ours, algebraic (w/o conf)</cell><cell>33.4</cell></row><row><cell>Ours, algebraic</cell><cell>21.3</cell></row><row><cell>Ours, volumetric (softmax aggregation)</cell><cell>13.7</cell></row><row><cell>Ours, volumetric (sum aggregation)</cell><cell>13.7</cell></row><row><cell>Ours, volumetric (conf aggregation)</cell><cell>14.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://saic-violet.github.io/ learnable-triangulation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="69" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A generalizable approach for multi-view 3D human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018-02-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A survey of advances in vision-based human motion capture and analysis. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krüger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="90" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxelto-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno>abs/1811.11742</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Monocular 3D Human Pose Estimation from Multi-view Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sporri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2500" to="2509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking Pose in 3D: Multi-stage Refinement and Recovery for Markerless Motion Capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human pose estimation from video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1533" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01598</idno>
		<title level="m">Monocular total capture: Posing face, body, and hands in the wild</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00281</idno>
		<title level="m">Humbi 1.0: Human multiview behavioral imaging dataset</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
