<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-Quality Facial Photo-Sketch Synthesis Using Multi-Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidan</forename><surname>Wang</surname></persName>
							<email>lidan.wang@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The State University of New Jersey</orgName>
								<address>
									<addrLine>94 Brett Road</addrLine>
									<postCode>08854</postCode>
									<settlement>Rutgers, Piscataway</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwanath</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
							<email>vishwanath.sindagi@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The State University of New Jersey</orgName>
								<address>
									<addrLine>94 Brett Road</addrLine>
									<postCode>08854</postCode>
									<settlement>Rutgers, Piscataway</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
							<email>vishal.m.patel@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The State University of New Jersey</orgName>
								<address>
									<addrLine>94 Brett Road</addrLine>
									<postCode>08854</postCode>
									<settlement>Rutgers, Piscataway</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">High-Quality Facial Photo-Sketch Synthesis Using Multi-Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>face photo sketch synthesis</term>
					<term>image-to-image translation</term>
					<term>face recognition</term>
					<term>multi-adversarial networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Synthesizing face sketches from real photos and its inverse have many applications. However, photo/sketch synthesis remains a challenging problem due to the fact that photo and sketch have different characteristics. In this work, we consider this task as an image-to-image translation problem and explore the recently popular generative models (GANs) to generate high-quality realistic photos from sketches and sketches from photos. Recent GANbased methods have shown promising results on image-toimage translation problems and photo-to-sketch synthesis in particular, however, they are known to have limited abilities in generating high-resolution realistic images. To this end, we propose a novel synthesis framework called Photo-Sketch Synthesis using Multi-Adversarial Networks, (PS 2 -MAN) that iteratively generates low resolution to high resolution images in an adversarial way. The hidden layers of the generator are supervised to first generate lower resolution images followed by implicit refinement in the network to generate higher resolution images. Furthermore, since photosketch synthesis is a coupled/paired translation problem, we leverage the pair information using CycleGAN framework. Both Image Quality Assessment (IQA) and Photo-Sketch Matching experiments are conducted to demonstrate the superior performance of our framework in comparison to existing state-of-the-art solutions. Code available at: https://github.com/lidan1/PhotoSketchMAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Research on biometrics has made significant progress in the past few decades and face remains the most commonly studied biometric primarily due to convenience of data collection. In law enforcement and criminal cases, automatic retrieval of photos of suspects from the police mug shot database can enable the authorities to rapidly narrow down potential suspects <ref type="bibr" target="#b34">[35]</ref>. In practice, photos of suspects are usually hard to acquire and it is known that commercial softwares or experienced artists are sought to generate sketches of a suspect based on the description of eyewitness. Other than the applications in security, face photo-sketch synthesis also has several applications in digital entertainment. Photo sketches have also become increasingly popular among the users of smart phones and social networks where sketches are used as profile photos or avatars. Thus, photo-sketch synthesis and matching are important and practical problems.</p><p>Earlier studies on face photo-sketch matching have focused on directly matching photos to sketches and vice versa <ref type="bibr" target="#b34">[35]</ref>. However, due to differences in style and appearance of photo and sketch, it is not practical to directly perform matching between the two modalities. A common approach to reduce this domain gap between photo and sketch is to perform face photo-synthesis technique prior to matching. Several algorithms are proposed on this topic in the literature. Existing approaches can be generally classified into four categories based on the types of sketches used <ref type="bibr" target="#b21">[22]</ref>: (i) hand-drawn viewed sketch <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b47">[48]</ref>, (ii) hand-drawn semi-forensic sketch <ref type="bibr" target="#b18">[19]</ref>, (iii) hand-drawn forensic sketch <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and (iv) softwaregenerated composite sketch <ref type="bibr" target="#b8">[9]</ref>.</p><p>Several works have successfully exploited Convolutional Neural Networks (CNNs) to perform different image-to-image translation tasks. Recently, generative models such as Generative Adversarial Networks (GANs) <ref type="bibr" target="#b7">[8]</ref> and Variational Auto-Encoders (VAE) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b25">[26]</ref> have been more successful in such tasks due to their powerful generative abilities. In particular, GANs <ref type="bibr" target="#b7">[8]</ref> have achieved impressive results in image generation, image editing and representation learning <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Recent studies also adopt the original method for conditional image generation tasks such as image-to-image translation <ref type="bibr" target="#b10">[11]</ref>. While Isola et al.( <ref type="bibr" target="#b10">[11]</ref>) considered paired data for learning the image-to-image translation, Zhu et al. <ref type="bibr" target="#b48">[49]</ref> and Yi et al. <ref type="bibr" target="#b38">[39]</ref> separately proposed unsupervised imageto-image translation methods without the use of paired data. Similar to <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b48">[49]</ref>, in this work, face photo-sketch synthesis is considered as an image-to-image translation task. In fact, Yi et al. <ref type="bibr" target="#b38">[39]</ref> presented some preliminary results specifically for photo-sketch synthesis. On evaluating these methods in detail for our task, it was found that they had limitations in generating higher resolution images (as shown in <ref type="figure" target="#fig_0">Fig.1</ref>). As argued in <ref type="bibr" target="#b41">[42]</ref>, it is difficult to train GANs to generate high-resolution realistic images as they tend to generate images with artifacts. This is attributed to the fact that as pixel space dimension increases, the overlap between natural distribution of images and learned model distribution reduces. To overcome this issue, a novel high-quality face photo-sketch synthesis framework based on GANs is proposed. Since in our task, both photo-to-sketch synthesis and sketch-tophoto synthesis have practical applications, we adopt the recently introduced CycleGAN <ref type="bibr" target="#b48">[49]</ref> framework. Similar to <ref type="bibr" target="#b48">[49]</ref>, the proposed method has two generators G A and G B which generate sketch from photo and photo from sketch, respectively. In contrast to <ref type="bibr" target="#b48">[49]</ref>, two major differences can be noted: 1) To address the issue of artifacts in highresolution image generation, we propose the use of multidiscriminator network. 2) While CycleGAN uses only cycle-consistency loss, we additionally use L 1 reconstruction error between generated output and target image. The use of additional loss functions behaves as a regularization during the learning process.</p><p>Existing GANs use generators that are constructed similar to encoder-decoder style where the input image is first forwarded through a series of convolutions, nonlinearities and max-pooling resulting in lower resolution feature maps which are then forwarded through a series of deconvolutions and non-linearities. Noting that the deconvolutions iteratively learn the weights to upsample the feature maps, this implicit presence of feature maps at different resolutions is leveraged in this work by applying adversarial supervision at every level of resolution. Specifically, the feature maps at every deconvolution layer are convolved using 3 × 3 convolutions to produce outputs at different resolutions (3 in particular). A discriminator network is introduced at every resolution. By doing so, supervision is provided directly to hidden layers of the network which will enable iterative refinement of the feature maps and hence the output image. To summarize, this paper makes the following contributions:</p><p>• A novel face photo-sketch synthesis framework based on GANs involving multi-adversarial networks where adversarial supervision is provided to hidden layers of the network. • While <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b26">[27]</ref> present generic adversarial methods to perform image-to-image translation and show some preliminary results on face photo-sketch synthesis, to the best of our knowledge, ours is the first work to study in detail the use of adversarial networks specifically for face photo-sketch synthesis. • Detailed experiments are conducted to demonstrate improvements in the synthesis results. Further, ablation studies are conducted to verify the effectiveness of iterative synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, previous work on face photo-sketch synthesis and generative modeling techniques that are applied for image-to-image translation tasks are reviewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Face photo-sketch synthesis</head><p>Existing works can be categorized based on multiple factors. Wang et al. <ref type="bibr" target="#b33">[34]</ref> categorize photo-sketch synthesis methods based on model construction techniques into three main classes: 1) subspace learning-based, 2) sparse representation-based, and 3) Bayesian inference-based approaches. Peng et al. <ref type="bibr" target="#b19">[20]</ref> perform the categorization based on representation strategies and come up with three broad approaches: 1) holistic image-based, 2) independent local patch-based, and 3) local patch with spatial constraintsbased methods.</p><p>Subspace learning based methods involve the use of linear and non-linear subspace methods such as Principal Component Analysis (PCA) and Local Linear Embedding (LLE). Tang and Wang <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b28">[29]</ref> assume linear mapping between photo and sketch and synthesized the sketch by taking a linear combination of the Eigen vectors of sketch images. Finding that the assumption of linear mapping to be unreasonable, Liu et al. <ref type="bibr" target="#b15">[16]</ref> proposed a non-linear method based on LLE where they perform a patch-based sketch synthesis. The input photo image is divided into overlapping patches and transformed to corresponding sketch patches using the LLE method. The whole sketch image is then obtained by averaging the overlapping areas between neighboring sketch patches. However, it leads to blurring effect and ignores the neighboring relationships among the patches and thus is unable to take advantage of global structure. This work was extended by Wang et al. <ref type="bibr" target="#b31">[32]</ref>, Gao et al. <ref type="bibr" target="#b5">[6]</ref> and Change et al. <ref type="bibr" target="#b0">[1]</ref> using sparse representation-based techniques. In a different approach, several methods were developed using Bayesian inference techniques. Gao et al. <ref type="bibr" target="#b6">[7]</ref> and Xiao et al. <ref type="bibr" target="#b37">[38]</ref> employed Hidden Markov Model (HMMs) to model non-linear relationship between sketches and photos. Wang and Tang <ref type="bibr" target="#b34">[35]</ref> proposed Markov Random Field (MRF) based technique to incorporate relationship among neighboring patches. Zhou et al. <ref type="bibr" target="#b47">[48]</ref> improved over <ref type="bibr" target="#b34">[35]</ref> by proposing Markov weight fields (MWF) model that is capable of synthesizing new target patches not existing in the training set. Wang et al. <ref type="bibr" target="#b32">[33]</ref> proposed a novel face sketch synthesis method based on transductive learning.</p><p>More recently, Peng et al. <ref type="bibr" target="#b20">[21]</ref> proposed a multiple representations-based face sketch photo-synthesis method that adaptively combines multiple representations to represent an image patch by combining multiple features from face images processed using multiple filters. Additionally, they employ Markov networks to model the relationship between neighboring patches. Zhang et al. <ref type="bibr" target="#b44">[45]</ref> employed a sparse representation-based greedy search strategy to first estimate an initial sketch. Candidate image patches from the initial estimated sketch and the template sketch are then selected using multi-scale features. These candidate patches are refined and assembled to obtain the final sketch which is further enhanced using a cascaded regression strategy. Peng et al. <ref type="bibr" target="#b19">[20]</ref> proposed a superpixelbased synthesis method involving two stage synthesis procedure. Wang et al. <ref type="bibr" target="#b30">[31]</ref> recently proposed the use of Bayesian framework consisting of neighbor selection model and weight computation model. They consider spatial neighboring constraint between adjacent image patches for both models in contrast to existing methods where the adjacency constraint is considered for only one of the models. CNN-based method such as <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b1">[2]</ref> were proposed recently showing promising results. There is also a recent work on face synthesis from facial attribute <ref type="bibr" target="#b2">[3]</ref> applying sketch to photo synthesis as a second stage in their approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Image-to-image translation</head><p>In contrast to the traditional methods for photo-sketch synthesis, several researchers have exploited the success of CNNs for synthesis and cross-domain photo-sketch recognition. Face photo-sketch synthesis is considered as an image-to-image translation problem. Zhang et al. <ref type="bibr" target="#b42">[43]</ref> proposed an end-to-end fully convolutional network-based photo-sketch synthesis method. Several methods have been developed for related tasks such as general sketch synthesis <ref type="bibr" target="#b26">[27]</ref>, photo-caricature translation <ref type="bibr" target="#b46">[47]</ref> and creation of parameterized avatars <ref type="bibr" target="#b36">[37]</ref>.</p><p>In this work, we explore generative modeling techniques which have been highly successful for several imageto-image translation tasks. GANs <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b48">[49]</ref> and VAEs <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b12">[13]</ref> are two recently popular classes of generative techniques. GANs <ref type="bibr" target="#b7">[8]</ref> are used to synthesize realistic images by learning the distribution of training images. GANs, motivated by game theory, consist of two competing networks: generator G and discriminator D. The goal of GAN is to train G to produce samples from training distribution such that the synthesized samples are indistinguishable from actual distribution by discriminator D. In another variant called Conditional GAN , the generator is conditioned on additional variables such as discrete labels, text and images <ref type="bibr" target="#b10">[11]</ref>. Recently, several variants based on original GAN have been proposed for image-to-image translation tasks. Isola et al. <ref type="bibr" target="#b10">[11]</ref> proposed Conditional GANs for several tasks such as labels to street scenes, labels to facades, image colorization, etc. In an another variant, Zhu et al. <ref type="bibr" target="#b48">[49]</ref> proposed CycleGAN that learns image-to-image translation in an unsupervised fashion. Similar to the above approach, Yi et al. <ref type="bibr" target="#b38">[39]</ref> proposed an unsupervised method to perform translation tasks based on unpaired data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, the problem formulation is presented followed by a detailed description of the proposed method PS 2 -MAN. Also, details of generator's and discriminator's network architecture are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Formulation</head><p>Given a dataset (D) consisting of a set of face photosketch pairs represented by</p><formula xml:id="formula_0">{(A i , B i )} N i=1</formula><p>, the goal of photo-sketch synthesis is to learn two functions: (1) B = f ps (A) that represents photo (A) to sketch (B) synthesis and (2) A = f sp (B) that represents sketch (B) to photo (A) synthesis. In this work, we consider this problem as an image-to-image translation task. Since both forward (photo to sketch) and inverse (sketch to photo) transformations are of equal practical importance, this problem can be easily accommodated into the CycleGAN <ref type="bibr" target="#b48">[49]</ref> framework. Similar to <ref type="bibr" target="#b48">[49]</ref>, the proposed method consists of two generator sub-networks G A and G B which transform from photo to sketch and from sketch to photo, respectively. G A takes in a real face photo image R A as input and produces synthesized (fake) sketch F B as output. The aim of G B is to transform sketch to photo, hence, it should transform F B back to input photo itself, which we represent as Rec A here. Thus, the general process can be expressed as:</p><formula xml:id="formula_1">F B = G A (R A ), Rec A = G B (F B ).<label>(1)</label></formula><p>Similarly, sketch to photo generation can be expressed as:</p><formula xml:id="formula_2">F A = G B (R B ), Rec B = G A (F A ),<label>(2)</label></formula><p>where R B , F A and Rec B are real sketch, synthesized (fake) photo, and reconstructed sketch from fake photo, respectively. Note that in the following context, the term "fake" is same as "synthesized".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Objective</head><p>As in GAN framework <ref type="bibr" target="#b7">[8]</ref>, the generators (G A and G B ) are trained using adversarial losses that come from discriminator sub-networks. The goal of the generator sub-networks is to produce images that are as realistic as possible so as to fool the discriminator sub-networks, where as the goal of the discriminator sub-networks is to learn to classify between generated and real samples. The use of adversarial loss is known to overcome the issue of blurred outputs that is often encountered when only L1 or L2 loss is minimized <ref type="bibr" target="#b10">[11]</ref>. In theory, GANs can learn a mapping that produce outputs identically distributed as target domain and although generic image-to-image translation GANs have been successful in generating visually appealing results, they tend to produce artifacts in the output (as shown in <ref type="figure" target="#fig_0">Fig 1)</ref> which adversely affects the face/sketch matching performance. Hence, it is crucial to generate outputs that are free from artifacts.</p><p>As discussed in <ref type="bibr" target="#b41">[42]</ref>, these artifacts arise due to known training instabilities while generating high-resolution images. These instabilities are potentially caused due to the fact that the supports of natural image distribution and implied model distribution may not overlap in highdimensional space. The severity of this problem increases with an increase in the image resolution. Thus, to avoid these artifacts while generating realistic images, we propose a stage-by-stage multi-scale refinement framework by leveraging the implicit presence of features maps of different resolutions in the generator sub-network. Considering that most GAN frameworks have generators similar to encoder-decoder style with a stack of convolutional and max-pooling layers followed by a series of deconvolution layers. The deconvolution layers sequentially upsample the feature maps from lower resolution to higher resolution. Feature maps from every deconvolutional layers are <ref type="figure">Figure 2</ref>: Network structure of the proposed PS 2 -MAN framework. Adversarial supervision is provided through multiple discriminators at the hidden layers. Note that, in addition to adversarial loss, cycle-consistency and L1-loss are also used to train the network. However, for the purpose of illustration, we show only adversarial loss in this <ref type="figure">figure.</ref> forwarded through 3 × 3 convolutional layer to generate output images at different resolutions. As shown in <ref type="figure">Fig 2,</ref> output images are generated at three resolution levels: 64 × 64, 128 × 128 and 256 × 256 for both generators G A and G B . Further, three separate discriminator subnetworks are employed to provide adversarial feedback to the generators. By doing so, we are providing supervision directly to hidden layers of the network which will enable implicit iterative refinement of the feature maps resulting in high-quality synthesis. For simplicity, images at different resolutions are represented as: Thus, as shown in <ref type="figure">Fig. 2</ref>, for a photo image R A , G A generates {F B 1 , F B 2 , F B 3 } as outputs. Then F B 3 , which is the output at the last deconvolution layer, is sent as input to G B resulting in three reconstructions {Rec A 1 ,Rec A 2 ,Rec A 3 }. Similarly, for a sketch input, G B will output {F A 1 ,F A 2 ,F A 3 }. And G A will produce {Rec B 1 ,Rec B 2 ,Rec B 3 } by taking F A 3 as input. We then add supervision at these different outputs to force outputs to be closer to targets at different resolution levels. Three discriminators are defined for each generator: D A64 , D A128 , D A256 for G A and D B64 , D B128 , D B256 for G B , which are applied on deconvolution layers with resolutions of 64 × 64, 128 × 128 and 256 × 256, respectively. The objective function is expressed as:</p><formula xml:id="formula_3">R A i , F A i , Rec A i , R B i , F B i ,</formula><formula xml:id="formula_4">L GAN A i =E B i ∼p data (B i ) [log D A i (B i )] + E A∼p data (A) [log(1 − D A i (G A (R A )) i )],<label>(3)</label></formula><p>and</p><formula xml:id="formula_5">L GAN B i =E A i ∼p data (A i ) [log D A i (A i )] + E B∼p data (B) [log(1 − D B i (G B (R B )) i )],<label>(4)</label></formula><p>where (G A (R A )) i = F B i , (G B (R B )) i = F A i and i = 1, 2, 3 corresponds to discriminators at different levels.</p><p>To generate images which are as close to target images as possible, we also minimize synthesis error L syn which is defined as the L 1 difference between synthesized image and corresponding target image. Similar to adversarial loss, L syn is minimized for all three resolution levels and is defined as:</p><formula xml:id="formula_6">L syn A i = F A i − R A i 1 = G B (R B ) i − R A i 1 L syn B i = F B i − R B i 1 = G A (R A ) i − R B i 1 .<label>(5)</label></formula><p>In spite of using L syn and the adversarial loss, as discussed in <ref type="bibr" target="#b48">[49]</ref>, we may have many mappings due to the large capacity of networks. Hence, similar to <ref type="bibr" target="#b48">[49]</ref>, the network is additionally regularized using forwardbackward consistency thereby reducing the space of possible mapping functions. This is achieved by introducing cycle consistency losses at different resolution stages, which are defined as:</p><formula xml:id="formula_7">L cyc A i = Rec A i − R A i 1 = G B (G A (R A )) i − R A i 1 L cyc B i = Rec B i − R B i 1 = G A (G B (R B )) i − R B i 1 .<label>(6)</label></formula><p>The final objective function is defined as:</p><formula xml:id="formula_8">L(G A , G B , D A , D B ) = 3 ∑ i=1 (L GAN A i + L GAN B i + λ A i L syn A i + λ B i L syn B i + η A i L cyc A i + η B i L cyc B i ).</formula><p>To summarize, the final objective function is constructed using L 1 error between synthesized and target images, adversarial loss and cycle-consistency loss. L 1 error enables the network to synthesize images that are closer to the target, however, they often result in blurry images. Adversarial loss overcomes this issue thereby resulting in relatively sharper images. However, the use of adversarial loss at the final stage results in artifacts, which we overcome by providing supervision to the hidden layers. Cycle-consistency loss provides additional regularization while learning the network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture</head><p>The generator sub-networks are constructed using stride-2 convolutions, residual blocks <ref type="bibr" target="#b9">[10]</ref> and fractionally strided convolutional layers. The network configuration is specified as follows:</p><p>C7S1-64, C3-128, C3-256, RB256×9, TC64, TC32, C7S1-3, where, C7S1 − k denotes 7 × 7 Convolution-BatchNormReLU layer with k filters and stride 1, Ck denotes a 3 × 3 Convolution-BatchNorm-ReLU layer with k filters, and stride 2, RBk × m denotes m residual block that contains two 3 × 3 convolutional layers with the same number of filters on both layers, TC denotes a 3 × 3 Transposed-Convolution-BatchNorm-ReLU layer with k filters and stride <ref type="bibr">1 2</ref> . The discriminator networks are constructed using 70 × 70 PatchGANs <ref type="bibr" target="#b10">[11]</ref> that classify whether 70 × 70 overlapping image patches are real or fake. The network configuration is specified as: C64-C128-C256-C512, where Ck denotes a 4 × 4 Convolution-BatchNorm-LeakyReLU layer with k filters and stride 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, experimental settings and evaluation of the proposed method are discussed in detail. We present the qualitative and quantitative results of the proposed method on two popular datasets: CUFS <ref type="bibr" target="#b34">[35]</ref> and CUFSF <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b45">[46]</ref>. In addition to performance comparison with state-of-the-art methods, ablation studies are conducted to verify the effectiveness of various components of the proposed method. Both visual and quantitative comparisons are presented in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The proposed method is evaluated on existing viewed sketches datasets. CUHK Face Sketch database (CUFS) <ref type="bibr" target="#b34">[35]</ref> is a viewed sketch database which includes 188 faces from the Chinese University of Hong Kong (CUHK) student database, 123 faces from the AR database <ref type="bibr" target="#b16">[17]</ref>, and 295 faces from the XM2VTS database <ref type="bibr" target="#b17">[18]</ref>. For each face, there is a sketch drawn by an artist based on a photo taken in a frontal pose under normal lighting condition, and with a neutral expression.</p><p>CUFSF <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b45">[46]</ref> database includes 1,194 persons from the FERET database <ref type="bibr" target="#b24">[25]</ref>. For each person, there is a face photo with lighting variation and a sketch with shape exaggeration drawn by an artist when viewing this photo <ref type="bibr" target="#b45">[46]</ref>. This dataset is particularly challenging since the photos are taken under different illumination conditions and sketches have shape exaggeration as compared to photos, however, the dataset is closer to forensic sketch scenario.</p><p>Both datasets contain facial landmark coordinates which can be easily applied for alignments. There also exist several recent datasets without landmark information, recent face alignment algorithms such as <ref type="bibr" target="#b22">[23]</ref> can be applied in the preprocessing stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Details</head><p>During model training procedure, each input image is resized to the size of 256 × 256. Data augmentation is performed on the fly by adding random noise to input images. The network is trained from scratch, similar to the network initialization setup in <ref type="bibr" target="#b48">[49]</ref>, the learning rate is set to 0.0002 for the first 100 epochs, and linearly decaying down to 0 for next 100 epochs. λ i are all set to 1 and η i are all set to 0.7 in (III-B). Weights were initialized from a Gaussian distribution with mean 0 and standard deviation 0.02. The network is trained using the Adam solver <ref type="bibr" target="#b11">[12]</ref>. For the CUHK dataset, 188 face-sketch pairs are divided such that 60 pairs are used for training, 28 pairs for validation and 100 pairs for testing. We augmented training set by horizontally flipping images so that training set has 120 images in total. For the CUFSF dataset, 1194 image pairs are divided to 600 for training, 297 for validation and 297 for testing. All images are preprocessed by simply aligning center of the eyes to the fixed position and cropping to the size of 200 × 250. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>To demonstrate the advantage of our multi-adversarial network structure over the single adversarial approach, we compare the results of the following network configurations on the CUHK dataset:</p><p>• C − D 256 : Proposed method with single discriminator at the final resolution level (256 × 256). • C − D 256,128 : Proposed method with two discriminators at last two resolution levels (256 × 256 and 128 × 128). • C − D 256,128,64 : Proposed method with two discriminators at three resolution levels (256 ×256, 128×128 and 64 × 64). <ref type="figure" target="#fig_2">Fig. 3</ref> shows sample results from the above configurations on the CUHK dataset. It can be observed that the performance in terms of visual quality improves as more levels of supervision are added. Similar observations can be made using the quantitative measurements such as SSIM <ref type="bibr" target="#b35">[36]</ref> and FSIM <ref type="bibr" target="#b43">[44]</ref>) as shown in <ref type="table" target="#tab_0">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with state-of-the-art methods</head><p>In addition to ablation studies, the proposed method is compared with recent state-of-the-art photo-sketch synthe-    <ref type="bibr" target="#b47">[48]</ref>, MrFSS <ref type="bibr" target="#b20">[21]</ref>, Pix2Pix <ref type="bibr" target="#b10">[11]</ref>, CycleGAN <ref type="bibr" target="#b48">[49]</ref> and DualGAN <ref type="bibr" target="#b38">[39]</ref>. Sample sketch and photo synthesis results on the CUHK dataset are shown in <ref type="figure" target="#fig_3">Fig. 4</ref> and <ref type="figure" target="#fig_4">Fig. 5</ref>, respectively. It can be observed that MrFSS synthesis results in blurred outputs. The generative models (Pix2Pix, CycleGAN and DualGAN) overcome the blurred effect by using adversarial loss in addition to L1 loss. However, they tend to have undesirable artifacts due to instabilities in training while generating high-resolution images. In contrast, the proposed method (PS 2 -MAN) is able to preserve high-frequency details and minimize the artifacts simultaneously. Also, photo synthesis using CycleGAN results in color distortion. A potential reason is the lack of L1 loss while training the network. Hence, in our case, we use L1 reconstruction error between target and synthesized image to train the network, thus providing the network with further regularization. Sample sketch and photo synthesis results on the CUFSF dataset for the generative techniques are shown  in <ref type="figure" target="#fig_5">Fig. 6 and Fig. 7</ref>, respectively. The CUFSF dataset is particularly challenging since the sketches have overexaggerated features as compared to the ones present in the real photos. It can be observed that in case of both sketch and photo synthesis that the generative methods (Pix2Pix, CycleGAN and DualGAN) introduce undesirable artifacts especially at facial features resulting. In contrast, the proposed method is able to minimize the artifacts while generating realistic images as compared to the other methods. Similar to ablation studies, we also compare the results of all the above methods using quantitative measures (SSIM and FSIM) as shown in <ref type="table" target="#tab_0">Table II</ref>. The proposed method achieves the best results in terms of SSIM and FSIM as compared to the other methods. Additionally, the methods are also compared using photo-sketch face matching rates using two approaches: (1) Synthesize sketches from photos and used these synthesized sketches to match with real sketch gallery. (2) Synthesize photos from sketches and use these synthesized photos to match with real photos gallery. The matching rates were calculated by computing the LBP features and cosine distance. The matching rates using generative techniques on the CUHK and CUFSF datasets for various are illustrated in <ref type="figure">Fig. 8  and 9</ref> and respectively in terms of the Cumulative Matching Characteristic (CMC) curves. <ref type="table" target="#tab_0">Table III</ref> summarize the rank-1 matching rates. It can be observed from <ref type="figure">Fig. 8</ref> and 9 that the proposed method achieves best matching rates at all ranks.</p><p>To summarize, through various experiments it is demonstrated that the proposed method PS 2 -MAN is able to generate realistic results with minimal artifacts as compared to existing methods. This is mainly due to the multiadversarial network used in our approach. Additionally, the proposed method achieves significant improvements over the other techniques in terms of various quality measures (such as SSIM and FSIM ) and matching rates while generating visually appealing outputs. (a) (b) <ref type="figure">Figure 9</ref>: Matching rates using generative techniques on CUFSF dataset for different ranks (a) Photo matching rates (b) Sketch matching rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We explored the problem of photo-sketch synthesis using the recently introduced generative models. A novel synthesis method using multi-adversarial networks is presented. The proposed method is developed specifically to enable GANs to generate high resolution images. This is achieved by providing adversarial supervision to hidden layers of the generator sub-network. Additionally, the forward and backward synthesis are trained iteratively in the CycleGAN framework, i.e., in addition to minimizing L1 reconstruction error, cycle-consistency loss is also used in the objective function. These additional loss functions provide appropriate regularization thereby generating high-quality and high resolution photo-sketch synthesis. Evaluations are performed on two popular datasets and the results are compared with recent state-of-the-art generative methods. It is clearly demonstrated that the proposed method achieves significant improvements in terms of visual quality and photo/sketch matching rates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sample results on photo and sketch synthesis. Top Row: Photo Synthesis, Bottom Row: Sketch synthesis. (a) Input Image. (b) Synthesis using single stage adversarial network. (c) Synthesis using multi-stage adversarial network (proposed method). (d) Ground truth. Artifacts in (b) are marked with red rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and Rec B i , where i = 1, 2 and three corresponds to resolution of 64 × 64, 128 × 128 and final output size, which is 256 × 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Results of ablation study: (a) Input. (b) Ground truth. (c) C − D 256 . (d) C − D 256,128 . (e) C − D 256,128,64 . Row 1 and Row 2: Photo synthesis from sketch. Row 3 and Row 4: Sketch synthesis from photo. It can be observed from (e) that the artifacts are minimized and the results are more realistic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of photo to sketch synthesis results on the CUHK dataset. From top to bottom: Input, Ground truth, MrFSPS, Pix2Pix, DualGAN, CycleGAN and PS 2 -MAN. PS 2 -MAN has minimal artifacts while generating realistic and sharper images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of Sketch to photo synthesis results on the CUHK dataset. From top to bottom: Input, Ground truth, MrFSPS, Pix2Pix, DualsAN, CycleGAN and PS 2 -MAN. PS 2 -MAN has minimal artifacts while generating realistic and sharper images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of photo to sketch synthesis results on the CUHK dataset. From top to bottom: Input, Ground truth, Pix2Pix, DualGAN, CycleGAN and PS 2 -MAN. PS 2 -MAN has minimal artifacts while generating realistic and sharper images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of sketch to photo synthesis results on the CUHK dataset. From top to bottom: Input, Ground truth, Pix2Pix, DualGAN, CycleGAN and PS 2 -MAN. PS 2 -MAN has minimal artifacts while generating realistic and sharper images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>ABLATION STUDY: QUANTITATIVE RESULTS FOR PHOTO AND SKETCH SYNTHESIS FOR DIFFERENT CONFIGURATIONS ON CUHK DATASET</figDesc><table><row><cell></cell><cell>C − D 256</cell><cell>C − D 256,128</cell><cell>C − D 256,128,64</cell></row><row><cell>SSIM (Photo Synthesis)</cell><cell>0.7626</cell><cell>0.7851</cell><cell>0.7915</cell></row><row><cell>SSIM (Sketch Synthesis)</cell><cell>0.5991</cell><cell>0.6034</cell><cell>0.6156</cell></row><row><cell>FSIM (Photo Synthesis)</cell><cell>0.7826</cell><cell>0.7920</cell><cell>0.8062</cell></row><row><cell>FSIM (Sketch Synthesis)</cell><cell>0.7271</cell><cell>0.7280</cell><cell>0.7361</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II</head><label>II</label><figDesc></figDesc><table><row><cell cols="7">: PERFORMANCE COMPARISON: QUANTITATIVE RESULTS FOR</cell></row><row><cell cols="5">PHOTO AND SKETCH SYNTHESIS ON CUHK DATASET</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MWF</cell><cell cols="3">MrFSPS pix2pix CycleGAN</cell><cell>DualGAN</cell><cell>Ours</cell></row><row><cell>SSIM (Photo Synthesis)</cell><cell>0.6057</cell><cell>0.6326</cell><cell>0.6606</cell><cell>0.7626</cell><cell>0.7908</cell><cell>0.7915</cell></row><row><cell cols="2">SSIM (Sketch Synthesis) 0.4996</cell><cell>0.5130</cell><cell>0.4669</cell><cell>0.5991</cell><cell>0.6003</cell><cell>0.6156</cell></row><row><cell>FSIM (Photo Synthesis)</cell><cell>0.7996</cell><cell>0.8031</cell><cell>0.6997</cell><cell>0.7826</cell><cell>0.7939</cell><cell>0.8062</cell></row><row><cell cols="2">FSIM (Sketch Synthesis) 0.7121</cell><cell>0.7339</cell><cell>0.6174</cell><cell>0.7271</cell><cell>0.7312</cell><cell>0.7361</cell></row><row><cell cols="3">sis methods such as MWF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III :</head><label>III</label><figDesc>RANK-1 MATCHING RATES FOR GENERATIVE METHODS ON</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.995</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.95</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Matching Rate</cell><cell cols="2">0.98 0.985 0.99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Matching Rate</cell><cell cols="2">0.8 0.85 0.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CycleGan</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CycleGan</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.975</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pix2Pix DualGAN</cell><cell></cell><cell cols="2">0.75</cell><cell></cell><cell></cell><cell>Pix2Pix DualGAN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PS 2 -MAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PS 2 -MAN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Rank</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rank</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="18">Figure 8: Matching rates using generative techniques on</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="18">CUHK dataset for different ranks (a) Photo matching rates</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="13">(b) Sketch matching rates.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Matching Rate</cell><cell>0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Matching Rate</cell><cell>0.5 0.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CycleGan</cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell>CycleGan</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pix2Pix</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pix2Pix</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DualGAN</cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell>DualGAN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PS 2 -MAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PS 2 -MAN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell></cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>18</cell><cell>20</cell><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>18</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Rank</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Rank</cell></row><row><cell cols="3">CUHK AND CUFSF DATASETS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Photo/Sketch</cell><cell>Pix2Pix</cell><cell cols="3">CycleGAN DualGAN PS 2 -MAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CUHK</cell><cell>Photo Matching Sketch Matching</cell><cell>100 78</cell><cell>99 95</cell><cell>100 98</cell><cell>100 99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CUFSF</cell><cell>Photo Matching Sketch Matching</cell><cell>37 40</cell><cell>25 44</cell><cell>35 40</cell><cell>47 51</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face sketch synthesis via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2146" to="2149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face sketch synthesis with style transfer using pyramid column feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Face synthesis from visual attributes via sketch using conditional vaes and gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00077</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00962</idno>
		<title level="m">Gp-gan: Gender preserving gan for synthesizing faces from landmarks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Composition-aided sketch-realistic portrait generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00899</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face sketch-photo synthesis and retrieval using sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CSVT</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1213" to="1226" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face sketch synthesis algorithm based on e-hmm and selective ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CSVT</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="487" to="496" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Matching composite sketches to face photos: A component-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bonnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE IFS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="204" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matching forensic sketches to mug shot photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="639" to="646" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition using kernel prototype similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1410" to="1422" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A nonlinear approach for face sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2005</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1005" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The ar face database, cvc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Xm2vtsdb: The extended m2vts database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second international conference on audio and video-based biometric person authentication</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">964</biblScope>
			<biblScope unit="page" from="965" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Forgetmenot: memory-aware forensic facial sketch matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Superpixel-based face sketch-photo synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CSVT</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="288" to="299" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiple representations-based face sketch-photo synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE NNLS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2201" to="2215" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face recognition from multiple stylistic sketches: Scenarios, datasets, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Piefa: Personalized incremental and ensemble face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3880" to="3888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised multi-image-to-image translation using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abavisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09334</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The feret database and evaluation procedure for face-recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<title level="m">Stochastic backpropagation and approximate inference in deep generative models</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00835</idno>
		<title level="m">Scribbler: Controlling deep image synthesis with sketch and color</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face photo recognition using sketch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face sketch recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CSVT</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="57" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1264" to="1274" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Heterogeneous image transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transductive face sketch-photo synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE NNLS</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1364" to="1376" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comprehensive survey to face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="30" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Face photo-sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1955" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised creation of parameterized avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05693</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A new approach for face recognition by sketches in photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1576" to="1588" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Image deraining using a conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Joint transmission map estimation and dehazing using deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00581</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">End-toend photo-sketch generation via fully convolutional representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="627" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fsim: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust face sketch style synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="220" to="232" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Coupled informationtheoretic encoding for face photo-sketch recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="513" to="520" />
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Photo-to-caricature translation on faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10735</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Markov weight fields for face sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1091" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
