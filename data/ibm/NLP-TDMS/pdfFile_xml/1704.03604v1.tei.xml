<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance-Level Salient Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<email>liguanbin@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
							<email>yizhouy@acm.org</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance-Level Salient Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image saliency detection has recently witnessed rapid progress due to deep convolutional neural networks. However, none of the existing methods is able to identify object instances in the detected salient regions. In this paper, we present a salient instance segmentation method that produces a saliency mask with distinct object instance labels for an input image. Our method consists of three steps, estimating saliency map, detecting salient object contours and identifying salient object instances. For the first two steps, we propose a multiscale saliency refinement network, which generates high-quality salient region masks and salient object contours. Once integrated with multiscale combinatorial grouping and a MAP-based subset optimization framework, our method can generate very promising salient object instance segmentation results. To promote further research and evaluation of salient instance segmentation, we also construct a new database of 1000 images and their pixelwise salient instance annotations. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks for salient region detection as well as on our new dataset for salient instance segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Salient object detection attempts to locate the most noticeable and eye-attracting object regions in images. It is a fundamental problem in computer vision and has served as a pre-processing step to facilitate a wide range of vision applications including content-aware image editing <ref type="bibr" target="#b3">[4]</ref>, object detection <ref type="bibr" target="#b37">[38]</ref>, and video summarization <ref type="bibr" target="#b35">[36]</ref>.</p><p>Recently the accuracy of salient object detection has been improved rapidly <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref> due to the deployment of deep convolutional neural networks. Nevertheless, most of previous methods are only designed to detect pixels that belong to any salient object, i.e. a dense saliency map, but are unaware of individual instances of salient objects. We refer to the task performed by these methods "salient region detection", as in <ref type="bibr" target="#b50">[51]</ref>. In this paper, we tackle a more challenging task, instance-level salient object segmentation (or salient instance segmentation for short), which aims to identify individual object instances in the detected salient regions <ref type="figure" target="#fig_0">(Fig. 1)</ref>. The next generation of salient object detection methods need to perform more detailed parsing within detected salient regions to achieve this goal, which is crucial for practical applications, including image captioning <ref type="bibr" target="#b24">[25]</ref>, multilabel image recognition <ref type="bibr" target="#b45">[46]</ref> as well as various weakly supervised or unsupervised learning scenarios <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>We suggest to decompose the salient instance segmentation task into the following three sub-tasks. 1) Estimating binary saliency map. In this sub-task, a pixel-level saliency mask is predicted, indicating salient regions in the input image. 2) Detecting salient object contours. In this sub-task, we perform contour detection for individual salient object instances. Such contour detection is expected to suppress spurious boundaries other than object contours and guide the generation of salient object proposals. 3) Identifying salient object instances. In this sub-task, salient object proposals are generated, and a small subset of salient object proposals are selected to best cover the salient regions. Finally, a CRF based refinement method is applied to improve the spatial coherence of salient object instances.</p><p>A number of recent papers have explored the use of fully convolutional neural networks for saliency mask generation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref>. Though these methods are efficient and can produce favorable results, they have their own limitations. Most of these methods infer saliency by learning con-trast from the internal multi-layer structure of a single VGG network <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b32">33]</ref>. As their output is derived from receptive fields with a uniform size, they may not perform well on images with salient objects at multiple different scales. Though Li et al. <ref type="bibr" target="#b29">[30]</ref> combined a multiscale fully convolutional network and a segment-level spatial pooling stream to make up for this deficiency, the resolution of their final saliency map is only one eighth of the resolution of the original input image, making it infeasible to accurately detect the contours of small salient object instances.</p><p>Given the aforementioned sub-tasks of salient instance segmentation, we propose a deep multiscale saliency refinement network, which can generate very accurate results for both salient region detection and object contour detection. Our deep network consists of three parallel streams processing scaled versions of the same input image and a learned attention model to fuse results at different scales from the three streams. The three streams share the same network architecture, a refined VGG network, and its associated parameters. This refined VGG network is designed to integrate the bottom-up and top-down information in the original network. Such information integration is paramount for both salient region detection <ref type="bibr" target="#b5">[6]</ref> and contour detection <ref type="bibr" target="#b4">[5]</ref>. The attention model in our deep network is jointly trained with the refined VGG network in the three streams.</p><p>Given the detected contours of salient object instances, we apply multiscale combinatorial grouping (MCG) <ref type="bibr" target="#b2">[3]</ref> to generate a number of salient object proposals. Though the generated object proposals are of high quality, they are still noisy and tend to have severe overlap. We further filter out noisy or overlapping proposals and produce a compact set of segmented salient object instances. Finally, a fully connected CRF model is employed to improve spatial coherence and contour localization in the initial salient instance segmentation.</p><p>In summary, this paper has the following contributions:</p><p>• We develop a fully convolutional multiscale refinement network, called MSRNet, for salient region detection.</p><p>MSRNet can not only integrate bottom-up and top-down information for saliency inference but also attentionally determine the pixel-level weight of each salient map by looking at different scaled versions of the same image. The proposed network can achieve significantly higher precision in salient region detection than previous methods.</p><p>• MSRNet generalizes well to salient object contour detection, making it possible to separate distinct object instances in detected salient regions. When integrated with object proposal generation and screening techniques, our method can generate high-quality segmented salient object instances.</p><p>• A new challenging dataset is created for further research and evaluation of salient instance segmentation. We have generated benchmark results for salient contour detection as well as salient instance segmentation using MSRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, deep convolutional neural networks have achieved great successes in computer vision topics such as image classification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b20">21]</ref>, object detection <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41]</ref> and semantic segmentation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b6">7]</ref>. In this section, we discuss the most relevant work on salient region detection, object proposal generation and instance-aware semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Salient Region Detection</head><p>Traditional saliency detection can be divided into bottom-up methods based on low-level features <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b9">10]</ref> and top-down methods incorporating high-level knowledge <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22]</ref>. Recently, deep CNNs have pushed the research on salient region detection into a new phase. Deep CNN based methods can be divided into two categories, segmentation or patch based methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">52]</ref> and endto-end saliency inference methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref>. Methods in the former category treat image patches as independent training and testing samples, and are generally inefficient due to redundancy among overlapping patches. To overcome this deficiency, deep end-to-end networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref> have been developed for saliency inference. Most recently, recurrent neural networks have also been integrated into such networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref>. Though these end-to-end networks improve both accuracy and efficiency, all of them consider a single scale of the input image and may not perform well on images with object instances at multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object Proposals</head><p>Object proposal generation aims at localizing target objects with a minimum number of object window (or segment) hypotheses. Previous work on this topic can be grouped into two approaches. The first produces a list of object proposal windows, ranked by a measure of objectness (the probability of an image window containing an object) <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b10">11]</ref> while the other generates object proposals by merging image segments resulting from multiple levels of segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref>. Though they have been widely used as a foregoing step for object detection, they are not tailored for salient object localization. Though Feng et al. <ref type="bibr" target="#b15">[16]</ref> proposed to generate a ranked list of salient object proposals, the overall quality of their result needs much improvement. Recently, Zhang et al. <ref type="bibr" target="#b50">[51]</ref> proposed a MAP-based subset optimization formulation to optimize both the number and locations of detection windows given a set of salient object proposals. However, due to the coarse mechanism they use, their "filtered" object windows cannot well match groundtruth objects. In this paper, we generate salient ob- ject proposals on the basis of salient object contour detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Instance-Aware Semantic Segmentation</head><p>Instance-aware semantic segmentation is defined as a unified task of object detection and semantic segmentation. This problem was first raised in <ref type="bibr" target="#b19">[20]</ref>, and has been much studied in recent years. It is either formulated as a multitask learning problem <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b12">13]</ref> or solved in an end-to-end integrated model <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b11">12]</ref>. Inspired by this problem, we propose salient instance segmentation, which simultaneously detects salient regions and identifies object instances inside them. Because salient object detection is not associated with a predefined set of semantic categories, it is a challenging problem closely related to generic object detection and segmentation. We believe solutions to such generic problems are valuable in practice as it is not possible to enumerate all object categories and prepare pixel-level training data for each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Salient Instance Segmentation</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our method for salient instance segmentation consists of four cascaded components, including salient region detection, salient object contour detection, salient instance generation and salient instance refinement. Specifically, we propose a deep multiscale refinement network and apply it to both salient region detection and salient object contour detection. Next, we generate a fixed number of salient object proposals on the basis of the results of salient object contour detection and apply a subset optimization method for further screening these object proposals. Finally, the results from the previous three steps are integrated in a CRF model to generate the final salient instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multiscale Refinement Network</head><p>We formulate both salient region detection and salient object contour detection as a binary pixel labeling problem. Fully convolutional networks have been widely used in image labeling problems and have achieved great successes in salient region detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref> and object contour detection <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b49">50]</ref>. However, none of them addresses these two problems in a unified network architecture. Since salient objects could have different scales, we propose a multiscale refinement network (MSRNet) for both salient region detection and salient object contour detection. MSRNet is composed of three refined VGG network streams with shared parameters and a learned attentional model for fusing results at different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Refined VGG Network</head><p>Salient region detection and salient object contour detection are closely related and both of them require low-level cues as well as high-level semantic information. Information from an input image needs to be passed from the bottom layers up in a deep network before being transformed into high-level semantic information. Meanwhile, such highlevel semantic information also needs to be passed from the top layers down and further integrated with high-resolution low-level cues, such as colors and textures, to produce highprecision region and contour detection results. Therefore, a network should consider both bottom-up and top-down information propagation and output a label map with the same resolution as the input image. We propose a refined VGG network architecture to achieve this goal. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the refined VGG network is essentially a VGG network augmented with a top-down refinement process.</p><p>We transform the original VGG16 into a fully convolutional network, which serves as our bottom-up backbone network. The two fully connected layers of VGG16 are first converted into convolutional layers with 1 × 1 kernels as described in <ref type="bibr" target="#b34">[35]</ref>. We also skip subsampling in the last two pooling layers to make the bottom-up feature map denser and replace the convolutional layers after the penultimate pooling layer with atrous convolution in order to retain the original receptive field of the filters. Thus the output resolution of the transformed VGG network is 1/8 of the original input resolution.</p><p>To augment the backbone network with a top-down refinement stream, we first attach one extra convolutional layer to each of the five max-pooling layers of VGG16. Each extra layer has 3 × 3 kernels and 64 channels which play a role in dimension reduction. Inspired by <ref type="bibr" target="#b39">[40]</ref>, we integrate a "refinement module" R to invert the effect of each pooling layer and double the resolution of its input feature map if necessary. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the refinement stream consists of five stacked refinement modules, each of which corresponds to one pooling layer in the backbone network. Each refinement module R i takes as input the output feature map F i td of the previous refinement module in the top-down pass along with the output feature map F i bu of the aforementioned extra convolutional layer attached to the corresponding pooling layer in the bottom-up pass. It learns to merge the information from these inputs to produce a new</p><formula xml:id="formula_0">feature map F i+1 td , i.e. F i+1 td = R i (F i td , F i bu ).</formula><p>The refinement module R i works by first concatenating F i td and F i bu and then feeding them to another 3 × 3 convolutional layer with 64 channels. Finally, an up-sampling layer is optionally added to double the spatial resolution to guarantee that F i td and F i bu have the same spatial resolution. Specifically, an up-sampling layer is added in each refinement module corresponding to any of the first three pooling layers in the bottom-up pass. We denote a refinement operation without up-sampling as R A and that with up-sampling as R B . Note that F 1 td is the output feature map encoding from the last layer of the backbone network and serves as the input to the entire top-down refinement stream. The final output of the refinement stream is a probability map with the same resolution as the original input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Multiscale Fusion with Attentional Weights</head><p>As it has been widely confirmed that feeding multiple scales of an input image to networks with shared parameters are beneficial for accurately localizing objects of different scales in pixel labeling problems <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref>, we replicate the refined VGG network in the previous section three times, each responsible for one of the scales. An input image is resized to three different scales (s ∈ {1, 0.75, 0.5}). Each scale s of the input image passes through one of the three replicated refined VGG networks, and comes out as a two-channel probability map in the resolution of scale s, denoted as M s c , where c ∈ {0, 1} denotes the two classes for saliency detection. The three probability maps are resized to the same resolution as the raw input image using bilinear interpolation. The final output from our MSRNet is computed as a weighted sum of the three probability maps in a pixelwise manner, which means the weights for the probabilistic scores at a pixel are not fixed but spatially varying. Let F c be the fused probability map of class c and W s be the weight map for scale s. The fused map is calculated by summing the elementwise multiplication between each probability map and its corresponding weight map:</p><formula xml:id="formula_1">F c = Σ s∈{1,0.75,0.5} W s M s c .<label>(1)</label></formula><p>We call W s attentional weights as in <ref type="bibr" target="#b18">[19]</ref> because it reflects how much attention should be paid to features at different spatial locations and image scales. These spatially varying attentional weights can be viewed as probability maps themselves and can be learned in a fully convolutional network as well. We simultaneously learn attentional weights along with saliency maps by adding an attention module to our MSRNet. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the attention module takes as input the concatenation of three output feature maps of the penultimate layers in the three top-down refinement streams, and it consists of two convolutional layers for attentional weight inference. The first convolutional layer has 512 channels with 3 × 3 kernels and the second layer has three channels with 1 × 1 kernels. Each of the three channels in the output feature map corresponds to attentional weights for one of the three scales. Thus the attention module learns a soft weight for each spatial location and each scale. As the convolutions and elementwise multiplications in our attention module are differentiable, they allow the gradient of the loss function to be propagated through. Therefore, the attention module can be jointly trained in our MSRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Multiscale Refinement Network Training</head><p>We train two deep models based on the same multiscale refinement network architecture to perform two subtasks, salient region detection and salient object contour detection. These subtasks have separate training sets. As the number of training images for salient contour detection is much smaller, in practice, we first train a network for salient region detection. A duplicate of this trained network is further fine-tuned for salient contour detection. The loss functions of these two subtasks have different weights for sample balance. As the number of "contour" and "non-contour" pixels are extremely imbalanced in each training batch for salient object contour detection, the penalty for misclassifying "contour" pixels is 10 times the penalty for misclassifying "non-contour" pixels while, for salient region detection, the penalty for misclassifying "salient" pixels is twice the penalty for misclassifying "non-salient" pixels. When training MSRNet for salient region detection, we initialize the bottom-up backbone network with a VGG16 network pretrained on ImageNet and the top-down refinement stream with random values. We jointly fine-tune the three refined VGG networks in MSRNet, and their shared parameters are optimized using standard stochastic gradient descent. The learning rate for the backbone networks is set to 10 −4 while that for other newly added layers is set to 10 −3 . To save memory and increase the mini-batch size, we fix the resolution of training images to 320 × 320. However, as MSRNet is a fully convolutional network, it can take an image of any size as the input and produce a saliency map with the same resolution as the input during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Salient Instance Proposal</head><p>We choose the multiscale combinatorial grouping (MCG) algorithm <ref type="bibr" target="#b2">[3]</ref> to generate salient object proposals from the detected salient object contours. MCG is a unified approach for bottom-up hierarchical image segmentation and object candidate generation. We simply replace the contour detector gPb in MCG with our MSRNet based salient object contour detector. Specifically, given an input image, we first generate four salient object contour maps (three from scaled versions of the input and one from the fused map). Each of these four contour maps is used to generate a distinct hierarchical image segmentation represented as an ultrametric contour map (UCM). These four hierarchies are aligned and combined into a single hierarchical segmentation, and a ranked list of object proposals are obtained as in <ref type="bibr" target="#b2">[3]</ref>.</p><p>To ensure a high recall rate of salient object instances, we generate 800 salient object proposals for any given image. We discard those proposals with fewer than 80% salient pixels to guarantee that any remaining proposal mostly resides inside a detected salient region. Given the set of initially screened salient object proposals, we further apply a MAPbased subset optimization method proposed in <ref type="bibr" target="#b50">[51]</ref> to produce a compact set of object proposals. The number of remaining object proposals in the compact set forms the final number of predicted salient object instances in the image. We call each remaining salient object proposal a detected salient instance. We can easily obtain an initial result for salient instance segmentation by labeling the pixels in each salient instance with a unique instance id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Refinement of Salient Instance Segmentation</head><p>As salient object proposals and salient regions are obtained independently, there exist discrepancies between the union of all detected salient instances and the union of all detected salient regions. In this section, we propose a fully connected CRF model to refine the initial salient instance segmentation result.</p><p>Suppose the number of salient instances is K. We treat the background as the K + 1 st class, and cast salient instance segmentation as a multi-class labeling problem. At the end, every pixel is assigned with one of the K + 1 labels using a CRF model. To achieve this goal, we first define a probability map with K + 1 channels, each of which corresponds to the probability of the spatial location being assigned with one of the K + 1 labels. If a salient pixel is covered by a single detected salient instance, the probability of the pixel having the label associated with that salient instance is 1. If a salient pixel is not covered by any detected salient instance, the probability of the pixel having any label is 1 K . Note that salient object proposals may have overlaps and some object proposals may occupy non-salient pixels. If a salient pixel is covered by k overlapping salient instances, the probability of the pixel having a label associated with one of the k salient instances is 1 k . If a background pixel is covered by k overlapping salient instances, the probability of the pixel having a label associated with one of the k salient instances is 1 k+1 , and the probability of the pixel having the background label is also 1 k+1 . Given this initial salient instance probability map, we employ a fully connected CRF model <ref type="bibr" target="#b25">[26]</ref> for refinement. Specifically, pixel labels are optimized with respect to the following energy function of the CRF:</p><formula xml:id="formula_2">E (x) = − i log P (x i ) + i,j θ ij (x i , x j ) ,<label>(2)</label></formula><p>where x represents a complete label assignment for all pixels and P (x i ) is the probability of pixel i being assigned with the label prescribed by x. θ ij (x i , x j ) is a pairwise potential defined as follows,</p><formula xml:id="formula_3">θ ij = µ (x i , x j ) ω 1 exp − p i − p j 2 2σ 2 α − I i − I j 2 2σ 2 β + ω 2 exp − p i − p j 2 2σ 2 γ ,<label>(3)</label></formula><p>where µ (x i , x j ) = 1 if x i = x j , and zero otherwise. θ ij involves two kernels. The first kernel depends on pixel positions (p) and pixel intensities (I), and encourages nearby pixels with similar colors to take similar salient instance labels, while the second kernel only considers spatial proximity when enforcing smoothness. The hyperparameters, σ α , σ β and σ γ , control the scale of Gaussian kernels. In this paper, we apply the publicly available implementation of <ref type="bibr" target="#b25">[26]</ref> to minimize the above energy. The parameters in this CRF are determined through cross validation on the validation set of our dataset introduced in the next section. The actual values of w 1 , w 2 , σ α , σ β and σ γ are set to 4.0, 3.0, 49.0, 5.0 and 3.0, respectively in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A New Dataset for Salient Object Instances</head><p>As salient instance segmentation is a completely new problem, no suitable datasets exist. In order to promote the study of this problem, we have built a new dataset with pixelwise salient instance labels. We initially collected 1, 388 images. To reduce the ambiguity in salient region detection results, these images were mostly selected from existing datasets for salient region detection, including EC-SSD <ref type="bibr" target="#b47">[48]</ref>, DUT-OMRON <ref type="bibr" target="#b48">[49]</ref>, HKU-IS <ref type="bibr" target="#b28">[29]</ref>, and MSO datasets <ref type="bibr" target="#b50">[51]</ref>. Two-thirds of the chosen images contain multiple occluded salient object instances while the remaining one-third consists of images with no salient regions, a single salient object instance or multiple salient instances without occlusion. To reduce label inconsistency, we asked three human annotators to label detected salient regions with different instance IDs in all selected images using a custom designed interactive segmentation tool. We only kept the images where salient regions were divided into an identical number of salient object instances by all the three annotators. At the end, our new salient instance dataset contains 1,000 images with high-quality pixelwise salient instance labeling as well as salient object contour labeling. We randomly divide the dataset into three parts, including 500 for training, 200 for validation and 300 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation</head><p>Our proposed MSRNet has been implemented on the public DeepLab code base <ref type="bibr" target="#b6">[7]</ref>, which was implemented in the Caffe framework <ref type="bibr" target="#b22">[23]</ref>. A GTX Titan X GPU is used for both training and testing. We combine the training sets of both the MSRA-B dataset (2500 images) <ref type="bibr" target="#b33">[34]</ref> and the HKU-IS dataset (2500 images) <ref type="bibr" target="#b28">[29]</ref> as our training set (5000 images) for salient region detection. The validation sets in the aforementioned two datasets are also combined as our validation set (1000 images). We augment the image dataset by horizontal flipping. During training, the mini-batch size is set to 6 and we choose to update the loss every 5 iterations. We set the momentum parameter to 0.9 and the weight decay to 0.0005 for both subtasks. The total number of iteration is set to 20K. We test the softmax loss on the validation set every 500 iterations and select the model with the lowest validation loss as the best model for testing. As discussed in Section 3.1.3, this trained model is used as the initial model for salient contour detection, and is further fine-tuned on the training set of our new dataset for salient instances and contour detection. As our new dataset only contains 500 training images, we perform data augmentation as in <ref type="bibr" target="#b46">[47]</ref>. Specifically, we rotate the images to 8 different orientations and crop the largest rectangle in the rotated image. With horizontal flipping at each orientation, the training set is enlarged by 16 times. We fine-tune MSRNet on the augmented dataset for 10K iterations and keep the model with the lowest validation error as our final model for salient object contour detection.</p><p>It takes around 50 hours to train our multiscale refinement network for salient region detection and another 20 hours for salient object contour detection. As MSRNet is a fully convolutional network, the testing phase is very efficient. In our experiments, it takes 0.6 seconds to perform either salient region detection or salient object contour detection on a testing image with 400x300 pixels. It takes 20 seconds to generate a salient instance segmentation with MCG being the bottleneck which needs 18 seconds to generate salient object proposals for a single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation on Salient Region Detection</head><p>To evaluate the performance of our MSRNet on salient region detection, we conduct testing on six benchmark datasets: MSRA-B <ref type="bibr" target="#b33">[34]</ref>, PASCAL-S <ref type="bibr" target="#b30">[31]</ref>, DUT-OMRON <ref type="bibr" target="#b48">[49]</ref>, HKU-IS <ref type="bibr" target="#b28">[29]</ref>, ECSSD <ref type="bibr" target="#b47">[48]</ref> and SOD <ref type="bibr" target="#b36">[37]</ref>. As we train our network on the combined training sets of MSRA-B and HKU-IS, we evaluate our trained model on the testing sets of these two datasets and on the combined training and testing sets of other datasets.</p><p>We adopt precision-recall curves (PR), maximum Fmeasure and mean absolute error (MAE) as our performance measures. The F-measure is defined as F β = (1+β 2 )·P recision·Recall β 2 ·P recision+Recall , where β 2 is set to 0.3. We report the maximum F-measure computed from all precision-recall pairs. MAE is defined as the average pixelwise absolute difference between the binary ground truth and the saliency map <ref type="bibr" target="#b38">[39]</ref>. It is a more meaningful measure in evaluating the applicability of a saliency model in salient instance segmentation. In the supplemental materials, we also report the average precision, recall and F-measure using an adaptive threshold which is set to twice the mean saliency value of each saliency map as suggested in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Comparison with the State of the Art</head><p>We compare the proposed MSRNet with other 8 state-ofthe-art salient region detection methods, including GC <ref type="bibr" target="#b9">[10]</ref>, DRFI <ref type="bibr" target="#b23">[24]</ref>, LEGS <ref type="bibr" target="#b43">[44]</ref>, MC <ref type="bibr" target="#b51">[52]</ref>, MDF <ref type="bibr" target="#b28">[29]</ref>, DCL + <ref type="bibr" target="#b29">[30]</ref>,  <ref type="figure">Figure 6</ref>. Comparison of precision-recall curves among 9 salient region detection methods on 3 datasets. Our MSRNet consistently outperforms other methods across all the testing datasets. Note that DHSNet <ref type="bibr" target="#b32">[33]</ref> includes the testing set of DUT-OMRON in its training data, therefore DHSNet is not included in the comparison on this dataset.</p><p>DHSNet <ref type="bibr" target="#b32">[33]</ref> and RFCN <ref type="bibr" target="#b44">[45]</ref>. The last six are the latest deep learning based methods. We use the original implementations provided by the authors in this comparison.</p><p>A visual comparison is given in <ref type="figure" target="#fig_4">Fig. 5</ref>. As we can see, our proposed MSRNet can not only accurately detect salient objects at different scales but also generate more precise saliency maps in various challenging cases. As a part of quantitative evaluation, we show a comparison of PR curves in <ref type="figure">Fig. 6</ref>. Refer to the supplemental materials for the performance comparison on the MSRA-B, ECSSD and SOD datasets. Furthermore, a quantitative comparison of maximum F-measure and MAE is given in <ref type="table">Table 1</ref>. As shown in <ref type="figure">Fig. 6</ref> and <ref type="table">Table 1</ref>, our proposed MSRNet consistently outperforms existing methods across all the datasets with a considerable margin. Specifically, MSRNet improves the maximum F-measure achieved by the best-performing existing algorithm by 1.53%, 1.33%, 3.70%, 1.33%, 2.4% and 1.8% respectively on MSRA-B, HKU-IS, DUT-OMRON, ECSSD, PASCAL-S and SOD. And at the same time, MSR-Net lowers the previoiusly best MAE by 10.6%, 20.4%, 13.8%, 8.5%, 13.8% and 11.1% respectively on MSRA-B, HKU-IS, DUT-OMRON, ECSSD, PASCAL-S and SOD. It is worth noting that MSRNet outperforms all the other six deep learning based saliency detection methods without resorting to any post-processing techniques such as CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Effectiveness of Multiscale Refinement Network</head><p>Our proposed MSRNet consists of three refined VGG streams and a learned attentional model for fusing results at different scales. To demonstrate the effectiveness and necessity of each component, we have trained three additional models for comparison. These three models are respectively a single backbone network (VGG16), a single-scale refinement network (SSRNet) and a multiscale VGG network with the same attentional module but without refinement (MSVGG). These three additional models are trained using the same setting as MSRNet training. Quantitative results from the four methods are obtained on the testing part of HKU-IS dataset. As shown in <ref type="figure" target="#fig_5">Fig. 7</ref>, MSRNet consistently achieves the best performance in terms of the PR curve as well as average precision, recall and F-measure. Both SSR-Net and MSVGG perform much better than VGG16, which respectively demonstrates the effectiveness of the refinement module and attention based multiscale fusion in MSR-Net. Moreover, these two components are complementary to each other, which makes MSRNet not only capable of detecting more precise salient regions (with higher resolution) but also discovering salient objects at multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation on Salient Instance Segmentation</head><p>To evaluate the effectiveness of our proposed framework for salient instance segmentation as well as to promote fur-  <ref type="table">Table 1</ref>. Comparison of quantitative results including maximum F-measure (larger is better) and MAE (smaller is better). The best three results on each dataset are shown in red, blue, and green , respectively. Note that the training set of DHSNet <ref type="bibr" target="#b32">[33]</ref> includes the testing set of MSRA-B and Dut-OMRON, and the entire MSRA-B dataset is used as part of the training set of RFCN <ref type="bibr" target="#b44">[45]</ref>. Corresponding test results are excluded here. ther research on this new problem, we adopt two types of performance measures and demonstrate the results from our framework according to these measures. We use the same performance measures used for traditional contour detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47]</ref> to evaluate the performance of salient object contour detection, and adopt three standard measures: fixed contour threshold (ODS), per-image best threshold (OIS), and average precision (AP). Refer to <ref type="bibr" target="#b1">[2]</ref> for detailed definitions. We define performance measures for salient instance segmentation by drawing inspirations from the evaluation of instance-aware semantic segmentation. Specifically, we adopt mean Average Precision, referred to as mAP r <ref type="bibr" target="#b19">[20]</ref>. In this paper, we report mAP r using IoU thresholds at 0.5 and 0.7, denoted as mAP r @0.5 and mAP r @0.7 respectively.</p><p>Benchmark results from our proposed method in both salient object contour detection and salient instance segmentation are given in <ref type="table" target="#tab_1">Table 2</ref>. <ref type="figure">Fig. 8</ref> demonstrates examples from our results on our testing set. Our method can handle challenging cases where multiple salient object instances are spatially connected to each other.  <ref type="figure">Figure 8</ref>. Examples of salient instance segmentation results by our MSRNet based framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have introduced salient instance segmentation, a new problem related to salient object detection, and also presented a framework for solving this problem. The most important component of our framework is a multiscale saliency refinement network, which generates highquality salient region masks and salient object contours. To promote further research and evaluation of salient instance segmentation, we have also constructed a new database with pixelwise salient instance annotations. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public datasets for salient region detection as well as on our new dataset for salient instance segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An example of instance-level salient object segmentation. Left: input image. Middle left: detected salient region. Middle right: filtered salient object proposals. Right: result of salient instance segmentation. Different colors indicate different object instances in the detected salient region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Our overall framework for instance-level salient object segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The architecture of our multiscale refinement network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The architecture of the attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visual comparison of saliency maps from state-of-the-art methods, including our MSRNet. The ground truth (GT) is shown in the last column. MSRNet consistently produces saliency maps closest to the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Componentwise efficacy of the proposed multiscale refinement network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative benchmark results of salient object contour detection and salient instance segmentation on our new dataset.</figDesc><table><row><cell cols="5">Salient Contour Detection Salient Instance Segmentation</cell></row><row><cell>ODS</cell><cell>OIS</cell><cell>AP</cell><cell cols="2">M P r @0.5(%) M P r @0.7(%)</cell></row><row><cell cols="2">0.719 0.757</cell><cell>0.765</cell><cell>65.32</cell><cell>52.18</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Seam carving for content-aware image resizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepedge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Boosting bottom-up and top-down visual features for saliency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03339</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bing: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Instancesensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08678</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04412</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Salient object detection by composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Category-independent object-level saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.5644</idno>
		<title level="m">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saliency guided dictionary learning for weakly-supervised image parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01013</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A user attention model for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An integrated model of top-down and bottom-up attention for optimizing detection speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Navalpakkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08250</idno>
		<title level="m">Recurrent instance segmentation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5726</idno>
		<title level="m">Cnn: Single-label to multi-label</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unconstrained salient object detection via proposal subset optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
