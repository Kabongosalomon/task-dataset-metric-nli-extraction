<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Complementary and Unlabeled Learning for Arbitrary Losses and Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhou</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Science</orgName>
								<orgName type="institution">China Agricultural University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Science</orgName>
								<orgName type="institution">China Agricultural University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Science</orgName>
								<orgName type="institution">China Agricultural University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Complementary and Unlabeled Learning for Arbitrary Losses and Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A weakly-supervised learning framework named as complementary-label learning has been proposed recently, where each sample is equipped with a single complementary label that denotes one of the classes the sample does not belong to. However, the existing complementary-label learning methods cannot learn from the easily accessible unlabeled samples and samples with multiple complementary labels, which are more informative. In this paper, to remove these limitations, we propose the novel multi-complementary and unlabeled learning framework that allows unbiased estimation of classification risk from samples with any number of complementary labels and unlabeled samples, for arbitrary loss functions and models. We first give an unbiased estimator of the classification risk from samples with multiple complementary labels, and then further improve the estimator by incorporating unlabeled samples into the risk formulation. The estimation error bounds show that the proposed methods are in the optimal parametric convergence rate. Finally, the experiments on both linear and deep models show the effectiveness of our methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ordinary supervised classification problems require that each training sample should be equipped with an exact label that denotes the class the sample belongs to. However, the preparation of massive exactly labeled data is usually laborious and unrealistic in practical. Therefore, a lot of studies on learning from weak supervision have been made to tackle this problem in different scenarios, e.g. semi-supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, partial label learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, and positive-unlabeled learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Recently, another weakly-supervised learning scenario called complementary-label learning (CLL) has been proposed. In the CLL setting, each ordinary label is substituted with the complementary label, which denotes one of the classes that a training sample does not belong to. It is obvious that the preparation of complementarily labeled data is much more labor-saving than that of ordinarily labeled data.</p><p>The complementary-label learning problem has been investigated in previous studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. In these works, different risk estimators were proposed to recover classification risk only from complementarily labeled data under the empirical risk minimization (ERM) framework. In <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b12">[13]</ref>, the proposed risk estimators had restrictions on loss functions and unbiasedness respectively. <ref type="bibr" target="#b13">[14]</ref> overcame the shortcomings by giving an unbiased risk estimator without any restriction on models and loss functions while guaranteeing the superior performance in terms of classification accuracy over the previous two methods.</p><p>It is noticeable that in these works, each training sample was given only a single complementary label. However, in quite a few cases, the training samples can be multi-complementarily labeled, namely each training sample is equipped with multiple complementary labels. For example, in the stage of data annotation, an annotator who has no idea of a training sample's exact label may be able to recognize multiple classes that the sample does not belong to, which results in a sample with multiple complementary labels. In crowdsourcing scenario <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, the quality of crowdsourcing label is especially crucial <ref type="bibr" target="#b16">[17]</ref>. Instead of being ordinarily labeled, a sample can be complementarily labeled to alleviate the effect of low-quality noisy crowdsourcing labels. Since a sample can be complementarily labeled by different crowdworkers, each training sample may have more than one complementary label. Moreover, compared with the single-complementary-label setting in previous CLL studies, the samples with multiple complementary labels are more informative. To sum up, a framework for learning from data with arbitrary 1 number of complementary labels is in demand.</p><p>Furthermore, the information concealed in the easily accessible unlabeled data proved to be helpful in many other weakly-supervised learning scenario both theoretically and practically <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Therefore, it is promising to further enhance the capability of CLL framework by incorporating the unlabeled data.</p><p>In this paper, we study the multi-complementary label and unlabeled learning (MCUL) problem, where both multi-complementarily labeled data and unlabeled data are leveraged to obtain better classifiers. In our method, we propose a novel unbiased risk estimator for MCUL problem with no limitation on loss functions and models. By using a mild assumption, we first derive the risk estimator for multi-complementary label learning (MCL) problem. Then we further utilize the unlabeled data to construct the risk estimator for MCUL problem. With no more assumption on loss functions and models, we show that the estimation error bounds of MCL and MCUL are in optimal parametric convergence rate <ref type="bibr" target="#b20">[21]</ref>. The effectiveness of the proposed MCUL is demonstrated through experiments on both linear and deep models.</p><p>The main contributions are summarized as follows:</p><p>• We propose the novel MCL framework that allows unbiased estimation of the classification risk only from samples with arbitrary number of complementary labels and can be applied on arbitrary losses and models.</p><p>• We further propose the MCUL framework to utilize the unlabeled samples, which are neglected in previous studies on CLL <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and validate the benefits of the incorporation of unlabeled samples both experimentally and theoretically.</p><p>• The previous CLL framework and ordinary classification problems are proven to be special cases of the MCUL framework, which shows the comprehensiveness of the MCUL framework as a weakly-supervised leaning framework.</p><p>This rest of this paper is organized as follows. We give the review of complementary-label learning in Section 2. The MCL and MCUL frameworks are proposed in Section 3. Moreover, we analyse the estimation error bounds of the proposed methods in Section 4 and discuss the helpfulness of integrating the class-prior information in Section 5. Finally, we give the experimental results of our frameworks on both linear and deep models in Section 6 and conclude the paper in Section 7. The detailed proof is shown in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Review of Complementary-Label Learning</head><p>To begin with, we first show the classification risk of learning from ordinary labels and then review how the previous risk estimators of learning from complementarily labeled samples recover the classification risk under the ERM framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ordinary Classification Problem</head><p>Let's denote the feature space with X ∈ R d and Y = {1, 2, . . . , K} is the label space. The training samples are drawn independently and identically from the unknown distribution D, which is the joint distribution over X × Y with density p(x, y). Then the critical work is to find a decision function g : X → R K that minimizes the classification risk with loss function :</p><formula xml:id="formula_0">X × Y → R + : R(g) := E p(x,y) [ (g(x), y)].<label>(1)</label></formula><p>Since the density p(x, y) is unknown, the classification risk is approximated by the empirical risk:R</p><formula xml:id="formula_1">(g) := 1 n n i=1 (g(x i ), y i ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Complementary-Label Learning</head><p>In the CLL setting, each sample is equipped with a complementary label. The complementarily labeled data {(x i , y i )} n i=1 are sampled independently and identically from a joint distribution with density p(x, y).</p><p>In <ref type="bibr" target="#b11">[12]</ref>, an assumption on density p(x, y) was made:</p><formula xml:id="formula_2">p(x, y) = 1 K − 1 y =y p(x, y).<label>(3)</label></formula><p>Under this assumption, <ref type="bibr" target="#b11">[12]</ref> proved that classification risk (1) can be recovered by an unbiased estimator only from complementarily labeled data. However, the loss functions are restricted to one-versus-all and pairwise comparison multi-class loss functions <ref type="bibr" target="#b21">[22]</ref>. Moreover, the binary loss functions (z) : R → R + used in the two multi-class loss functions are required to fulfill symmetric condition: (z)+ (−z) = 1. Obviously, the popular softmax cross-entropy loss and all the other convex loss functions do not meet these conditions. Since the softmax cross-entropy loss is widely used in deep learning, this requirement will be a serious limitation for the application of state-of-the-art deep models.</p><p>To make deep models available, <ref type="bibr" target="#b12">[13]</ref> proposed another risk estimator limited to softmax cross-entropy loss. Though the risk estimator is not necessarily unbiased, the method is ensured to identify the optimal classifier that minimizes classification risk (1) by minimizing its learning object. The method also introduces bias into the choice of complementary labels. However, in the stages of bias estimation, ordinarily labeled data are required. The severe requirement might not align with the motivation of complementary-label learning. The limitations above were removed in <ref type="bibr" target="#b13">[14]</ref>. An unbiased risk estimator with only complementarily labeled data was deduced by taking a different approach than <ref type="bibr" target="#b11">[12]</ref>. With the same assumption (3) adopted, the risk formulation is valid for arbitrary losses and models. Experiments on both linear and deep models showed the superiority of the estimator in <ref type="bibr" target="#b13">[14]</ref> than those in previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Nevertheless, the estimator is still confined within single-complementary-label setting, where each sample is given merely one complementary label. The unlabeled data are also neglected in previous CLL studies, which prevents the CLL from being a more general framework.</p><p>The MCUL framework proposed in this paper further enables learning from both multicomplementarily labeled samples and unlabeled samples. <ref type="figure" target="#fig_0">Figure 1</ref> describes the differences between the previous CLL setting and the proposed MCUL setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Frameworks</head><p>In this section, we propose our framework to enable unbiased estimation of classification risk from both multi-complementarily labeled data and unlabeled data.</p><p>We first prove that the classification risk can be recovered from multi-complementarily labeled data under a mild assumption by employing the risk rewrite technique <ref type="bibr" target="#b22">[23]</ref>. Then we further present the risk formulation of MCUL and show the estimation error bounds of the two methods.</p><p>Notations and Settings: Denote by Y = {1, . . . , K} the complementary label space. Y c is the collection of all the possible combinations of c different complementary labels, e.g.</p><formula xml:id="formula_3">Y c = {1, . . . , c} ∈ Y c .</formula><p>Y c is referred to as complementary-label set in the following sections.</p><p>Suppose the training samples are sampled as follows:</p><formula xml:id="formula_4">S u := {x u i } nu i=1 i.i.d. ∼ p(x), S c := {(x c i , Y c i )} nc i=1 i.i.d. ∼ p c (x, Y ), c = 1, . . . , K −1.</formula><p>where p(x) is the marginal density and p c (x, Y ) is the density on X ×Y c . S u are the unlabeled data and S c are the multi-complementarily labeled data with complementary-label sets of size c. The size of complementary-label set Y is denoted by |Y | and |S c | = n c , c = 1, . . . , K − 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Complementary Label Learning (MCL)</head><p>In this section, we give an account of the risk minimization framework of multi-complementary label learning. As in the previous works, we first make assumptions on the relation between density p c (x, Y ) and p(x, y).</p><formula xml:id="formula_5">p c (x, Y ) = 1 K−1 c y / ∈Y p(x, y).<label>(4)</label></formula><p>The assumption implies each combination of c complementary labels are selected uniformly, which is a mild generalized assumption of those in the previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Under this assumption, we prove that the multi-complementary loss allows unbiased estimation of classification risk (1) from samples with complementary-label sets: Lemma 1. Suppose the density p c (x, Y ) and p(x, y) follow the assumption <ref type="bibr" target="#b3">(4)</ref>. For any loss function and decision function g, the classification risk (1) is equal to the risk formulation below:</p><formula xml:id="formula_6">R c (g) = E p c (x,Y ) [ (g(x), Y )].<label>(5)</label></formula><p>where is the multi-complementary loss:</p><formula xml:id="formula_7">(x, Y ) := K y=1 (g(x), y) − K −1 |Y | y∈Y (g(x), y).<label>(6)</label></formula><p>The proof can be found in the Appendix 7. For ease of notation, we the following notation for cumulative loss K y=1 (g(x), y):</p><formula xml:id="formula_8">L(g(x)) := K y=1 (g(x), y).<label>(7)</label></formula><p>Due to the notation, we can further rewrite the multi-complementary loss into the form below:</p><formula xml:id="formula_9">(x, Y ) := L(g(x)) − K −1 |Y | y∈Y (g(x), y).<label>(8)</label></formula><p>Notice that the cumulative loss L(g(x)) = K y=1 (g(x), y) is obtained by summing up the loss of the prediction g(x) w.r.t. all the potential labels y ∈ {1, . . . , K}, so it only relies on the sample x and the classifier g(·). As a result, the label information used in the calculation of multi-complementary loss <ref type="formula" target="#formula_9">(8)</ref> is only complementary-label set Y . Therefore, the multi-complementary label learning setting totally gets rid of the dependence on true labels. The risk formulation in Lemma 1 shows that the classification risk can be recovered only from samples with complementary-label sets of fixed size c. However, the complementary-label sets of samples are not necessarily limited to a certain size in reality. To completely remove the limitation on the size of complementary-label set, we consider the convex combination of R c (g) called multi-complementary risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1. (Multi-Complementary Risk) For any decision function g, its MCL risk is defined as:</head><formula xml:id="formula_10">R MCL (g) = K−1 c=1 α c R c (g),<label>(9)</label></formula><p>where α is any vector in {α</p><formula xml:id="formula_11">K−1 c=1 α c = 1, α 0}. Theorem 2.</formula><p>The MCL risk is equal to classification risk <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_12">R MCL (g) = R(g).<label>(10)</label></formula><p>Proof. Due to Lemma 1, we can get R c (g) = R(g). Then the following equations holds:</p><formula xml:id="formula_13">R MCL (g) = K−1 c=1 α c R c (g) = K−1 c=1 α c R(g) = R(g).</formula><p>The empirical MCL risk is as below:</p><formula xml:id="formula_14">R MCL (g) = K−1 c=1 α c n c nc i=1   L(g(x c i )) − K −1 c y∈Y c i (g(x c i ), y)   .<label>(11)</label></formula><p>Then the following work is to find the minimizerĝ MCL of empirical MCL risk:</p><formula xml:id="formula_15">g MCL = arg min g∈G KR MCL (g).<label>(12)</label></formula><p>where G = {g(x)} is a real function class and</p><formula xml:id="formula_16">G K = [G i ] K i=1</formula><p>is a K-dimensional function class. In <ref type="bibr" target="#b10">(11)</ref>, all the samples are taken into consideration regardless of the size of their complementary-label sets. Since there is no restriction on loss function and classifier g, any loss and model is available for the multi-complementary learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 1.</head><p>There are some special cases in the multi-complementary label learning setting. If α 1 = 1, the proposed estimator will reduce to the estimator in single-complementary-label setting <ref type="bibr" target="#b13">[14]</ref>. If α K−1 = 1, the proposed estimator will be the same with that in ordinary classification problem <ref type="bibr" target="#b1">(2)</ref>. According to the special cases, the proposed MCL proved to be a comprehensive weakly-supervised learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Complementary and Unlabeled Learning (MCUL)</head><p>To utilize both multi-complementarily labeled data and unlabeled data, we further rewrite the risk formulation and propose the MCUL framework. Based on Lemma 1, we can incorporate the unlabeled data to construct an unbiased estimator of classification risk (1): Lemma 3. The classification risk <ref type="bibr" target="#b0">(1)</ref> is equal to the risk formulation below:</p><formula xml:id="formula_17">R u c (g) = E p c (x,Y )   (1−γ)L(g(x))− K −1 c y∈Y (g(x), y)   +γE p(x) [L(g(x))]<label>(13)</label></formula><p>where γ ∈ [0, 1] is the trade-off coefficient.</p><p>Proof. The cumulative loss L(g(x)) is independent of Y , and thus:</p><formula xml:id="formula_18">E p(x) [L(g(x))] = E p c (x,Y ) [L(g(x))] .</formula><p>According to the equation above and Lemma 1, we can obtain:</p><formula xml:id="formula_19">R u c (g) = R c (g) = R(g).</formula><p>In the same manner as in the derivation of (9), we can derive the multi-complementary and unlabeled risk:</p><formula xml:id="formula_20">Definition 2. (Multi-Complementary&amp;Unlabeled Risk)</formula><p>For any decision function g, its MCUL risk is defined as:</p><formula xml:id="formula_21">R MCUL (g) = K−1 c=1 α c R u c (g).<label>(14)</label></formula><p>where α is any vector in {α</p><formula xml:id="formula_22">K−1 c=1 α c = 1, α 0}.</formula><p>When the trade-off coefficient γ is set to 0, the MCUL risk is the same with MCL risk <ref type="bibr" target="#b8">(9)</ref>. The following Theorem allows unbiased estimation with both unlabeled data and multi-complementarily labeled data. <ref type="formula" target="#formula_0">(1)</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 4. The MCUL risk is equal to classification risk</head><formula xml:id="formula_23">R MCUL (g) = R(g).<label>(15)</label></formula><p>The Theorem can be proven in the same way as in Theorem 9. We can approximate the MCUL risk by the empirical MCUL risk below:</p><formula xml:id="formula_24">R MCUL (g) = γ n u nu i=1 L(g(x u i )) + K−1 c=1 α c (1 − γ) n c L(g(x c i )) − K−1 c=1 α c (K −1) c·n c nc i=1 y∈Y c i (g(x c i ), y).<label>(16)</label></formula><p>Notice that the unlabeled data are used for construct the estimator of cumulative loss since the calculation of L(g(x)) does not need any label information. Theorem 3 shows that this incorporation can still yields an unbiased estimator of classification risk <ref type="bibr" target="#b0">(1)</ref>. Then the following work is to find the minimizerĝ MCUL of empirical MCUL risk:</p><formula xml:id="formula_25">g MCUL = arg min g∈G KR MCUL (g).<label>(17)</label></formula><p>Compared with the empirical MCL risk <ref type="bibr" target="#b10">(11)</ref>, the empirical MCUL risk <ref type="formula" target="#formula_0">(14)</ref> further incorporates the unlabeled data into the risk formulation. With the incorporation of easily accessible unlabeled data, the estimation error bound will be tighter, which indicates a better decision function g. The claim is further validated in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Estimation Error Bounds of MCL and MCUL</head><p>In this section, we give the estimation error bounds of the proposed MCL and MCUL frameworks.</p><p>Suppose the non-negative loss function does not exceed C on feature space X and let L be the Lipschitz constant of . R n (G) is the Rademacher complexity <ref type="bibr" target="#b23">[24]</ref> of function class G with sample size of n from p(x) and we suppose it decays in the rate of O(1/ √ n). We have the following estimation error bounds, which show the convergence ofĝ MCL andĝ MCUL to the optimal decision function g * = arg min </p><formula xml:id="formula_26">R(ĝ MCL )−R(g * ) ≤ K−1 c=1 2K K − 1 c + 1 α c ×   2KL R nc (G) + C ln(2K/δ) 2n c   .<label>(18)</label></formula><p>Theorem 6. (Estimation error bound of MCUL) For any δ &gt; 0, with probability at least 1 − δ:</p><formula xml:id="formula_27">R(ĝ MCUL )−R(g * ) = O(1/ √ n u + K−1 c=1 1/ √ n c ).<label>(19)</label></formula><p>The proof of the theorems above can be found in the Appendix 7.</p><p>Remark 2. From Theorems 5 and 6, we can learn the estimation error bounds of the proposed methods are in the optimal convergence rate without any additional assumption <ref type="bibr" target="#b20">[21]</ref>. Moreover, with increasing number of unlabeled data, the error bound of MCUL will get tighter, which implies the helpfulness of utilizing unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Integration of Class-Prior Information</head><p>In the previous sections, a sample with c complementary labels is considered to be sampled from the distribution with density p c (x, Y ), which is independent from the class-prior probability p(|Y | = c). In practical situations, however, the class-prior may be accessible. For example, in <ref type="bibr" target="#b24">[25]</ref>, the class-prior can be estimated from the given data; the prior is simply approximated by the relative frequency in <ref type="bibr" target="#b13">[14]</ref>. <ref type="bibr" target="#b25">[26]</ref> proves the helpfulness of integrating the class-prior into learning algorithm. As can be seen, it is promising to further enhance the capability of proposed MCL and MCUL framework by utilizing the class-prior information. Notice that p c (x, Y ) is the conditional density p x, Y |Y | = c in essence. Then the following equation holds:</p><formula xml:id="formula_28">p(x, Y ) = p c (x, Y )π c .<label>(20)</label></formula><p>where π c = p(|Y | = c) and |Y | = c. Due to the equation <ref type="formula" target="#formula_1">(20)</ref>, we can integrate the class-prior information into the risk formulations of MCL and MCUL as follows:</p><p>Theorem 7. (MCL cl risk and MCUL cl risk)</p><formula xml:id="formula_29">R(g) = E p(x,Y )   L(g(x)) − λ y∈Y (g(x), y)   (21) = E p(x,Y )   (1 − γ)L(g(x)) − λ y∈Y (g(x), y)   + γE p(x) [L(g(x))]<label>(22)</label></formula><p>where λ = (K − 1)/ K−1 c=1 cπ c and γ ∈ [0, 1] is a trade-off parameter. <ref type="bibr" target="#b20">(21)</ref> and <ref type="bibr" target="#b21">(22)</ref> are called MCL cl risk and MCUL cl risk respectively.</p><p>The proof can be found in the Appendix 7.</p><p>From the Theorem 7, the coefficient α in <ref type="formula" target="#formula_10">(9)</ref> and <ref type="formula" target="#formula_0">(14)</ref> is substituted by the class-prior π c . Compared with (9) that converges in the rate of O(</p><formula xml:id="formula_30">K−1 c=1 1/ √ n c ), MCL cl risk (21) converges in O(1/ √ n)</formula><p>, which often indicates a faster convergence rate. We will experimentally evaluate the helpfulness of integrating class-prior information in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we experimentally evaluate the proposed methods on nine benchmark datasets including: PENDIGITS, LETTER, SATIMAGE, USPS, MNIST <ref type="bibr" target="#b26">[27]</ref>, Fashion-MNIST <ref type="bibr" target="#b27">[28]</ref>, Kuzushi-MNIST <ref type="bibr" target="#b28">[29]</ref>, EMNIST-balanced <ref type="bibr" target="#b29">[30]</ref> and SVHN <ref type="bibr" target="#b30">[31]</ref>. The first three datasets can be downloaded from the UCI machine learning repository and all the other datasets are available on public websites. We compare three complementary-label learning baseline methods: Pairwise Comparison(PC ) with sigmoid loss from <ref type="bibr" target="#b11">[12]</ref>, Forward Correction(Fwd) from <ref type="bibr" target="#b12">[13]</ref> and Gradient Ascent(GA) from <ref type="bibr" target="#b13">[14]</ref>.</p><p>The details of the datasets are shown in the following sections. The implementation is based on Pytorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>In the experiments, the empirical risk minimization of MCL and MCUL is conducted by minimizing the risk formulation <ref type="bibr" target="#b10">(11)</ref> and <ref type="formula" target="#formula_0">(16)</ref> w.r.t. softmax cross-entropy loss. GA, and Fwd follow the setting above and PC is trained with pairwise-comparison loss. Adam <ref type="bibr" target="#b31">[32]</ref> is applied for optimization. All the datasets are split into training/testing sets with a 9:1 ratio and the training sets are further divided into training/validation sets with the same ratio.</p><p>To ensure that the assumption (4) is satisfied, each complementary-label set of size c is generated by randomly choosing c labels from the candidate labels other than the true label. Though a sample with c complementary labels can be simply split into c samples with one complementary label each, it's obvious that the c samples are not independent of each other. Therefore this approach will lead to serious violation of the fundamental i.i.d. assumption. For fair comparison in these experiments, the complementary labels are generated in the same way as in <ref type="bibr" target="#b13">[14]</ref>.</p><p>For PENDIGITS, LETTER, SATIMAGE, USPS, and MNIST, a linear-in-input model with a bias term is used. For MNIST, the learning rate is fixed to 1e-4; weight decay 1e-4; maximum iterations 60000; and batch size is set to 100. For the rest datasets, the learning rate is selected from {1e-1, 1e-2, 1e-3, 1e-4} and the number of maximum iterations is changed to 5000.</p><p>For Fashion-MNIST, Kuzushi-MNIST and EMNIST-balanced, a MLP model(d-500-K ) is trained for 300 epochs. The learning rate and weight decay are fixed to 1e-4 and the batch size is 256. For SVHN, Resnet-18 <ref type="bibr" target="#b32">[33]</ref> is deployed and trained for 120 epochs. The learning rate is selected from {1e-2, 1e-3, 1e-4} and weight decay is fixed to 5e-4. To alleviate overfitting by forcing the non-negativity of loss functions <ref type="bibr" target="#b33">[34]</ref>, in the experiments on flexible models, MCL and MCUL losses are replaced by their absolute values.</p><p>Experiments on datasets with or without unlabeled samples are both conducted. When the unlabeled samples are incorporated, we randomly set 99% of training samples to be unlabeled for datasets with less than 50000 samples, which is a common setting in previous studies of weakly-supervised learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>. The fraction is further increased to 99.5% for datasets with more than 50000 samples.</p><p>In respect of parameter setting, the setting of baseline methods follow the previous work <ref type="bibr" target="#b13">[14]</ref>. For MCUL, we use γ = 0.1. The parameter α is set according to the equations below:</p><formula xml:id="formula_31">α i : α j = ni (K−i) 2 : nj (K−j) 2 . K−1 i=1 α 1 = 1.</formula><p>In our methods, we make no assumption on the distribution of the size of complementarylabel sets. To generate the multi-complementarily labeled samples as close to the reality, we suppose that samples with too few or too many complementary labels are less likely to appear. Then the n i follows the equation below:</p><formula xml:id="formula_32">n i : n j = e −(i−µ) 2 : e −(j−µ) 2 .</formula><p>In the experiments, µ = K 2 is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments on Linear Model and Flexible Models</head><p>The experimental results of linear and flexible models are summarized in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> respectively. The experimental results under the presence of unlabeled samples are shown in the second row corresponding to each dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of MCL:</head><p>First we compare the proposed MCL framework with the three baseline methods. From the experimental results, we can see that MCL framework outperforms the baseline methods on all the datasets regardless of which model is applied. The superiority of MCL is especially apparent when the datasets have a large number of classes. Due to the experimental results on LETTER and EMNIST-balanced, it can be seen that the baseline methods can hardly generate effective classifiers. Furthermore, the training curves in <ref type="figure" target="#fig_2">Figure  2</ref> show that in most cases, MCL can generate better classifiers and converge in a faster rate. Compared with the baseline methods, MCL remains valid owing to the capability of utilizing multi-complementarily labeled samples.  <ref type="table" target="#tab_0">Table 1</ref>, as can be seen, under the presence of a great percentage of unlabeled samples, the baseline methods suffer from the lack of complementarily labeled samples, while MCUL can still enhance its performance by incorporating unlabeled samples. Moreover, in the cases that only a small number of complementarily labeled samples are available, the performance of baseline methods GA is seriously degraded due to the imprecise estimation of class prior, which is one of the reasons that the performance of GA is relatively weak with a small fraction of complementarily labeled samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of MCUL: From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experiments with Accessible Class-Prior Probability</head><p>In this section, we further show the benefits of integrating the class-prior information. We compare the performance of MCL and MCUL with MCL cl and MCUL cl on linear model. The experimental results are summarized in <ref type="table" target="#tab_2">Table 3</ref> and the setup is consistent with that in the previous experiments.</p><p>From the results, we can see that by integrating the class-prior probability, MCL cl outperforms MCL on most datasets and MCUL cl performs better than MCUL on all the datasets, which shows the helpfulness of utilizing the class-prior information when it is accessible. Furthermore, under the presence of a great percentage of unlabeled samples, the integration of class-prior information can always boost the performance of models. A rational explanation is that when supervision information is inadequate, the benefits of incorporating class-prior information is more conspicuous. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We first derive the MCL framework to learn from samples with any number of complementary labels for arbitrary losses and models. Then we incorporate unlabeled data into the risk formulation and propose the MCUL framework to enhance the performance of MCL by learning from multi-complementarily labeled data and unlabeled data simultaneously. We further show that the estimation error bounds of the proposed methods are in the optimal parametric convergence rate. Finally, we conduct experiments and show our methods outperform the current state-of-the-art methods on both linear and deep model. A promising direction is applying our methods on crowdsourcing and other weakly-supervised classification scenarios, which is our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Theorem 6</head><p>The proof of Theorem 5 is omitted since it is the special case of Theorem 6 by setting γ to 0. First we introduce the Talagrand's contraction lemma <ref type="bibr" target="#b34">[35]</ref>: To apply Talagrand's contraction lemma, we use the shifted loss˜ (z) = (z) − (0) instead of (z). Then we abbreviate some complex terms in these forms:</p><formula xml:id="formula_33">Lemma</formula><formula xml:id="formula_34">                       A = γE p(x) [ y˜ (g(x), y)] B c = E p c (x,Y ) [(1 − γ) y˜ (g(x), y) − K −1 c y∈Y˜ (g(x), y)]</formula><p>Let I(·) be the indicator function and α i = 2I(y ∈ Y i ) − 1. Then we have the conclusion below:</p><formula xml:id="formula_35">E Sc E σ   sup g∈G K 1 n (xi,Y i)∈Sc σ i y∈Y i˜ (g(x i ), y)   = E Sc E σ   sup g∈G K 1 2n (xi,Y i)∈Sc σ i y˜ (g(x i ), y)(α i + 1)   ≤ E Sc E σ   sup g∈G K 1 2n (xi,Y i)∈Sc α i σ i y˜ (g(x i ), y)   + E Sc E σ   sup g∈G K 1 2n (xi,Y i)∈Sc σ i y˜ (g(x i ), y)   = E X E σ sup g∈G K 1 n xi∈X σ i y˜ (g(x i ), y)</formula><p>Then the inequalities hold:</p><formula xml:id="formula_36">R n,p c (˜ c B • G K ) ≤ (1 − γ)E X E σ sup g∈G K 1 n xi∈X σ i y˜ (g(x i ), y) + (K − 1) c E X E σ sup g∈G K 1 n xi∈X σ i y˜ (g(x i ), y) = (K − 1 γc + (1 − γ) γ R n (˜ A • G K ) ≤ K − 1 c + 1 − γ K 2 L R n (G)</formula><p>We can bound sup g∈G K A −Â and sup g∈G K B c −B c using Mcdiarmid's inequality <ref type="bibr" target="#b35">[36]</ref>:</p><p>Lemma 10. For a certain c, the inequalities below hold with probability at least 1 − δ:</p><formula xml:id="formula_37">sup g∈G K A −Â ≤ γK   2KL R nu (G) + C ln 2/δ 2n u   sup g∈G K B c −B c ≤ K − 1 c + 1 − γ K   2KL R nc (G) + C ln 2/δ 2n c   .</formula><p>Proof. We are going to prove the first inequality and the second can be proved in a similar way. Firstly, we consider the single direction sup g∈G K (A −Â). Theˆ A will not exceed γKC due to the definition, then the change of sup g∈G K (A −Â) will not exceed γKC /n when we replace a single x i with x i . Due to the Mcdiarmid's inequality, the inequality below holds with probability at least 1 − δ/2:</p><formula xml:id="formula_38">sup g∈G K (A −Â) ≤ E sup g∈G K (A −Â) + γKC ln 2/δ 2n u</formula><p>Due to the symmetrization inequality <ref type="bibr" target="#b23">[24]</ref>, we can obtain that:</p><formula xml:id="formula_39">E sup g∈G K (A −Â) ≤ 2R n ( A • G K ) ≤ 2γK 2 L R n (G)</formula><p>The other direction is similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we can prove the Theorem 6:</head><p>Proof. Notice thatR MCUL (g MCUL ) ≤R MCUL (g * ). Due to Lemma 3, we can get:</p><formula xml:id="formula_40">R(ĝ MCUL ) − R(g * ) = R MCUL (ĝ MCUL ) − R MCUL (g * ) = R MCUL (ĝ MCUL ) −R MCUL (ĝ MCUL ) +R MCUL (ĝ MCUL ) −R MCUL (g * ) +R MCUL (g * ) − R MCUL (g * ) ≤ R MCUL (ĝ MCUL ) −R MCUL (ĝ MCUL ) +R MCUL (g * ) − R MCUL (g * ) ≤ 2 sup g∈G K R MCUL (g) −R MCUL (g) .</formula><p>According to the sub-additivity of supremum, we can get the inequality below:</p><formula xml:id="formula_41">sup g∈G K R MCUL (g) −R MCUL (g) ≤ K−1 c=1 α c sup g∈G K R u c (g) −R u c (g) ≤ K−1 c=1 α c sup g∈G K A −Â + sup g∈G K B c −B c = sup g∈G K A −Â + K−1 c=1 α c sup g∈G K B c −B c</formula><p>Due to the union bound and Lemma 10, the inequality below holds with probability at least 1 − δ:</p><formula xml:id="formula_42">R(ĝ MCUL ) − R(g * ) ≤ 2 sup g∈G K R MCUL (g) −R MCUL (g) ≤ 2 sup g∈G K A −Â + K−1 c=1 α c sup g∈G K B c −B c ≤ 2K   γ   2KL R nu (G) + C ln(2K/δ) 2n u   + K − 1 c + 1 − γ K−1 c=1 α c   2KL R nc (G) + C ln(2K/δ) 2n c     .</formula><p>which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 7</head><p>Proof. Denote {Y |Y ∈ Y, y ∈ Y } with Y y . Due to the equation <ref type="formula" target="#formula_1">(20)</ref>, the equations following hold:</p><formula xml:id="formula_43">       Y ∈Y y p(x, Y ) = p(x, y) + (1 − cπc K−1 ) ŷ =y p(x,ŷ). Y ∈Y y p(x, Y ) = cπc K−1 ŷ =y p(x,ŷ).</formula><p>By substituting the second equation into the first one, we can get:</p><formula xml:id="formula_44">p(x, y) = Y ∈Y p(x, Y ) − K − 1 cπ c Y ∈Y y p(x, Y ).<label>(23)</label></formula><p>By denoting K−1 cπc with λ, the following equations hold: </p><formula xml:id="formula_45">R(g) = E p(x,</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The demonstration of previous CCL setting and the proposed MCL&amp;MCUL settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 5 .</head><label>5</label><figDesc>(Estimation error bound of MCL) For any δ &gt; 0, with probability at least 1 − δ:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The training curves of baseline methods and MCL on flexible models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>8 .</head><label>8</label><figDesc>Let G be a class of real functions and G K = [G i ] K i=1 be a K-dimensional function class and : R K → R a Lipschitz function with constant L and (0) = 0. Then R n ( • G K ) ≤ KL R n (G).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>y) [ (g(x), y)] = K y=1 (g(x), y)p(x, y)dx =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test mean and standard deviation of the classification accuracy of linear model for 10 trials. The best one is emphasized in bold. #n, #f and #c denote the number of samples, features and classes of each dataset. The results of experiments under the presence of unlabeled samples are shown in the second row corresponding to each dataset.</figDesc><table><row><cell>Datasets</cell><cell>#n</cell><cell>#f</cell><cell>#c</cell><cell>PC</cell><cell>Fwd</cell><cell>GA</cell><cell>MCL</cell><cell>MCUL</cell></row><row><cell cols="2">PENDIGITS 10092</cell><cell>16</cell><cell>10</cell><cell>62.98±5.09 5.98±4.24</cell><cell>77.91±2.83 8.09±1.71</cell><cell>15.01±6.71 11.05±5.05</cell><cell>84.32±1.09 11.44±5.68</cell><cell>-± -28.03±8.10</cell></row><row><cell>LETTER</cell><cell>20000</cell><cell>16</cell><cell>26</cell><cell>9.17±2.44 4.66±2.78</cell><cell>9.37±2.52 3.99±0.88</cell><cell>4.68±0.81 5.12±2.01</cell><cell>45.75±2.31 5.33±0.76</cell><cell>-± -8.41±1.86</cell></row><row><cell>SATIMAGE</cell><cell>6435</cell><cell>36</cell><cell>6</cell><cell cols="3">74.05±4.65 15.92±6.67 17.77±10.98 17.68±12.08 77.45±5.43 38.32±1.82</cell><cell>82.10±1.82 21.13±7.57</cell><cell>-± -51.35±7.94</cell></row><row><cell>USPS</cell><cell>9298</cell><cell>256</cell><cell>10</cell><cell cols="2">41.75±5.45 46.15±13.10 9.66±4.72 11.56±7.23</cell><cell>14.19±6.07 8.57±3.38</cell><cell>82.31±2.32 9.66±6.62</cell><cell>-± -26.20±3.98</cell></row><row><cell>MNIST</cell><cell cols="2">70000 784</cell><cell>10</cell><cell>51.21±4.87 13.78±4.16</cell><cell>52.17±4.88 13.58±2.11</cell><cell>67.80±1.89 14.86±1.77</cell><cell>77.36±1.27 25.86±3.49</cell><cell>-± -38.21±5.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test mean and standard deviation of the classification accuracy of flexible models for 4 trials. The best one is emphasized in bold. #n, #f and #c denote the number of samples, features and classes of each dataset. The results of experiments under the presence of unlabeled samples are shown in the second row corresponding to each dataset.</figDesc><table><row><cell>Datasets</cell><cell>#n</cell><cell>#f</cell><cell>#c</cell><cell>PC</cell><cell>Fwd</cell><cell>GA</cell><cell>MCL</cell><cell>MCUL</cell></row><row><cell>Fashion-MNIST</cell><cell>70000</cell><cell>784</cell><cell>10</cell><cell cols="4">77.34±0.88 83.49±0.18 81.73±0.25 84.97±0.09 25.75±4.39 23.36±3.05 22.34±3.18 49.81±4.42</cell><cell>-± -56.93±1.66</cell></row><row><cell>Kuzushi-MNIST</cell><cell>70000</cell><cell>784</cell><cell>10</cell><cell cols="4">59.31±1.07 66.46±0.17 70.68±0.88 79.25±0.28 16.35±4.92 12.94±2.72 15.13±2.42 23.42±1.15</cell><cell>-± -31.04±1.89</cell></row><row><cell cols="2">EMNIST-balanced 131600</cell><cell>784</cell><cell>47</cell><cell cols="2">14.28±1.18 18.21±2.93 2.36±0.36 2.68±0.08</cell><cell>4.25±0.71 2.54±0.36</cell><cell>65.41±0.22 4.14±1.13</cell><cell>-± -6.77±0.51</cell></row><row><cell>SVHN</cell><cell>99289</cell><cell>1024</cell><cell>10</cell><cell cols="3">20.74±2.61 76.27±2.07 14.77±1.27 12.95±0.75 17.56±1.54 6.87±0.41</cell><cell>83.27±0.55 19.05±0.44</cell><cell>-± -19.47±0.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test mean and standard deviation of the classification accuracy of linear model for 10 trials. The best one is emphasized in bold. The results of experiments under the presence of unlabeled samples are shown in the second row corresponding to each dataset.</figDesc><table><row><cell>Datasets</cell><cell>MCL</cell><cell>MCUL</cell><cell>MCL cl</cell><cell>MCUL cl</cell></row><row><cell>PENDIGITS</cell><cell>84.32±1.09 11.44±5.68</cell><cell>-± -28.03±8.10</cell><cell>84.27±0.26 13.82±2.39</cell><cell>-± -38.47±5.68</cell></row><row><cell>LETTER</cell><cell>45.75±2.31 5.33±0.76</cell><cell>-± -8.41±1.86</cell><cell>53.29±5.31 17.32±1.84</cell><cell>-± -18.54±0.92</cell></row><row><cell>SATIMAGE</cell><cell>82.10±1.82 21.13±7.57</cell><cell>-± -51.35±7.94</cell><cell>83.53±1.19 39.61±9.57</cell><cell>-± -54.23±6.14</cell></row><row><cell>USPS</cell><cell>82.31±2.32 9.66±6.62</cell><cell>-± -26.20±3.98</cell><cell>81.07±1.58 24.29±4.72</cell><cell>-± -39.58±2.99</cell></row><row><cell>MNIST</cell><cell>77.36±1.27 25.86±3.49</cell><cell>-± -38.21±5.24</cell><cell>81.47±2.46 27.54±2.71</cell><cell>-± -41.58±5.27</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">'Arbitrary' means the samples can be equipped with different numbers of complementary labels.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. Due to assumption <ref type="bibr" target="#b3">(4)</ref>, the equations below hold:</p><p>We can get the following equation by substituting the second equation above into the first equation:</p><p>Then we can rewrite the classification risk (1) due to the equation above:</p><p>By exchanging the order of summation, we can get an equal version of the last equation above and finally conclude the proof:</p><p>We give the following conclusions:</p><p>Proof. The first inequality can be deduced from Lemma 8 directly. By definition and the sub-additivity of supremum: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Semi-Supervised Learning</title>
		<editor>Olivier Chapelle, Bernhard Schölkopf, and Alexander Zien</editor>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised classification based on classification from positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marthinus</forename><surname>Christoffel Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2998" to="3006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised optimal margin distribution machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3104" to="3110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast semi-supervised discriminant analysis for binary classification of large data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Tavernier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaak</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Meerbergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><forename type="middle">Kurt</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Ceulemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="86" to="99" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining deep generative and discriminative models for bayesian semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José Miguel Hernández-Lobato</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">107156</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning from partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1501" to="1536" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Disambiguation-free partial label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cai-Zhi</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2155" to="2167" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning classifiers from only positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Noto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convex formulation for learning from positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marthinus</forename><surname>Christoffel Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1386" to="1394" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient training for positive unlabeled learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Francesco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>De Natale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2584" to="2598" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classification from positive, unlabeled and biased negative data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Guan</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2820" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning from complementary labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Ishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5639" to="5649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning with biased complementary labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (1)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11205</biblScope>
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Complementary-label learning for arbitrary losses and models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Ishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2971" to="2980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Crowdsourcing: Why the Power of the Crowd Is Driving the Future of Business</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Crwon Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised online clustering and detection algorithms using crowdsourced data for malaria diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alba</forename><surname>Pagès-Zamora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margarita</forename><surname>Cabrera-Bean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Diaz-Vilor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="209" to="223" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crowdsourcing label quality: a theoretical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SCIENCE CHINA Information Sciences</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">When can unlabeled data improve the learning rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Göpfert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1500" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from incomplete and inaccurate supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Partial label learning with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An overview of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="988" to="999" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Statistical analysis of some multi-category large margin classification methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1225" to="1251" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the minimal supervision for training any binary classifier from only unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Foundations of Machine Learning. Adaptive computation and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Class-prior estimation for learning from positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marthinus</forename><surname>Christoffel Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="463" to="492" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Theoretical comparisons of positive-unlabeled learning against positivenegative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marthinus</forename><surname>Christoffel Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="1199" to="1207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>abs/1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep learning for classical japanese literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarin</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asanobu</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<idno>abs/1812.01718</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">EMNIST: extending MNIST to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Positive-unlabeled learning with non-negative risk estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Kiryo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Marthinus Christoffel du Plessis, and Masashi Sugiyama</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1675" to="1685" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Probability in Banach Spaces: isoperimetry and processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Talagrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On the method of bounded differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Mcdiarmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="148" to="188" />
		</imprint>
	</monogr>
	<note>Surveys in combinatorics. which concludes the proof of equation (21). (22) can be proved in the same way as in the proof of Lemma 13</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
