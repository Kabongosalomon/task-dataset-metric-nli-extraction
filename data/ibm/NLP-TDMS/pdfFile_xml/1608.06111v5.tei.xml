<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Incremental Parser for Abstract Meaning Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-04-10">10 Apr 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
							<email>m.damonte@sms.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<email>scohen@inf.ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
							<email>satta@dei.unipd.it</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Incremental Parser for Abstract Meaning Representation</title>
					</analytic>
					<monogr>
						<title level="m">From the Proceedings of EACL 2017</title>
						<meeting> <address><addrLine>Valencia, Spain</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2017-04-10">10 Apr 2017</date>
						</imprint>
					</monogr>
					<note type="submission">This version includes slightly more information than the published version (January, 2017).</note>
					<note>arXiv:1608.06111v5 [cs.CL]</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Meaning Representation (AMR) is a semantic representation for natural language that embeds annotations related to traditional tasks such as named entity recognition, semantic role labeling, word sense disambiguation and co-reference resolution. We describe a transition-based parser for AMR that parses sentences leftto-right, in linear time. We further propose a test-suite that assesses specific subtasks that are helpful in comparing AMR parsers, and show that our parser is competitive with the state of the art on the LDC2015E86 dataset and that it outperforms state-of-the-art parsers for recovering named entities and handling polarity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing aims to solve the problem of canonicalizing language and representing its meaning: given an input sentence, it aims to extract a semantic representation of that sentence. Abstract meaning representation <ref type="bibr" target="#b2">(Banarescu et al., 2013)</ref>, or AMR for short, allows us to do that with the inclusion of most of the shallow-semantic natural language processing (NLP) tasks that are usually addressed separately, such as named entity recognition, semantic role labeling and coreference resolution. AMR is partially motivated by the need to provide the NLP community with a single dataset that includes basic disambiguation information, instead of having to rely on different datasets for each disambiguation problem. The annotation process is straightforward, enabling the development of large datasets.</p><p>Several parsers for AMR have been recently developed <ref type="bibr" target="#b10">(Flanigan et al., 2014;</ref><ref type="bibr" target="#b26">Wang et al., 2015a;</ref><ref type="bibr" target="#b17">Peng et al., 2015;</ref><ref type="bibr" target="#b18">Pust et al., 2015;</ref><ref type="bibr" target="#b12">Goodman et al., 2016;</ref><ref type="bibr" target="#b19">Rao et al., 2015;</ref><ref type="bibr" target="#b25">Vanderwende et al., 2015;</ref><ref type="bibr" target="#b1">Artzi et al., 2015;</ref><ref type="bibr" target="#b3">Barzdins and Gosko, 2016;</ref><ref type="bibr" target="#b31">Zhou et al., 2016)</ref>. This line of research is new and current results suggest a large room for improvement. Greedy transition-based methods <ref type="bibr" target="#b16">(Nivre, 2008)</ref> are one of the most popular choices for dependency parsing, because of their good balance between efficiency and accuracy. These methods seem promising also for AMR, due to the similarity between dependency trees and AMR structures, i.e., both representations use graphs with nodes that have lexical content and edges that represent linguistic relations.</p><p>A transition system is an abstract machine characterized by a set of configurations and transitions between them. The basic components of a configuration are a stack of partially processed words and a buffer of unseen input words. Starting from an initial configuration, the system applies transitions until a terminal configuration is reached. The sentence is scanned left to right, with linear time complexity for dependency parsing. This is made possible by the use of a greedy classifier that chooses the transition to be applied at each step.</p><p>In this paper we introduce a parser for AMR that is inspired by the ARCEAGER dependency transition system of <ref type="bibr" target="#b15">Nivre (2004)</ref>. The main difference between our system and ARCEAGER is that we need to account for the mapping from word tokens to AMR nodes, non-projectivity of AMR structures and reentrant nodes (multiple incoming edges). Our AMR parser brings closer dependency parsing and AMR parsing by showing that dependency parsing algorithms, with some modifications, can be used for AMR. Key properties such as working left-to-right, incrementality 1 and linear complexity further strengthen its relevance.</p><p>The AMR parser of <ref type="bibr" target="#b26">Wang et al. (2015a)</ref>, called CAMR, also defines a transition system. It differs from ours because we process the sentence left-toright while they first acquire the entire dependency tree and then process it bottom-up. More recently <ref type="bibr" target="#b31">Zhou et al. (2016)</ref> presented a non-greedy transition system for AMR parsing, based on ARC-STANDARD <ref type="bibr" target="#b15">(Nivre, 2004)</ref>. Our transition system is also related to an adaptation of ARCEAGER for directed acyclic graphs (DAGs), introduced by <ref type="bibr" target="#b21">Sagae and Tsujii (2008)</ref>. This is also the basis for <ref type="bibr" target="#b20">Ribeyre et al. (2015)</ref>, a transition system used to parse dependency graphs. Similarly, <ref type="bibr" target="#b9">Du et al. (2014)</ref> also address dependency graph parsing by means of transition systems. Analogously to dependency trees, dependency graphs have the property that their nodes consist of the word tokens, which is not true for AMR. As such, these transition systems are more closely related to traditional transition systems for dependency parsing.</p><p>Our contributions in this paper are as follows:</p><p>• In §3 we develop a left-to-right, linear-time transition system for AMR parsing, inspired by the ARCEAGER transition system for dependency tree parsing;</p><p>• In §5 we claim that the Smatch score <ref type="bibr" target="#b6">(Cai and Knight, 2013)</ref> is not sufficient to evaluate AMR parsers and propose a set of metrics to alleviate this problem and better compare alternative parsers;</p><p>• In §6 we show that our algorithm is competitive with publicly available state-of-the-art parsers on several metrics.</p><p>2 Background and Notation AMR Structures AMRs are rooted and directed graphs with node and edge labels. An annotation example for the sentence I beg you to excuse me is shown in <ref type="figure">Figure 1</ref>, with the AMR graph reported in <ref type="figure">Figure 2</ref>. Concepts are represented as labeled nodes in the graph and can be either English words (e.g. I and you) or Propbank framesets (e.g. beg-01 and excuse-01). Each node in the graph is assigned to a variable in the AMR annotation so that a variable re-used in the annotation corresponds to reentrancies (multiple incoming edges) in the graph. Relations are represented as labeled and directed edges in the graph.</p><formula xml:id="formula_0">( b / beg-01 :ARG0 ( i / i :ARG1 ( y / you) :ARG2 ( e / excuse-01 :ARG0 y :ARG1 i ) )</formula><p>Notation For most sentences in our dataset, the AMR graph is a directed acyclic graph (DAG), with a few specific cases where cycles are permitted. These cases are rare, and for the purpose of this paper, we consider AMR as DAGs. We denote by [n] the set {1, . . . , n}. We define an AMR structure as a tuple <ref type="bibr">(G, x, π)</ref>, where x = x 1 · · · x n is a sentence, with each x i , i ∈ [n], a word token, and G is a directed graph G = (V, E) with V and E the set of nodes and edges, respectively. <ref type="bibr">2</ref> We assume G comes along with a node labeling function and an edge labeling function. Finally, π : V → [n] is a total alignment function that maps every node of the graph to an index i for the sentence x, with the meaning that node v represents (part of) the concept expressed by the word x π(v) . <ref type="bibr">3</ref> We note that the function π is not invertible, since it is neither injective nor surjective. For each i ∈ [n], we let</p><formula xml:id="formula_1">π −1 (i) = {v | v ∈ V, π(v) = i}</formula><p>be the pre-image of i under π (this set can be empty for some i), which means that we map a token in the sentence to a set of nodes in the AMR. In this way we can align each index i for x to the induced subgraph of G. More formally, we define <ref type="formula">(1)</ref> with the node and edge labeling functions of ← − π (i) inherited from G. Hence, ← − π (i) returns the AMR subgraph aligned with a particular token in the sentence.</p><formula xml:id="formula_2">← − π (i) = (π −1 (i), E ∩ (π −1 (i) × π −1 (i))),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transition-Based AMR Parsing</head><p>Similarly to dependency parsing, AMR parsing is partially based on the identification of predicateargument structures. Much of the dependency parsing literature focuses on transition-based dependency parsing-an approach to parsing that scans the sentence from left to right in linear time and updates an intermediate structure that eventually ends up being a dependency tree.</p><p>The two most common transition systems for greedy dependency parsing are ARCSTANDARD and ARCEAGER. With ARCSTANDARD, a stack is maintained along with a buffer on which the left-to-right scan is performed. At each step, the parser chooses to scan a word in the buffer and shift it onto the stack, or else to create an arc between the two top-most elements in the stack and pop the dependent. ARCSTANDARD parses a sentence in a pure bottom-up, left-to-right fashion (similarly to shift-reduce context-free grammar parsers), and must delay the construction of right arcs until all the dependent node has been completed. This imposes strong limitations on the degree of incrementality of the parser. The ARCEAGER system was designed to improve on ARCSTANDARD by mixing bottom up and topdown strategies. More precisely, in the ARCEA-GER parser left arcs are constructed bottom-up and right arcs are constructed top-down, so that right dependents can be attached to their heads even if some of their own dependents are not identified yet. In this way arcs are constructed as soon as the head and the dependent are available in the stack.</p><p>• I beg you excuse <ref type="figure">Figure 3</ref>: AMR's edges for the sentence "I beg you to excuse me." mapped back to the sentence, according to the alignment. • is a special token representing the root.</p><p>Because of the similarity of AMR structures to dependency structures, transition systems are also helpful for AMR parsing. Starting from the ARCEAGER system, we develop here a novel transition system, called AMREAGER that parses sentences into AMR structures. There are three key differences between AMRs and dependency trees that require further adjustments for dependency parsers to be used with AMRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Projectivity</head><p>A key difference between English dependency trees and AMR structures is projectivity. Dependency trees in English are usually projective, roughly meaning that there are no crossing arcs if the edges are drawn in the semiplane above the words. While this restriction is empirically motivated in syntactic theories for English, it is no longer motivated for AMR structures.</p><p>The notion of projectivity can be generalized to AMR graphs as follows. The intuition is that we can use the alignment π to map AMR edges back to the sentence x, and test whether there exist pairs of crossing edges. <ref type="figure">Figure 3</ref> shows this mapping for the AMR of <ref type="figure">Figure 2</ref>, where the edge connecting excuse to I crosses another edge. More formally, consider an AMR edge e = (u, ℓ, v). Let π(u) = i and π(v) = j, so that u is aligned with x i and v is aligned with x j . The spanning set for e, written S(e), is the set of all nodes w such that π(w) = k and i &lt; k &lt; j if i &lt; j or j &lt; k &lt; i if j &lt; i. We say that e is projective if, for every node w ∈ S(e), all of its parent and child nodes are in S(e) ∪ {u, v}; otherwise, we say that e is non-projective. An AMR is projective if all of its edges are projective, and is non-projective otherwise. This corresponds to the intuitive definition of projectivity for DAGs introduced in <ref type="bibr" target="#b21">Sagae and Tsujii (2008)</ref> and is closely related to the definition of non-crossing graphs of <ref type="bibr" target="#b13">Kuhlmann and Jonsson (2015)</ref>.</p><p>Non-projective edges 8% Non-projective AMRs 35% Reentrant edges 7% AMRs with at least one reentrancy 51% <ref type="table">Table 1</ref>: Statistics for non-projectivity and reentrancies in 200 AMR manually aligned with the associated sentences. 5 <ref type="table">Table 1</ref> demonstrates that a relatively small percentage of all AMR edges are non-projective. Yet, 35% of the sentences contain at least one nonprojective edge.</p><p>Reentrancy AMRs are graphs rather than trees because they can have nodes with multiple parents, called reentrant nodes, as in the node you for the AMR of <ref type="figure">Figure 2</ref>. There are two phenomena that cause reentrancies in AMR: control, where a reentrant edge appears between siblings of a control verb, and co-reference, where multiple mentions correspond to the same concept. <ref type="bibr">6</ref> In contrast, dependency trees do not have nodes with multiple parents. Therefore, when creating a new arc, transition systems for dependency parsing check that the dependent does not already have a head node, preventing the node from having additional parents. To handle reentrancy, which is not uncommon in AMR structures as shown in Table 1, we drop this constraint.</p><p>Alignment Another main difference with dependency parsing is that in AMR there is no straightforward mapping between a word in the sentence and a node in the graph: words may generate no nodes, one node or multiple nodes. In addition, the labels at the nodes are often not easily determined by the word in the sentence. For instance expectation translates to expect-01 and teacher translates to the two nodes teach-01 and person, connected through an :ARG0 edge, expressing that a teacher is a person who teaches. A mechanism of concept identification is therefore required to map each token x i to a subgraph with the correct labels at its nodes and edges: if π is the gold alignment, this should be the subgraph ← − π (i) 5 https://github.com/jflanigan/jamr/ blob/master/docs/Hand_Alignments.md 6 A valid criticism of AMR is that these two reentrancies are of a completely different type, and should not be collapsed together. Co-reference is a discourse feature, working by extra-semantic mechanisms and able to cross sentence boundaries, which are not crossed in AMR annotation. defined in Equation <ref type="formula">(1)</ref>. To obtain alignments between the tokens in the sentence and the nodes in the AMR graph of our training data, we run the JAMR aligner. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Transition system for AMR Parsing</head><p>A stack σ = σ n | · · · |σ 1 |σ 0 is a list of nodes of the partially constructed AMR graph, with the top element σ 0 at the right. We use the symbol '|' as the concatenation operator. A buffer β = β 0 |β 1 | · · · |β n is a list of indices from x, with the first element β 0 at the left, representing the word tokens from the input still to be processed. A configuration of our parser is a triple (σ, β, A), where A is the set of AMR edges that have been constructed up to this point.</p><p>In order to introduce the transition actions of our parser we need some additional notation. We use a function a that maps indices from x to AMR graph fragments. For each i ∈ [n], a(i) is a graph G a = (V a , E a ), with single root root(G a ), representing the semantic contribution of word x i to the AMR for x. As already mentioned, G a can have a single node representing the concept associated with x i , or it can have several nodes in case x i denotes a complex concept, or it can be empty.</p><p>The transition Shift is used to decide if and what to push on the stack after consuming a token from the buffer. Intuitively, the graph fragment a(β 0 ) obtained from the token β 0 , if not empty, is "merged" with the graph we have constructed so far. We then push onto the stack the node root(a(β 0 )) for further processing. LArc(ℓ) creates an edge with label ℓ between the top-most node and the second top-most node in the stack, and pops the latter. RArc(ℓ) is the symmetric operation, but does not pop any node from the stack.</p><p>Finally, Reduce pops the top-most node from the stack, and it also recovers reentrant edges between its sibling nodes, capturing for instance several control verb patterns. To accomplish this, Reduce decides whether to create an additional edge between the node being removed and the previously created sibling in the partial graph. This way of handling control verbs is similar to the REEN-TRANCE transition of <ref type="bibr" target="#b26">Wang et al. (2015a)</ref>.</p><p>The choice of popping the dependent in the LArc transition is inspired by ARCEAGER, where left-arcs are constructed bottom-up to increase the incrementality of the transition system <ref type="bibr" target="#b15">(Nivre, 2004)</ref>. This affects our ability to recover some reentrant edges: consider a node u with two parents v and v ′ , where the arc v → u is a left-arc and v ′ → u is any arc. If the first arc to be processed is v → u, we use LArc that pops u, hence making it impossible to create the second arc v ′ → u. Nevertheless, we discovered that this approach works better than a completely unrestricted allowance of reentrancy. The reason is that if we do not remove dependents at all when first attached to a node, the stack becomes larger, and nodes which should be connected end up being distant from each other, and as such, are never connected.</p><p>The initial configuration of the system has a • node (representing the root) in the stack and the entire sentence in the buffer. The terminal configuration consists of an empty buffer and a stack with only the • node. The transitions required to parse the sentence The boy and the girl are shown in <ref type="table" target="#tab_1">Table 2</ref>, where the first line shows the initial configuration and the last line shows the terminal configuration.</p><p>Similarly to the transitions of the ARCEAGER, the above transitions construct edges as soon as the head and the dependent are available in the stack, with the aim of maximizing the parser incrementality. We now show that our greedy transitionbased AMR parser is linear-time in n, the length of the input sentence x. We first claim that the output graph has size O(n). Each token in x is mapped to a constant number of nodes in the graph by Shift. Thus the number of nodes is O(n). Furthermore, each node can have at most three parent nodes, created by transitions RArc, LArc and Reduce, respectively. Thus the number of edges is also O(n). It is possible to bound the maximum number of transitions required to parse x: the number of Shift is bounded by n, and the number of Reduce, LArc and RArc is bounded by the size of the graph, which is O(n). Since each transition can be carried out in constant time, we conclude that our parser runs in linear time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training the System</head><p>Several components have to be learned: (1) a transition classifier that predicts the next transition given the current configuration, (2) a binary classifier that decides whether or not to create a reentrancy after a Reduce, (3) a concept identification step for each Shift to compute a(β 0 ), and 3) another classifier to label edges after each LArc or RArc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Oracle</head><p>Training our system from data requires an oracle-an algorithm that given a gold-standard AMR graph and a sentence returns transition sequences that maximize the overlap between the gold-standard graph and the graph dictated by the sequence of transitions.</p><p>We adopt a shortest stack, static oracle similar to <ref type="bibr" target="#b7">Chen and Manning (2014)</ref>. Informally, static means that if the actual configuration of the parser has no mistakes, the oracle provides a transition that does not introduce any mistake. Shortest stack means that the oracle prefers transitions where the number of items in the stack is minimized. Given the current configuration (σ, β, A) and the goldstandard graph G = (V g , A g ), the oracle is defined as follows, where we test the conditions in the given order and apply the action associated with the first match:</p><formula xml:id="formula_3">1. if ∃ℓ[(σ 0 , ℓ, σ 1 ) ∈ A g ] then LArc(ℓ); 2. if ∃ℓ[(σ 1 , ℓ, σ 0 ) ∈ A g ] then RArc(ℓ); 3. if ¬∃i, ℓ[(σ 0 , ℓ, β i ) ∈ A g ∨ (β i , ℓ, σ 0 ) ∈ A g ]</formula><p>then Reduce;</p><p>4. Shift otherwise.</p><p>The oracle first checks whether some goldstandard edge can be constructed from the two elements at the top of the stack (conditions 1 and 2). If LArc or RArc are not possible, the oracle checks whether all possible edges in the gold graph involving σ 0 have already been processed, in which case it chooses Reduce (conditions 3). To this end, it suffices to check the buffer, since LArc and RArc have already been excluded and elements in the stack deeper than position two can no longer be accessed by the parser. If Reduce is not possible, Shift is chosen.</p><p>Besides deciding on the next transition, the oracle also needs the alignments, which we generate with JAMR, in order to know how to map the next token in the sentence to its AMR subgraph ← − π (i) defined in (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transition Classifier</head><p>Like all other transition systems of this kind, our transition system has a "controller" that predicts a transition given the current configuration (among  As a classifier, we use a feed-forward neural network with two hidden layers of 200 tanh units and learning rate set to 0.1, with linear decaying. The input to the network consists of the concatenation of embeddings for words, POS tags and Stanford parser dependencies, one-hot vectors for named entities and additional sparse features, extracted from the current configuration of the transition system; this is reported in more details in <ref type="table" target="#tab_2">Table 3</ref>. The embeddings for words and POS tags were pre-trained on a large unannotated corpus consisting of the first 1 billion characters from Wikipedia. 8 For lexical information, we also extract the leftmost (in the order of the aligned words) child (c), leftmost parent (p) and leftmost grandchild (cc). Leftmost and rightmost items are common features for transition-based parsers <ref type="bibr" target="#b30">(Zhang and Nivre, 2011;</ref><ref type="bibr" target="#b7">Chen and Manning, 2014)</ref> but we found only leftmost to be helpful in our case. All POS tags, dependencies and named entities are generated using Stanford CoreNLP . The accuracy of this classifier on the development set is 84%.</p><p>Similarly, we train a binary classifier for deciding whether or not to create a reentrant edge after a Reduce: in this case we use word and POS embeddings for the two nodes being connected and their parent as well as dependency label embeddings for the arcs between them. 8 http://mattmahoney.net/dc/enwik9.zip</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Concept Identification</head><p>This routine is called every time the transition classifier decides to do a Shift; it is denoted by a(·) in §3. This component could be learned in a supervised manner, but we were not able to improve on a simple heuristic, which works as follows: during training, for each Shift decided by the oracle, we store the pair (β 0 , ← − π (i)) in a phrase-table. During parsing, the most frequent graph H for the given token is then chosen. In other words, a(i) approximates ← − π (i) by means of the graph most frequently seen among all occurrences of token x i in the training set.</p><p>An obvious problem with the phrase-table approach is that it does not generalize to unseen words. In addition, our heuristic relies on the fact that the mappings observed in the data are correct, which is not the case when the JAMR-generated alignments contain a mistake. In order to alleviate this problem we observe that there are classes of words such as named entities and numeric quantities that can be disambiguated in a deterministic manner. We therefore implement a set of "hooks" that are triggered by the named entity tag of the next token in the sentence. These hooks override the normal Shift mechanism and apply a fixed rule instead. For instance, when we see the token New York (the two tokens are collapsed in a single one at preprocessing) we generate the subgraph of <ref type="figure">Figure 4</ref> and push its root onto the stack. Similar subgraphs are generated for all states, cities, countries and people. We also use hooks for ordinal numbers, percentages, money and dates.</p><p>depth and #p count the number of children and parents, respectively, of a stack element. The function w maps a stack/buffer element to the word embedding for the associated word in the sentence. The function p gives the leftmost (according to the alignment) parent of a stack element, the function c the leftmost child and the function cc the leftmost grandchild. The function s maps a stack/buffer element to the part-of-speech embedding for the associated word. The function e maps a stack/buffer element to its entity. Finally, the function ℓ maps a pair of symbols to the dependency label embedding, according to the edge (or lack of) in the dependency tree for the two words these symbols are mapped to.</p><formula xml:id="formula_4">d(σ 0 ), d(σ 1 ) children #c(σ 0 ), #c(σ 1 ) parents #p(σ 0 ), #p(σ 1 ) lexical w(σ 0 ), w(σ 1 ), w(β 0 ), w(β 1 ), w(p(σ 0 )), w(c(σ 0 )), w(cc(σ 0 )), w(p(σ 1 )), w(c(σ 1 )), w(cc(σ 1 )) POS s(σ 0 ), s(σ 1 ), s(β 0 ), s(β 1 ) entities e(σ 0 ), e(σ 1 ), e(β 0 ), e(β 1 ) dependency ℓ(σ 0 , σ 1 ), ℓ(σ 1 , σ 0 ), ∀i ∈ {0, 1}: ℓ(σ i , β 0 ), ℓ(β 0 , σ i ) ∀i ∈ {1, 2, 3}: ℓ(β 0 , β i ), ℓ(β i , β 0 ) ∀i ∈ {1, 2, 3}: ℓ(σ 0 , β i ), ℓ(β i , σ 0 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Edge Labeling</head><p>Edge labeling determines the labels for the edges being created. Every time the transition classifier decides to take an LArc or RArc operation, the edge labeler needs to decide on a label for it. There are more than 100 possible labels such as :ARG0, :ARG0-of, :ARG1, :location, :time and :polarity. We use a feed-forward neural network similar to the one we trained for the transition classier, with features shown in <ref type="table" target="#tab_4">Table 4</ref>. The accuracy of this classifier on the development set is 77%.</p><p>Labeling Rules Sometimes the label predicted by the neural network is not a label that satisfies the requirements of AMR. For instance, the label :top can only be applied when the node from which the edge starts is the special • node. In order to avoid generating such erroneous labels, we use a set of rules, shown in  created edge so that we only consider those during prediction. Also ARG roles cannot always be applied: each Propbank frame allows a limited number of arguments. For example, while add-01 and add-02 allow for :ARG1 and :ARG2 (and their inverse :ARG1-of and :ARG2-of ), add-03 and add-04 only allow :ARG2 (and :ARG2-of ).</p><formula xml:id="formula_5">name feature template depth d(σ 0 ), d(σ 1 ) children #c(σ 0 ), #c(σ 1 ) parents #p(σ 0 ), #p(σ 1 ) lexical w(σ 0 ), w(σ 1 ), w(p(σ 0 )), w(c(σ 0 )), w(cc(σ 0 )), w(p(σ 1 )), w(c(σ 1 )), w(cc(σ 1 )) POS s(σ 0 ), s(σ 1 ) entities e(σ 0 ), e(σ 1 ) dependency ℓ(σ 0 , β 0 ), ℓ(β 0 , σ 0 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Fine-grained Evaluation</head><p>Until now, AMR parsers were evaluated using the Smatch score. 9 Given the candidate graphs and the gold graphs in the form of AMR annotations, Smatch first tries to find the best alignments between the variable names for each pair of graphs and it then computes precision, recall and F1 of the concepts and relations. We note that the Smatch score has two flaws: (1) while AMR parsing involves a large number of subtasks, the Smatch score consists of a single number that does not assess the quality of each subtasks separately; (2) the Smatch score weighs different types of errors in <ref type="table" target="#tab_3">Table 5</ref>: Labeling rules: For each edge label, we provide regular expressions that must hold on the labels at the start node (start) and the end node (end) of the edge. Ex. indicates when the rule is exclusive, d-ent is the AMR concept date-entity, inter. is the AMR constant interrogative, expr. is the AMR constant expressive, imp. is the AMR constant imperative. a way which is not necessarily useful for solving a specific NLP problem. For example, for a specific problem concept detection might be deemed more important than edge detection, or guessing the wrong sense for a concept might be considered less severe than guessing the wrong verb altogether.</p><p>Consider the two parses for the sentence Silvio Berlusconi gave Lucio Stanca his current role of modernizing Italy's bureaucracy in <ref type="figure">Figure 5</ref>. At the top, we show the output of a parser (Parse 1) that is not able to deal with named entities. At the bottom, we show the output of a parser (Parse 2) which, except for :name, :op and :wiki, always uses the edge label :ARG0. The Smatch scores for the two parses are 56 and 78 respectively. Both parses make obvious mistakes but the three named entity errors in Parse 1 are considered more important than the six wrong labels in Parse 2. However, without further analysis, it is not advisable to conclude that Parse 2 is better than Parse 1. In order to better understand the limitations of the different parsers, find their strengths and gain insight in which downstream tasks they may be helpful, we compute a set of metrics on the test set.</p><p>Unlabeled is the Smatch score computed on the predicted graphs after removing all edge labels. In this way, we only assess the node labels ( g / give-01 :ARG0 ( p3 / silvio :mod ( n4 / berlusconi ) ) :ARG1 ( r / role :time ( c2 / current ) :mod ( m / modernize-01 :ARG0 p4 :ARG1 ( b / bureaucracy :part-of ( c3 / italy ) ) ) :poss p4 ) :ARG2 ( p4 / person lucio :mod stanca ) ) ( g / give-01 :ARG0 ( p3 / person :wiki " S i l v i o B e r l u s c o n i " :name ( n4 / name :op1 " S i l v i o " :op2 " B e r l u s c o n i " ) ) :ARG0 ( r / role :ARG0 ( c2 / current ) :ARG0 ( m / modernize-01 :ARG0 p4 :ARG0 ( b / bureaucracy :ARG0 ( c3 / country :wiki " I t a l y " :name ( n6 / name :op1 " I t a l y " ) ) ) ) :ARG0 p4 ) :ARG0 ( p4 / person :wiki − :name ( n5 / name :op1 " L u c i o " :op2 " S t a n c a " ) ) ) <ref type="figure">Figure 5</ref>: Two parses for the sentence "Silvio Berlusconi gave Lucio Stanca his current role of modernizing Italy's bureaucracy."</p><p>and the graph topology, which may be enough to benefit several NLP tasks because it identifies basic predicate-argument structure. For instance, we may be interested in knowing whether two events or entities are related to each other, while not being concerned with the precise type of relation holding between them.</p><p>No WSD gives a score that does not take into account word sense disambiguation errors. By ignoring the sense specified by the Propbank frame used (e.g., duck-01 vs duck-02) we have a score that does not take into account this additional complexity in the parsing procedure. To compute this score, we simply strip off the suffixes from all Propbank frames and calculate the Smatch score.</p><p>Following <ref type="bibr" target="#b22">Sawai et al. (2015)</ref>, we also evaluate the parsers using the Smatch score on noun phrases only (NP-only), by extracting from the AMR dataset all noun phrases that do not include further NPs.</p><p>As we previously discussed, reentrancy is a very important characteristic of AMR graphs and it is not trivial to handle. We therefore implement a test for it (Reentrancy), where we compute the Smatch score only on reentrant edges.</p><p>Concept identification is another critical component of the parsing process and we therefore compute the F-score on the list of predicted concepts (Concepts) too. Identifying the correct concepts is fundamental: if a concept is not identified, it will not be possible to retrieve any edge involving that concept, with likely significant consequences on accuracy. This metric is therefore  <ref type="table">Table 6</ref>: Evaluation of the two parses in <ref type="figure">Figure 5</ref> with the proposed evaluation suite.</p><p>quite important to score highly on.</p><p>Similarly to our score for concepts, we further compute an F-score on the named entities (Named Ent.) and wiki roles for named entities (Wikification) that consider edges labeled with :name and :wiki respectively. These two metrics are strictly related to the concept score. However, since named entity recognition is the focus of dedicated research, we believe it is important to define a metric that specifically assesses this problem. Negation detection is another task which has received some attention. An F-score for this (Negations) is also defined, where we find all negated concepts by looking for the :polarity role. The reason we can compute a simple F-score instead of using Smatch for these metrics is that there are no variable names involved.</p><p>Finally we compute the Smatch score on :ARG edges only, in order to have a score for semantic role labeling (SRL), which is another extremely important subtask of AMR, as it is based on the identification of predicate-argument structures.</p><p>Using this evaluation suite we can evaluate AMRs on a wide range of metrics that can help us find strengths and weakness of each parser, hence speeding up the research in this area. <ref type="table">Table 6</ref> reports the scores for the two parses of <ref type="figure">Figure 5</ref>, where we see that Parse 1 gets a high score for semantic role labeling while Parse 2 is optimal for named entity recognition. Moreover, we can make additional observations such as that Parse 2 is optimal with respect to unlabeled score and that Parse 1 recovers more reentrancies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We compare our parser 10 against two available parsers: JAMR <ref type="bibr" target="#b10">(Flanigan et al., 2014)</ref> and CAMR <ref type="bibr" target="#b27">(Wang et al., 2015b;</ref><ref type="bibr" target="#b26">Wang et al., 2015a)</ref>, using the LDC2015E86 dataset for evaluation. Both parsers are available online 11 and were recently updated for SemEval-2016 Task 8 <ref type="bibr" target="#b11">(Flanigan et al., 2016;</ref><ref type="bibr" target="#b28">Wang et al., 2016)</ref>. However, CAMR's SemEval system, which reports a Smatch score of 67, is not publicly available. CAMR has a quadratic worst-case complexity (although linear in practice). In JAMR, the concept identification step is quadratic and the relation identification step is O(|V | 2 log |V |), with |V | being the set of nodes in the AMR graph. <ref type="table" target="#tab_6">Table 7</ref> shows the results obtained by the parsers on all metrics previously introduced. On Smatch, our system does not give state-of-the-art results. However, we do obtain the best results for Unlabeled and Concept and outperform the other parses for Named Ent. and Negations. Our score of Reentrancy is also close the best scoring system, which is particularly relevant given the importance of reentrancies in AMR. The use of the Reduce transition, which targets reentrancies caused by control verbs, is critical in order to achieve this result.</p><p>The relatively high results we obtain for the un-labeled case suggests that our parser has difficulty in labeling the arcs. Our score for concept identification, which is on par with the best result from the other parsers, demonstrates that there is a relatively low level of token ambiguity. State-of-theart results for this problem can be obtained by choosing the most frequent subgraph for a given token based on a phrase-table constructed from JAMR alignments on the training data. The scores for named entities and wikification are heavily dependent on the hooks mentioned in §4.3, which in turn relies on the named entity recognizer to make the correct predictions. In order to alleviate the problem of wrong automatic alignments with respect to polarity and better detect negation, we performed a post-processing step on the aligner output where we align the AMR constant -(minus) with words bearing negative polarity such as not, illegitimate and asymmetry.</p><p>Our experiments demonstrate that there is no parser for AMR yet that conclusively does better than all other parsers on all metrics. Advantages of our parser are the worst-case linear complexity and the fact that is possible to perform incremental AMR parsing, which is both helpful for realtime applications and to investigate how meaning of English sentences can be built incrementally left-to-right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The first data-driven AMR parser is due to <ref type="bibr" target="#b10">Flanigan et al. (2014)</ref>. The problem is addressed in two separate stages: concept identification and relation identification. They use a sequence labeling algorithm to identify concepts and frame the relation prediction task as a constrained combinatorial optimization problem. <ref type="bibr" target="#b29">Werling et al. (2015)</ref> notice that the difficult bit is the concept identification and propose a better way to handle that task: an action classifier to generate concepts by applying predetermined actions. Other proposals involve a synchronous hyperedge replacement grammar solution <ref type="bibr" target="#b17">(Peng et al., 2015)</ref>, a syntaxbased machine translation approach <ref type="bibr" target="#b18">(Pust et al., 2015)</ref> where a grammar of string-to-tree rules is created after reducing AMR graphs to trees by removing all reentrancies, a CCG system that first parses sentences into lambda-calculus representations <ref type="bibr" target="#b1">(Artzi et al., 2015)</ref>. A systematic translation from AMR to first order logic formulas, with a special treatment for quantification, reentrancy and negation, is discussed in <ref type="bibr" target="#b5">Bos (2016)</ref>. In <ref type="bibr" target="#b25">Vanderwende et al. (2015)</ref>, a pre-existing logical form parser is used and the output is then converted into AMR graphs. Yet another solution is proposed by <ref type="bibr" target="#b19">Rao et al. (2015)</ref> who discuss a parser that uses SEARN <ref type="bibr" target="#b8">(Daumé III et al., 2009</ref>), a "learning to search" algorithm.</p><p>Transition-based algorithms for AMR parsing are compelling because traditional graph-based techniques are computationally expensive. <ref type="bibr" target="#b27">Wang et al. (2015b)</ref> and <ref type="bibr" target="#b26">Wang et al. (2015a)</ref> propose a framework that parses a sentence into its AMR structure through a two-stage process: a dependency tree is generated from the input sentence through a transition-based parser and then another transition-based parser is used to generate the AMR. The main benefit of this approach is that the dependency parser can be trained on a training set much larger than the training set for the treeto-graph algorithm. Others further built on this parser: <ref type="bibr" target="#b12">Goodman et al. (2016)</ref> use imitation learning to alleviate the probem of error propagation in the greedy parser, while <ref type="bibr" target="#b3">Barzdins and Gosko (2016)</ref> create a wrapper around it to fix frequent mistakes and investigate ensembles with a character level neural parser. More recently <ref type="bibr" target="#b31">Zhou et al. (2016)</ref> presented a non-greedy transition system for AMR parsing, based on ARCSTANDARD <ref type="bibr" target="#b15">(Nivre, 2004)</ref>.</p><p>AMR parsing as a whole is a complex task because it involves many subtasks including named entity recognition, co-reference resolution and semantic role labeling. <ref type="bibr" target="#b22">Sawai et al. (2015)</ref> do not attempt at parsing AMR graphs for entire sentences but they instead handle simple noun phrases (NPs). They extract NPs from the AMR dataset only when they do not include further NPs, do not include pronouns nor named entities. Due to these restrictions, the AMRs are mostly trees and easier to handle than the original AMR graphs. They approach this task using a transition based system inspired by ARCSTANDARD.</p><p>AMR is not the only way to represent meaning in natural language sentences. Alternative semantic representations have been developed and studied, such as Boxer <ref type="bibr" target="#b4">(Bos et al., 2004)</ref>, CCG <ref type="bibr" target="#b23">(Steedman, 1996;</ref><ref type="bibr" target="#b24">Steedman, 2000)</ref> and UCCA <ref type="bibr" target="#b0">(Abend and Rappoport, 2013)</ref>.</p><p>We presented a transition system that builds AMR graphs in linear time by processing the sentences left-to-right, trained with feed-forward neural networks. The parser demonstrates that it is possible to perform AMR parsing using techniques inspired by dependency parsing.</p><p>We also noted that it is less informative to evaluate the entire parsing process with Smatch than to use a collection of metrics aimed at evaluating the various subproblems in the parsing process. We further showed that our left-to-right transition system is competitive with publicly available state-of-the-art parsers. Although we do not outperform the best baseline in terms of Smatch score, we show on par or better results for several of the metrics proposed. We hope that moving away from a single-metric evaluation will further speed up progress in AMR parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Annotation for the sentence "I beg you to excuse me." Variables are in boldface and concepts and edge labels are in italics. AMR graph representation forFigure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Parsing steps for the sentence "The boy and the girl."</figDesc><table><row><cell>Shift, LArc, RArc and Reduce). The examples</cell></row><row><cell>from which we learn this controller are based on</cell></row><row><cell>features extracted from the oracle transition se-</cell></row><row><cell>quences, where the oracle is applied on the train-</cell></row><row><cell>ing data.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Features used in transition classifier. The function d maps a stack element to the depth of the associated graph fragment. The functions #c</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>. These rules</cell></row><row><cell>determine which labels are allowed for the newly</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Features used in edge labeling. See Table 3 for a legend of symbols.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Results on test split of LDC2015E86 for JAMR, CAMR and our AMREAGER. J stands for JAMR and C for CAMR (followed by the year of publication). Best systems are in bold.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Strictly speaking, transition-based parsing cannot achieve full incrementality, which requires to have a single connected component at all times<ref type="bibr" target="#b15">(Nivre, 2004)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We collapse all multi-word named entities in a single token (e.g., United Kingdom becomes United Kingdom) both in training and parsing.3 π is a function because we do not consider co-references, which would otherwise cause a node to map to multiple indices. This is in line with current work on AMR parsing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Since Smatch is an approximate randomized algorithm, decimal points in the results vary between different runs and are not reported. This approach was also taken by<ref type="bibr" target="#b27">Wang et al. (2015b)</ref> and others.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Our parser is available at https://github.com/ mdtux89/amr-eager, the evaluation suite at https: //github.com/mdtux89/amr-evaluation and a demo at http://cohort.inf.ed.ac.uk/ amreager.html 11 JAMR: https://github.com/jflanigan/ jamr, CAMR: https://github.com/c-amr/camr.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the three anonymous reviewers and Sameer Bansal, Jeff Flanigan, Sorcha Gilroy, Adam Lopez, Nikos Papasarantopoulos, Nathan Schneider, Mark Steedman, Sam Thomson, Clara Vania and Chuan Wang for their help and comments. This research was supported by a grant from Bloomberg and by the H2020 project SUMMA, under grant agreement 688139.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Universal conceptual cognitive annotation (UCCA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Broad-coverage CCG semantic parsing with AMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Linguistic Annotation Workshop</title>
		<meeting>Linguistic Annotation Workshop<address><addrLine>Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guntis</forename><surname>Barzdins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didzis</forename><surname>Gosko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01278</idno>
		<title level="m">RIGA at SemEval-2016 task 8: Impact of smatch extensions and character-level neural translation on AMR parsing accuracy</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Widecoverage semantic representations from a ccg parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>James R Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING. Association for Computational Linguistics</title>
		<meeting>COLING. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Expressive power of abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceesings of EMNLP</title>
		<meeting>eesings of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Peking: Profiling syntactic tree parsing techniques for semantic graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014)</title>
		<meeting>the 8th International Workshop on Semantic Evaluation (SemEval-2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="459" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
		<title level="m">CMU at SemEval-2016 task 8: Graph-based AMR parsing with infinite ramp loss. Proceedings of SemEval</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1202" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Noise reduction and targeted exploration in imitation learning for abstract meaning representation parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jonsson</surname></persName>
		</author>
		<title level="m">Parsing to noncrossing dependency graphs. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="559" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together. ACL</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together. ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A synchronous hyperedge replacement grammar based approach for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Using syntaxbased machine translation to parse english into abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06665</idno>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudh</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogarshi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.07586</idno>
		<title level="m">Parser for abstract meaning representation using learning to search</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Because syntax does matter: Improving predicate-argument structures parsing using syntactic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Ribeyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Villemonte De La Clergerie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shift-reduce dependency DAG parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COL-ING</title>
		<meeting>COL-ING</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic structure analysis of noun phrases using abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiro</forename><surname>Sawai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Surface Structure and Interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Syntactic Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An AMR parser for english, french, german, spanish and japanese and a new AMR-annotated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Boosting transition-based AMR parsing with refined actions and auxiliary analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A transition-based algorithm for AMR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">CAMR at SemEval-2016 task 8: An extended transition-based AMR parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proceedings of SemEval</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenon</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03139</idno>
		<title level="m">Robust subgraph generation improves abstract meaning representation parsing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AMR parsing with an incremental joint model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">U</forename><surname>Weiguang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhui</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
