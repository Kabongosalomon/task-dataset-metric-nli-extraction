<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly-Supervised Learning for Tool Localization in Laparoscopic Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armine</forename><surname>Vardazaryan</surname></persName>
							<email>vardazaryan@unistra.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>IHU</postCode>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">University Hospital of Strasbourg, IRCAD, IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Marescaux</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">University Hospital of Strasbourg, IRCAD, IHU Strasbourg</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
							<email>npadoy@unistra.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>IHU</postCode>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly-Supervised Learning for Tool Localization in Laparoscopic Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>surgical tool localization</term>
					<term>endoscopic videos</term>
					<term>weakly-supervised learning</term>
					<term>Cholec80</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surgical tool localization is an essential task for the automatic analysis of endoscopic videos. In the literature, existing methods for tool localization, tracking and segmentation require training data that is fully annotated, thereby limiting the size of the datasets that can be used and the generalization of the approaches. In this work, we propose to circumvent the lack of annotated data with weak supervision. We propose a deep architecture, trained solely on image level annotations, that can be used for both tool presence detection and localization in surgical videos. Our architecture relies on a fully convolutional neural network, trained end-to-end, enabling us to localize surgical tools without explicit spatial annotations. We demonstrate the benefits of our approach on a large public dataset, Cholec80, which is fully annotated with binary tool presence information and of which 5 videos have been fully annotated with bounding boxes and tool centers for the evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The automatic analysis of surgical videos is at the core of many potential assistance systems for the operating room. The localization of surgical tools, in particular, is required in many applications, such as the analysis of tool-tissue interactions, the development of novel human-robot assistance platforms and the automated annotation of video databases.</p><p>In the literature, surgical tool localization has traditionally been approached with fully supervised methods <ref type="bibr" target="#b0">[1]</ref>, with the most recent localization and segmentation methods relying on deep learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>. However, training fully supervised approaches require the data to be fully annotated with spatial information, which is tedious and expensive. This may explain why the datasets used so far for tool localization are small, namely in the order of a few thousand images and with a maximum of 5-6 sequences, as described in the recent review <ref type="bibr" target="#b0">[1]</ref>. This then limits the applicability and generalizability of the approaches that can be developed.</p><p>Recently, it has been shown that when a convolutional neural network is trained for the task of classification, the convolutional layers of the network learn general notions about the detected objects. Some recent works have used this fact to successfully localize objects in images without explicitly training for localization <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. The proposed deep learning approaches directly output spatial heat maps, where the detected position corresponds to the strongest activations. This is achieved by replacing all fully connected layers with equivalent convolutions or removing them altogether. The resulting architectures are called fully convolutional networks (FCNs). Others have extended this approach to address the challenging task of semantic segmentation with weak supervision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>. In the medical community as well, weakly supervised learning (WSL) has been applied to tasks such as detection of cancerous regions in medical images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Along with the recent release of large public surgical video datasets, such as Cholec80 <ref type="bibr" target="#b15">[16]</ref>, which contains 80 complete cholecystectomy videos fully annotated with binary tool presence information (∼180K frames in total), WSL techniques can potentially help develop tool localization methods that can scale up to larger datasets containing much more variability.</p><p>In this paper, we propose a method for detecting and localizing surgical tools. It is based on weakly-supervised learning using only image-level labels and does not require any spatial annotation. Our contributions are twofold: (1) we propose the first surgical tool localization approach based on weakly-supervised learning;</p><p>(2) we demonstrate our approach on the largest public endoscopic video dataset to date, namely Cholec80 <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this work, we present a method for the localization of surgical tools in endoscopic videos that does not require spatial annotations. This is possible with a FCN architecture that preserves the spatial information and permits us to observe activation regions where the tool is detected. Therefore, our method addresses two tasks: binary presence classification and tool localization, with the latter hinging on the former.</p><p>Our model takes an image as input and returns C localization heat maps, where C is the number of tools to be detected. For our task on the Cholec80 dataset, C = 7. The heatmaps are used to find confidence values for each class and perform the binary classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Architecture</head><p>As basis for our network (illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>), we use ResNet18 <ref type="bibr" target="#b4">[5]</ref> because it has been shown to perform well on a multitude of tasks. Since we want to preserve relative spatial information throughout our network, we remove the fully connected layer and average pooling from the end of the network. Additionally, we change the stride in the last two banks of ResNet from 2 to 1 pixel to obtain localization maps with a higher resolution. Note that reducing the strides for all Then, we convert the 512 feature maps into localization maps by adding a convolutional layer of 1 × 1 kernels. To obtain one map per class, we set the number of filters in this layer to C. Finally, with pooling we transform these maps into a vector of class-wise confidence values, which are, in turn, used for the binary classification of the tools. Instead of using conventional max pooling, we use the extended spatial pooling (ESP) s c = max z c +α min z c from <ref type="bibr" target="#b1">[2]</ref>, which extracts more details about the detection of the object. In the equation, z c ∈ IR 2 is the localization map for class c and α is 0.6 as advised by <ref type="bibr" target="#b1">[2]</ref>.</p><p>During inference, we use the raw localization maps to find the predicted position of the tools. First, the localization maps are resized to the original size of the input image with bilinear interpolation. Then, the position of the maximum activation is considered to be the predicted location of the tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head><p>Before training on Cholec80, the ResNet layers are initialized from ImageNet weights. During training, data is first randomly shuffled and batched, then data augmentation is applied independently to each image in a batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>During training, all images in the batch are augmented before being given to the network. Augmentation includes horizontal flipping, random rotation by +90/-90 degrees, as well as the masking procedure introduced in <ref type="bibr" target="#b14">[15]</ref>. Masking entails randomly replacing patches in the image with the mean pixel of the train set. This improves the quality of predicted localization maps. Loss The models are trained for multi-label classification with a weighted crossentropy loss L presented in Equation 1, where k c and v c are respectively the ground truth and predicted tool presence for class c, σ is the sigmoid function, and W c is the weight for class c. Weights are added to counteract the polarizing effect of class imbalance. The weight for each class is inversely proportional to the number of occurrences of the class in the train set.</p><formula xml:id="formula_0">L = C c=1 −1 N [W c k c log(σ(v c )) + (1 − k c ) log(1 − σ(v c ))]<label>(1)</label></formula><p>3 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>For our experiments, we use the Cholec80 dataset <ref type="bibr" target="#b15">[16]</ref> containing 80 videos of cholecystectomy procedures, fully annotated with image-level surgical tool labels for binary detection. Our training, validation and test sets consist of 40, 10 and 30 videos, respectively. Additionally, for the purpose of evaluating the performance of our localization method, our team has fully annotated 5 videos from the test set with bounding boxes and tool centers. The details of these annotations are presented in <ref type="table" target="#tab_0">Table 1</ref>. They are also illustrated in column 1 of <ref type="figure" target="#fig_2">Figure 3</ref>. As part of the preprocessing, we randomly mask patches of 30 × 30 by filling these squares with the average pixel value of the test set. For each patch, the probability of masking is 0.5. We train all the evaluated models for 120 epochs with an initial learning rate of 0.1, which decreases by a factor of 10 at [60, 100] epochs. That learning rate is applied to the new convolutional layer, while the layers of ResNet are trained with a learning rate smaller by a factor of 100. In our loss function, we use a weight decay of 10 −4 . The models were trained with the momentum optimizer (momentum µ = 0.9) and batch size of 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluated Models</head><p>We evaluate several variants of the architecture presented in section 2.1 in order to compare the differences and search for the best performing configuration. Multi-maps Our network architecture contains a convolutional layer of 7 kernels, each dedicated to one tool. Introduced in <ref type="bibr" target="#b1">[2]</ref>, the notion of multi-maps is based on the following idea: instead of using a single kernel for each class, multiple kernels can be used and be followed by class-wise averaging to obtain 7 localization maps. This helps the network to extract more details about the object than when a single feature map is used. The authors of <ref type="bibr" target="#b1">[2]</ref> advise to use 8 kernels per class. However, since the objects we detect are significantly simpler than the classes used in <ref type="bibr" target="#b1">[2]</ref>, we use only 4 kernels per class (28 filters altogether).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classification</head><p>As mentioned above, we use the dataset Cholec80 to test our method. Specifically, the 30 videos of the test set are used for testing the classification performance. To quantify the results, we use average precision (AP), which is defined as the area under the precision-recall curve. We illustrate the curve for architecture FCN ESP MM Msk in <ref type="figure" target="#fig_1">Figure 2</ref>, where we see that results for scissors and clipper fall behind the rest of the tools. A similar pattern can be observed in <ref type="table" target="#tab_1">Table  2</ref>. All models detect most tools quite well with AP values above 93%. However, the results for scissors (∼50%) and clipper (∼82%) are significantly worse than those of the other tools. This may be due to the fact that scissors and clipper are present only in 2% and 4% of annotations, respectively. In contrast, hook is present in 64% of all annotations (see <ref type="table" target="#tab_0">Table 1</ref>, row 2). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Localization</head><p>With our method, we are able to obtain localization maps that contain information about the positions of the tools in the frame. Multiple classes of tools can be detected in the same frame. Note, however, that our approach is not designed to detect multiple instances of the same class, because all instances would share the same localization map. In this work, we limit detection to a single instance of each type of tool, even though multiple instance detection could, for example, be possible with post-processing heuristics.</p><p>We evaluate the quality of the predictions by comparing them against the ground truth bounding boxes that we have annotated for that purpose. In the cases where multiple instances of the same tool are present in the frame, we pick the bounding box closest to the prediction.</p><p>Localization AP To evaluate the quality of localization, we compute AP as described in <ref type="bibr" target="#b5">[6]</ref>, which is based on a metric defined in <ref type="bibr" target="#b11">[12]</ref>. If the predicted location lies in a ground truth bounding box of the same class, with a tolerance of 8 pixels (the global stride of the network), the example is considered a true positive. Otherwise, it is a false positive. Taking that into account, we compute precision and recall as described in <ref type="bibr" target="#b2">[3]</ref>, where recall is defined as the proportion of positive predictions, and precision is the proportion of true positives in positive predictions. AP is then computed as the area under the precision-recall curve. For this evaluation, we use only the positive classes as the negative class corresponds to having no tool in the image and cannot be annotated with a bounding box. The results of this computation are presented in <ref type="table">Table 3</ref>. The localization AP values for all models are similar, ranging approximately between 87% and 89%. Our intuition is that all models are almost equally likely to predict a tool center that lies in the bounding box, without capturing the quality of the precise location inside the bounding box. In the next section, we quantify the accuracy of the predicted tool centers relative to ground truth. Distance Error Localization AP gives a coarse idea about the quality of obtained predictions. To get a better sense of the accuracy of the localization, we compute the distance between the predicted tool center and its ground truth. We normalize this value by the diagonal of the image. The results are presented in <ref type="table" target="#tab_2">Table 4</ref>. We can see that, generally, masking and ESP improve the quality of predicted tool centers. On the other hand, multi-maps do not seem to affect the outcome significantly. It is also noteworthy that specimen bag is localized significantly worse than the other tools. This can be explained by the varying shape of the bag, as well as the ambiguity of its center.</p><p>Qualitative Results For the sake of visual comparison, we present qualitative results for 8 evaluated models in <ref type="figure" target="#fig_2">Figure 3</ref>, where input images are overlaid with localization maps. Just as the quantitative results suggest, the performances of the networks are very similar and the detected tool centers are very close to one another in most cases. However, the models with masking and ESP generate more detailed maps that cover the tools better than other models and provide strong ROI for the tools.</p><p>In <ref type="figure" target="#fig_3">Figure 4</ref>, we present additional results for the architecture FCN ESP MM Msk. In the figure, we see which features the network finds most discriminative about each of the tools. Ideally, we aim to localize the working end of the tools only, as the shaft does not usually contain tool-specific features. In <ref type="figure" target="#fig_3">Figure 4</ref>, we can see that for scissors and irrigator (row 4 and 6 respectively) the shafts themselves are very distinctive and discriminative. In the case of scissors, the brightest detection corresponds to the shaft. This may explain why the localization AP values for scissors are the lowest among all tools, as the annotated bounding boxes are defined over tool tips only (see column 1 in <ref type="figure" target="#fig_2">Figure 3</ref>). Specimen bag (last row) is an exception since it is not connected to a shaft. We should also note that the second tool, bipolar, is not fully detected. The network detects the blue insulated section of the forceps but not the metal tips. Our intuition is that they look very similar to those of grasper and hence cannot be used to discriminate one tool from the other. Additional qualitative results can be seen in the supplementary video (https://youtu.be/7VWVY04Z0MA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we showed that reliable surgical tool detection and localization can be achieved without the use of spatial annotations during training. Our method relies on a FCN architecture that preserves relative spatial information of the input image. This enables us to localize the surgical tools while using only binary presence annotations for training. We evaluated several variants of our network, obtaining very promising AP values of around 87 and 88 for classification and localization on the test set, respectively. These results also suggest that the proposed approach could be used to ease the generation of spatial annotations within surgical video labeling software and extended for tool segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Our FCN for tool detection consists of a modified Resnet architecture, a 1 × 1 convolutional layer and a spatial pooling layer. banks would dramatically increase the dimensions of intermediate tensors during training, making it computationally infeasible. These changes have the collective effect of quadrupling the resolution of the output. Using images of size 480 × 854 as input to the network, we obtain a feature map tensor of 60 × 107 × 512 at the output of ResNet and a global stride of 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Precision-recall classification curve for FCN ESP MM Msk (best seen in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Column 1: Ground truth bounding box and tool center. Columns 2-6: input images overlaid with corresponding localization maps (after sigmoid) and predicted tool centers for FCN ESP, FCN ESP Msk, FCN MSP Msk, FCN MSP MM Msk, FCN ESP MM Msk, in that order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Input images (left) and corresponding localization maps after sigmoid layer (right). Each row shows 3 examples of the same tool. The tools in rows 1-7 are presented in the following order: grasper, bipolar, hook, scissors, clipper, irrigator, specimen bag. These results correspond to architecture FCN ESP MM Msk. (Best seen in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Grasper Bipolar Hook Scissors Clipper Irrigator Spec.Bag Total Sp.Dataset statistics. (Row 1) Number of frames where each tool is present, for the 5 spatially annotated videos. (Row 2) Number of frames where each tool is present in the complete Cholec80 dataset.</figDesc><table><row><cell>Annot. 4774</cell><cell>379</cell><cell>4313</cell><cell>327</cell><cell>384</cell><cell>332</cell><cell>375</cell><cell>7175</cell></row><row><cell>Cholec80 102588</cell><cell cols="3">8876 103106 3254</cell><cell>5986</cell><cell>9814</cell><cell cols="2">11462 161397</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The models we devised are as follows: FCN ESP (M1), FCN ESP Msk (M2), FCN ESP MM (M3), FCN ESP MM Msk (M4), FCN MSP (M5), Grasper Bipolar Hook Scissors Clipper Irrigator Spec.bag mAP Average precision (AP) for binary tool presence classification.</figDesc><table><row><cell>FCN ESP</cell><cell>96.5</cell><cell>94.9 99.5 51.4</cell><cell>81.4</cell><cell>93.2</cell><cell>93.7</cell><cell>87.2</cell></row><row><cell>FCN ESP Msk</cell><cell>96.7</cell><cell>95.5 99.6 50.0</cell><cell>82.3</cell><cell>94.3</cell><cell>93.5</cell><cell>87.4</cell></row><row><cell>FCN ESP MM</cell><cell>96.6</cell><cell>95.0 99.6 50.2</cell><cell>82.8</cell><cell>94.0</cell><cell>93.5</cell><cell>87.4</cell></row><row><cell cols="2">FCN ESP MM Msk 96.7</cell><cell>94.8 99.5 44.2</cell><cell>81.9</cell><cell>92.9</cell><cell>93.2</cell><cell>86.1</cell></row><row><cell>FCN MSP</cell><cell>96.6</cell><cell>93.9 99.5 49.6</cell><cell>81.6</cell><cell>92.1</cell><cell>92.4</cell><cell>86.5</cell></row><row><cell>FCN MSP Msk</cell><cell>96.7</cell><cell>94.1 99.5 49.4</cell><cell>83.2</cell><cell>92.7</cell><cell>93.4</cell><cell>87.0</cell></row><row><cell>FCN MSP MM</cell><cell>96.7</cell><cell>93.8 99.5 50.4</cell><cell>81.8</cell><cell>91.5</cell><cell>92.7</cell><cell>86.6</cell></row><row><cell cols="2">FCN MSP MM Msk 96.8</cell><cell>94.2 99.6 49.8</cell><cell>83.0</cell><cell>93.3</cell><cell>94.0</cell><cell>87.2</cell></row></table><note>FCN MSP Msk (M6), FCN MSP MM (M7), FCN MSP MM Msk (M8). The models M1-M4 use the ESP method seen in section 2.1. To see whether that spatial pooling method is beneficial, we included identical models that use max pooling (MSP) instead: M5-M8. Similarly, to evaluate the benefit of masking images during training, architectures M2, M4, M6 and M8 incorporate masking, while M1, M3, M5 and M7 do not. Finally, models M3, M4, M7 and M8 use multi-maps [2], described below.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Grasper Bipolar Hook Scissors Clipper Irrigator Spec.bag mAP Mean distance from predicted tool center to true center in percents (relative to the image diagonal).</figDesc><table><row><cell>FCN ESP</cell><cell>97.9</cell><cell cols="3">99.3 98.6 63.9</cell><cell>97.5</cell><cell>94.2</cell><cell>69.5</cell><cell>88.7</cell></row><row><cell>FCN ESP Msk</cell><cell>96.6</cell><cell cols="3">99.6 99.0 52.6</cell><cell>98.8</cell><cell>93.3</cell><cell>72.1</cell><cell>87.4</cell></row><row><cell>FCN ESP MM</cell><cell>97.1</cell><cell cols="3">99.7 98.9 64.8</cell><cell>98.3</cell><cell>88.7</cell><cell>72.1</cell><cell>88.5</cell></row><row><cell cols="2">FCN ESP MM Msk 96.9</cell><cell cols="3">99.5 97.9 58.1</cell><cell>97.9</cell><cell>91.8</cell><cell>78.4</cell><cell>88.7</cell></row><row><cell>FCN MSP</cell><cell>97.4</cell><cell cols="3">99.6 98.0 57.4</cell><cell>98.2</cell><cell>94.3</cell><cell>70.7</cell><cell>88.0</cell></row><row><cell>FCN MSP Msk</cell><cell>96.5</cell><cell cols="3">99.5 98.7 66.4</cell><cell>98.4</cell><cell>94.3</cell><cell>67.5</cell><cell>88.8</cell></row><row><cell>FCN MSP MM</cell><cell>97.9</cell><cell cols="3">99.7 98.7 46.3</cell><cell>97.7</cell><cell>94.4</cell><cell>73.5</cell><cell>86.9</cell></row><row><cell cols="2">FCN MSP MM Msk 97.6</cell><cell cols="3">99.5 98.9 57.9</cell><cell>97.2</cell><cell>92.9</cell><cell>72.7</cell><cell>88.1</cell></row><row><cell cols="8">Table 3. Localization average precision (AP) for the 8 evaluated models.</cell><cell></cell></row><row><cell></cell><cell cols="8">Grasper Bipolar Hook Scissors Clipper Irrigator Spec.bag mean</cell></row><row><cell>FCN ESP</cell><cell>6.7</cell><cell>5.4</cell><cell>6.0</cell><cell>12.0</cell><cell>8.4</cell><cell>4.4</cell><cell>9.7</cell><cell>7.5</cell></row><row><cell>FCN ESP Msk</cell><cell>6.7</cell><cell>4.6</cell><cell>4.7</cell><cell>11.3</cell><cell>6.7</cell><cell>4.7</cell><cell>9.1</cell><cell>6.8</cell></row><row><cell>FCN ESP MM</cell><cell>6.8</cell><cell>5.0</cell><cell>5.5</cell><cell>11.0</cell><cell>8.3</cell><cell>4.5</cell><cell>9.1</cell><cell>7.1</cell></row><row><cell>FCN ESP MM Msk</cell><cell>6.9</cell><cell>5.3</cell><cell>5.7</cell><cell>9.6</cell><cell>6.9</cell><cell>4.6</cell><cell>9.1</cell><cell>6.9</cell></row><row><cell>FCN MSP</cell><cell>7.4</cell><cell>4.4</cell><cell>5.9</cell><cell>17.8</cell><cell>8.7</cell><cell>4.0</cell><cell>10.4</cell><cell>8.4</cell></row><row><cell>FCN MSP Msk</cell><cell>7.1</cell><cell>4.6</cell><cell>5.3</cell><cell>10.1</cell><cell>7.4</cell><cell>4.2</cell><cell>9.4</cell><cell>6.9</cell></row><row><cell>FCN MSP MM</cell><cell>7.6</cell><cell>4.8</cell><cell>5.8</cell><cell>17.7</cell><cell>9.2</cell><cell>3.9</cell><cell>10.0</cell><cell>8.4</cell></row><row><cell cols="2">FCN MSP MM Msk 6.7</cell><cell>4.7</cell><cell>5.3</cell><cell>11.6</cell><cell>7.9</cell><cell>4.3</cell><cell>9.2</cell><cell>7.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by French state funds managed within the Investissements d'Avenir program by BPI France (project CONDOR) and by the ANR (references ANR-11-LABX-0004 and ANR-10-IAHU-02). The authors would also like to acknowledge the support of NVIDIA with the donation of a GPU used in this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision-based and marker-less surgical tool detection and tracking: a review of the literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="633" to="654" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5957" to="5966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toolnet: Holistically-nested real-time segmentation of robotic surgical tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Garcia-Peraza-Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gruijthuijsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Devreker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Attilakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deprest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vander Poorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-transfer learning for weakly supervised lesion localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MIC-CAI)</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Constrained deep weak supervision for histopathology image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">I C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2376" to="2388" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tool detection and operative skill assessment in surgical videos using region-based convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jopling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Azagury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="691" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two-phase learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3554" to="3563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous recognition and pose estimation of instruments in minimally invasive surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Concurrent segmentation and localization for tracking of surgical instruments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rieke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Vizcaíno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="664" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weaklysupervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Addressing multi-label imbalance problem of surgical tool detection using cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zachow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1013" to="1020" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incorporating network built-in priors in weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1382" to="1396" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Endonet: A deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
