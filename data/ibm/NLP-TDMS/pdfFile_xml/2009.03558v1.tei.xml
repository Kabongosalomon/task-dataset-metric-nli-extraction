<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Region Comparison Network for Interpretable Few-shot Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
							<email>lxduan@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chen</surname></persName>
							<email>gggchenlin@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Wyze Labs, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<email>jluo@cs.rochester.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Region Comparison Network for Interpretable Few-shot Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While deep learning has been successfully applied to many real-world computer vision tasks, training robust classifiers usually requires a large amount of welllabeled data. However, the annotation is often expensive and time-consuming. Few-shot image classification has thus been proposed to effectively use only a limited number of labeled examples to train models for new classes. Recent works based on transferable metric learning methods have achieved promising classification performance through learning the similarity between the features of samples from the query and support sets. However, rare of them explicitly consider the model interpretability. For that, in this work, we propose a metric learning based method named Region Comparison Network (RCN), which aims to reveal how fewshot learning works as in a neural network, to learn specific regions that are related to each other in images coming from the query and support sets. Moreover, we design a visualization strategy named Region Activation Mapping (RAM) to intuitively explain what our method has learned by visualizing intermediate variables in our network. We also present a new way to generalize the interpretability from the task level to the category level, which can also be viewed as a way to find the prototypical parts for supporting the final decision of our RCN. Extensive experiments on four benchmark datasets clearly show the effectiveness of our method over existing baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Benefiting from the power of large-scale training data, deep learning models have demonstrated promising performance on many computer vision tasks <ref type="bibr" target="#b16">(Huang et al. 2017;</ref><ref type="bibr" target="#b13">He et al. 2016;</ref><ref type="bibr" target="#b43">Szegedy et al. 2017;</ref><ref type="bibr" target="#b21">Krizhevsky, Sutskever, and Hinton 2012;</ref><ref type="bibr" target="#b15">Hu, Shen, and Sun 2018)</ref>. However, it is still a big challenge to apply deep learning to a task with only limited data available, which is often the case in real-world applications. As a result, few-shot learning, which aims to learn a classifier for a given set of classes with only limited labeled training samples, has been attracting more and more attention from the community in recent years <ref type="bibr" target="#b17">(Huang et al. 2019;</ref><ref type="bibr" target="#b24">Li et al. 2017;</ref>).</p><p>Copyright c 2020, All rights reserved.</p><p>Many works have been proposed to address the few-shot learning problem based on various principles, e.g., meta learning, metric learning <ref type="bibr" target="#b40">(Snell, Swersky, and Zemel 2017;</ref><ref type="bibr" target="#b44">Vinyals et al. 2016;</ref><ref type="bibr" target="#b42">Sung et al. 2018)</ref>, however, rare attention was paid to the interpretability of few-shot learning models, except for some brand new works <ref type="bibr" target="#b41">(Sun et al. 2020;</ref><ref type="bibr" target="#b2">Cao, Brbic, and Leskovec 2020)</ref> in 2020. Although some concrete results are shown in previous works <ref type="bibr" target="#b9">(Garcia and Bruna 2017;</ref><ref type="bibr" target="#b37">Santoro et al. 2016)</ref>, it is still unclear how the model explicitly performs the recognition and comparison process. In other words, we are still confused about the incidence relation between the final classification and the pairs of support and query samples. To this end, in this work, we take one step towards the interpretability of few-shot learning by exploiting the relation between representative regions of different images. We are keen to find answers to the following questions, which parts of a given test(query) image are essential for classification, while which parts of a training(support) sample matter?</p><p>The recognition process of humans partially inspires our method. It is known that human is able to recognize a new object by only seeing a few examples <ref type="bibr" target="#b22">(Lake et al. 2011;</ref><ref type="bibr" target="#b11">Gidaris and Komodakis 2018;</ref><ref type="bibr" target="#b33">Qi, Brown, and Lowe 2018)</ref>. As shown in <ref type="bibr" target="#b3">(Chen et al. 2019a;</ref><ref type="bibr" target="#b42">Sung et al. 2018</ref>), if we ask humans to describe how they identify objects in the real world, most people might view that focusing on partitions of an image and comparing them with prototypical parts of images from a given category can help them achieve this goal. For example, humans can classify an image of woodpecker mainly because this woodpecker's beak is closely similar to the beaks of woodpeckers they have seen.</p><p>To study this issue, we design a new metric learning based model for few-shot learning. The motivation of our model is to find which parts in a query sample are most similar to the manually selected regions in a support sample, by comparing the computed similarity between them. To achieve this goal, our model is designed to generate a region weight in the final stage, in order to define which common parts between support sample and query sample can influence the final similarity score mostly. Also, we develop Region Activation Mapping (RAM) to acquire some concrete visualization results about interpretability in few-shot image classification, which have rarely been considered in previous works <ref type="bibr" target="#b9">(Garcia and Bruna 2017;</ref><ref type="bibr" target="#b37">Santoro et al. 2016)</ref>. Con-sidering the difference between interpretability in normal image classification and few-shot image classification, it is reasonable to think about what our model can do under the circumstance of data limitation, which means we cannot access sufficient samples to discover the prototypical regions. Moreover, we also need to find out how much a single region similarity score can contribute to the final similarity score. The difference of interpretability between normal tasks and few-shot tasks can be shown in <ref type="figure">Fig. 1(a)</ref>, and our key idea of building the interpretable few-shot learning model can be shown in <ref type="figure">Fig. 1(b)</ref>.</p><p>Our contributions can be concluded as follows: • In this paper, we propose a metric learning based model to solve the problem of interpretable few-shot image classification. Compared to attention mechanism, our model indicates the relationship between final classification decision and region similarities directly in the last layer, which can be viewed as a simple and easily explained linear process. • We present an easily explainable module to make the final prediction for few-shot image classification. By learning a generated weight of regions, this module can explain the question as "what kind of regions in a support sample are similar to somewhere in a query sample, and which of them do the model like to compare?". For that, we develop a so-called region meta learner, which can be viewed as a dynamic system aiming to adapt different meta tasks in the training/testing stage. • We also present an easy-to-implement visualization strategy named Region Activation Mapping (RAM) to intuitively show the interpretability of our RCN model, by visualizing the weight and similarity scores of regions. We also present a statistic-based method to generalize and quantify the explanations into a set of standard rules for the comparison process, as well as a generalization method to find the prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Few-shot Learning is a research issue aiming to learn the concept from only few examples per class <ref type="bibr" target="#b22">(Lake et al. 2011)</ref>. It requires an efficient representation learning which can extract knowledge from only a few labeled samples and generalize this learned knowledge among many unlabeled samples. It is closely relevant to meta learning <ref type="bibr" target="#b14">Hou et al. 2019)</ref>, because we need a model to handle tasks from different tasks. Investigating many recent works of few-shot learning, we group them into metric-based models <ref type="bibr" target="#b19">(Koch, Zemel, and Salakhutdinov 2015;</ref><ref type="bibr" target="#b44">Vinyals et al. 2016;</ref><ref type="bibr" target="#b40">Snell, Swersky, and Zemel 2017;</ref><ref type="bibr" target="#b42">Sung et al. 2018)</ref> and gradient-based models <ref type="bibr" target="#b7">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b34">Ravi and Larochelle 2017;</ref><ref type="bibr" target="#b24">Li et al. 2017)</ref>. Metric based methods like matching networks <ref type="bibr" target="#b44">(Vinyals et al. 2016</ref>) address the few-shot classification problem by learning to compare <ref type="bibr" target="#b4">(Chen et al. 2019b)</ref>, which means the models can achieve classification score by computing the similarity between support sample and query sample using some metric methods, such as Euclidean distance <ref type="bibr" target="#b40">(Snell, Swersky, and Zemel 2017)</ref>. As for gradient-based methods, like MAML <ref type="bibr" target="#b7">(Finn, Abbeel, and Levine 2017)</ref> and MetaSGD  aiming to find an appropriate gradient-based optimization method for meta learning, they are usually model agnostic and can be used with some metric learning models to achieve higher performance on few-shot learning tasks. Our framework is related to the category of metric-based model. However, not like most exiting methods comparing the features on the level of the whole image, our model tends to compare each region between support sample and query sample, which can explore more fine-grained information and find the critical regions related to the final decision. Interpretability of Deep Learning is made to find the crucial factors resulting in the final decision of deep neural networks. Decision models learned on a considerable amount of data produced by humans may lead to unfair and wrong decisions since the training data may contain some human biases and prejudices <ref type="bibr" target="#b12">(Guidotti et al. 2018)</ref>. For example, a well-trained cat-dog CNN may classify dog images into the right category successfully. However, the most important foundation may be the same lawn background, not the same dog heads, probably because we collect dog images outdoors while collecting cat images indoor. We need to know what actually happens inside deep neural networks. According to <ref type="bibr" target="#b36">(Rudin 2018)</ref>, the current methods of interpretability can be divided into interpretable models <ref type="bibr" target="#b3">(Chen et al. 2019a;</ref><ref type="bibr" target="#b50">Zhang, Nian Wu, and Zhu 2018;</ref><ref type="bibr" target="#b45">Wang et al. 2017)</ref> and model diagnosis <ref type="bibr" target="#b38">(Selvaraju et al. 2017;</ref><ref type="bibr" target="#b39">Simonyan, Vedaldi, and Zisserman 2013)</ref>. The objective of model diagnosis is using some visualization methods or sampling functions, such as RISE <ref type="bibr" target="#b32">(Petsiuk, Das, and Saenko 2018)</ref> for visualizing the feature maps and LIME <ref type="bibr" target="#b35">(Ribeiro, Singh, and Guestrin 2016)</ref> for restructuring a more straightforward model by sampling nearby examples to supersede the original model. On the contrary, some recent works related to interpretable model such as InterpretableCNN <ref type="bibr" target="#b50">(Zhang, Nian Wu, and Zhu 2018)</ref> and ProtoPNet <ref type="bibr" target="#b3">(Chen et al. 2019a</ref>), firmly claim it is useless and meaningless to find explanations on black-box models, which is just likely to perpetuate the wrong practice <ref type="bibr" target="#b36">(Rudin 2018)</ref>, because standard deep learning models are unexplainable intrinsically no matter what diagnosis methods you use.</p><p>Following the idea of building interpretable models to set a white-box reasoning system for the learning process directly <ref type="bibr" target="#b36">(Rudin 2018)</ref>, our model achieves the interpretability by quantifying the contributions of important parts in support sample to the final classification decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>The training process in few-shot learning aims to learn the concepts from meta training tasks and generalize among meta testing tasks, where the category distributions are entirely disjoint. We can acquire meta tasks by sampling from a big dataset containing various examples such as Mini-ImageNet.</p><p>Unlike normal training strategy owning train dataset and test dataset related to a same category distribution. We use episodic training <ref type="bibr" target="#b44">(Vinyals et al. 2016</ref>) paradigm in few-shot learning to minimize the generalization error by sample different meta task per episode. In episodic training, we first  <ref type="figure">Figure 1</ref>: 1(a): In traditional image classification tasks, we often explain our reasoning by dissecting the image and pointing out some prototypical parts that can impact the final classification crucially <ref type="bibr" target="#b3">(Chen et al. 2019a</ref>). However, this theory of reasoning about usual image classification is not suitable for few-shot image classification, since the training strategy and model structure are completely different. We set a new theory for the reasoning in few-shot classification as an ensemble process, which means combining all the region similarity scores into a final similarity score by giving a region weight. 1(b): Our motivation to solve this issue is dividing each support sample into several parts manually. For each query sample, we compute its feature similarity to these parts one by one. In the last procedure, we combine all the region similarity scores into a final classification decision by using a generated weight. </p><formula xml:id="formula_0">D te with |C te | classes, where |C tr |+|C te | = |C| and C tr ∩C te = ∅.</formula><p>For N-way K-shot task in meta-training procedure, we first sample N classes from C tr per episode, and then disjointly sample K examples per class as the support set S and B examples as the query set Q, respectively. These two sets can be represented as</p><formula xml:id="formula_1">D S = {(x i , y i )} N ×K i=1 and D Q = {(x i , y i )} N ×B i=1 , where B</formula><p>is a hyperparameter that we need to fix in our experiments. The few-shot learning models can get the basic knowledge on the support set and minimize the empirical error on the query set.</p><p>We use the strategy as same as we mentioned above to evaluate our model on meta-testing dataset D te .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Approach</head><p>The Region Comparison Network(RCN) is partially inspired by ProtoPNet <ref type="bibr" target="#b3">(Chen et al. 2019a)</ref>. ProtoPNet aims to explain the learning process by comparing the inputting images and some selected prototypical parts of each category. However, instead of projecting the prototypical parts of some class onto the latent training patch by a manual updating rule automatically like ProtoPNet, we use a region meta learner inputted with some representative features for the meta task, to generate a region weight indicating the importance of each region in support sample. This dynamic process can provide different explanations for different meta tasks, which is an ability that ProtoPNet does not have.</p><p>The main idea of our model is to compare each selected region in support sample to the whole range of query sample by computing each region similarity score between them, and then find out somewhere in query sample similar to this specific selected region in support sample mostly by using a max pooling kernel. As for interpretability, we consider it as a region weight representing the importance of each corresponding region in support sample compared to query sample in the classification process. In other words, the region weight can help us to point out which similarity between region-to-region can mainly determine the similarity between images-to-images by quantifying the contributions of regions. We achieve this goal by using the region matching network and explaining network that we will introduce in detail in the following section.</p><p>Our framework contains three modules: feature extractor, region matching network and explain network. The architecture can be shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Feature extractor f (·) is a simple CNN without full-connection layer, which is utilized to map an inputting image into representative feature maps. The region matching network g(·) aims to get the region similarity scores between support sample and query sample, and explain network h(·) can get the final classification decisions by combining the region similarity scores with a weight generated from the region meta learner, which can be taken as an explainable inference process. We will introduce some details of g(·) and h(·) in the following article, respectively.</p><p>For loss function, we use mean square error (MSE) for the loss function of our model(Equal 1). It is not a standard choice for classification problem <ref type="bibr" target="#b42">(Sung et al. 2018</ref>), but considering our final classification decision is a classification score,it can be taken as a regression problem to achieve our predictions closer to the ground truth generated discretely from {0, 1}. Also, the MSE loss is introduced to measure the gap between the estimated similarity and true similarity of each pair of a query image and a support image, since the similarity is real-valued, we believe the MSE loss is more suitable.</p><formula xml:id="formula_2">M SE = (x S ,y S )∈D S (x Q ,y Q )∈D Q (s S,Q − 1(y S == y Q )) 2 (1)</formula><p>where s S,Q denotes the final classification score for support sample x S and query sample x Q , as well as the similaity between x Q and x S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region Matching Network</head><p>The Region Matching Network <ref type="figure" target="#fig_2">(Figure 3</ref>) is built as a method of combination and similarity computing module, which does not have any parameter to learn during the meta training stage. Moreover, the time and space complexity of this module are both lower than that of the regular convolutional layer, which will be explicitly analyzed in our supplementary material. We denote a support sample as (x S , y S ) and a query sample as (x Q , y Q ). The feature maps outputting from the feature extractor f (·) can be represented as f (x) ∈ R n×w×h , where n, w, h represent the number of channels, width and height for feature maps f (x) respectively.</p><p>We first decompose the feature maps f (</p><formula xml:id="formula_3">x S ) into sev- eral region vectors {f (x S ) i } w×h i=1 among width and height, where f (x S ) i ∈ R n×1×1 and i ∈ [1, 2...w × h].</formula><p>We view it as the representative features of specific regions, which is located in i-th parts of support sample x S . For dimensional unification in the similarity computing process, we define a operating function r(·) to repeat a single region vector f (x S ) i on the dimensionality of width and height, to make them be the same value as those in f (x Q ). We set this oper-</p><formula xml:id="formula_4">ation as f (x S ) i = r(f (x S ) i ), where f (x S ) i ∈ R n×w×h .</formula><p>In order to avoid internal covariate shift, we restrict the similarities into the range of 0 and 1 by utilizing cosine similarity(Eq 2) as the metric method, which measures the similarity of two vector by the cosine of the angle between them. Also, we find it is the best metric function by some empirical study, which will be shown in our supplement material.</p><formula xml:id="formula_5">CosineSimilarity(a, b) = a · b a b<label>(2)</label></formula><p>The region similarity maps</p><formula xml:id="formula_6">{S i S,Q ∈ R 1×w×h } h×w i=1</formula><p>are computed by using cosine similarity between f (x S ) i and f (x Q )on the dimensionality of channels. It can be shown in Eq 3 regularly.</p><formula xml:id="formula_7">(S i S,Q ) a,b = CosineSimilarity((f (x S ) i ) a,b , (f (x Q )) a,b ) where f (x) a,b ∈ R n×1×1 , a ∈ [1, 2...w], b ∈ [1, 2...h]<label>(3)</label></formula><p>After that, we use a global max pooling kernel to select the most salient information in region similarity maps</p><formula xml:id="formula_8">{S i S,Q } h×w i=1 , which can be denoted as {P i S,Q } h×w i=1</formula><p>. P i S,Q is regarded as a similarity score between f (x S ) i and somewhere similar to f (x S ) i mostly in f (x Q ). Take two bird images for example, P 1 S,Q may represent how similar the backgrounds are between support sample and query sample, while P 2 S,Q may represent the similarity of the birds' wings or something else.</p><p>Explain Network Explain Network aims to explain how much that each item in P S,Q contributes to the final classification decisions. In this module, we use a region meta learner to generate the region weight W p , and then combine the region similarity scores P S,Q to get the final classification score by using the region weight W p .</p><p>Considering the important parts are changing from metatasks, such as we classify dog images by their heads while birds images by their wings. We utilize a region meta learner to generate a dynamic region weight adapting to each specific meta-task. We will introduce the details of the region meta learner's structure in the experimental section.</p><p>Moreover, region meta learner generates region weight by learning from some representative information, which is set as the concatenation of support feature maps and query feature maps on the dimensionality of channel. This process can be represented in Eq 4. <ref type="figure">Figure 4</ref>: The structure of explain network for 2-shot task(images are from Mini-ImageNet)</p><formula xml:id="formula_9">Wp = m([f (X S ), f (X Q )]) h(P S,Q ) = W T p P S,Q<label>(4)</label></formula><p>where h(·) denotes the explain network and m(·) denotes the region meta learner that we will introduce its structure carefully in the Section .</p><p>Why do we not use a learnable linear liner layer to acquire region weight, but use a meta learner to generate the region weight instead? It is mainly because the meta task in each episode is different. For example, we may identify sparrows by their heads, but we identify woodpeckers mainly by their beaks. A simple linear hidden layer may not be able to generalize among different support-query pairs(meta tasks), while using a mete learner instead can alleviate this problem, since it can generate different region weights by giving different meta inputs adapting to different meta tasks. We will demonstrate this assumption by the experimental ablation results in Section .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>To compare our proposed framework with exiting stateof-art few-shot learning methods, We evaluate our proposed framework on four benchmark datasets. The four datasets are introduced as follows: Mini-Imagenet <ref type="bibr" target="#b44">(Vinyals et al. 2016</ref>) is a dataset containing 60,000 colorful images coming from 100 classes, with 600 images in each class, and it can be taken as a subset of Im-ageNet <ref type="bibr" target="#b6">(Deng et al. 2009</ref>). In our experiments, we use the same splits of <ref type="bibr" target="#b40">(Snell, Swersky, and Zemel 2017)</ref>, who employ 64 classes for meta-training, 16 for meta-validation and 20 for meta-testing. CIFAR-FS <ref type="bibr" target="#b1">(Bertinetto et al. 2019</ref>) is randomly sampled from CIFAR-100 (Krizhevsky, Hinton, and others 2009) by applying the same criteria in <ref type="bibr" target="#b1">(Bertinetto et al. 2019</ref>) as same as MiniImageNet, which means we split the 100 classes to 64 classes for meta-training, 16 for meta-validation and 20 for meta-testing. <ref type="bibr">CUB-200 (Welinder et al. 2010</ref>) is a fine-grained with 6033 images from 200 bird species. Due to the different split method, we perform experiments following <ref type="bibr" target="#b26">(Li et al. 2019b</ref>) (130 classes for meta-training, 20 classes for meta-validation and 50 classes for meta-testing) and <ref type="bibr" target="#b4">(Chen et al. 2019b</ref>) (100 classes for meta-training, 50 classes for meta-validation and 50 classes for meta-testing), respectively. Stanford Dogs <ref type="bibr" target="#b18">(Khosla et al. 2011</ref>) contains 20580 images with 120 classes of dogs. Without loss of generality, we use the same criterion in <ref type="bibr" target="#b26">(Li et al. 2019b)</ref> to split it to few-shot dataset, which means 70,20 and 30 classes for meta-training, meta-validation and meta-testing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Feature Extractor: We use ResNet-12 following <ref type="bibr">Mishra et al. 2017)</ref> and Conv4( a standard 4-layer convolutional network with 64 filters per layer) <ref type="bibr" target="#b42">(Sung et al. 2018;</ref><ref type="bibr" target="#b26">Li et al. 2019b;</ref><ref type="bibr" target="#b44">Vinyals et al. 2016</ref>) as our feature extractor. Both of them have been used extensively.For ResNet-12, we use DropBlock regularization <ref type="bibr" target="#b10">(Ghiasi, Lin, and Le 2018)</ref> with keep rate = 0.5 to prevent overfitting. Region Matching Network: In region matching network, the values of width and height of the inputting feature maps f (x) are 5. In the ablation experiments, we use an adaptive average pooling kernel to change the size of feature maps to 1 and 3, in order to find the best outputting size for f (x). Explain Network: For region meta learner m(·), we use a simple CNN to generate the region weights from the concatenation of query feature maps and support feature maps. We take the block of this CNN as [Conv(1 × 1, in channels, out channels), BN, Relu], where in channels and out channels respectively denote the numbers of channels for input feature maps and output feature maps, and BN is batch normalization. We stack these blocks as the channels of 640 → 64 and 64 → 1. Data Argumentation: Data argumentation is an effective trick to prevent overfitting in training deep learning models. In our experiments, we only apply data argumentation methods on the query set in the meta-training stage. We use a group of random resize crop, random color jittering, random horizontal flip and random erasing <ref type="bibr" target="#b51">(Zhong et al. 2020)</ref> as our data argumentation method. Optimization: Adam is used as the optimization method in the meta-training stage. The learning rate is initially set to 0.001 and later reduces to 0.5 times if the average accuracy on the meta-validation dataset over 600 episodes does not increase. The model is trained following a strategy that set an iteration as 500 meta-training episodes, 600 meta-validation episodes and 600 meta-testing episodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We sample 15 query images per class for evaluation in both 1-shot and 5-shot tasks following <ref type="bibr" target="#b42">(Sung et al. 2018)</ref>, and the final few-shot classification accuracies are computed by averaging over 600 episodes in meta-testing stage. Some meta learning models need to pretrain on a lager task of N-way K-shot before training on 5-way 5-shot(1-shot), which is called meta pretraining <ref type="bibr" target="#b40">(Snell, Swersky, and Zemel 2017)</ref>. Moreover, some models use self-supervised pretraining <ref type="bibr" target="#b28">(Mangla et al. 2019)</ref> or pertrained feature extractor ). However, our framework can be meta-trained end-to-end without any method of pretraining.</p><p>We present the results of our method comparing other baselines on the normal datasets and fine-grained datasets(Tables 1 2 and 3)  We find our framework can both achieve promising performances on normal datasets and fine-grained datasets, especially for fine-grained datasets. Due to the motivation of comparing the specific regions instead of the whole images, our model can explore more fine-grained information of each sample and superior the state-of-the-art baselines on the task of few-shot fine-grained image classification. Also, our model is very easy-implemented, since the structures of the region matching network and explain network are simple and can be trained end-to-end on only one training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In order to demonstrate it is correct and reasonable for using a meta learner to generate region weight and also in order to find the impact of the feature maps' size(width and height) to the final classification, we did controlled experiments including using fixed linear layer, learnable linear layer and meta learner. For fixed linear layer, we just add all the region similarity scores by mean operation. For learnable linear layer, all the values of weight items need to be bigger than 0 during the optimization stage. We use an average pool layer to control the height and width of the feature maps. We use Mini-ImageNet and CUB-200 representing normal benchmarks and fine-grained benchmarks, respectively. The results can be shown in <ref type="table" target="#tab_3">Table 4</ref>. We will show the results of ablation experiments of other two datasets(CIFAR-FS and Stanford Dogs) in the supplementary materials.</p><p>According to the table 4, we can prove that using a region meta learner to generate different region weights for different support-query pairs can improve our model's performance in some cases like 5 × 5, and we will demonstrate that it can improve the interpretability by showing some visual results in Section . However, we find the meta learner cannot outperform than learnable linear layer in some cases where the width and height are smaller than 5. It can be reasoned as the adaptive average pool layer may cause some loss of representative information.</p><p>As for the size of feature maps and the number of regions we select from the support sample, it needs to be trade-off. In other words, this hyper-parameter must be a value that is not too big or too small. If the size is too small, we cannot capture the important regions and explore the fine-grained According to the visualization results about RAM for query sample and region weight for support sample, it is safe to claim that our model classifies these two Saynornis images (left) mainly depending on their heads. As for Black Tern images (right), we find out that our model mainly makes the final classification decision by comparing their wings. This figure shows that our model tends to focus on different important regions for different kinds of images. 5(b): We show the samples in category number n07697537 of Mini-ImageNet, which can be considered as the category of hot dog bun. Unlike CUB-200 or Stanford Dogs, whose samples almost locate the main objects in the center of the images with plain backgrounds, the main objects in the images of Mini-ImageNet are more difficult to be located and recognized(Like a man holds a hot dog bud located in the left corner). Our model can still find the essential regions due to the help of region meta learner. Moreover, we can explain the issue that our model classifies the hot dog bud images by comparing the sausages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Model Interpretability</head><p>Attribution methods <ref type="bibr" target="#b32">(Petsiuk, Das, and Saenko 2018;</ref><ref type="bibr" target="#b52">Zhou et al. 2016</ref>) focus on explaining neural networks by finding which parts of the input samples are the most responsible for determining the output of model. When they are applied to deep convolutional models, they can output saliency maps pointing out the important regions in input image <ref type="bibr" target="#b8">(Fong, Patrick, and Vedaldi 2019)</ref>. To make our explanation more comprehensive and user-friendly, we present an easy-implemented visualization method named Region Activation Mapping (RAM) to show the important regions in query sample. Also, the important regions of support sample can be shown by region weight W p .</p><p>In Class Activation Mapping (CAM) <ref type="bibr" target="#b52">(Zhou et al. 2016</ref>), the authors hold the view that the feature maps located in different channels focus on different regions in input feature maps, and they use a weight to average them into a saliency map for characterizing the import regions. In RAM, it is reasonable to say that the i-th region similarity map S i S,Q represents the similarities between the i-th region in support sample and everywhere in query sample. Therefore, we can make ensemble all the region similarity maps by region weight to indicate the important areas in query sample.</p><p>In RAM, we denote the region weight generated by region meta learner as W p ∈ R h×w and the region similarity maps {s i } h×w i=0 where s i ∈ R 1×h×w . Our method can be shown in Eq 5, where W p [i] denotes the i-th item in W p . It is easy to find that our final classification decisions for query samples are influenced by the region weight and region similarity maps together so that RAM can show this combined influence very clearly. Note that k(·) is a nonlinear function to enhance the impact of similar regions between query sample and support sample. Here it is set as k(x) = e 2x .</p><formula xml:id="formula_10">RAM = h×w i=1 Wp[i] · k(S i S,Q )<label>(5)</label></formula><p>We use RAM to visualize the important regions in query samples, while use region weight for the visualization of support sample. In addition, similarity maps are applied to find out the regions in query sample similar to the determinate regions in support sample. These results are shown in <ref type="figure">Fig. 5</ref>. In this figure, bilinear upsampling is used to match the input image's size and the results in the visualization process.</p><p>The <ref type="figure">Figure 5(a)</ref> can demonstrate that our model can focus on different important regions for different categories, while the <ref type="figure">Figure 5(b)</ref> can prove that our model can locate different regions for different images from the same category. Due to the limitation of pages, we will show more visualization results including the similarity maps in our supplement materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization and Quantification of Model Interpretability</head><p>Our framework can provide the interpretability for a specific meta task, which means it can only explain which regions are essential in a single episode. Therefore, we cannot find which parts are important among the class level, as well as a prototype part or a common rule for classification.</p><p>In this section, we apply an algorithm based on some statistical analysis to generalize the interpretability from meta tasks into categories, and we also present a criterion to metric the importance of regions at the level of class.</p><p>We assume a sample set {(x i , y i )} N i=1 related to a specific category c(y i = c). We select one sample randomly as a support sample x S , while other images are considered as query samples {x i Q } N −1 i=1 . We compute the region weight W i between x S and x i Q iteratively, and stack them into a region weight matrix W ∈ R (N −1)×w×h , where W i,j denotes the j-th item in i-th region vector W i . If W :,j is a zero vector, we will remove this vector from matrix W , since it denotes the j-th region in support sample is meaningless.</p><p>We compute the mean value µ j and the standard deviation σ j of vector W :,j , and the distribution of W :,j is assumed as a Gaussian distribution. We use the probability density function of one-dimensional Gaussian distribution to simulate the distribution of W :,j , which can be represented as Eq 6:</p><formula xml:id="formula_11">f (x, W:,j ) = 1 σj √ 2π exp (− (x − µj ) 2 2σ 2 j )<label>(6)</label></formula><p>where µ j and σ j denote the mathematical expectation and the standard deviation, respectively. It is safe to say that µ j represents the importance of j-th region among the whole class, while σ j denotes the degree of dispersion for each support-query pair. Therefore, if µ j is bigger and σ j is smaller, the j-th selected region in x S will be more likely to be a prototypical part representing the whole class c. We present a criterion about the importance of a region by using the mathematical expectation of N (µ j , σ j ) in range of [µ j − 2a, µ j + 2a], This indicator can describe how much the j-th region in the selected support image can represent the decision basis of the whole class c. It is shown in Eq 7 more minutely:</p><formula xml:id="formula_12">Ij = µ j +2a µ j −2a</formula><p>f (x, W:,j )xdx a = 1 M j σj <ref type="formula">(7)</ref> where a is a mean value for standard deviations, which can be taken as a standard deviation for all the similarity values of the selected regions in support sample. Our generalization method can be shown in Alg 1 briefly, where f (·), g(·) and m(·) denote the feature extractor, region matching network and region meta learner respectively. In this algorithm, we finally rank {I j } M j=1 in ascending order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Generalization Method</head><p>Input:</p><formula xml:id="formula_13">x S , {x i Q } N −1 i=1</formula><p>Output: µj −2a f (x, W :,j )xdx 14: end for In order to show the results, we take a class in CUB-200 as an example <ref type="figure" target="#fig_5">(Fig. 6)</ref>, and we will show the examples of other datasets(e.g. MiniImageNet) in the supplementary material.</p><p>According to <ref type="figure" target="#fig_5">Fig. 6</ref>, the results of interpretability among the whole class are reasonable and do not against our common sense. Through this generalization method, we can explain which parts of images that the model would like to pay attention to at the level of category, as well as general types of rules to classify images. In addition, it is also a diagnostic method to determine whether our model has focused on the reasonable and explainable areas that do not against our common sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we present an interpretable deep learning framework named Region Comparison Network (RCN) to solve the problem of few-shot image classification. We also present a simple yet useful visualization method named Region Active Mapping (RAM) to show the intermediate variables of our network, which intuitively explains what RCN has learned. Moreover, we present a criterion to measure the importance of regions in each category and develop a strategy to generalize the quantitative explanations from a We select two support samples from these two different categories, and apply our generalization method on them. The maximal I j can be shown clearly in the bar graph(region 8 and region 7). Obviously, they are different parts of the objects, where head part is the prototype <ref type="bibr" target="#b3">(Chen et al. 2019a</ref>) for class fox sparrow and tail part for black tern. In other words, our model tends to classify different objects by different parts, just as same as humans. specific support-query pair to the whole class. Experiments on four benchmark datasets demonstrate the effectiveness of our RCN. Since little work on the explicit interpretability of few-shot learning has been focused on in the literature, we believe our pioneer work is important and can pave the way for future study on this topic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of 2-way 2-shot split the whole dataset D with |C| classes into meta-training dataset D tr with |C tr | classes and meta-testing dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The structure of region matching network for w = h = 5, where X S and X Q denote support sample and query sample respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>a): We show examples of class Sayornis and class Black Tern in CUB-200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>{I j } M j=1 1: W = [] is a two-dimensional matrix 2: M = 0 3: for x i Q ∈ {x i Q } N −1 i=0 do 4: S i = m(g(f (x S ), f (x i Q )))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>This figure uses the examples from class fox sparrow and class black tern in CUB-200 with the size of w = h = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean accuracies (%) of different methods on the MiniImageNet and CIFAR-FS dataset. Results are obtained over 600 test episodes with 95% confidence intervals. Note that Conv4-n denotes 4-layer convolution network outputting feature maps with n channels. *:) uses feature extractor as 6-layer convolution network with deformable convolution kernel<ref type="bibr" target="#b5">(Dai et al. 2017)</ref> </figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>Type</cell><cell cols="2">Mini-ImageNet (5-way)</cell><cell cols="2">CIFAR-FS (5-way)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>META LSTM (Ravi and Larochelle 2017)</cell><cell>Conv4-32</cell><cell>Meta</cell><cell cols="2">43.44±0.77 60.60±0.71</cell><cell>-</cell><cell>-</cell></row><row><cell>MAML (Finn, Abbeel, and Levine 2017)</cell><cell>Conv4-32</cell><cell>Meta</cell><cell cols="2">48.70±1.84 63.11±0.92</cell><cell>58.9±1.9</cell><cell>71.5±1.0</cell></row><row><cell>Dynamic-Net (Gidaris and Komodakis 2018)</cell><cell>Conv4-64</cell><cell>Meta</cell><cell cols="2">56.20±0.86 72.81±0.62</cell><cell>-</cell><cell>-</cell></row><row><cell>Dynamic-Net (Gidaris and Komodakis 2018)</cell><cell>Res12</cell><cell>Meta</cell><cell cols="2">55.45±0.89 70.13±0.68</cell><cell>-</cell><cell>-</cell></row><row><cell>SNAIL (Mishra et al. 2017)</cell><cell>Res12</cell><cell>Meta</cell><cell cols="2">55.71±0.99 68.88±0.92</cell><cell></cell></row><row><cell>AdaResNet (Lee et al. 2019)</cell><cell>Res12</cell><cell>Meta</cell><cell cols="2">56.88±0.62 71.94±0.57</cell><cell>-</cell><cell>-</cell></row><row><cell>MATCHING NETS (Vinyals et al. 2016)</cell><cell>Conv4-64</cell><cell cols="3">Metric 43.56±0.84 55.31±0.73</cell><cell>-</cell><cell>-</cell></row><row><cell>PROTOTYPICAL NETS (Snell, Swersky, and Zemel 2017)</cell><cell>Conv4-64</cell><cell cols="3">Metric 49.42±0.78 68.20±0.66</cell><cell>55.5±0.7</cell><cell>72.0±0.6</cell></row><row><cell>RELATION NETS (Sung et al. 2018)</cell><cell>Conv4-64</cell><cell cols="3">Metric 50.44±0.82 65.32±0.70</cell><cell>55.0±1.0</cell><cell>69.3±0.8</cell></row><row><cell>GNN (Garcia and Bruna 2017)</cell><cell>Conv4-64</cell><cell cols="3">Metric 50.33±0.36 66.41±0.63</cell><cell>61.9</cell><cell>75.3</cell></row><row><cell>PABN (Huang et al. 2019)</cell><cell>Conv4-64</cell><cell cols="3">Metric 51.87±0.45 65.37±0.68</cell><cell>-</cell><cell>-</cell></row><row><cell>TPN (Liu et al. 2019)</cell><cell>Conv4-64</cell><cell cols="3">Metric 52.78±0.27 66.59±0.28</cell><cell>-</cell><cell>-</cell></row><row><cell>DN4 (Li et al. 2019b)</cell><cell>Conv4-64</cell><cell cols="3">Metric 51.24±0.74 71.02±0.64</cell><cell>-</cell><cell>-</cell></row><row><cell>R2-D2 (Bertinetto et al. 2019)</cell><cell cols="3">Conv4-512 Metric 51.80±0.20</cell><cell>68.4±0.20</cell><cell>65.3±0.2</cell><cell>79.4±0.1</cell></row><row><cell>GCR (Li et al. 2019a)</cell><cell cols="4">Conv4-512 Metric 53.21±0.40 72.32±0.32</cell><cell>-</cell><cell>-</cell></row><row><cell>PARN (Wu et al. 2019)</cell><cell>*</cell><cell cols="3">Metric 55.22±0.82 71.55±0.66</cell><cell>-</cell><cell>-</cell></row><row><cell>RCN</cell><cell>Conv4-64</cell><cell cols="5">Metric 53.47±0.84 71.63±0.70 61.61±0.96 77.63±0.75</cell></row><row><cell>RCN</cell><cell>Res12</cell><cell cols="5">Metric 57.40±0.86 75.19±0.64 69.02±0.92 82.96±0.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Mean accuracies (%) of different methods on the CUB-200. Results are obtained over 600 test episodes with 95% confidence intervals. †: Split CUB as<ref type="bibr" target="#b26">(Li et al. 2019b</ref>).</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>Type</cell><cell cols="2">CUB-200 (5-way)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>PCM  † (Wei et al. 2019)</cell><cell cols="4">Conv4-64 Metric 42.10±1.96 62.48±1.21</cell></row><row><cell>MATCHING NETS  † (Vinyals et al. 2016)</cell><cell cols="4">Conv4-64 Metric 45.30±1.03 59.50±1.01</cell></row><row><cell cols="5">PROTOTYPICAL NETS  † (Snell, Swersky, and Zemel 2017) Conv4-64 Metric 37.36±1.00 45.28±1.03</cell></row><row><cell>GNN  † (Garcia and Bruna 2017)</cell><cell cols="4">Conv4-64 Metric 51.83±0.98 63.69±0.94</cell></row><row><cell>DN4  † (Li et al. 2019b)</cell><cell cols="4">Conv4-64 Metric 53.15±0.84 81.90±0.60</cell></row><row><cell>RCN  †</cell><cell cols="4">Conv4-64 Metric 66.48±0.90 82.04±0.58</cell></row><row><cell>RCN  †</cell><cell>Res12</cell><cell cols="3">Metric 78.64±0.88 90.10±0.50</cell></row><row><cell>Baseline++  ‡ (Chen et al. 2019b)</cell><cell>Res10</cell><cell cols="3">Metric 69.55±0.89 85.17±0.50</cell></row><row><cell>MAML++(High-End)+SCA  ‡ (Antoniou and Storkey 2019)</cell><cell>-</cell><cell>Meta</cell><cell cols="2">70.46±1.18 85.63±0.66</cell></row><row><cell>GPShot(CosSim)  ‡ (Patacchiola et al. 2019)</cell><cell>Res10</cell><cell>Meta</cell><cell cols="2">70.81±0.52 83.26±0.50</cell></row><row><cell>GPShot(BNCosSim)  ‡ (Patacchiola et al. 2019)</cell><cell>Res10</cell><cell>Meta</cell><cell cols="2">72.27±0.30 85.64±0.29</cell></row><row><cell>RCN  ‡</cell><cell cols="4">Conv4-64 Metric 67.06±0.93 82.36±0.61</cell></row><row><cell>RCN  ‡</cell><cell>Res12</cell><cell cols="3">Metric 74.65±0.86 88.81±0.57</cell></row></table><note>‡: Split CUB as (Chen et al. 2019b)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mean accuracies (%) of different methods on the Stanford Dogs. Results are obtained over 600 test episodes with 95% confidence intervals.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>Type</cell><cell cols="2">CUB-200 (5-way)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>PCM (Wei et al. 2019)</cell><cell cols="4">Conv4-64 Metric 28.78±2.33 46.92±2.00</cell></row><row><cell>MATCHING NETS (Vinyals et al. 2016)</cell><cell cols="4">Conv4-64 Metric 45.30±1.03 59.50±1.01</cell></row><row><cell cols="5">PROTOTYPICAL NETS (Snell, Swersky, and Zemel 2017) Conv4-64 Metric 37.59±1.00 48.19±1.03</cell></row><row><cell>GNN (Garcia and Bruna 2017)</cell><cell cols="4">Conv4-64 Metric 46.98±0.98 62.27±0.95</cell></row><row><cell>DN4 (Li et al. 2019b)</cell><cell cols="4">Conv4-64 Metric 45.73±0.76 66.33±0.66</cell></row><row><cell>RCN</cell><cell cols="4">Conv4-64 Metric 54.29±0.96 72.65±0.72</cell></row><row><cell>RCN</cell><cell>Res12</cell><cell cols="3">Metric 66.24±0.96 81.50±0.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Mean accuracies (%) of different methods on the Mini-ImageNet and CUB-200(using split criterion as<ref type="bibr" target="#b26">(Li et al. 2019b)</ref>). Results are obtained over 600 test episodes with 95% confidence intervals. Note that the items in the region weight of fixed linear layer are all fixed as 1 h×w</figDesc><table><row><cell>Version</cell><cell cols="2">Mini-ImageNet</cell><cell cols="2">CUB-200</cell></row><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Fixed Linear Layer (5×5)</cell><cell>49.30±0.89</cell><cell>55.51±0.71</cell><cell>62.61±1.63</cell><cell>67.26±0.83</cell></row><row><cell>Learnable Linear Layer (5×5)</cell><cell>55.97±0.86</cell><cell>72.80±0.63</cell><cell>73.23±0.90</cell><cell>88.12±0.56</cell></row><row><cell>Meta Learner (5×5)</cell><cell>57.40±0.86</cell><cell>75.19±0.64</cell><cell>78.64±0.88</cell><cell>90.10±0.50</cell></row><row><cell>Fixed Linear Layer (4×4)</cell><cell>51.79±0.90</cell><cell>57.40±0.70</cell><cell>65.18±1.08</cell><cell>71.65±0.83</cell></row><row><cell>Learnable Linear Layer (4×4)</cell><cell>55.18±0.84</cell><cell>73.25±0.64</cell><cell>75.12±0.89</cell><cell>87.63±0.54</cell></row><row><cell>Meta Learner (4×4)</cell><cell>55.73±0.83</cell><cell>72.78±0.62</cell><cell>76.48±0.86</cell><cell>87.89±0.57</cell></row><row><cell>Fixed Linear Layer (3×3)</cell><cell>51.51±0.90</cell><cell>56.02±0.70</cell><cell>65.97±1.03</cell><cell>74.59±0.89</cell></row><row><cell>Learnable Linear Layer (3×3)</cell><cell>56.50±0.87</cell><cell>73.48±0.62</cell><cell>76.15±0.87</cell><cell>88.10±0.51</cell></row><row><cell>Meta Learner (3×3)</cell><cell>55.41±0.85</cell><cell>72.16±0.68</cell><cell>75.63±0.88</cell><cell>86.96±0.57</cell></row><row><cell>Fixed Linear Layer (2×2)</cell><cell>51.58±0.91</cell><cell>57.59±0.70</cell><cell>68.95±1.05</cell><cell>77.64±0.81</cell></row><row><cell>Learnable Linear Layer (2×2)</cell><cell>56.03±0.85</cell><cell>72.23±0.64</cell><cell>73.79±0.85</cell><cell>87.42±0.57</cell></row><row><cell>Meta Learner (2×2)</cell><cell>55.65±0.83</cell><cell>72.36±0.64</cell><cell>75.79±0.87</cell><cell>86.64±0.55</cell></row><row><cell>Fixed Linear Layer (1×1)</cell><cell>52.22±1.03</cell><cell>57.34±0.75</cell><cell>70.70±0.78</cell><cell>78.43±0.43</cell></row><row><cell>Learnable Linear Layer (1×1)</cell><cell>54.80±0.86</cell><cell>71.80±0.69</cell><cell>75.83±0.85</cell><cell>86.97±0.53</cell></row><row><cell>Meta Learner (1×1)</cell><cell>55.40±0.89</cell><cell>72.78±0.62</cell><cell>73.83±0.98</cell><cell>84.77±0.54</cell></row><row><cell cols="5">information, while we may have too much noise region vec-</cell></row><row><cell cols="5">tors like the regions of backgrounds if the size is too big.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material Different Metrics</head><p>The results of empirical study about using different metric methods in Region Matching Network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity Analysis of Region Matching Network</head><p>The time and space complexity of Region Matching Network(RMN) is low compared to other layers in the backbone networks. Let us assume the size of the feature map from the backbone networks is C * H * W , where C, H and W denote the number of channels, height, and width, respectively. Then, the time complexity is O(H * W * C * W * H) and space complexity is O(H * W * W * H) for storing the similarity matrix.</p><p>To compare with a normal convolutional layer, assuming its output size equals to the input size, the time complexity is O(H * W * K * K * C1 * C2), where K, C1, C2 denotes the kernel size, the numbers of in and out channels, respectively. Note that the size of the feature map is usually small (e.g., H = W = 5, for Res12), and so O(H * W * C * W * H) is often not as large as O(H * W * K * K * C1 * C2). The space complexity of RMN O(W * H * H * W ) is also not high, as the space complexity of an ordinary feature map is O(W * H * C) where C can be quite large at high layers (e.g., C = 640, for Res12).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by self-critique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9940" to="9950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brbic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07375</idno>
		<title level="m">Concept learners for generalizable few-shot learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">This looks like that: Deep learning for interpretable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8930" to="8941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deformable convolutional networks. In ICCV</title>
		<imprint>
			<biblScope unit="page" from="764" to="773" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding deep networks via extremal perturbations and smooth masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2950" to="2958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey of methods for explaining black box models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4005" to="4016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compare more nuanced: Pairwise alignment bilinear network for few-shot fine-grained learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Edgelabeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cvpr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Fine-Grained Visual Categorization</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CogSci</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9715" to="9724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7260" to="7268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12087</idno>
		<title level="m">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05199</idno>
		<title level="m">Deep kernel transfer in gaussian processes for fewshot learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rise: Randomized input sampling for explanation of black-box models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Petsiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07421</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot; explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10154</idno>
		<title level="m">Please stop explaining black box models for high stakes decisions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<title level="m">Meta-learning with memory-augmented neural networks. In ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Explanation-guided training for cross-domain few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08790</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A bayesian framework for learning rule sets for interpretable classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klampfl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Macneille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2357" to="2393" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Piecewise classifier mappings: Learning fine-grained learners for novel categories with few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6116" to="6125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Parn: Positionaware relation networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6659" to="6667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Metagan: An adversarial approach to few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2365" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Interpretable convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nian Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
