<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequential End-to-end Network for Efficient Person Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Key Laboratory of Embedded System and Service Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duoqian</forename><surname>Miao</surname></persName>
							<email>dqmiao@tongji.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Key Laboratory of Embedded System and Service Computing</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<postCode>201804</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequential End-to-end Network for Efficient Person Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person search aims at jointly solving Person Detection and Person Re-identification (re-ID). Existing works have designed end-to-end networks based on Faster R-CNN. However, due to the parallel structure of Faster R-CNN, the extracted features come from the low-quality proposals generated by the Region Proposal Network, rather than the detected high-quality bounding boxes. Person search is a fine-grained task and such inferior features will significantly reduce re-ID performance. To address this issue, we propose a Sequential End-to-end Network (SeqNet) to extract superior features. In SeqNet, detection and re-ID are considered as a progressive process and tackled with two sub-networks sequentially. In addition, we design a robust Context Bipartite Graph Matching (CBGM) algorithm to effectively employ context information as an important complementary cue for person matching. Extensive experiments on two widely used person search benchmarks, CUHK-SYSU and PRW, have shown that our method achieves state-of-the-art results. Also, our model runs at 11.5 fps on a single GPU and can be integrated into the existing end-to-end framework easily.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Pedestrian detection <ref type="bibr" target="#b12">(Girshick et al. 2014;</ref><ref type="bibr" target="#b11">Girshick 2015;</ref><ref type="bibr" target="#b24">Ren et al. 2015)</ref> aims at detecting the bounding boxes (BBoxes) of all people in the image. Person re-identification (re-ID) <ref type="bibr" target="#b33">Zhao et al. 2017;</ref><ref type="bibr" target="#b26">Wang et al. 2019;</ref><ref type="bibr" target="#b8">Fu et al. 2019;</ref><ref type="bibr" target="#b15">Hao et al. 2019;</ref><ref type="bibr" target="#b32">Zhao et al. 2020</ref>) is used to match the interested person with hand-cropped person images. Although these two fields are widely studied in recent years, they can not be directly applied to real-world applications due to their limited functionality. To close the gap, Xu et al. introduce person search task which aims at locating a target person in the scene image <ref type="bibr" target="#b29">(Xu et al. 2014)</ref>. Person search can be seen as a combination of pedestrian detection and person re-ID. It has broad application prospects in video surveillance, finding lost children, and self-service supermarket, etc.</p><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, existing works divide the task into generating BBoxes of all people in the image and person re-ID. They either tackle the problem separately with two independent models (two-stage methods) or jointly with a multi-task model (end-to-end methods).</p><p>For end-to-end methods <ref type="bibr" target="#b28">(Xiao et al. 2017</ref><ref type="bibr" target="#b27">(Xiao et al. , 2019</ref><ref type="bibr" target="#b21">Munjal et al. 2019)</ref>, they design a multi-task framework based on Faster R-CNN <ref type="bibr" target="#b24">(Ren et al. 2015)</ref>. A Region Proposal Network (RPN) is built to generate region proposals, which are then fed into the subsequent parallel detection and re-ID branches. However, these features extracted by the network come from low-quality proposals rather than detected accurate BBoxes. Although these inferior features have little impact on the coarse-grained classification task, they will significantly reduce the performance of the fine-grained re-ID task. This problem is caused by the parallel structure of Faster R-CNN. Because detection and re-ID are processed at the same time, the accurate BBoxes are not available before extracting re-ID features. For two-stage methods, there is no such problem, because detection and re-ID are tackled sequentially with two separate models. However, they are time-consuming and resource-consuming.</p><p>Motivated by the above observations, we propose a Sequential End-to-end Network (SeqNet) illustrated in <ref type="figure" target="#fig_0">Figure  1</ref>   <ref type="figure">Figure 2</ref>: An example of matching query image and gallery image. Blue box denotes the query person and green box denotes the ground truth. The number attached to each yellow line represents the similarity between the two people connected by the line.</p><p>our model employs an extra Faster R-CNN head as an enhanced RPN to provide high-quality BBoxes. Then an unmodified baseline head is used to extract the discriminative features of these BBoxes. At test time, the non-maximum suppression (NMS) is applied to remove redundant BBoxes before re-ID stage for efficiency. Moreover, to improve the classification ability of baseline head for high Intersection over Union (IoU) samples, we adopt the more reliable classification result of detection head. In general, our SeqNet not only inherits the sequential process of two-stage methods, which can provide accurate BBoxes for re-ID stage, but also retains the end-to-end training fashion and efficiency. Another challenge for person search is how to utilize context information to perform more robust matching. As shown in <ref type="figure">Figure 2</ref>, given a query person (a), (c) is the corresponding ground truth, but the (d) with largest similarity (0.6) will be mistakenly predicted as top-1 result. If context information (b) is taken into consideration, to maximize the total similarity, the optimal matching should be (a) ↔ (c), (b) ↔ (d). In this way, the wrong prediction (d) can be revised to (c). Inspired by this, we design a Context Bipartite Graph Matching (CBGM) algorithm to exploit context information as a complement to individual feature. Specifically, we treat all people in the query image and each gallery image as two sets of vertices respectively. A complete bipartite graph is built upon the two sets of vertices, and the weight of each edge is the similarity between corresponding vertices calculated by the person search network. Then the Kuhn-Munkres (K-M) algorithm <ref type="bibr" target="#b18">(Kuhn 1955;</ref><ref type="bibr" target="#b22">Munkres 1957</ref>) is exploited to discovery the optimal matching with maximum weight. In this matching, the person connected with the querier is taken as top-1 result.</p><p>The contributions of this paper are three-fold:</p><p>• We notice that the performance of previous end-to-end framework is limited by inferior features and formulate a Sequential End-to-end Network (SeqNet) to refine them.</p><p>• To make full use of context information, we propose a Context Bipartite Graph Matching algorithm to perform more robust matching.</p><p>• Our method outperforms all other state-of-the-art ones on the two widely used benchmarks CUHK-SYSU <ref type="bibr" target="#b28">(Xiao et al. 2017</ref>) and PRW <ref type="bibr" target="#b34">(Zheng et al. 2017)</ref>. Moreover, our method can be integrated into the existing end-to-end framework easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Person Search</head><p>Person search has raised a lot of interest in computer vision community since the publication of two large scale datasets, CUHK-SYSU <ref type="bibr" target="#b28">(Xiao et al. 2017</ref>) and PRW <ref type="bibr" target="#b34">(Zheng et al. 2017</ref>). It's a straightforward solution to tackle the problem with a pedestrian detector and a re-ID descriptor sequentially. Zheng et al. make a systematic evaluation on various detectors and descriptors, and propose a re-weighting algorithm adjusting the matching similarity to suppress the false positive detections <ref type="bibr" target="#b34">(Zheng et al. 2017</ref>). Lan, Zhu, and Gong point out the performance of person search is limited by the multi-scale matching, and formulates a Cross-Level Semantic Alignment (CLSA) method capable of learning more discriminative identity representations <ref type="bibr" target="#b19">(Lan, Zhu, and Gong 2018)</ref>  <ref type="bibr" target="#b6">(Dong et al. 2020a</ref>). To reconcile the contradictory goals of the two subtasks, Chen et al. present a novel approach called Norm-Aware Embedding (NAE) to disentangle the person embedding into norm and angle for detection and re-ID respectively <ref type="bibr" target="#b4">(Chen et al. 2020b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-stage Faster R-CNN</head><p>Some researchers extend Faster R-CNN to a multi-stage fashion. Gidaris and Komodakis propose a post-processing step that the network iterate several times in inference stage to achieve better localization performance <ref type="bibr">Komodakis 2015, 2016)</ref>. Cai and Vasconcelos design a cascade framework containing a sequence of detectors trained with increasing IoU thresholds to be sequentially more selective against close false positives <ref type="bibr" target="#b0">(Cai and Vasconcelos 2018)</ref>. Inspired by them, our model is designed as a multi-stage framework to introduce the sequential process into end-toend person search network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>In this section, we first revisit the end-to-end person search network, then discuss its shortcoming. Next, we describe our proposed Sequential End-to-end Network (SeqNet). Finally, we formulate a Context Bipartite Graph Matching (CBGM) algorithm to utilize the context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-end Network for Person Search</head><p>We take the multi-task network NAE <ref type="bibr" target="#b4">(Chen et al. 2020b)</ref> as our baseline. The overview of this baseline is illustrated in <ref type="figure">Figure 3</ref> (a). It adopt ResNet50 <ref type="bibr" target="#b16">(He et al. 2016</ref>) as the backbone network. Specifically, res1∼res4 are taken as the stem network to extract the 1024-channel stem feature maps of the image. A Region Proposal Network (RPN) is built upon these feature maps to generate region proposals. After NMS, we keep 128 proposals, and exploit RoI-Align to pool a 1024 × 14 × 14 region for each of them. Next these regions are fed into res5 to extract 2048-dim features, which are then mapped to 256-dim. It uses these 2048-dim features to calculate regressors and 256-dim features to perform classification and re-ID tasks. The Norm-Aware-Embedding is designed to supervise the classification and re-ID branches and the Smooth-L 1 -Loss <ref type="bibr" target="#b11">(Girshick 2015</ref>) is adopted to supervise the regression branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problems of the End-to-end Framework</head><p>As aforementioned, the baseline suffers from the inferior features. To investigate its influence, we train the baseline model and report the results under two evaluation settings in <ref type="table">Table 1</ref>. The original setting is denoted by parallelization. In the second setting called serialization, the network will iterate twice to solve the detection and re-ID in turn. The first iteration will output detected BBoxes. Then we exploit RoI-Align to pool a fixed size region for each BBox and feed them into res5 to extract re-ID features. In this way, superior BBoxes can be obtained before re-ID stage. <ref type="table">Table 1</ref> shows the mAP of re-ID is increased by 0.75% on CUHK-SYSU, 1.12% on PRW. This demonstrates the re-ID ability of the network is greatly limited by the inferior features of proposals. We also notice that the detection performance has a slight decline. This is caused by the inconsistency between </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Sequential End-to-end Network</head><p>The overview of our model is shown in <ref type="figure">Figure 3</ref> (b). It consists of two head networks to solve person detection and person re-ID respectively. The first standard Faster R-CNN head is employed to generate accurate BBoxes. The second unmodified baseline head is applied to further fine-tune these BBoxes and extract their discriminative features. The main idea is to exploit Faster R-CNN as a stronger RPN to provide fewer but more accurate candidate BBoxes. These high-quality BBoxes lead to more discriminative embeddings.</p><p>Training During training phase, these two heads are trained with 0.5 IoU threshold to distinguish positive and negtive samples, and the feature learning is supervised by the following 5 losses. • L reg1 /L reg2 : The regression loss of the first/second head.</p><p>N p is the number of positive samples, r i is the calculated regressor of i-th positive sample, i is the corresponding ground truth regressor, and L loc is the Smooth-L 1 -Loss.</p><formula xml:id="formula_0">L reg = 1 N p Np i=1 L loc (r i , i )<label>(1)</label></formula><p>• L cls1 : The classification loss of the first head. N is the number of samples, p i is the predicted classification probability of i-th sample, and c i is the ground truth label.</p><formula xml:id="formula_1">L cls1 = − 1 N N i=1 c i log(p i )<label>(2)</label></formula><p>• L cls2 , L reid : The classification and re-ID losses of the second head. It is calculated by the Norm-Aware-Embedding L nae (.). f is the extracted 256-dim features.</p><formula xml:id="formula_2">L cls2 , L reid = L nae (f )<label>(3)</label></formula><p>The overall learning objective function is given as: L = λ 1 L reg1 +λ 2 L cls1 +λ 3 L reg2 +λ 4 L cls2 +λ 5 L reid (4) λ 1 is set to 10, and the others are 1. Inference In inference stage, NMS is applied to remove redundant BBoxes before re-ID stage. In this way, the inference speed will be greatly accelerated.</p><p>First Classification Score (FCS) <ref type="figure">Figure 4</ref> shows that there are a lot of detected BBoxes with IoU &gt; 0.8 in test phase. The second head is trained with 0.5 IoU threshold, so it may fail to classify these high IoU samples correctly. Hence we take the more reliable classification scores predicted by the first head as output.</p><p>Discussion Our SeqNet shares the similar structure with previous works <ref type="bibr">Komodakis 2015, 2016;</ref><ref type="bibr" target="#b0">Cai and Vasconcelos 2018)</ref>, but our method differs from them significantly from the following aspects:</p><p>• Motivation Previous multi-stage Faster R-CNN is proposed to achieve better detection performance. However, our method aims at solving the detection and re-ID sequentially with a jointly optimized network to extract more discriminative features.</p><p>• Efficiency Our SeqNet owns an extra NMS, which ensures the efficiency in test phase. In contrast, each head of their networks needs to handle all BBoxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Bipartite Graph Matching</head><p>In this section, we present a novel Context Bipartite Graph Matching (CBGM) algorithm used in test phase to integrate context information into the matching process.</p><p>Traditional person search task can be seen as a singlepoint matching strategy, which takes the person most similar to the querier in the gallery image as the search result. But it may fail when there are multiple people with very similar appearances in the gallery image. We extend it to a multipoint matching strategy, which matches both the querier and its surrounding people with all the detected pedestrians in the gallery image. In this way, when the single-point matching strategy fails, as long as the surrounding people can be correctly matched, the query person can still be identified.</p><p>Taking <ref type="figure">Figure 2</ref> for example, we define the following symbols.</p><p>• Q/G: The query/gallery image (the upper/lower image).</p><p>• q: The query person in Q (the blue box, i.e., person (a)).</p><p>• V : All people in image (V G = {(c), (d)}).</p><p>• sim(p 1 , p 2 ): The cosine similarity between person p 1 and p 2 calculated by extracted features.</p><p>• SIM (q, G): The similarity between q and G. It is defined as the maximum value among these similarities between q and all people in G.</p><formula xml:id="formula_3">SIM (q, G) = max p∈V G sim(q, p)<label>(5)</label></formula><p>In graph theory, a matching M = (V, E) in an undirected graph is a set of edges without common vertices. V is the set of vertices and E is the set of edges. We further define the following concepts.</p><p>• weight(e i , e j ): The weight of the edge (e i , e j ) ∈ E. </p><p>• C(M ): The confidence of matching M . It is defined as the maximum value among all weights.</p><formula xml:id="formula_5">C(M ) = max (ei,ej )∈E weight(e i , e j )<label>(7)</label></formula><p>Based on these two sets of vertices V Q and V G , we firstly build a complete bipartite graph G = (V, E), in which V = V Q ∪ V G . The graph has the following properties:</p><formula xml:id="formula_6">• For every two vertices v 1 ∈ V Q and v 2 ∈ V G , (v 1 , v 2 ) is</formula><p>an edge in E. • No edge has both endpoints in the same set of vertices.</p><p>• For each edge (e i , e j ) ∈ E, its weight is the similarity of corresponding vertices, i.e., weight(e i , e j ) = sim(e i , e j ).</p><p>Then the Kuhn-Munkres (K-M) algorithm <ref type="bibr" target="#b18">(Kuhn 1955;</ref><ref type="bibr" target="#b22">Munkres 1957</ref>) is exploited to find the optimal matching with largest weight(M ). In <ref type="figure">Figure 2</ref>, the matching is (a) ↔ (c), (b) ↔ (d), and the query person (a) can be correctly matched with the ground truth (c).</p><p>The proposed Context Bipartite Graph Matching (CBGM) algorithm is described in Algorithm 1. We rank all the gallery images in descending order by SIM (q, G), and remain the top-k 1 to be processed. In this way, most gallery images in which q does not appear can be removed. Additionally, excessive context information may bring noise. Therefore we only regard the people with top-k 2 detection confidence in the query image as context information. After the optimal matching M is found, C(M ) is taken as the similarity between q and its matched person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we first introduce the datasets and evaluation protocols. Then we describe the implementation details, followed by ablation studies on the efficacy of each component. Finally, we compare our method with state-of-the-art ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Evaluation Protocol</head><p>CUHK-SYSU CUHK-SYSU <ref type="bibr" target="#b28">(Xiao et al. 2017</ref>) is a large scale person search dataset containing 18,184 scene images and 96,143 annotated BBoxes, which are collected from two sources: street snap and movie. All people are divided into 8,432 labeled identities and other unknown ones. The training set contains 11,206 images and 5,532 different identities. The test set contains 6,978 images and 2,900 query people. The training and test sets have no overlap on images and query people. For each query, different gallery sizes from 50 to 4000 are pre-defined to evaluate the search performance. If not specify, gallery size of 100 is used by default.</p><p>PRW PRW is another widely used dataset <ref type="bibr" target="#b34">(Zheng et al. 2017)</ref> containing 11,816 video frames captured by 6 cameras in Tsinghua university. 34,304 BBoxes are annotated manually. Similar to CUHK-SYSU, all people are divided into labeled and unlabeled identities. The training set contains 5,704 images and 482 different people, while the test set includes 6,112 images and 2,057 query people. For each query, the gallery is the whole test set, i.e., the gallery size is 6112. Evaluation Protocol Following the settings in previous works <ref type="bibr" target="#b21">(Munjal et al. 2019;</ref><ref type="bibr" target="#b4">Chen et al. 2020b)</ref>, the Cumulative Matching Characteristic (CMC) and the mean Averaged Precision (mAP) are adopted as the performance metrics. The formal is widely used in person re-ID, and the latter is inspired by object detection task. The higher the two metrics, the better the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We implement our model with PyTorch <ref type="bibr" target="#b23">(Paszke et al. 2017)</ref> and run all experiments on one NVIDIA Tesla V100 GPU. We adopt ResNet50 <ref type="bibr" target="#b16">(He et al. 2016</ref>) pretrained on the Im-ageNet <ref type="bibr" target="#b5">(Deng et al. 2009</ref>) as the backbone network. During training, batch size is 5 and each image is resized to 900 × 1500 pixels. Our model is optimized by Stochastic Gradient Descent (SGD) for 20 epochs (18 epochs for PRW) with initial learning rate of 0.003 which is warmed up during the first epoch and decreased by 10 at the 16-th epoch. The momentum and weight decay of SGD are set to 0.9 and 5 × 10 −4 individually. For CUHK-SYSU/PRW, the circular queue size of OIM is set to 5000/500. At test time, NMS with 0.4/0.5 threshold is used to remove redundant boxes detected by the first/second head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In this section, we perform several analytical experiments on CUHK-SYSU to better understand our proposed method.  Different detectors and re-identifiers We first explore whether the improvement brought by SeqNet comes from better detection or more discriminative features. We separate the person search task into two stages: detection stage with different detectors and re-ID stage with different reidentifiers. When using NAE re-identifier, we remove its RPN module and set the proposals manually to the BBoxes detected by the specified detector (e.g. SeqNet). In particular, NAE detector + NAE re-identifier is equivalent to the serialization mentioned in last section. The results are summarized in <ref type="table" target="#tab_3">Table 2</ref>, from which we can draw the following conclusions:</p><p>• The detection of SeqNet is better We can see from the second column that SeqNet (Recall: 92.1, AP: 89.2) achieves better detection than NAE (Recall: 92.6, AP: 86.6) in overall. It is mainly because that each head (RPN head/Faster R-CNN head/baseline head) of SeqNet will perform regression to BBoxes, which makes our model more selective against false positives. • SeqNet is more discriminative for re-ID When using NAE detector, the mAP and top-1 accuracy of SeqNet outperform that of NAE by 0.6% and 0.3% respectively. Similar improvement (mAP ↑ 0.5%, top-1 ↑ 0.7%) can be observed when using SeqNet detector. This demonstrates our SeqNet can extract more discriminative features with the same detection ability. This is caused by the inconsistency of NAE, i.e., trained by low-quality proposals but testd by high-quality detected BBoxes. In contrast, the baseline head of SeqNet is trained with detected BBoxes, which makes it more suitable for test scenario. • Detection is not the performance bottleneck If ground truth BBoxes are adopted as detection results, the mAP of NAE can be increased by 2.4%, while SeqNet can only be increased by 0.8%. It indicates that SeqNet gains very little from better detection, and future research should focus on how to achieve a better re-ID.</p><p>FCS and NMS SeqNet has two key components: FCS to improve the classification ability, NMS to accelerate the inference speed. The upper block of <ref type="table">Table 3</ref> shows that FCS greatly improves the detection (Recall: 91.5→92.7, AP: 86.7→89.7), which leads to a better re-ID (mAP: 93.1→93.8, top-1: 94.0→94.5). In addition, although NMS slightly reduces detection performance (Recall: 92.7→92.1, AP: 89.7→89.2), it does not affect re-ID and increases the FPS (processed Frames Per Second) from 7.4 to 11.5.</p><p>The multi-task framework of baseline head The lower block of <ref type="table">Table 3</ref> reports the impact of each task of the  <ref type="table">Table 3</ref>: Influence of different components on accuracy and speed. The ablation study about FCS and NMS is in the upper block. The multi-task framework of baseline head is discussed in the lower block.  . baseline head. Since the detection of Faster R-CNN head is strong enough, the regression and classification branches of baseline head will not have much impact on the overall detection, but they will facilitate the re-ID branch to learn more discriminative features. We can observe that neither regression nor classification branch alone can achieve the best performance, suggesting that the two are complementary.</p><p>Different k 1 and k 2 of CBGM We evaluate the performance with different k 1 and k 2 of CBGM. <ref type="figure">Figure 4</ref> shows that CBGM is robust to these two parameters. On CUHK-SYSU, k 1 /k 2 = 10/3 achieves the best performance, while on PRW, k 1 /k 2 = 30/4 is the best choice. This is because that the gallery size of PRW (6112) is much larger than that of CUHK-SYSU (100). Therefore, k 1 and k 2 needs to be larger to capture more context information for precise search. <ref type="table" target="#tab_8">Table 5</ref> reports the average time to search a query person under different gallery sizes. For CUHK-SYSU dataset, after sorting all gallery images in descending order by SIM (q, G), CBGM is applied to the top-10 gallery images. <ref type="table" target="#tab_8">Table 5</ref> shows that the additional computation brought by CBGM is fixed (about 2ms) and light. The larger the gallery size, the smaller the impact of CBGM on speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency of CBGM</head><p>Integrated into another method To verify the universality of our method, we integrate SeqNet into the existing end-to-end framework. We choose the widely studied OIM    <ref type="bibr" target="#b28">(Xiao et al. 2017)</ref> as the base network and its implementation is the same as in <ref type="bibr" target="#b4">(Chen et al. 2020b</ref>). As shown in <ref type="table" target="#tab_9">Table  6</ref>, SeqNet improves the mAP of OIM by 6.3% and 11.8% on CUHK-SYSU and PRW benchmarks respectively, which demonstrates that our method is insensitive to base network. Particularly, OIM+SeqNet+CBGM further achieves 94.3 of mAP and 95.0 of top-1 accuracy, outperforming all other competitors. It indicates the great potential of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the State-of-the-art Methods</head><p>In this section, we compare our method with the state-ofthe-art models on CUHK-SYSU and PRW.</p><p>CUHK-SYSU <ref type="table" target="#tab_9">Table 6</ref> shows that both the mAP and top-1 accuracy of our method are higher than other competitors. Compared with the state-of-the-art two-stage model TCTS, our NAE+SeqNet+CBGM outperforms it by 0.9% and 0.6% w.r.t mAP and top-1 accuracy though it adopts more tricks, e.g., label smooth, random erasing <ref type="bibr" target="#b35">(Zhong et al. 2020)</ref>, and triplet loss <ref type="bibr" target="#b17">(Hermans, Beyer, and Leibe 2017)</ref>. This demonstrates the effectiveness of solving detection and re-ID jointly, which can avoid sub-optimal solution. We also compare these methods under different gallery sizes. <ref type="figure">Figure 5</ref> shows that the mAP of all algorithms decreases monotonically with the increase of gallery size,  which indicates the difficulty of locating a target person in a large search scope. We can observe that our method still ranks best at all gallery sizes.</p><p>PRW The right column of <ref type="table" target="#tab_9">Table 6</ref> summarizes the results on PRW dataset. Our NAE+SeqNet+CBGM still surpasses the others. PRW has fewer training data than CUHK-SYSU, which indicates that our method is robust and effective for a relatively small dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime Comparison</head><p>We compare the speed of different models, and show the Tera-Floating Point Operation persecond (TFLOPs) for each GPU for fair comparison. Our SeqNet is implemented in PyTorch, and images are resized to 900 × 1500 pixels, which is the same as MGTS and QEEPS. <ref type="table" target="#tab_11">Table 7</ref> shows that our method is around 2 times faster than QEEPS and MGTS. Finally, our SeqNet costs 86 milliseconds per-frame on a V100 GPU, which is only a bit slower than NAE. The fast speed reveals the great potential of SeqNet to real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we notice the performance of previous end-toend framework is limited by inferior features. To address the issue, we propose a Sequential End-to-end Network to solve the detection and re-ID in turn. Besides, we design a Context Bipartite Graph Matching algorithm to exploit context information as a complement to individual feature. Extensive experiments demonstrate that our method can significantly improve the performance of previous end-to-end models at an acceptable time cost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of three methods for person search. (a). Existing end-to-end framework. (b). Existing two-stage framework. (c). Ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(c) to extract high-quality features. Specifically, detection and re-ID share the stem representations, but solved with two head networks sequentially. Compared with baseline, arXiv:2103.10148v1 [cs.CV] 18 Mar 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. Chen et al. first reveal the inherent optimization conflict between the pedestrian detection and person re-ID, and present a Mask-Guided Two-Stream (MGTS) method to eliminate the conflict (Chen et al. 2018). Han et al. introduce a RoI transform layer to jointly optimize the detection and re-ID models (Han et al. 2019). Wang et al. notice the consistency requirements between the two subtasks in person search, and adopt a Task-Consistent Two-Stage (TCTS) framework to solve the inconsistency existing in previous works (Wang et al. 2020). Dong et al. propose a Instance Guided Proposal Network (IGPN) to reduce the number of proposals to relieve the burden of re-ID (Dong et al. 2020b). Besides the two-stage framework, the faster and simpler end-to-end methods based on Faster R-CNN are also popular. Xiao et al. design the first end-to-end person search network, which is trained with standard Faster R-CNN losses and their proposed Online Instance Matching (OIM) loss (Xiao et al. 2017). Xiao et al. introduce center loss to increase the intra-class compactness of feature representations (Xiao et al. 2019). Instead of generating BBoxes for all people in the image, Liu et al. propose to recursively shrinking the search area under the guidance of the query (Liu et al. 2017). Chang et al. adopt a similar idea and first introduce the deep reinforcement learning into person search framework (Chang et al. 2018). Yan et al. build a graph model to exploit context information as an complementary cue for person matching (Yan et al. 2019). Munjal et al. propose a query-guided region proposal network (QRPN) to produce query-relevant proposals, and a query-guided similarity subnetwork (QSimNet) to learn a query-guided re-ID score (Munjal et al. 2019). Chen et al. propose a Hierarchical Online Instance Matching (HOIM) loss which exploits the hierarchical relationship between detection and re-ID to guide the feature learning of their network (Chen et al. 2020a). Dong et al. design a Bi-directional Interaction Network (BINet) to remove rebundent context information outside BBoxes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>(a).Baseline (b). Our Sequential End-to-end Network, in which yellow parts are modifications and NMS only be applied in inference stage. The structure before RoI-Align is the same as baseline, so it is not shown here for simplification. The IoU statistics of the BBoxes of labeled pedestrians detected by the first head in test phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>•</head><label></label><figDesc>weight(M ): The weight of matching M . It is defined as the sum of the weights of all edges. weight(M ) = (ei,ej )∈E weight(e i , e j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1: Influence of inferior features on the performance on CUHK-SYSU and PRW datasets. We separate the person search into detection and re-ID, and evaluate their performance individually. The bold font represents the best result. Most experiment results will be presented in this form.the training and test phase, i.e., the network is trained by the proposals generated by RPN, but tested by the detected BBoxes. Therefore it is necessary to introduce serialization into model training, rather than just in test phase.</figDesc><table><row><cell>Method</cell><cell cols="3">Detection Recall AP mAP top-1 re-ID</cell></row><row><cell></cell><cell></cell><cell>CUHK-SYSU</cell><cell></cell></row><row><cell>parallelization</cell><cell>92.6</cell><cell>86.6 91.7</cell><cell>92.8</cell></row><row><cell>serialization</cell><cell>90.9</cell><cell>85.7 92.5</cell><cell>93.7</cell></row><row><cell></cell><cell></cell><cell>PRW</cell><cell></cell></row><row><cell>parallelization</cell><cell>93.8</cell><cell>88.7 43.6</cell><cell>80.0</cell></row><row><cell>serialization</cell><cell>93.7</cell><cell>89.4 44.7</cell><cell>80.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Query person, q ∈ V Q Gallery images, S = {G 1 , G 2 , ...} Number of processed gallery images, k 1 Maximum context, k 2 Output: Most similar person in each gallery image Similarities between q and these most similar people 1 Rank S in descending order by SIM (q, G) 2 Remain top-k 1 gallery images, S = {G 1 , G 2 , ..., G k1 } 3 Rank V Q in descending order by detection confidence 4 Remain top-k 2 people, V Q = {q 1 , q 2 , ..., q k2 } 5 Set people, sims to empty list 6 for each G ∈ S do</figDesc><table><row><cell cols="2">Algorithm 1: CBGM</cell></row><row><cell></cell><cell>Input:</cell></row><row><cell></cell><cell>Query image, Q</cell></row><row><cell>7</cell><cell>Based on V Q and V G , build a complete bipartite</cell></row><row><cell></cell><cell>graph G = (V, E)</cell></row><row><cell>8</cell><cell>Exploit K-M algorithm to find the optimal</cell></row><row><cell></cell><cell>matching M with largest weight</cell></row></table><note>9 for each edge (e i , e j ) of M do10 if e i = q then11 Insert e j into people12 Insert C(M ) into sims13 break 14 return people, sims</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Analytical experiment results with different detectors and re-identifiers on CUHK-SYSU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The performance on CUHK-SYSU (the upper block) and PRW (the lower block) datasets with different k 1 and k 2 of CBGM. We evaluate the performance by</figDesc><table><row><cell>mAP +top−1</cell></row><row><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>The average time to search a query person under different gallery sizes on CUHK-SYSU. The unit is milliseconds.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="4">CUHK-SYSU mAP top-1 mAP top-1 PRW</cell></row><row><cell></cell><cell>DPM(Girshick et al. 2015)</cell><cell>-</cell><cell>-</cell><cell cols="2">20.5 48.3</cell></row><row><cell>two-stage</cell><cell cols="2">MGTS(Chen et al. 2018) CLSA(Lan, Zhu, and Gong 2018) 87.2 83.0 RDLR(Han et al. 2019) 93.0 IGPN(Dong et al. 2020b) 90.3</cell><cell>83.7 88.5 94.2 91.4</cell><cell cols="2">32.6 72.1 38.7 65.0 42.9 70.2 47.2 87.0</cell></row><row><cell></cell><cell>TCTS(Wang et al. 2020)</cell><cell>93.9</cell><cell>95.1</cell><cell cols="2">46.8 87.5</cell></row><row><cell></cell><cell>OIM(Xiao et al. 2017)</cell><cell>75.5</cell><cell>78.7</cell><cell cols="2">21.3 49.9</cell></row><row><cell></cell><cell>IAN(Xiao et al. 2019)</cell><cell>76.3</cell><cell>80.1</cell><cell cols="2">23.0 61.9</cell></row><row><cell></cell><cell>NPSM(Liu et al. 2017)</cell><cell>77.9</cell><cell>81.2</cell><cell cols="2">24.2 53.1</cell></row><row><cell></cell><cell>RCAA(Chang et al. 2018)</cell><cell>79.3</cell><cell>81.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>CTXGraph(Yan et al. 2019)</cell><cell>84.1</cell><cell>86.5</cell><cell cols="2">33.4 73.6</cell></row><row><cell>end-to-end</cell><cell>QEEPS(Munjal et al. 2019) HOIM(Chen et al. 2020a) BINet(Dong et al. 2020a) NAE(Chen et al. 2020b) NAE+(Chen et al. 2020b)</cell><cell>88.9 89.7 90.0 91.5 92.1</cell><cell>89.1 90.8 90.7 92.4 92.9</cell><cell cols="2">37.1 76.7 39.8 80.4 45.3 81.7 43.3 80.9 44.0 81.1</cell></row><row><cell></cell><cell>OIM(ours)</cell><cell>87.1</cell><cell>88.5</cell><cell cols="2">34.0 75.9</cell></row><row><cell></cell><cell>OIM+SeqNet(ours)</cell><cell>93.4</cell><cell>94.1</cell><cell cols="2">45.8 81.7</cell></row><row><cell></cell><cell>OIM+SeqNet+CBGM(ours)</cell><cell>94.3</cell><cell>95.0</cell><cell cols="2">46.6 84.9</cell></row><row><cell></cell><cell>NAE+SeqNet(ours)</cell><cell>93.8</cell><cell>94.6</cell><cell cols="2">46.7 83.4</cell></row><row><cell></cell><cell>NAE+SeqNet+CBGM(ours)</cell><cell>94.8</cell><cell>95.7</cell><cell cols="2">47.6 87.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison of mAP and top-1 accuracy with the state-of-the-art methods on CUHK-SYSU and PRW. Our models are shown in italics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The mAP under different gallery sizes. The dashed lines represent two-stage methods and the solid lines represent end-to-end ones.</figDesc><table><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NAE+SeqNet+CBGM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NAE</cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RCAA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IAN</cell></row><row><cell>mAP</cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NPSM OIM BINet</cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CTXGraph</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MGTS</cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLSA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDLR</cell></row><row><cell></cell><cell>50</cell><cell>0</cell><cell>1,000</cell><cell>2,000</cell><cell>3,000</cell><cell>4,000</cell><cell>TCTS</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Gallery size</cell><cell></cell><cell></cell></row><row><cell cols="8">Figure 5: GPU(TFLOPs) MGTS QEEPS NAE NAE+ SeqNet</cell></row><row><cell cols="3">K80(4.1)</cell><cell cols="2">1269</cell><cell>-</cell><cell>663</cell><cell>606</cell><cell>-</cell></row><row><cell cols="3">P6000(12.6)</cell><cell>-</cell><cell></cell><cell>300</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">P40(11.8)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>158</cell><cell>161</cell><cell>-</cell></row><row><cell cols="3">V100(14.1)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>83</cell><cell>98</cell><cell>86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparison of running time on different GPUs. The unit is milliseconds.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">RCAA: Relational contextaware agents for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="84" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical Online Instance Matching for Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10518" to="10525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person search via a mask-guided two-stream cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Norm-Aware Embedding for Efficient Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12615" to="12624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bi-Directional Interaction Network for Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2839" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instance Guided Proposal Network for Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2585" to="2594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8295" to="8302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attend refine repeat: Active box proposal generation via in-out localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04446</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="437" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Re-id driven localization refinement for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9814" to="9823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HSME: Hypersphere manifold embedding for visible thermal person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8385" to="8392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person search by multi-scale matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="536" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural person search machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="493" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Query-guided end-to-end person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Munjal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Algorithms for the assignment and transportation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the society for industrial and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TCTS: A Task-Consistent Two-Stage Framework for Person Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11952" to="11961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatialtemporal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8933" to="8940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">IAN: the individual aggregation network for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="332" to="340" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person search in a scene by jointly modeling people commonness and person uniqueness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="937" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning context graph for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2158" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-level descriptors for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4306" to="4312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep fusion feature representation learning with hard mining center-triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Consistent iterative multi-view transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1087" to="1094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Random Erasing Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
