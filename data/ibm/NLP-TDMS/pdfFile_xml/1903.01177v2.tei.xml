<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff and Things</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaku</forename><surname>Narita</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Seno</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Ishikawa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohsuke</forename><surname>Kaji</surname></persName>
						</author>
						<title level="a" type="main">PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff and Things</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose PanopticFusion, a novel online volumetric semantic mapping system at the level of stuff and things. In contrast to previous semantic mapping systems, PanopticFusion is able to densely predict class labels of a background region (stuff) and individually segment arbitrary foreground objects (things). In addition, our system has the capability to reconstruct a large-scale scene and extract a labeled mesh thanks to its use of a spatially hashed volumetric map representation. Our system first predicts pixel-wise panoptic labels (class labels for stuff regions and instance IDs for thing regions) for incoming RGB frames by fusing 2D semantic and instance segmentation outputs. The predicted panoptic labels are integrated into the volumetric map together with depth measurements while keeping the consistency of the instance IDs, which could vary frame to frame, by referring to the 3D map at that moment. In addition, we construct a fully connected conditional random field (CRF) model with respect to panoptic labels for map regularization. For online CRF inference, we propose a novel unary potential approximation and a map division strategy.</p><p>We evaluated the performance of our system on the ScanNet (v2) dataset. PanopticFusion outperformed or compared with state-of-the-art offline 3D DNN methods in both semantic and instance segmentation benchmarks. Also, we demonstrate a promising augmented reality application using a 3D panoptic map generated by the proposed system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Geometric and semantic scene understanding in 3D environments has an important role in autonomous robotics and context-aware augmented reality (AR) applications. Geometric scene understanding such as visual simultaneous localization and mapping (SLAM) and 3D reconstruction has been widely discussed since the early days of both the robotics and computer vision communities. In recent years, semantic mapping, which not only reconstructs the 3D structure of a scene but also recognizes what exists in the environment, has attracted much attention because of the great progress of deep neural networks.</p><p>Semantic mapping systems could take a variety of approaches in terms of geometry and semantics. When we think about robotic and AR applications that deeply interact with the real world, what kind of properties are required for the ideal semantic mapping system? In terms of geometry, it needs to be able to reconstruct a large-scale scene, not sparsely but densely. Additionally, the 3D reconstruction desirably needs to be represented as a volumetric map, not just point clouds or surfels, because it is difficult to directly utilize point clouds and surfels for robot-object collision <ref type="bibr" target="#b0">1</ref> The authors are with R&amp;D Center, Sony Corporation.</p><p>{gaku.narita, takashi.seno, tomoya.ishikawa, yohsuke.kaji}@sony.com detection or robot navigation. In terms of semantics, which we mainly focus on in this paper, we believe that it is important for the mapping system to have a holistic scene understanding capability, that is to say, dense semantic labeling as well as individual object discrimination. This is because densely labeled semantics is a crucial cue for intelligent robot navigation, and also, discriminating individual objects is essential for robot-object interaction.</p><p>Turning our eyes to the field of 2D image recognition, an image understanding task called panoptic segmentation has been proposed recently <ref type="bibr" target="#b10">[11]</ref>. In the panoptic segmentation task, semantic classes are defined as a set of stuff classes (amorphous regions, such as floors, walls, the sky and roads) and thing classes (countable objects, such as chairs, tables, people and vehicles) and one needs to predict class labels on stuff regions and both class labels and instance IDs on thing regions, where the predictions should be performed for each pixel. Extending this point of view to 3D mapping, in this paper we propose the PanopticFusion system. To the best of our knowledge, it is the first semantic mapping system that realizes scene understanding at the level of stuff and things. Our system incrementally performs large-scale 3D surface reconstruction online, as well as dense class label prediction on the background region and segmentation and recognition of individual foreground objects, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Our approach first passes the incoming RGB frame to 2D semantic and instance segmentation networks and obtains a panoptic label image in which class labels are assigned to stuff pixels and instance IDs to thing pixels. The predicted panoptic labels and depth measurements are integrated into the volumetric map. Before integration, we keep the consistency of instance IDs, which possibly change from frame to frame, by referring to the volumetric map at that moment. In addition, we regularize the map using a fully connected CRF model with respect to panoptic labels. For CRF inference, we propose a unary potential approximation using limited information stored in the map. We also present a map division strategy that achieves a significant reduction in computational time without a drop in accuracy.</p><p>We evaluated the performance of our system on the ScanNet v2 dataset <ref type="bibr" target="#b3">[4]</ref>, a richly annotated large-scale dataset for indoor scene understanding. The results revealed that PanopticFusion is superior or comparable to the state-of-theart offline 3D DNN methods in the both 3D semantic and instance segmentation tasks. Note that our system is not limited to indoor scenes. Finally, we demonstrated a promising AR application using the 3D panoptic map generated by our system.</p><p>The main contributions of this paper are the following:</p><p>• The first reported semantic mapping system that realizes scene understanding at the level of stuff and things. • Large-scale 3D reconstruction and labeled mesh extraction thanks to the use of a spatially hashed volumetric map representation. • Map regularization using a fully connected CRF with a novel unary potential approximation and map division strategy. • Superior or comparable results in both 3D semantic and instance segmentation tasks, in comparison with the state-of-the-art offline 3D DNN methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Previously proposed representative semantic mapping systems related to our PanopticFusion system are shown in <ref type="table" target="#tab_0">Table  I</ref>. These systems can be divided into two categories from the perspective of semantics: the dense labeling approach and the object-oriented approach.</p><p>The dense labeling approach builds a single 3D map and assigns a class label or a probability distribution of class labels to each surfel or voxel to realize a dense 3D semantic segmentation. Hermans et al. <ref type="bibr" target="#b7">[8]</ref> utilize random decision forests for 2D semantic segmentation and transfer the inferred probability distributions to point clouds with a Bayesian update scheme. Extending the approach of Hermans et al. <ref type="bibr" target="#b7">[8]</ref>, SemanticFusion <ref type="bibr" target="#b16">[17]</ref> improves the recognition performance by using CNNs for 2D semantic segmentation and makes use of ElasticFusion [31] for a SLAM system to generate a globally consistent map. Xiang et al. <ref type="bibr" target="#b31">[32]</ref> presented KinectFusion <ref type="bibr" target="#b18">[19]</ref>-based volumetric mapping with novel data associated RNNs for improving the segmentation accuracy. While these methods realize dense scene understanding, they suffer from the drawback that they are not able to distinguish individual objects in the scene.  <ref type="bibr" target="#b28">[29]</ref> SemanticFusion <ref type="bibr" target="#b16">[17]</ref> DA-RNN <ref type="bibr" target="#b31">[32]</ref> MaskFusion <ref type="bibr" target="#b23">[24]</ref> Fusion++ <ref type="bibr" target="#b15">[16]</ref> PanopticFusion (Ours) Methods adopted in the early days of the object-oriented approach leverage 3D model databases. SLAM++ <ref type="bibr" target="#b24">[25]</ref> performs point pair feature-based object detection and feeds the detected objects into a pose graph. Tateno et al. <ref type="bibr" target="#b28">[29]</ref> proposed a 3D object detection and pose estimation system that combines unsupervised geometric segmentation and global 3D descriptor matching. These methods, however, require the shapes of objects in the scene to be exactly the same as the 3D models in the database. Recently, several studies on the object-oriented approach using a CNN-based 2D object detector have been reported. Sünderhauf et al. <ref type="bibr" target="#b26">[27]</ref> and Nakajima et al. <ref type="bibr" target="#b17">[18]</ref> combine a 2D object detector and unsupervised geometric segmentation in order to detect objects in point clouds or a surfel map. MaskFusion <ref type="bibr" target="#b23">[24]</ref>, Fusion++ <ref type="bibr" target="#b15">[16]</ref> and MID-Fusion <ref type="bibr" target="#b32">[33]</ref> introduced an objectoriented map representation that individually builds 3D maps for each object based on 2D object detection. The objectoriented map representation enables tracking of individual objects <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref> and an object-level pose graph optimization <ref type="bibr" target="#b15">[16]</ref>. However, the quantitative recognition performance of these methods is not clear because they mainly evaluate the camera trajectory accuracy. Furthermore, they focus on foreground objects, resulting in a lack of semantics and/or geometry of background regions.</p><p>In contrast to these related studies, PanopticFusion realizes holistic scene reconstruction and dense semantic labeling with the ability to discriminate individual objects. Our system builds a single volumetric map, similar to dense labeling approaches, yet each voxel stores neither class labels nor class probability distributions but DNN-predicted panoptic labels in order to seamlessly manage both stuff and things semantics. The class labels of foreground objects can be restored by a probability integration process. In addition, our 3D reconstruction leverages the truncated signed distance field (TSDF) volumetric map with the voxel hashing data structure <ref type="bibr" target="#b19">[20]</ref>, which allows us to reconstruct a large-scale scene as well as extract labeled meshes by using marching cubes <ref type="bibr" target="#b14">[15]</ref>, in contrast to the 3D maps of previous methods, which are based on point clouds <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b26">[27]</ref>, surfels <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref> and a fixed-sized voxel grid <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b15">[16]</ref>. It should be noted that, with 3D DNN methods that directly apply deep networks to 3D data such as point clouds or voxel grids, high recognition performance has been reported <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Nevertheless, with those methods, it is basically necessary to reconstruct the whole scene in advance, requiring offline processing, which could limit their application to robotics and AR. On the contrary, PanopticFusion is an online and incremental framework.</p><p>III. METHOD <ref type="figure" target="#fig_1">Fig. 2</ref> shows the system overview of PanopticFusion. Our system first feeds an incoming RGB frame into 2D semantic and instance segmentation networks and obtains pixel-wise panoptic labels by fusing the two outputs (Section III-C). The panoptic labels are carefully tracked by referring to the volumetric map at that moment (Section III-D) and are integrated into the map with depth measurements (Section III-E). Probability distributions of class labels for foreground objects are also incrementally integrated (Section III-F). In addition, online map regularization with a fully-connected CRF model is performed for a further improvement of the recognition accuracy. Note that camera poses with respect to the volumetric map are given by an external vSLAM, and labeled meshes are extracted by using marching cubes <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notations</head><p>We denote all class labels by L, and they are divided into stuff labels L St and thing labels L Th : such that L = L St ∪ L Th and L St ∩ L Th = ∅. A set of instance IDs for discriminating individual things is denoted by Z. Here we define a set of panoptic labels L Pa = L St ∪ Z ∪ l unk in order to seamlessly manage stuff and things level semantics in the 3D map. l unk denotes the unknown label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Volumetric Map</head><p>We use the TSDF-based volumetric map representation with a voxel hashing approach <ref type="bibr" target="#b19">[20]</ref>, which manages spatially hashed small regular voxel grids called voxel blocks. This approach is memory efficient compared with a single voxel grid approach like the original KinectFusion <ref type="bibr" target="#b18">[19]</ref> and enables us to reconstruct large-scale scenes. Our implementation is based on voxblox <ref type="bibr" target="#b20">[21]</ref>, which is a CPU-based TSDF mapping system, but we extend it to integrate the semantics.</p><p>Our volumetric map stores the truncated signed distance <ref type="bibr" target="#b18">[19]</ref>. Our system additionally stores the panoptic label</p><formula xml:id="formula_0">D t (v) ∈ R, the RGB color C t (v) ∈ R 3 and the associated weight W D t (v) ∈ R ≥0 at each voxel location v ∈ R 3 , as with</formula><formula xml:id="formula_1">L Pa t (v) ∈ L Pa and its weight W L t (v) ∈ R ≥0 .</formula><p>Here t denotes the time index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 2D Panoptic Label Prediction</head><p>For the incoming RGB frame, we predict pixel-wise panoptic labels by fusing both 2D semantic and instance segmentation outputs. We utilize the state-of-the-art CNN architectures of PSPNet <ref type="bibr" target="#b35">[36]</ref> and Mask R-CNN <ref type="bibr" target="#b6">[7]</ref> for 2D semantic and instance segmentation, respectively. PSPNet infers pixel-wise class labels L t (u) ∈ L, where u ∈ R 2 denotes the image coordinates. Mask R-CNN outputs instance IDs for each pixel Z t (u) ∈ Z ∪ l unk , where the regions without any foreground objects are filled with l unk . The foreground object probability p t (z, O) and conditional probability distribution of thing labels p t (z, l Th |O) with respect to instance z are utilized in the probability integration step described in Section III-F. We obtain pixel-wise panoptic labels L Pa t (u) from L t (u) and Z t (u) preceding the instance IDs:</p><formula xml:id="formula_2">L Pa t (u) =      Z t (u) Z t (u) = l unk L t (u) Z t (u) = l unk ∧ L t (u) ∈ L St l unk otherwise.</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Panoptic Label Tracking</head><p>Direct integration of raw panoptic labels L Pa t (u) into the volumetric map induces label inconsistency because Mask R-CNN does not necessarily output a consistent instance ID for the same object through multiple frames. To avoid this problem, we need to estimate consistency-resolved panoptic labelsL Pa t (u) before the integration. The simplest way is to track the foreground objects in the 2D image sequence using a visual object tracker. This approach unfortunately is not able to re-identify an object in the case of a loopy camera trajectory. Therefore, we take a map reference approach similar to <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>We first prepare the reference panoptic labelsL Pa t−1 (u) by accessing the map. Here, T t denotes the live camera pose, K the camera intrinsic parameters, and D t (u) the live depth map:</p><formula xml:id="formula_3">L Pa t−1 (u) = L Pa t−1 (T t K −1 D t (u)[u, 1] T ).<label>(2)</label></formula><p>To track labels, we compute the intersection over union (IoU) U (z, z) of instance ID z of raw panoptic labels L Pa t (u) and instance IDz of reference panoptic labelsL Pa t−1 (u):</p><formula xml:id="formula_4">U (z, z) = IoU {u|L Pa t−1 (u) =z}, {u|L Pa t (u) = z}<label>(3)</label></formula><p>Here, IoU is defined as IoU(A, B) = |A ∩ B|/|A ∪ B|.</p><p>When the maximum value of IoU exceeds a threshold θ U , z giving the maximum value is associated with z. Otherwise a new instance ID is assigned to z:</p><formula xml:id="formula_5">z = arg maxz U (z, z) maxz U (z, z) &gt; θ U z new otherwise.<label>(4)</label></formula><p>The association is processed in descending order in the mask area |{u|L Pa t (u) = z}|. Once a reference instance IDz is associated with z, that instance ID is not associated with any other z. The utilization of IoU instead of an overlap ratio, as used in <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b15">[16]</ref>, and the exclusive label association is for avoiding under-segmentation of foreground objects in the map. From the associated instance IDs and raw stuff labels, we obtain the consistency-resolved panoptic labelsL Pa t (u) as follows, which are used in the integration step:</p><formula xml:id="formula_6">L Pa t (u) =      L Pa t (u) L Pa t (u) ∈ L St z L Pa t (u) ∈ Z l unk otherwise.<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Volumetric Integration</head><p>For integration, we take the raycasting approach, as with <ref type="bibr" target="#b20">[21]</ref>. For each pixel u, we cast a ray from the sensor origin s to the back-projected 3D point p u = T t K −1 D t (u)[u, 1] T and update the voxels along the ray within a truncated distance. Regarding TSDF values, we update them by weighted averaging, similar to <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_7">D t (v) = W D t−1 (v)D t−1 (v) + w t (v, p u )d t (v, p u , s) W D t−1 (v) + w t (v, p u ) ,<label>(6)</label></formula><formula xml:id="formula_8">W D t (v) = W D t−1 (v) + w t (v, p u ).<label>(7)</label></formula><p>Here, d t denotes the distance between the voxel and the surface boundary, and w t a quadric weight <ref type="bibr" target="#b20">[21]</ref> that takes the reliability of depth measurements into account. Similar updating is applied to the voxel color C t (v).</p><p>In contrast to TSDF and colors of continuous values, weighted averaging cannot be applied to panoptic labels of discrete values. The most reliable and simplest way to manage panoptic labels is to record all integrated labels. This, unfortunately, results in a significant increase in memory usage and frequent memory allocation. Instead we store a single label at each voxel and update its weight by the increment/decrement strategy. If the pixel-wise panoptic labelL Pa t (u) estimated in the previous section is the same as the current voxel panoptic label L Pa t−1 (v), we increment the weight W L t (v) with the quadric weight:</p><formula xml:id="formula_9">L Pa t (v) = L Pa t−1 (v), W L t (v) = W L t−1 (v) + w t (v, p u ).<label>(8)</label></formula><p>In contrast, if those panoptic labels do not coincide, we decrement the weight:</p><formula xml:id="formula_10">L Pa t (v) = L Pa t−1 (v), W L t (v) = W L t−1 (v) − w t (v, p u ).<label>(9)</label></formula><p>Note that in the case where w t &gt; W L t−1 , that is, when the weight considerably falls, we replace the voxel label with the newly estimated label:</p><formula xml:id="formula_11">L Pa t (v) =L Pa t (u), W L t (v) = w t (v, p u ) − W L t−1 (v). (10) F.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thing Label Probability Integration</head><p>The thing label predicted by Mask R-CNN is frequently uncertain even while the segmentation mask is accurate, especially in the case where a small part of the object is visible. Hence we probabilistically integrate thing labels instead of assigning a single label to each foreground object:</p><formula xml:id="formula_12">p 1···t (z, l Th ) = t p t (z, O)p t (z, l Th |O) t p t (z, O) .<label>(11)</label></formula><p>Weighting the probability distributions with the detection confidence p t (z, O) allows the final distribution to preferentially reflect reliable detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Online Map Regularization</head><p>While the integration scheme described above yields a reliable 3D panoptic map, it is possible to further improve the recognition accuracy by using a map regularization with a fully connected CRF model. A fully connected CRF with Gaussian edge potentials has been widely used in 2D image segmentation since an efficient inference method was proposed <ref type="bibr" target="#b11">[12]</ref>. Recently, several studies that apply it to a 3D map, such as surfels or occupancy grids, have been reported <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b33">[34]</ref>. In those approaches, CRF models are constructed with respect to class labels whose number is fixed, whereas we consider the CRF with respect to panoptic labels whose number depends on the scene and is theoretically not limited. Here we are faced with two problems: how to properly compute unary potentials for panoptic labels, and how to infer a CRF whose number of labels is potentially large within a practical time.</p><p>1) Problem Setting: We construct a fully connected graph whose nodes are individual voxels. We assign a label variable x v ∈ L Pa to each node and infer the optimal labels x = {x v } that minimize the Gibbs energy E by the mean-field approximation and a message passing scheme:</p><formula xml:id="formula_13">E(x) = v ψ u (x v ) + v&lt;v ψ p (x v , x v ).<label>(12)</label></formula><p>While it is non-trivial which unary potentials should be used for a panoptic label CRF, we use a negative logarithm of a probability distribution following a standard class label CRF:</p><formula xml:id="formula_14">ψ u (x v ) = − log p(x v ).<label>(13)</label></formula><p>We utilize a linear combination of Gaussian kernels for pairwise potentials because the efficient inference method <ref type="bibr" target="#b11">[12]</ref> can be applied:</p><formula xml:id="formula_15">ψ p (x v , x v ) = µ(x v , x v ) m w (m) k (m) (f v , f v ).<label>(14)</label></formula><p>Here, µ(x s , x s ) = 1 [xs =x s ] is a simple Potts model. As in <ref type="bibr" target="#b11">[12]</ref>, we chose the following two kernels which regularize the map with respect to voxel colors and locations, respectively:</p><formula xml:id="formula_16">k (1) (f v , f v ) = exp − |v − v | 2 2θ 2 α − |C(v) − C(v )| 2 2θ 2 β ,<label>(15)</label></formula><formula xml:id="formula_17">k (2) (f v , f v ) = exp − |v − v | 2 2θ 2 α .<label>(16)</label></formula><p>2) Unary Potential Approximation: Previous approaches <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b33">[34]</ref> assigned a probability distribution to each surfel or voxel, which can be used directly to compute unary potentials; in contrast, from the viewpoint of memory efficiency, we store only a single label in each voxel. Therefore, we approximate the unary potentials using only a single label, and weights stored in a voxel, based on a certain assumption described as follows.</p><p>Here let us focus on the integration scheme of panoptic labels shown in Eq. (8)- <ref type="bibr" target="#b9">(10)</ref>. We denote the set of times when the predicted panoptic label is the same as, and not the same as, the current voxel label by</p><formula xml:id="formula_18">T + = {τ |L Pa τ (u) = L Pa t (v)} and T − = {τ |L Pa τ (u) = L Pa t (v)}, respectively. If L Pa τ (v) = L Pa t (v)</formula><p>for all τ = 1, · · · , t − 1, that is to say, the voxel label has not changed, Eq. (17) holds strictly. If p(x v = L Pa t (v)) &gt; 0.5 and the number of integrations is sufficiently large, Eq. (17) holds asymptotically:</p><formula xml:id="formula_19">t∈T+ w t (v, p u ) − t∈T− w t (v, p u ) W L t (v).<label>(17)</label></formula><p>In addition, from the TSDF update scheme in Eq. <ref type="formula" target="#formula_8">(7)</ref> we have,</p><formula xml:id="formula_20">t∈T+ w t (v, p u ) + t∈T− w t (v, p u ) = W D t (v).<label>(18)</label></formula><p>Consequently, the probability that the current panoptic label in the voxel is actually correct can be calculated as,</p><formula xml:id="formula_21">p(x v = L Pa t (v)) = t∈T+ w t (v, p u ) t∈T+ w t (v, p u ) + t∈T− w t (v, p u ) 1 2 1 + W L t (v) W D t (v) .<label>(19)</label></formula><p>It is unfortunately not possible to calculate the exact probability that the voxel takes a label other than the current label because the map does not record all the information about previously integrated labels. Therefore, we approximate the probability as follows, where M denotes the number of panoptic labels in the map:</p><formula xml:id="formula_22">p(x v ) = 1 M − 1 1 − p(x v = L Pa t (v)) (x v = L Pa t (v)). (20)</formula><p>Finally, we obtain the unary potential from Eq. (13), <ref type="bibr" target="#b18">(19)</ref> and <ref type="bibr" target="#b19">(20)</ref>. In spite of the approximated approach, it realizes quantitative and qualitative improvements in recognition accuracy, as shown in Section IV-C.</p><p>3) Map Division for Online Inference: The computational complexity of the inference algorithm proposed by Krähenbühl et al. <ref type="bibr" target="#b11">[12]</ref> is O(N M ), where N and M are the numbers of voxels and panoptic labels, respectively. In our problem setting, however, M is theoretically limitless and could in practice be large, e.g. several hundreds, which would make online inference impracticable. To solve this problem, we present a map division strategy. When we divide the volumetric map into S spatially contiguous submaps, the number of panoptic labels in each submap can be expected to be O(M/S). Hence, the total computational complexity could be reduced to S ×O(N/S ×M/S) = O(N M )/S. The map is divided by the block-wise region growing approach based on the predefined maximum number of voxel blocks. The division process has little effect on computational time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION A. Experimental Setup</head><p>For evaluating the performance of our system, we used the ScanNet v2 dataset <ref type="bibr" target="#b3">[4]</ref>, a large-scale dataset for indoor scene understanding. It provides RGB-D images captured by hand-held consumer-grade depth sensors, camera trajectories, reconstructed 3D models, and 2D/3D semantic annotations. In the following experiments, we used RGB-D images of size 640×480 pixels and the provided camera trajectories for fair comparison. The dataset was composed of 1201 training scenes and 312 open test scenes. In addition, 100 hidden test scenes without publicly available semantic annotations are provided for the ScanNet Benchmark Challenge <ref type="bibr" target="#b1">[2]</ref>. For quantitative evaluations, 20 class annotations are generally used. In this paper, we define the wall and floor as the stuff class L St and the other 18 classes, such as chairs and sofas, as the thing class L Th . Note that our system is not limited to indoor scenes, and the numbers of stuff and thing classes can be arbitrarily defined.</p><p>We employed ResNet-50 for the backbone of PSPNet. The network was initialized with the ADE20K <ref type="bibr" target="#b36">[37]</ref> pre-trained weights, and was then fine-tuned using a SGD optimizer for 30 epochs with a learning rate of 0.01 and a batch size of 2. We leveraged ResNet-101-FPN for the Mask R-CNN's backbone. After initialization with MS COCO <ref type="bibr" target="#b12">[13]</ref> pre-trained weights, the network was fine-tuned by 4-step alternating learning <ref type="bibr" target="#b22">[23]</ref> using an ADAM optimizer for 25 epochs with a learning rate of 0.001 and a batch size of 1 1 .</p><p>We used the following parameters for the integration process: voxel size of 0.024 m, a truncation distance of 4×0.024 m, 16×16×16 voxels per voxel block, IoU threshold θ U = 0.25. In the map regularization, w (1) = 10, w (2) = 15, θ α = 0.05 m, θ β = 20 were used with 5 iterations of CRF inference. The following experiments were performed on a computer equipped with an Intel Core i7-7800X CPU at 3.50 GHz and two NVIDIA GeForce GTX 1080Ti GPUs. <ref type="figure">Fig. 3</ref> shows examples of 3D panoptic maps generated by our system. Unfortunately, there are no semantic mapping systems or 3D DNNs that can recognize a 3D scene at the level of stuff and things. Therefore, we evaluated the performance on two sub-tasks, 3D semantic segmentation and instance segmentation, for a quantitative comparison. In this evaluation, we used the hidden test set of ScanNet v2. We show the results in Tables II and III. In the tables, the state-of-the-art methods that apply 3D DNNs to points or volumetric grids are listed. Note that the methods of <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref> leverage RGB images with associated camera poses as well. Our system that uses only 2D-based recognition modules surprisingly achieves comparable or superior performance compared with those methods, thanks to the careful integration of multi-view predictions. In terms of the classwise accuracy, the results revealed that our system has advantages especially in the case of small objects such as sinks and pictures, and objects that are confusing to recognize only by their geometry, such as beds, bookshelves, and curtains. In <ref type="table" target="#tab_0">Table II</ref>, several semantic segmentation methods outperform <ref type="figure">Fig. 3</ref>. Qualitative results obtained with PanopticFusion system. From left to right, typical scenes in ScanNet v2 of scene0608 00, scene0643 00 and scene0488 01 are displayed. Note that ground truth and our results leverage different reconstruction algorithms, and the colors of things in our results are not necessarily the same as the ground truth. our system because of their large receptive fields in 3D space. However, these methods basically need to reconstruct the entire scene in advance, assuming offline process, while our system is an online and incremental framework. How to apply 3D DNNs to partial observations and how to integrate them into an online mapping system are left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative and Qualitative Results</head><p>Additionally, we evaluated 3D panoptic quality on the open test set of ScanNet v2, although there are no quantitatively comparable methods. We employed the evaluation criteria originally proposed in <ref type="bibr" target="#b10">[11]</ref>. Note that the quality was evaluated with respect to each vertex instead of each pixel, and, as with the ScanNet 3D semantic instance benchmark, we ignored the predicted things with less than 100 vertices. We show the panoptic quality (PQ) as well as the segmentation quality (SQ) and recognition quality (RQ) in <ref type="table" target="#tab_0">Table IV</ref>. We hope these results will invigorate research in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of Map Regularization</head><p>In this section, we evaluate the map regularization proposed in Section III-G. First, we evaluated the effects of the map division on the recognition accuracy and computational time.</p><p>We used the open test set for the recognition accuracy and typical scenes in ScanNet v2 for the computational time. The result is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Note that, in this experiment, we applied regularization to the pre-generated map as a post process to evaluate solely the effects of CRF.</p><p>As can been seen, the recognition performance was improved by the map regularization with the proposed unary potential approximation regardless of whether or not map division was used. The results also show that the map division strategy drastically reduced the computational time without a decrease in recognition performance, compared with the case of building a CRF model for a whole map.</p><p>Based on the above results, our online system employed map regularization with the map division strategy. We chose a maximum number of voxel blocks of 25 because of the better recognition accuracy and acceptable computational time. <ref type="table" target="#tab_0">Table IV</ref> shows the difference in recognition performance due to whether or not map regularization was used in online processing. This result shows that the map regularization improved the recognition performance even when the system ran online. Note that the scores of almost all the classes were boosted by the proposed regularization. See <ref type="figure" target="#fig_3">Fig. 5</ref> for qualitative effects of the map regularization.      <ref type="figure" target="#fig_0">Fig. 1</ref>). PSPNet and Mask R-CNN each run on GPUs, and the other components are processed on a CPU. All components are basically processed in parallel. The throughput of our system is around 4.3 Hz, which is determined by Mask R-CNN, the bottleneck process of our system. Although our current implementation is not highly optimized, our system is able to run at a rate allowing interaction. Note that the computational time except for the map regularization does not depend on the scale of scenes nor the number of things because we utilize the raycasting approach for the integrations. The processing time of the map regularization increases to about 10 seconds at the end of the sequence, but it could be reduced by processing only the voxel blocks near the camera frustum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Run-time Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. APPLICATIONS</head><p>In this section, we demonstrate a promising AR application utilizing a 3D panoptic map generated online by the proposed system. A 3D panoptic map reconstructed as 3D meshes allows us to realize the following visualizations according to the context of the scene:</p><p>• Path planning on stuff regions such as floors and walls. • Interaction with individual objects, or the thing regions. • Interaction appropriate for the semantics of each region. • Natural occlusion and collision visualization. <ref type="figure">Fig. 6</ref>. An example of AR application using a 3D panoptic map generated by PanopticFusion system.</p><p>We show an example of an AR application utilizing the above visualizations in <ref type="figure">Fig. 6</ref>. Humanoids and insect-type robots are able to locomote on the floor and wall meshes, respectively, according to the automatic path planning. Additionally, the semantics of each object realizes contextaware interactions such that humanoids sit and lie on chairs and sofas, respectively, and CG objects appear on tables. Moreover, we can naturally visualize the occlusion effects, which are important for AR, because the 3D meshes of the scene are extracted. Note that, taking advantage of the accurately recognized 3D panoptic map, we can easily estimate the poses of seats of chairs and sofas, and top panels of tables by using simple normal-and curvature-based segmentation and plane detection.</p><p>We believe that our system is useful not only for AR scenarios but also for autonomous robots that explore scenes and manipulate objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we have introduced a novel online volumetric semantic mapping system at the level of stuff and things. It performs dense semantic labeling while discriminating individual objects, as well as large-scale 3D reconstruction and labeled mesh extraction thanks to the use of a spatially hashed volumetric map representation. This was realized by pixel-wise panoptic label prediction and its volumetric integration with careful label tracking. In addition, we constructed a fully connected CRF model with respect to panoptic labels and inferred it online with a novel unary potential approximation and a map division strategy, which further improved the recognition performance. The experimental results showed that our system outperformed or compared well with state-of-the-art offline 3D DNN methods in terms of both 3D semantic and instance segmentation. In future work, we plan to extend our system to ensure global consistency against long-term pose drift, to perform highthroughput mapping by network reduction, and to support dynamic environments.</p><p>We believe that the stuff and things-level semantic mapping will open the way to new applications of intelligent autonomous robotics and context-aware augmented reality that deeply interact with the real world.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>PanopticFusion realizes an online volumetric semantic mapping at the level of stuff and things. The system performs large-scale 3D reconstruction, as well as dense semantic labeling on stuff regions and segmentation of individual things in an online manner, as shown in the top figure. It is also able to restore the class labels of things and yield a colored mesh, as shown in the bottom figures. The results obtained with scene0645 01 of ScanNet v2 are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>System overview of PanopticFusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Results of the map regularization with the map division strategy. The relationship between the maximum number of voxel blocks and (a) recognition accuracy and (b) computational time. Note the computational time is shown in a logarithmic scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results of map regularization. The noisy predictions within red circles are appropriately regularized, taking a spatial context into account.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Semantic mapping systems related to PanopticFusion.</figDesc><table><row><cell>Method</cell><cell>Speed Online</cell><cell>Geometry TSDF Volume Surfels</cell><cell>Large-scale</cell><cell>Model-free</cell><cell>Semantics Dense Labeling Object-level</cell><cell>Map Reg.</cell></row><row><cell>SLAM++ [25]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2.5D is not enough</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table V</head><label>V</label><figDesc></figDesc><table><row><cell>shows computational times for each component</cell></row><row><cell>of our system, which are measured on scene0645 01, a</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>3D semantic segmentation results on ScanNet (v2) 3D semantic label benchmark (hidden test set)<ref type="bibr" target="#b1">[2]</ref>. This table shows IoU (%). Note that the bold and underlined numbers denote first and second ranks, respectively. 78.<ref type="bibr" target="#b5">6</ref> 31.1 36.6 52.4 34.8 30.0 18.9 18.2 50.1 10.2 21.1 34.2 0.2 24.5 15.2 46.0 31.8 20.3 14.5 PointNet++ [22] 33.9 52.3 67.7 25.6 47.8 36.0 34.6 23.2 26.1 25.2 45.8 11.7 25.0 27.8 24.7 21.2 14.5 54.8 36.4 58.4 18.3 SPLATNet [26] 39.3 69.9 92.7 31.1 51.1 65.6 51.0 38.3 19.7 26.7 60.6 0.0 24.5 32.8 40.5 0.1 24.9 59.3 27.1 47.2 22.7 Tangent Conv. [28] 43.8 63.3 91.8 36.9 64.6 64.5 56.2 42.7 27.9 35.2 47.4 14.7 35.3 28.2 25.8 28.3 29.4 61.9 48.7 43.7 29.8 3DMV [5] 48.4 60.2 79.6 42.4 53.8 60.6 50.7 41.3 37.8 53.9 64.3 21.4 31.0 43.3 57.4 53.7 20.8 69.3 47.2 48.4 30.1 TextureNet [10] 56.6 68.0 93.5 49.4 66.4 71.9 63.6 46.4 39.6 56.8 67.1 22.5 44.5 41.1 67.8 41.2 53.5 79.4 56.5 67.2 35.6 SparseConvNet [6] 72.5 86.5 95.5 72.1 82.1 86.9 82.3 62.8 61.4 68.3 84.6 32.5 53.3 60.3 75.4 71.0 87.0 93.4 72.4 64.7 57.2 PanopticFusion (Ours) 52.9 60.2 81.5 38.6 68.8 63.2 64.9 44.2 29.3 56.1 60.4 24.1 22.5 43.4 70.5 49.9 66.9 79.6 50.7 49.1 34.8</figDesc><table><row><cell></cell><cell>avg.</cell><cell>wall floor cab</cell><cell>bed chair sofa tabl door wind bkshf pic</cell><cell>cntr desk curt fridg showr toil</cell><cell>sink bath ofurn</cell></row><row><cell>ScanNet [4]</cell><cell>30.6</cell><cell>43.7</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>3D instance segmentation results on ScanNet (v2) 3D semantic instance benchmark (hidden test set)<ref type="bibr" target="#b1">[2]</ref>. This table shows AP 0.5 , average precision with IoU threshold of 0.5. Note that the bold and underlined numbers denote first and second ranks, respectively.</figDesc><table><row><cell></cell><cell>avg.</cell><cell>cab</cell><cell cols="2">bed chair sofa tabl door wind bkshf pic</cell><cell cols="6">cntr desk curt fridg showr toil</cell><cell>sink bath ofurn</cell></row><row><cell>SGPN [30]</cell><cell>14.3</cell><cell>6.5</cell><cell>39.0 27.5 35.1 16.8 8.7</cell><cell>13.8 16.9 1.4</cell><cell>2.9</cell><cell>0.0</cell><cell>6.9</cell><cell>2.7</cell><cell>0.0</cell><cell cols="2">43.8 11.2 20.8 4.3</cell></row><row><cell>GSPN [35]</cell><cell>30.6</cell><cell cols="3">34.8 40.5 58.9 39.6 27.5 28.3 24.5 31.1 2.8</cell><cell>5.4</cell><cell cols="2">12.6 6.8</cell><cell cols="4">21.9 21.4 82.1 33.1 50.0 29.0</cell></row><row><cell>3D-SIS [9]</cell><cell>38.2</cell><cell cols="3">19.0 43.2 57.7 69.9 27.1 32.0 23.5 24.5 7.5</cell><cell>1.3</cell><cell>3.3</cell><cell cols="5">26.3 42.2 85.7 88.3 11.7 100.0 24.0</cell></row><row><cell>MASC [14]</cell><cell>44.7</cell><cell cols="4">38.2 55.5 63.3 63.9 38.6 36.1 27.6 38.1 32.7 0.2</cell><cell cols="6">26.0 50.9 45.1 57.1 98.0 36.7 52.8 43.2</cell></row><row><cell>PanopticFusion (Ours)</cell><cell>47.8</cell><cell cols="4">25.9 71.2 55.0 59.1 26.7 25.0 35.9 59.5 43.7 0.0</cell><cell cols="6">17.5 61.3 41.1 85.7 94.4 48.5 66.7 43.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>3D panoptic segmentation results on ScanNet (v2) open test set.</figDesc><table><row><cell>method</cell><cell>metric</cell><cell>all</cell><cell cols="2">things stuff</cell><cell>wall floor cab</cell><cell>bed chair sofa tabl door wind bkshf pic</cell><cell>cntr desk curt fridg showr toil</cell><cell>sink bath ofurn</cell></row><row><cell>PanopticFusion w/o CRF</cell><cell>PQ SQ RQ</cell><cell>29.7 71.2 41.1</cell><cell>26.7 71.4 36.8</cell><cell>56.7 69.5 79.6</cell><cell cols="4">37.5 76.0 18.6 29.1 37.8 38.2 29.5 13.8 14.1 13.0 26.5 8.3 62.3 76.7 69.4 68.5 69.3 72.3 70.1 74.6 69.9 70.7 72.9 65.0 60.6 70.5 75.3 75.8 79.2 71.9 74.0 75.3 14.9 11.6 38.0 28.8 72.4 33.3 28.0 24.3 60.2 99.0 26.8 42.5 54.6 52.8 42.1 18.5 20.1 18.4 36.3 12.8 24.6 16.4 50.4 37.9 91.3 46.4 37.8 32.2</cell></row><row><cell>PanopticFusion with CRF</cell><cell>PQ SQ RQ</cell><cell>33.5 73.0 45.3</cell><cell>30.8 73.3 41.3</cell><cell>58.4 70.7 80.9</cell><cell cols="4">40.4 76.4 23.8 35.8 46.7 42.1 34.8 18.0 19.3 16.4 26.4 10.4 16.1 16.6 39.5 36.3 76.1 36.7 31.0 27.7 64.0 77.4 71.1 70.1 74.3 74.6 74.3 76.0 72.5 73.9 71.2 65.1 61.7 72.3 77.7 79.5 81.4 72.7 75.3 75.8 63.1 98.7 33.5 51.1 62.8 56.3 46.9 23.6 26.7 22.2 37.1 16.0 26.0 23.0 50.8 45.7 93.5 50.5 41.2 36.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Run-time analysis.</figDesc><table><row><cell>Frequency</cell><cell>Component</cell><cell>time</cell></row><row><cell cols="2">Every Mask R-CNN frames PSPNet</cell><cell>80 ms</cell></row><row><cell></cell><cell>Mask R-CNN</cell><cell>235 ms</cell></row><row><cell></cell><cell>Panoptic label fusion</cell><cell>2 ms</cell></row><row><cell></cell><cell>Reference panoptic label gen.</cell><cell>19 ms</cell></row><row><cell></cell><cell>Panoptic label tracking</cell><cell>9 ms</cell></row><row><cell></cell><cell>Volumetric integration</cell><cell>139 ms</cell></row><row><cell></cell><cell>Probability integration</cell><cell>∼ 1 ms</cell></row><row><cell>Every 10 sec.</cell><cell>Map regularization</cell><cell>4.5 s</cell></row><row><cell>Every 1 sec.</cell><cell>Mesh extraction</cell><cell>14 ms</cell></row><row><cell>Throughput</cell><cell></cell><cell>4.3 Hz</cell></row><row><cell cols="2">typical large-scale scene in ScanNet v2 (shown in</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We used a publicly available implementation of<ref type="bibr" target="#b0">[1]</ref> and<ref type="bibr" target="#b2">[3]</ref> for PSPNet and Mask R-CNN, respectively.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pspnet-Keras-Tensorflow</surname></persName>
		</author>
		<ptr target="https://github.com/Vladkryvoruchko/PSPNet-Keras-tensorflow" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="http://kaldir.vc.in.tum.de/scannet_benchmark" />
	</analytic>
	<monogr>
		<title level="j">Scannet benchmark challenge</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mask r-cnn for object detection and instance segmentation on keras and tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Abdulla</surname></persName>
		</author>
		<ptr target="https://github.com/matterport/Mask_RCNN" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10409</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dense 3d semantic mapping of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07003</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Texturenet: Consistent local parametrizations for learning from high-resolution signals on meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00020</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Panoptic segmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Masc: Multi-scale affinity with sparse convolution for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04478</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Marching cubes: A high resolution 3d surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harvey</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM siggraph computer graphics</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1987" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="163" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fusion++: Volumetric object-level slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semanticfusion: Dense 3d semantic mapping with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient object-oriented semantic mapping with object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikatsu</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideo</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3206" to="3213" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kinectfusion: Realtime dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Richard A Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symposium on Mixed and Augmented Reality (ISMAR)</title>
		<meeting><address><addrLine>Steve Hodges, and Andrew Fitzgibbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shahram Izadi, and Marc Stamminger. Real-time 3d reconstruction at scale using voxel hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Voxblox: Incremental 3d euclidean signed distance fields for on-board mav planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Oleynikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Fehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Runz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buffier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symposium on Mixed and Augmented Reality (ISMAR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Slam++: Simultaneous localisation and mapping at the level of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Renato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hauke</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meaningful maps with object-oriented semantic mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasir</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When 2.5 d is not enough: Simultaneous reconstruction, segmentation and recognition on dense slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Elasticfusion: Dense slam without a pose graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Da-rnn: Semantic mapping with data associated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mid-fusion: Octreebased object-level multi-instance dynamic slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimos</forename><surname>Tzoumanikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07976</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic 3d occupancy mapping through efficient high order crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03320</idno>
	</analytic>
	<monogr>
		<title level="m">Minhyuk Sung, and Leonidas Guibas</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
