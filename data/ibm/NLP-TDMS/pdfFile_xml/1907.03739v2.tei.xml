<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point-Voxel CNN for Efficient 3D Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Point-Voxel CNN for Efficient 3D Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Point-Voxel CNN (PVCNN) for efficient, fast 3D deep learning. Previous work processes 3D data using either voxel-based or point-based NN models. However, both approaches are computationally inefficient. The computation cost and memory footprints of the voxel-based models grow cubically with the input resolution, making it memory-prohibitive to scale up the resolution. As for pointbased networks, up to 80% of the time is wasted on structuring the sparse data which have rather poor memory locality, not on the actual feature extraction. In this paper, we propose PVCNN that represents the 3D input data in points to reduce the memory consumption, while performing the convolutions in voxels to reduce the irregular, sparse data access and improve the locality. Our PVCNN model is both memory and computation efficient. Evaluated on semantic and part segmentation datasets, it achieves much higher accuracy than the voxel-based baseline with 10× GPU memory reduction; it also outperforms the state-of-the-art point-based models with 7× measured speedup on average. Remarkably, the narrower version of PVCNN achieves 2× speedup over PointNet (an extremely efficient model) on part and scene segmentation benchmarks with much higher accuracy. We validate the general effectiveness of PVCNN on 3D object detection: by replacing the primitives in Frustrum PointNet with PVConv, it outperforms Frustrum PointNet++ by 2.4% mAP on average with 1.5× measured speedup and GPU memory reduction. * indicates equal contributions. The first two authors are listed in the alphabetical order.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D deep learning has received increased attention thanks to its wide applications: e.g., AR/VR and autonomous driving. These applications need to interact with people in real time and therefore require low latency. However, edge devices (such as mobile phones and VR headsets) are tightly constrained by hardware resources and battery. Therefore, it is important to design efficient and fast 3D deep learning models for real-time applications on the edge.</p><p>Collected by the LiDAR sensors, 3D data usually comes in the format of point clouds. Conventionally, researchers rasterize the point cloud into voxel grids and process them using 3D volumetric convolutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref>. With low resolutions, there will be information loss during voxelization: multiple points will be merged together if they lie in the same grid. Therefore, a high-resolution representation is needed to preserve the fine details in the input data. However, the computational cost and memory requirement both increase cubically with voxel resolution. Thus, it is infeasible to train a voxel-based model with high-resolution inputs: e.g., 3D-UNet <ref type="bibr" target="#b50">[51]</ref> requires more than 10 GB of GPU memory on 64×64×64 inputs with batch size of 16, and the large memory footprint makes it rather difficult to scale beyond this resolution.</p><p>Recently, another stream of models attempt to directly process the input point clouds <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. These point-based models require much lower GPU memory than voxel-based models thanks to the sparse representation. However, they neglect the fact that the random memory access is also very inefficient. As the points are scattered over the entire 3D space in an irregular manner, processing Bandwidth (GB/s) (a) Off-chip DRAM accesses take two orders of magnitude more energy than arithmetic operations (640pJ vs. 3pJ <ref type="bibr" target="#b9">[10]</ref>), while the bandwidth is two orders of magnitude less (30GB/s vs. 668GB/s <ref type="bibr" target="#b15">[16]</ref>). Efficient 3D deep learning should reduce the memory footprint, which is the bottleneck of conventional voxel-based methods. Bandwidth (GB/s) (b) Random memory access is inefficient since it cannot take advantage of the DRAM burst and will cause bank conflicts <ref type="bibr" target="#b27">[28]</ref>, while contiguous memory access does not suffer from the above issue. Efficient 3D deep learning should avoid random memory accesses, which is the bottleneck of conventional point-based methods. <ref type="figure">Figure 1</ref>: Efficient 3D models should reduce memory footprint and avoid random memory accesses.</p><p>them introduces random memory accesses. Most point-based models <ref type="bibr" target="#b22">[23]</ref> mimic the 3D volumetric convolution: they extract the feature of each point by aggregating its neighboring features. However, neighbors are not stored contiguously in the point representation; therefore, indexing them requires the costly nearest neighbor search. To trade space for time, previous methods replicate the entire point cloud for each center point in the nearest neighbor search, and the memory cost will then be O(n 2 ), where n is the number of input points. Another overhead is introduced by the dynamic kernel computation. Since the relative positions of neighbors are not fixed, these point-based models have to generate the convolution kernels dynamically based on different offsets.</p><p>Designing efficient 3D neural network models needs to take the hardware into account. Compared with arithmetic operations, memory operations are particularly expensive: they consume two orders of magnitude higher energy, having two orders of magnitude lower bandwidth ( <ref type="figure">Figure 1a</ref>). Another aspect is the memory access pattern: the random access will introduce memory bank conflicts and decrease the throughput <ref type="figure">(Figure 1b</ref>). From the hardware perspective, conventional 3D models are inefficient due to large memory footprint and random memory access.</p><p>This paper provides a novel perspective to overcome these challenges. We propose Point-Voxel CNN (PVCNN) that represents the 3D input data as point clouds to take advantage of the sparsity to reduce the memory footprint, and leverages the voxel-based convolution to obtain the contiguous memory access pattern. Extensive experiments on multiple tasks demonstrate that PVCNN outperforms the voxel-based baseline with 10× lower memory consumption. It also achieves 7× measured speedup on average compared with the state-of-the-art point-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Hardware-Efficient Deep Learning. Extensive attention has been paid to hardware-efficient deep learning for real-world applications. For instance, researchers have proposed to reduce the memory access cost by pruning and quantizing the models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49]</ref> or directly designing the compact models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref>. However, all these approaches are general-purpose and are suitable for arbitrary neural networks. In this paper, we instead design our efficient primitive based on some domain-specific properties: e.g., 3D point clouds are highly sparse and spatially structured.</p><p>Voxel-Based 3D Models. Conventionally, researchers relied on the volumetric representation to process the 3D data <ref type="bibr" target="#b44">[45]</ref>. For instance, Maturana et al. <ref type="bibr" target="#b26">[27]</ref> proposed the vanilla volumetric CNN; Qi et al. <ref type="bibr" target="#b30">[31]</ref> extended 2D CNNs to 3D and systematically analyzed the relationship between 3D CNNs and multi-view CNNs; Wang et al. <ref type="bibr" target="#b39">[40]</ref> incoporated the octree into volumetric CNNs to reduce the memory consumption. Recent studies suggest that the volumetric representation can also be used in 3D shape segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref> and 3D object detection <ref type="bibr" target="#b49">[50]</ref>.</p><p>Point-Based 3D Models. PointNet <ref type="bibr" target="#b29">[30]</ref> takes advantage of the symmetric function to process the unordered point sets in 3D. Later research <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref> proposed to stack PointNets hierarchically to model neighborhood information and increase model capacity. Instead of stacking PointNets as basic blocks, another type of methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46]</ref> abstract away the symmetric function using dynamically generated convolution kernels or learned neighborhood permutation function. Other research, such as SPLATNet <ref type="bibr" target="#b35">[36]</ref> which naturally extends the idea of 2D image SPLAT to 3D, and SONet <ref type="bibr" target="#b21">[22]</ref> which uses the self-organization mechanism with the theoretical guarantee of invariance to point order, also shows great potential in general-purpose 3D modeling with point clouds as input.</p><p>Special-Purpose 3D Models. There are also 3D models tailored for specific tasks. For instance, SegCloud <ref type="bibr" target="#b37">[38]</ref>, SGPN <ref type="bibr" target="#b41">[42]</ref>, SPGraph <ref type="bibr" target="#b18">[19]</ref>, ParamConv <ref type="bibr" target="#b40">[41]</ref>, SSCN <ref type="bibr" target="#b5">[6]</ref> and RSNet <ref type="bibr" target="#b12">[13]</ref> are specialized in 3D semantic/instance segmentation. As for 3D object detection, F-PointNet <ref type="bibr" target="#b28">[29]</ref> is based on the RGB detector and point-based regional proposal networks; PointRCNN <ref type="bibr" target="#b34">[35]</ref> follows the similar idea while abstracting away the RGB detector; PointPillars <ref type="bibr" target="#b19">[20]</ref> and SECOND <ref type="bibr" target="#b46">[47]</ref> focus on the efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation</head><p>3D data can be represented in the format of</p><formula xml:id="formula_0">x = {x k } = {(p k , f k )},</formula><p>where p k is the 3D coordinate of the k th input point or voxel grid, and f k is the feature corresponding to p k . Both voxel-based and point-based convolution can then be formulated as</p><formula xml:id="formula_1">y k = xi∈N (x k ) K(x k , x i ) × F(x i ).<label>(1)</label></formula><p>During the convolution, we iterate the center x k over the entire input. For each center, we first index its neighbors x i in N (x k ), then convolve the neighboring features F(x i ) with the kernel K(x k , x i ), and finally produces the corresponding output y k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Voxel-Based Models: Large Memory Footprint</head><p>Voxel-based representation is regular and has good memory locality. However, it requires very high resolution in order not to lose information. When the resolution is low, multiple points are bucketed into the same voxel grid, and these points will no longer be distinguishable. A point is kept only when it exclusively occupies one voxel grid. In <ref type="figure">Figure 2a</ref>, we analyze the number of distinguishable points and the memory consumption (during training with batch size of 16) with different resolutions. On a single GPU (with 12 GB of memory), the largest affordable resolution is 64, which will lead to 42% of information loss (i.e., non-distinguishable points). To keep more than 90% of the information, we need to double the resolution to 128, consuming 7.2× GPU memory (82.6 GB), which is prohibitive for deployment. Although the GPU memory increases cubically with the resolution, the number of distinguishable points has a diminishing return. Therefore, the voxel-based solution is not scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Point-Based Models: Irregular Memory Access and Dynamic Kernel Overhead</head><p>Point-based 3D modeling methods are memory efficient. The initial attempt, PointNet <ref type="bibr" target="#b29">[30]</ref>, is also computation efficient, but it lacks the local context modeling capability. Later research <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref> improves the expressiveness of PointNet by aggregating the neighborhood information in the point domain. However, this will lead to the irregular memory access pattern and introduce the dynamic kernel computation overhead, which becomes the efficiency bottlenecks.</p><p>Irregular Memory Access. Unlike the voxel-based representation, neighboring points x i ∈ N (x k ) in the point-based representation are not laid out contiguously in memory. Besides, 3D points are scattered in R 3 ; thus, we need to explicitly identify who are in the neighboring set N (x k ), rather than by direct indexing. Point-based methods often define N (x k ) as nearest neighbors in the coordinate space <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b45">46]</ref> or feature space <ref type="bibr" target="#b42">[43]</ref>. Either requires explicit and expensive KNN computation. After KNN, gathering all neighbors x i in N (x k ) requires large amount of random memory accesses, which is not cache friendly. Combining the cost of neighbor indexing and data movement, we summarize in <ref type="figure">Figure 2b</ref> that the point-based models spend 36% <ref type="bibr" target="#b22">[23]</ref>, 52% <ref type="bibr" target="#b42">[43]</ref> and 57% <ref type="bibr" target="#b45">[46]</ref> of the total runtime on structuring the irregular data and random memory access.</p><p>Dynamic Kernel Computation. For the 3D volumetric convolutions, the kernel K(x k , x i ) can be directly indexed as the relative positions of the neighbor x i are fixed for different center x k : e.g., each axis of the coordinate offset p i − p k can only be 0, ±1 for the convolution with size of 3. However, for the point-based convolution, the points are scattered over the entire 3D space irregularly; therefore, the relative positions of neighbors become unpredictable, and we will have to calculate the kernel K(x k , x i ) for each neighbor x i on the fly. For instance, SpiderCNN <ref type="bibr" target="#b45">[46]</ref> leverages the third-order Taylor expansion as a continuous approximation of the kernel K(x k , x i ); PointCNN <ref type="bibr" target="#b22">[23]</ref> permutes the neighboring points into a canonical order with the feature transformer F(x i ). Both will introduce additional matrix multiplications. Empirically, we find that for PointCNN, the overhead of dynamic kernel computation can be more than 50% (see <ref type="figure">Figure 2b</ref>)!</p><p>In summary, the combined overhead of irregular memory access and dynamic kernel computation ranges from 55% (for DGCNN) to 88% (for PointCNN), which indicates that most computations are wasted on dealing with the irregularity of the point-based representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Point-Voxel Convolution</head><p>Based on our analysis on the bottlenecks, we introduce a hardware-efficient primitive for 3D deep learning: Point-Voxel Convolution (PVConv), which combines the advantages of point-based methods (i.e., small memory footprint) and voxel-based methods (i.e., good data locality and regularity).</p><p>Our PVConv disentangles the fine-grained feature transformation and the coarse-grained neighbor aggregation so that each branch can be implemented efficiently and effectively. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, the upper voxel-based branch first transforms the points into low-resolution voxel grids, then it aggregates the neighboring points by the voxel-based convolutions, followed by devoxelization to convert them back to points. Either voxelization or devoxelization requires one scan over all points, making the memory cost low. The lower point-based branch extracts the features for each individual point. As it does not aggregate the neighbor's information, it is able to afford a very high resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Voxel-Based Feature Aggregation</head><p>A key component of convolution is to aggregate the neighboring information to extract local features. We choose to perform this feature aggregation in the volumetric domain due to its regularity.</p><p>Normalization. The scale of different point cloud might be significantly different. We therefore normalize the coordinates {p k } before converting the point cloud into the volumetric domain. First, we translate all points into the local coordinate system with the gravity center as origin. After that, we normalize the points into the unit sphere by dividing all coordinates by max p k 2 , and we then scale and translate the points to [0, 1]. Note that the point features {f k } remain unchanged during the normalization. We denote the normalized coordinates as {p k }. where r denotes the voxel resolution, I[·] is the binary indicator of whether the coordinatep k belongs to the voxel grid (u, v, w), f k,c denotes the c th channel feature corresponding top k , and N u,v,w is the normalization factor (i.e., the number of points that fall in that voxel grid). As the voxel resolution r does not have to be large to be effective in our formulation (which will be justified in Section 5), the voxelized representation will not introduce very large memory footprint.</p><p>Feature Aggregation. After converting the points into voxel grids, we apply a stack of 3D volumetric convolutions to aggregate the features. Similar to conventional 3D models, we apply the batch normalization <ref type="bibr" target="#b14">[15]</ref> and the nonlinear activation function <ref type="bibr" target="#b25">[26]</ref> after each 3D convolution.</p><p>Devoxelization. As we need to fuse the information with the point-based feature transformation branch, we then transform the voxel-based features back to the domain of point cloud. A straightforward implementation of the voxel-to-point mapping is the nearest-neighbor interpolation (i.e., assign the feature of a grid to all points that fall into the grid). However, this will make the points in the same voxel grid always share the same features. Therefore, we instead leverage the trilinear interpolation to transform the voxel grids to points to ensure that the features mapped to each point are distinct.</p><p>As our voxelization and devoxelization are both differentiable, the entire voxel-based feature aggregation branch can then be optimized in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Point-Based Feature Transformation</head><p>The voxel-based feature aggregation branch fuses the neighborhood information in a coarse granularity. However, in order to model finer-grained individual point features, low-resolution voxel-based methods alone might not be enough. To this end, we directly operate on each point to extract individual point features using an MLP. Though simple, the MLP outputs distinct and discriminative features for each point. Such high-resolution individual point information is very critical to supplement the coarse-grained voxel-based information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Feature Fusion</head><p>With both individual point features and aggregated neighborhood information, we can efficiently fuse two branches with an addition as they are providing complementary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussions</head><p>Efficiency: Better Data Locality and Regularity. Our PVConv is more efficient than conventional point-based convolutions due to its better data locality and regularity. Our proposed voxelization and devoxelization both require O(n) random memory accesses, where n is the number of points, since we only need to iterate over all points once to scatter them to their corresponding voxel grids. However, for conventional point-based methods, gathering the neighbors for all points requires at least O(kn) random memory accesses, where k is the number of neighbors. Therefore, our PVCNN is k× more efficient from this viewpoint. As the typical value for k is 32/64 in PointNet++ <ref type="bibr" target="#b31">[32]</ref> and 16 in PointCNN <ref type="bibr" target="#b22">[23]</ref>, we empirically reduce the number of incontiguous memory accesses by 16× to  64× through our design and achieve better data locality. Besides, as our convolutions are done in the voxel domain, which is regular, our PVConv does not require KNN computation and dynamic kernel computation, which are usually quite expensive.</p><p>Effectiveness: Keeping Points in High Resolution. As our point-based feature extraction branch is implemented as MLP, a natural advantage is that we are able to maintain the same number of points throughout the whole network while still having the capability to model neighborhood information. Let us make a comparison between our PVConv and set abstraction (SA) module in PointNet++ <ref type="bibr" target="#b31">[32]</ref>. Suppose we have a batch of 2048 points with 64-channel features (with batch size of 16). We consider to aggregate information from 125 neighbors of each point and transform the aggregated feature to output the features with the same size. The SA module will require 75.2 ms of latency and 3.6 GB of memory consumption, while our PVConv will only require 25.7 ms of latency and 1.0 GB of memory consumption. The SA module will have to downsample to 685 points (i.e., around 3× downsampling) to match up with the latency of our PVConv, while the memory consumption will still be 1.5× higher. Thus, with the same latency, our PVConv is capable of modeling the full point cloud, while the SA module has to downsample the input aggressively, which will inevitably induce information loss. Therefore, our PVCNN is more effective compared to its point-based counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We experimented on multiple 3D tasks including object part segmentation, indoor scene segmentation and 3D object detection. Our PVCNN achieves superior performance on all these tasks with lower measured latency and GPU memory consumption. More details are provided in the appendix.   <ref type="table">Table 3</ref>: Results of more ablation studies.</p><p>(a) Top row: features extracted from coarse-grained voxel-based branch (large, continuous).</p><p>(b) Bottom row: features extracted from fine-grained point-based branch (isolated, discontinuous). <ref type="figure">Figure 6</ref>: Two branches are providing complementary information: the voxel-based branch focuses on the large, continuous parts, while the point-based focuses on the isolated, discontinuous parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Object Part Segmentation</head><p>Setups. We first conduct experiments on the large-scale 3D object dataset, ShapeNet Parts <ref type="bibr" target="#b2">[3]</ref>. For a fair comparison, we follow the same evaluation protocol as in Li et al. <ref type="bibr" target="#b22">[23]</ref> and Graham et al. <ref type="bibr" target="#b5">[6]</ref>. The evaluation metric is mean intersection-over-union (mIoU): we first calculate the part-averaged IoU for each of the 2874 test models and average the values as the final metrics. Besides, we report the measured latency and GPU memory consumption on a single GTX 1080Ti GPU to reflect the efficiency. We ensure the input data to have the same size with 2048 points and batch size of 8.</p><p>Models. We build our PVCNN by replacing the MLP layers in PointNet <ref type="bibr" target="#b29">[30]</ref> with our PVConv layers. We adopt PointNet <ref type="bibr" target="#b29">[30]</ref>, RSNet <ref type="bibr" target="#b12">[13]</ref>, PointNet++ <ref type="bibr" target="#b31">[32]</ref> (with multi-scale grouping), DGCNN <ref type="bibr" target="#b42">[43]</ref>, SpiderCNN <ref type="bibr" target="#b45">[46]</ref> and PointCNN <ref type="bibr" target="#b22">[23]</ref> as our point-based baselines. We reimplement 3D-UNet <ref type="bibr" target="#b50">[51]</ref> as our voxel-based baseline. Note that most baselines make their implementation publicly available, and we therefore collect the statistics from their official implementation. <ref type="table" target="#tab_1">Table 1</ref>, our PVCNN outperforms all previous models. PVCNN directly improves the accuracy of its backbone (PointNet) by 2.5% with even smaller overhead compared with PointNet++. We also design narrower versions of PVCNN by reducing the number of channels to 25% (denoted as 0.25×C) and 50% (denoted as 0.5×C). The resulting model requires only 53.5% latency of PointNet, and it still outperforms several point-based methods with sophisticated neighborhood aggregation including RSNet, PointNet++ and DGCNN, which are almost an order of magnitude slower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. As in</head><p>In <ref type="figure" target="#fig_3">Figure 4, PVCNN</ref>    Furthermore, we also measure the latency of PVCNN on three edge devices. In <ref type="figure">Figure 5</ref>, PVCNN consistently achieves a speedup of 2× over PointNet and PointCNN on different devices. Especially, PVCNN is able to run at 19.9 objects per second on Jetson Nano with PointNet++-level accuracy and 20.2 objects per second on Jetson Xavier with PointCNN-level accuracy.</p><p>Analysis. Conventional voxel-based methods have saturated the performance as the input resolution increases, but the memory consumption grows cubically. PVCNN is much more efficient, and the memory increases sub-linearly ( <ref type="table" target="#tab_3">Table 2</ref>). By increasing the resolution from 16 (0.5×R) to 32 (1×R), the GPU memory usage is increased from 1.55 GB to 1.59 GB, only 1.03×. Even if we squeeze the volumetric resolution to 16 (0.5×R), our method still outperforms 3D-UNet that has much higher voxel resolution (96) by a large margin (1%). PVCNN is very robust even with small resolution in the voxel branch, thanks to the high-resolution point-based branch maintaining the individual point's information. We also compared different implementations of devoxelization in <ref type="table">Table 3</ref>. The trilinear interpolation performs better than the nearest neighbor, which is because the points near the voxel boundaries will introduce larger fluctuations to the gradient, making it harder to optimize.</p><p>Visualization. We illustrate the voxel and point branch features from the final PVConv in <ref type="figure">Figure 6</ref>, where warmer color represents larger magnitude. We can see that the voxel branch captures large, continuous parts (e.g.  Models. Apart from PVCNN (which is based on PointNet), we also extend PointNet++ <ref type="bibr" target="#b31">[32]</ref> with our PVConv to build PVCNN++. We compare our two models with the state-of-the-art point-based models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43]</ref> and the voxel-based baseline <ref type="bibr" target="#b50">[51]</ref>.</p><p>Results. As in <ref type="table" target="#tab_5">Table 4</ref>, PVCNN improves its backbone (PointNet) by more than 13% in mIoU, and it also outperforms DGCNN (which involves sophisticated graph convolutions) by a large margin in both accuracy and latency. Remarkably, our PVCNN++ outperforms the state-of-the-art point-based model (PointCNN) by 1.7% in mIoU with 4× lower latency, and the voxel-based baseline (3D-UNet) by 4% in mIoU with more than 8× lower latency and GPU memory consumption.</p><p>Similar to object part segmentation, we design compact models by reducing the number of channels in PVCNN to 12.5%, 25% and 50% and PVCNN++ to 50%. Remarkably, the narrower version of our PVCNN outperforms DGCNN with 15× measured speedup, and RSNet with 9× measured speedup. Furthermore, it achieves 4% improvement in mIoU upon PointNet while still being 2.5× faster than this extremely efficient model (which does not have any neighborhood aggregation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">3D Object Detection</head><p>Setups. We finally conduct experiments on the driving-oriented dataset, KITTI <ref type="bibr" target="#b4">[5]</ref>. We follow Qi et al. <ref type="bibr" target="#b28">[29]</ref> to construct the val set from the training set so that no instances in the val set belong to the same video clip of any training instance. The size of val set is 3769, leaving the other 3711 samples for training. We evaluate all models for 20 times and report the mean 3D average precision (AP).</p><p>Models. We build two versions of PVCNN based on F-PointNet <ref type="bibr" target="#b28">[29]</ref>: (a) an efficient version where we only replace the MLP layers within the instance segmentation network, and (b) a complete version where we further replace the MLP layers in the box estimation network. We compare our two models with F-PointNet (whose backbone is PointNet) and F-PointNet++ (whose backbone is PointNet++).</p><p>Results. In <ref type="table" target="#tab_7">Table 5</ref>, even if our efficient model does not aggregate neighboring features in the box estimation network while F-PointNet++ does, ours still outperform it in most classes with 1.8× lower latency. Improving the box estimation network with PVConv, our complete model outperforms both baselines in all categories significantly. Compared with F-PointNet baseline, our PVCNN obtains up to 8% mAP improvement in pedestrians and 3.5-6.8% mAP improvement in cyclist, which indicates that our proposed PVCNN is both efficient and expressive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose Point-Voxel CNN (PVCNN) for fast and efficient 3D deep learning. We bring the best of both worlds together: voxels and points, reducing the memory footprint and irregular memory access. We represent the 3D input data efficiently with the sparse, irregular point representation and perform the convolutions efficiently in the dense, regular voxel representation. Extensive experiments on multiple tasks consistently demonstrate the effectiveness and efficiency of our proposed method. We believe that our research will break the stereotype that the voxel-based convolution is naturally inefficient and shed light on co-designing the voxel-based and point-based architectures for fast and efficient 3D deep learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Add 32b SRAM Read 32b DRAM Read Energy (pJ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 (Figure 2 :</head><label>12</label><figDesc>b) Point-based: large memory/computation overheads Both voxel-based and point-based NN models are inefficient. Left: the voxel-based model suffers from large information loss at acceptable GPU memory consumption (model: 3D-UNet [51]; dataset: ShapeNet Part [3]). Right: the point-based model suffers from large irregular memory access and dynamic kernel computation overheads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Voxelization.Figure 3 :</head><label>3</label><figDesc>We transform the normalized point cloud {(p k , f k )} into the voxel grids {V u,v,w } by averaging all features f k whose coordinatep k = (x k ,ŷ k ,ẑ k ) falls into the voxel grid (u, v, w):V u,v,w,c = 1 N u,v,w n k=1 I[floor(x k × r) = u, floor(ŷ k × r) = v, floor(ẑ k × r) = w] × f k,c , (2)Devoxelize Voxelize Convolve Multi-Layer Perceptron (MLP) (a) Voxel-Based Feature Aggregation (Coarse-Grained) (b) Point-Based Feature Transformation (Fine-Grained) Normalize Fuse PVConv is composed of a low-resolution voxel-based branch and a high-resolution pointbased branch. The voxel-based branch extracts coarse-grained neighborhood information, which is supplemented by the fine-grained individual point features extracted from the point-based branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Trade-off: accuracy vs. memory consumption Comparisons between PVCNN and point/voxel-based baselines on ShapeNet Part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Trade-off: accuracy vs. memory consumption Comparisons between PVCNN and point/voxel-based baselines on S3DIS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of object part segmentation on ShapeNet Part. On average, PVCNN outperforms the point-based models with 5.5× measured speedup and 3× memory reduction, and outperforms the voxel-based baseline with 59× measured speedup and 11× memory reduction.</figDesc><table><row><cell></cell><cell>Input Data</cell><cell cols="2">Convolution Mean IoU</cell><cell>Latency</cell><cell>GPU Memory</cell></row><row><cell>PointNet [30]</cell><cell>points (8×2048)</cell><cell>none</cell><cell>83.7</cell><cell>21.7 ms</cell><cell>1.5 GB</cell></row><row><cell>3D-UNet [51]</cell><cell>voxels (8×96 3 )</cell><cell>volumetric</cell><cell>84.6</cell><cell>682.1 ms</cell><cell>8.8 GB</cell></row><row><cell>RSNet [13]</cell><cell>points (8×2048)</cell><cell>point-based</cell><cell>84.9</cell><cell>74.6 ms</cell><cell>0.8 GB</cell></row><row><cell>PointNet++ [32]</cell><cell>points (8×2048)</cell><cell>point-based</cell><cell>85.1</cell><cell>77.9 ms</cell><cell>2.0 GB</cell></row><row><cell>DGCNN [43]</cell><cell>points (8×2048)</cell><cell>point-based</cell><cell>85.1</cell><cell>87.8 ms</cell><cell>2.4 GB</cell></row><row><cell cols="2">PVCNN (Ours, 0.25×C) points (8×2048)</cell><cell>volumetric</cell><cell>85.2</cell><cell>11.6 ms</cell><cell>0.8 GB</cell></row><row><cell>SpiderCNN [46]</cell><cell>points (8×2048)</cell><cell>point-based</cell><cell>85.3</cell><cell>170.7 ms</cell><cell>6.5 GB</cell></row><row><cell>PVCNN (Ours, 0.5×C)</cell><cell>points (8×2048)</cell><cell>volumetric</cell><cell>85.5</cell><cell>21.7 ms</cell><cell>1.0 GB</cell></row><row><cell>PointCNN [23]</cell><cell>points (8×2048)</cell><cell>point-based</cell><cell>86.1</cell><cell>135.8 ms</cell><cell>2.5 GB</cell></row><row><cell>PVCNN (Ours, 1×C)</cell><cell>points (8×2048)</cell><cell>volumetric</cell><cell>86.2</cell><cell>50.7 ms</cell><cell>1.6 GB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of different voxel resolutions.</figDesc><table><row><cell>∆mIoU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results of indoor scene segmentation on S3DIS. On average, our PVCNN and PVCNN++ outperform the point-based models with 8× measured speedup and 3× memory reduction, and outperform the voxel-based baseline with 14× measured speedup and 10× memory reduction.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>table top, lamp head) while the point branch captures isolated, discontinuous details (e.g., table legs, lamp neck). The two branches provide complementary information and can be explained by the fact that the convolution operation extracts features with continuity and locality. Easy Mod. Hard Easy Mod. Hard Easy Mod. Hard F-PointNet [29] 29.1 ms 1.3 GB 83.26 69.28 62.56 65.08 55.85 49.28 74.54 55.95 52.65 F-PointNet++ [29] 105.2 ms 2.0 GB 83.76 70.92 63.65 70.00 61.32 53.59 77.15 56.49 53.37 PVCNN (efficient) 58.9 ms 1.4 GB 84.22 71.11 63.63 69.16 60.28 52.52 78.67 57.79 54.16 PVCNN (complete) 69.6 ms 1.4 GB 84.02 71.54 63.81 73.20 64.71 56.78 81.40 59.97 56.24</figDesc><table><row><cell>Efficiency</cell><cell>Car</cell><cell>Pedestrian</cell><cell>Cyclist</cell></row><row><cell>Latency GPU Mem.</cell><cell></cell><cell></cell><cell></cell></row></table><note>5.2 Indoor Scene Segmentation Setups. We conduct experiments on the large-scale indoor scene segmentation dataset, S3DIS [1,2]. We follow Tchapmi et al. [38] and Li et al. [23] to train the models on area 1,2,3,4,6 and test them on</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results of 3D object detection on the val set of KITTI. The complete PVCNN outperforms F-PointNet++ in all categories significantly with 1.5× measured speedup and memory reduction. area 5 since it is the only area that does not overlap with any other area. Both data processing and evaluation protocol are the same as PointCNN<ref type="bibr" target="#b22">[23]</ref> for fair comparison. We measure the latency and memory consumption with 32768 points per batch at test time on a single GTX 1080Ti GPU.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank MIT Quest for Intelligence, MIT-IBM Watson AI Lab, Samsung, Facebook and SONY for supporting this research. We thank AWS Machine Learning Research Awards for providing the computation resource. We thank NVIDIA for donating Jetson AGX Xavier.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandar</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<title level="m">Joint 2D-3D-Semantic Data for Indoor Scene Understanding. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3D Semantic Parsing of Large-Scale Indoor Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Christopher Bongsoo Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Vision meets Robotics: The KITTI Dataset. IJRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D Semantic Segmentation With Submanifold Sparse Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning both Weights and Connections for Efficient Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AMC: AutoML for Model Compression and Acceleration on Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computing&apos;s Energy Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISSCC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">Searching for MobileNetV3. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent Slice Networks for 3D Segmentation on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<title level="m">SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and &lt; 0.5MB Model Size. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">In-Datacenter Performance Analysis of a Tensor Processing Unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-Scale Point Cloud Semantic Segmentation With Superpoint Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PointPillars: Fast Encoders for Object Detection from Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PointGrid: A Deep Network for 3D Shape Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SO-Net: Self-Organizing Network for Point Cloud Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution on X -Transformed Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fixed Point Quantization of Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darryl</forename><forename type="middle">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><forename type="middle">S</forename><surname>Talathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sreekanth Annapureddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rectifier Nonlinearities Improve Neural Network Acoustic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<idno>php?media=wiki:lectures:onur-740-fall11-lecture25-mainmemory.pdf. 2</idno>
		<ptr target="https://www.archive.ece.cmu.edu/~ece740/f11/lib/exe/fetch" />
		<title level="m">DDR Access Illustration</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frustum PointNets for 3D Object Detection from RGB-D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Volumetric and Multi-View CNNs for Object Classification on 3D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">OctNet: Learning Deep 3D Representations at High Resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse Lattice Networks for Point Cloud Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Octree Generating Networks: Efficient Convolutional Architectures for High-Resolution 3D Outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexley</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SEGCloud: Semantic Segmentation of 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">HAQ: Hardware-Aware Automated Quantization with Mixed Precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>O-Cnn</surname></persName>
		</author>
		<title level="m">Octree-based Convolutional Neural Networks for 3D Shape Analysis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>SIGGRAPH</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep Parametric Continuous Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic Graph CNN for Learning on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">VoxSegNet: Volumetric CNNs for Semantic Part Segmentation of 3D Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongji</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A Deep Representation for Volumetric Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SECOND: Sparsely Embedded Convolutional Detection. Sensors</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Özgün</forename><surname>Çiçek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soeren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
