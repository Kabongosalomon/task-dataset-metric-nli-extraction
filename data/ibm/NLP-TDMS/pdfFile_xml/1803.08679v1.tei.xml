<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<email>wmzuo@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>csdzhang@comp.polyu.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<email>mhyang@ucmerced.edu</email>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Discriminative Correlation Filters (DCF) are efficient in visual tracking but suffer from unwanted boundary effects. Spatially Regularized DCF (SRDCF) has been suggested to resolve this issue by enforcing spatial penalty on DCF coefficients, which, inevitably, improves the tracking performance at the price of increasing complexity. To tackle online updating, SRDCF formulates its model on multiple training images, further adding difficulties in improving efficiency. In this work, by introducing temporal regularization to SRDCF with single sample, we present our spatial-temporal regularized correlation filters (STRCF). The STRCF formulation can not only serve as a reasonable approximation to SRDCF with multiple training samples, but also provide a more robust appearance model than SRDCF in the case of large appearance variations. Besides, it can be efficiently solved via the alternating direction method of multipliers (ADMM). By incorporating both temporal and spatial regularization, our STRCF can handle boundary effects without much loss in efficiency and achieve superior performance over SRDCF in terms of accuracy and speed. Compared with SRDCF, STRCF with handcrafted features provides a 5× speedup and achieves a gain of 5.4% and 3.6% AUC score on OTB-2015 and Temple-Color, respectively. Moreover, STRCF with deep features also performs favorably against state-of-the-art trackers and achieves an AUC score of 68.3% on OTB-2015.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed the rapid advances of discriminative correlation filters (DCFs) in visual tracking. <ref type="bibr">Benefited</ref>  (b) <ref type="figure">Figure 1</ref>: (a) The results of STRCF and SRDCF <ref type="bibr" target="#b12">[13]</ref> on two sequences with occlusion and deformation. (b) A comparison of SRDCF variants and STRCF using HOG feature in terms of mean OP (%) and speed (FPS) on OTB-2015 and Temple-Color. The best three results are shown in red, blue and green fonts, respectively.</p><p>the DCF can be learned very efficiently in the frequency domain via fast Fourier transform (FFT). For example, the tracking speed of the earliest DCF-based tracker, i.e., MOSSE <ref type="bibr" target="#b3">[4]</ref>, can reach 700 frames per second (FPS). Along with the introduction of feature representation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref>, nonlinear kernel <ref type="bibr" target="#b18">[19]</ref>, scale estimation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, max-margin classifiers <ref type="bibr" target="#b42">[43]</ref>, spatial regularization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>, and continuous convolution <ref type="bibr" target="#b14">[15]</ref>, DCF-based trackers have been greatly improved and significantly advanced the state-ofthe-art tracking accuracy. However, such performance im-provement is not obtained without any extra cost. Most topranked trackers, e.g., SRDCF <ref type="bibr" target="#b12">[13]</ref> and C-COT <ref type="bibr" target="#b14">[15]</ref>, have gradually lost the characteristic speed and realtime capability of early DCF-based trackers. For example, the speed of SRDCF <ref type="bibr" target="#b12">[13]</ref> using the hand-crafted HOG feature is ∼6 FPS, while that of the baseline KCF <ref type="bibr" target="#b18">[19]</ref> is ∼170 FPS. For better understanding on this issue, we dissect the tradeoff between accuracy and speed in SRDCF. In general, the inefficiency of SRDCF can be attributed to three factors: (i) scale estimation, (ii) spatial regularization, and (iii) formulation on large training set. <ref type="figure">Fig. 1b</ref> lists the tracking speed and accuracy of SRDCF and its variants on two popular benchmarks, including SRDCF(−M) (i.e., removing (iii)), SRDCF(−MS) (i.e., removing (ii)&amp;(iii)), and KCF (i.e., removing (i)&amp;(ii)&amp;(iii)). We note that when removing (iii), linear interpolation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> is adopted as an alternative strategy for online model updating. From <ref type="figure">Fig. 1b</ref>, it can be seen that the tracker still maintains its real-time ability (∼ 33F P S) when adding scale estimation. But the tracking speed decreases significantly with the further introduction of spatial regularization and formulation on large training set. Therefore, it is valuable to develop a solution for taking use of (ii) and (iii) without much loss in efficiency.</p><p>In this paper, we study the solution for taking the benefit of spatial regularization and formulation on large training set without much loss in efficiency. On the one hand, the high complexity of SRDCF mainly comes at the formulation on multiple training images. By removing the constraint, SRDCF with single image can be efficiently solved via ADMM. Due to the convexity of SRDCF, the ADMM can also guarantee to converge to global optimum. On the other hand, in SRDCF spatial regularization is integrated into the formulation on multiple training images for the coupling of DCF learning and model updating, which does benefit the tracking accuracy. Motivated by online Passive-Aggressive (PA) learning <ref type="bibr" target="#b5">[6]</ref>, we introduce a temporal regularization to SRDCF with single image, resulting in our spatial-temporal regularized correlation filters (STRCF). STRCF is a rational approximation of the full SRDCF formulation on multiple training images, and can also be exploited for simultaneous DCF learning and model updating. Besides, the ADMM algorithm can also be directly used to solve STRCF. Thus, our STRCF incorporates both spatial and temporal regularization into DCF, and can be adopted to speed up SRDCF.</p><p>Furthermore, as an extension of online PA algorithm <ref type="bibr" target="#b5">[6]</ref>, STRCF can also provide a more robust appearance model than SRDCF in the case of significant appearance variations. <ref type="figure">Fig. 1a</ref> illustrates the tracking results on two sequences with occlusion and deformation. Compared with SRDCF, we can see that, with the introduction of the temporal regularization, STRCF performs more robustly to occlusion while adapting well to large appearance variation. From <ref type="figure">Fig. 1b</ref>, STRCF not only runs at real-time tracking speed (∼ 30F P S), but also leads to +5.7% performance gain over SRDCF by average mean OP on two datasets. To sum up, STRCF can achieve remarkable improvements over the baseline SRDCF on all the datasets, and runs at more than 5× faster tracking speed.</p><p>We perform comparative experiments on several benchmarks, including OTB-2015 <ref type="bibr" target="#b39">[40]</ref>, Temple-Color <ref type="bibr" target="#b24">[25]</ref>, and VOT-2016 <ref type="bibr" target="#b21">[22]</ref>. STRCF performs favorably in terms of accuracy, robustness and speed in comparison with the stateof-the-art CF-based and CNN trackers.</p><p>The contributions of this paper are as follows:</p><p>• A STRCF model is presented by incorporating both spatial and temporal regularization into the DCF framework. Based on online PA, STRCF can not only serve as a rational approximation of the SRDCF formulation on multiple training images, but also provide a more robust appearance model than SRDCF in the case of large appearance variations.</p><p>• An ADMM algorithm is developed for solving STRCF efficiently, where each sub-problem has the closedform solution. And our algorithm can empirically converge within very few iterations.</p><p>• Our STRCF with hand-crafted feature can run in realtime, achieves notable improvements over SRDCF by tracking accuracy. Furthermore, our STRCF with deep features performs favorably in comparison with the state-of-the-art trackers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section first provides a brief survey on DCF trackers and then focuses on spatial regularization and formulation on large training set that are most relevant to our STRCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Discriminative Correlation Filters</head><p>Using DCFs for adaptive tracking starts with MOSSE <ref type="bibr" target="#b3">[4]</ref>, which learns the CFs with few samples in the frequency domain. Notable improvements have been made to this popular tracker to address several limiting issues. For example, Henriques et al. <ref type="bibr" target="#b18">[19]</ref> learn the kernelized CFs (KCF) via kernel trick. The multi-channel version of MOSSE is also studied in <ref type="bibr" target="#b20">[21]</ref>. And more discriminative features are widely used, such as HOG <ref type="bibr" target="#b7">[8]</ref>, color names (CN) <ref type="bibr" target="#b13">[14]</ref> and deep CNN features <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>. To cope with the size change and occlusion, several scale-adaptive <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> and partbased trackers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> are further investigated. Besides, long-term tracking <ref type="bibr" target="#b28">[29]</ref>, continuous convolution <ref type="bibr" target="#b14">[15]</ref> and particle filter based methods <ref type="bibr" target="#b41">[42]</ref> are also developed to improve the tracking accuracy and robustness. Due to the space limitation, here we only review the methods from spatial regularization and formulation on large training set that are close to our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Spatial Regularization</head><p>The circulant shifted samples in DCF-based trackers always suffer from periodic repetitions on boundary positions, thereby significantly degrading the tracking performance. Several spatial regularization methods have been suggested to alleviate the unwanted boundary effects. Galoogahi et al. <ref type="bibr" target="#b17">[18]</ref> pre-multiply the image patches with a fixed masking matrix containing the target regions, and then solve the constrained optimization problem via ADMM. However, their method can only be applied to single channel DCFs. Danelljan et al. <ref type="bibr" target="#b12">[13]</ref> propose a spatial regularization term to penalize the DCF coefficients depending on their spatial locations and suggest the Gauss-Seidel algorithm to solve the resulting normal equations. The work <ref type="bibr" target="#b6">[7]</ref> also employs a similar spatial regularization term, but the spatial regularization matrix is predicted with a multidirectional RNN for identifying the reliable components. These two methods, however, are unable to exploit the circulant structure in learning, resulting in higher computational cost. More recently, Galoogahi et al. <ref type="bibr" target="#b16">[17]</ref> extend <ref type="bibr" target="#b17">[18]</ref> to multiple channels and further speed up the tracker towards real-time. Compared with these methods, our STRCF has several merits: (1) while STRCF serves as an approximation to <ref type="bibr" target="#b12">[13]</ref> on multiple training samples, it can be solved more efficiently with the proposed ADMM algorithm. <ref type="bibr" target="#b1">(2)</ref> with the introduction of the temporal regularization, STRCF can learn a more robust appearance model than <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>, thereby leading to superior tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Formulation on large training set</head><p>One of the most critical challenges in visual tracking is to learn and maintain a robust and fast appearance model in the case of large appearance variations. To this end, MOSSE <ref type="bibr" target="#b3">[4]</ref> implements simultaneous DCF learning and model updating by learning the CFs with multiple training samples from historical tracking results. Similar strategy of incorporating large training set into the formulation can also be found in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref>. In practice, robust CFs can be learned by taking the samples at different time instances into consideration. However, this leads to superior performance at the price of higher computational burden. In comparison with these methods, KCF <ref type="bibr" target="#b18">[19]</ref> and its variants <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref> decouple the DCF learning and model updating, and further exploit the circulant structure for high efficiency. As a result, KCF with HOG feature can run at more than 150 FPS on a single CPU. Following this work, there also exist several heuristic methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> to address the naive model updating issues. These methods, however, obtain inferior performance than DCF-based trackers with large training set. Compared with these trackers, STRCF can not only be solved efficiently by avoiding the deployment of large training set, but also benefit from simultaneous DCF learning and model updating by introducing the temporal regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatially Regularized DCF</head><p>In this section, we first revisit the SRDCF tracker, and then present our STRCF model motivated by online PA. Finally, an ADMM is developed to solve the STRCF model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisit SRDCF</head><formula xml:id="formula_0">Denote by D = {(x k y k )} T k=1 a training set of multiple images. Each sample x k = [x 1 k , ..., x D k ] consists of D fea- ture maps with size of M × N .</formula><p>And y k is the predefined Gaussian shaped labels. The SRDCF <ref type="bibr" target="#b12">[13]</ref> is formulated by minimizing the following objective,</p><formula xml:id="formula_1">arg min f T k=1 α k D d=1 x d k * f d − y k 2 + D d=1 w · f d 2 ,<label>(1)</label></formula><p>where · denotes the Hadamard product, * stands for the convolution operator, w and f are the spatial regularization matrix and correlation filter, respectively. α k indicates the weight to each sample x k and is set to emphasize more to the recent samples. In <ref type="bibr" target="#b12">[13]</ref>, Danelljan et al. employ the Gauss-Seidel method to iteratively update the filters f . Please refer to <ref type="bibr" target="#b12">[13]</ref> for more implementation details. However, although SRDCF is effective in suppressing the adverse boundary effects, it also increases the computational burden due to the following two reasons:</p><p>(i) The failure of exploiting circulant matrix structure. For the sake of learning a robust correlation filter f , DCF trackers incorporate several historical samples {(x k , y k )} T k=1 for training <ref type="bibr" target="#b10">[11]</ref>. However, unlike other CFbased trackers learned with only the sample from the current frame, the formulation on multiple images breaks the circulant matrix structure, resulting in high computation burden. As for SRDCF, the optimization becomes even more difficult due to the spatial regularization term.</p><p>(ii) The large linear equations and Gauss-Seidel solver. Eqn. (1) results in a DM N × DM N large sparse linear equation system. While the Gauss-Seidel method is suggested to solve Eqn. (1) using the property of sparse matrix, it still remains high computational complexity. In addition, the SRDCF tracker also needs a long start-up time to learn the discriminative correlation filters in the first frame due to the low convergence speed of Gauss-Seidel method.</p><p>Both the spatial regularization and formulation on multiple images will break the circulant matrix structure. Fortunately, these two issues can be circumvented to improve the tracking speed. The formulation on multiple images can be relaxed to a STRCF model on single image by introducing the temporal regularization. Furthermore, the introduction of spatial regularization can be addressed by exploiting an equivalent reformulation solved by ADMM efficiently.  </p><formula xml:id="formula_2">1 1 1 arg min + i D D d d d i k k k d d D ¦ ¦ ¦ f f x fy w f +1 i f 1 f 2 2 2 1 1 1 arg min D D d d d t t t d d P ¦ ¦ f f xf y w f ff ... ... 2 1 1 arg min i D D d i k d 1 d 1 2 d d k k D f f i w f + 2 + d d k 2 + d 2 2 2 1 1 1 arg min D D d d d i i i d d P ¦ ¦ f f xf y w f ff 2 2 1 1 1 arg min + t D D d d d t k k k d d D ¦ ¦ ¦ f f x fy w f 1 t f 1 i f 2 1 1 arg min t D D d t k d 1 d 1 2 d d k k D f f t w f + 2 + d d k 2 + d 2 2 2 2 2 1 1 arg min D D 2 d t t d d 1 1 1 d d t f f t w f f f d t P d P d d t d ... ... ... ... ... ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">STRCF</head><p>In online classification, when a new instance comes on each round, the algorithm first predicts its label, and then updates the classifier based on the newly instance-label pair. On the one hand, the learning algorithm should be passive to make the updated classifier similar to the previous one. On the other hand, the learning algorithm should be aggressive to guarantee the new instance be corrected classified. Thus, Crammer et al. <ref type="bibr" target="#b5">[6]</ref> suggest an online passiveaggressive (PA) algorithm by introducing a temporal regularization, and derive the bound on the cumulative loss of PA w.r.t. the best fixed predictor.</p><p>Motivated by PA, we introduce a temporal regularization term f − f t−1 2 , resulting in our spatial-temporal regularized CF (STRCF) model,</p><formula xml:id="formula_3">arg min f 1 2 D d=1 x d t * f d −y 2 + 1 2 D d=1 w · f d 2 + µ 2 f −ft−1 2 ,<label>(2)</label></formula><p>where f t−1 denotes the CFs utilized in the (t − 1)-th frame, and µ denotes the regularization parameter. Here, D d=1 w · f d 2 denotes the spatial regularizer, and f − f t−1 2 denotes the temporal regularizer. STRCF can also be treated as an extension of online PA from two aspects: (i) Instead of classification, STRCF is an online learning of linear regression; (ii) Instead of instancewise updating, the samples in STRCF come at the batch level (i.e. all the shift versions of an image) on each round. Therefore, STRCF naturally inherits the merits of online PA on adaptively balancing the tradeoff between aggressive and passive model learning, thus leading to more robust models in the case of large appearance variations. In <ref type="figure" target="#fig_1">Fig. 2</ref>, we compare STRCF with SRDCF on sequence Lemming to highlight their relationships on CF model learning. From it we can make the following observations: (i) Similar to SRDCF, STRCF also implements simultaneous DCF learn-ing and model updating with the introduction of temporal regularizer, thus can serve as a rational approximation of SRDCF with multiple training samples; (ii) In the case of occlusion, while SRDCF suffers from over-fitting to recent corrupted samples, STRCF can alleviate this by passively updating the CFs to keep it close to the previous ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization algorithm</head><p>The model in Eqn. <ref type="formula" target="#formula_3">(2)</ref> is convex, and can be minimized to obtain the globally optimal solution via ADMM. To this end, we first introduce an auxiliary variable g by requiring f = g and the stepsize parameter γ, then the Augmented Lagrangian form of Eqn. (2) can be formulated as</p><formula xml:id="formula_4">L(w, g, s) = 1 2 D d=1 x d t * f d −y 2 + 1 2 D d=1 w·g d 2 (3) + D d=1 (f d −g d ) T s d + γ 2 D d=1 f d −g d 2 + µ 2 f−ft−1 2 ,</formula><p>where s, µ are the Lagrange multiplier and penalty factor, respectively. By introducing h = 1 γ s, Eqn.</p><p>(3) can be reformulated as</p><formula xml:id="formula_5">L(w, g, h) = 1 2 D d=1 x d t * f d −y 2 + 1 2 D d=1 w·g d 2<label>(4)</label></formula><formula xml:id="formula_6">+ γ 2 D d=1 f d −g d +h d 2 + µ 2 f − ft−1 2 .</formula><p>The ADMM algorithm is then adopted by alternatingly solving the following subproblems,</p><formula xml:id="formula_7">           f (i+1) =argmin f D d=1 x d t * f d −y 2 +γ f −g+h 2 +µ f−ft−1 2 g (i+1) = arg min g D d=1 w · g d 2 + γ f − g + h 2 h (i+1) = h (i) + f (i+1) − g (i+1) .<label>(5)</label></formula><p>We detail the solution to each subproblem as follows: Subproblem f : Using the Parseval's theorem, the first row of Eqn. <ref type="bibr" target="#b4">(5)</ref> can be rewritten in the Fourier domain as </p><formula xml:id="formula_8">argmin V j (f ) Vj(xt) Vj(f )−ŷj 2 +µ Vj(f )−Vj(ft−1) 2 (7) +γ Vj(f )−Vj(ĝ)+Vj(ĥ) 2 .</formula><p>Taking the derivative of Eqn. <ref type="bibr" target="#b6">(7)</ref> be zero, we can get the closed-form solution for V j (f ),</p><formula xml:id="formula_9">Vj(f ) = (Vj(xt)Vj(xt) + (µ + γ)I) −1 q,<label>(8)</label></formula><p>where the vector q takes the form as</p><formula xml:id="formula_10">q = V j (x t )ŷ j + γV j (ĝ) − γV j (ĥ) + µV j (f t−1 ). Since V j (x t )V j (x t ) is rank-1 matrix, Eqn. (8)</formula><p>can be solved with the Sherman-Morrsion formula <ref type="bibr" target="#b32">[33]</ref>, and we have</p><formula xml:id="formula_11">Vj(f ) = 1 µ + γ (I − Vj(xt)Vj(xt) µ + γ + Vj(xt) Vj(xt) )q.<label>(9)</label></formula><p>Note that Eqn. (9) only contains vector multiply-add operation and thus can be computed efficiently. f can be further obtained by the inverse DFT off . Subproblem g: From the second sub-equation of Eqn. (5), each element of g can be computed independently, and thus the closed-form solution of g can be computed by,</p><formula xml:id="formula_12">g = (W W + γI) −1 (γf + γh).<label>(10)</label></formula><p>where W represents the DM N × DM N diagonal matrix concatenated with D diagonal matrices Diag(w).</p><p>Updating stepsize parameter γ: The stepsize parameter γ is updated as in Eqn. <ref type="bibr" target="#b10">(11)</ref>,</p><formula xml:id="formula_13">γ (i+1) = min(γ max , ργ (i) ),<label>(11)</label></formula><p>where γ max denotes the maximum value of γ and the scale factor ρ. 1 please refer to <ref type="bibr" target="#b42">[43]</ref> for more details.</p><p>Taking the DFT and inverse DFT into account, the complexity of solving f is O(DM N log(M N )). And the computational cost for g is O(DM N ). Hence, the overall cost of our algorithm is O(DM N log(M N )N I ), where N I represents the maximum number of iterations. In addition, compared with SRDCF, our ADMM algorithm does not need a start-up time to initialize the CFs in the first frame.</p><p>Convergence. Note that the STRCF model is convex, and each sub-problem in ADMM algorithm has closed-form solution. Therefore, it satisfies the Eckstein-Bertsekas condition <ref type="bibr" target="#b15">[16]</ref>, and is guaranteed to converge to global optimum. In addition, We empirically find that the proposed ADMM can converge within 2 iterations on most of the sequences, and thus N I is set to 2 for efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we first compare our STRCF with the state-of-the-art trackers in terms of both hand-crafted and CNN features on the OTB-2015 dataset. Then, we analyze the impacts of the temporal regularization and hyperparameter µ on tracking performance using OTB-2015. Finally, we conduct comparative experiments on Temple-Color and VOT-2016 benchmarks.</p><p>Following the settings in SRDCF <ref type="bibr" target="#b12">[13]</ref>, we crop the square region centered at the target, in which the side length of the region is √ 5W H (W and H represent the width and height of the target, respectively). Then we extract HOG, CN <ref type="bibr" target="#b13">[14]</ref> and CNN features for the image region. The features are further weighted by a cosine window to reduce the boundary discontinuities. As for the ADMM algorithm, we set the hyper-parameter in Eqn. (2) to µ = 16 throughout all the experiments. The initial stepsize parameter γ (0) , the maximum value γ max and scale factor ρ are set to 10, 100 and 1.2, respectively. Our STRCF is implemented with Matlab 2017a and all the experiments are run on a PC equipped with Intel i7 7700 CPU, 32GB RAM and a single NVIDIA GTX 1070 GPU. The source code of our tracker is publicly available at https://github.com/ lifeng9472/STRCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The OTB-2015 benchmark</head><p>The OTB-2015 benchmark <ref type="bibr" target="#b39">[40]</ref> is a popular tracking dataset which consists of 100 fully annotated video sequences with 11 different attributes, such as abrupt motion, illumination variation, scale variation and motion blurring. We evaluate the trackers based on the One Pass Evaluation (OPE) protocol provided in <ref type="bibr" target="#b39">[40]</ref>, where overlap precision (OP) metric is employed by calculating the bounding box overlaps exceeding 0.5 in a sequence. Besides, we also provide the overlap success plots containing the OP metric over a range of thresholds.</p><p>We compare STRCF with 20 state-of-the-art trackers, including trackers using hand-crafted features (i.e. SRDCF SRDCF <ref type="bibr" target="#b12">[13]</ref> BACF <ref type="bibr" target="#b19">[20]</ref>    [13], BACF <ref type="bibr" target="#b19">[20]</ref>, ECO-HC <ref type="bibr" target="#b8">[9]</ref>, SRDCFDecon <ref type="bibr" target="#b9">[10]</ref>, Staple <ref type="bibr" target="#b0">[1]</ref>, Staple+CA <ref type="bibr" target="#b29">[30]</ref>, SAMF+AT <ref type="bibr" target="#b2">[3]</ref>, DSST <ref type="bibr" target="#b10">[11]</ref>, SAMF <ref type="bibr" target="#b23">[24]</ref>, MEEM <ref type="bibr" target="#b40">[41]</ref> and KCF <ref type="bibr" target="#b18">[19]</ref>) and using CNN features (i.e. ECO <ref type="bibr" target="#b8">[9]</ref>, DeepSRDCF <ref type="bibr" target="#b11">[12]</ref>, HCF <ref type="bibr" target="#b27">[28]</ref>, HDT <ref type="bibr" target="#b33">[34]</ref>, C-COT <ref type="bibr" target="#b14">[15]</ref>, FCNT <ref type="bibr" target="#b36">[37]</ref>, SiameseFC <ref type="bibr" target="#b1">[2]</ref>, CF-Net <ref type="bibr" target="#b1">[2]</ref> and MSDAT <ref type="bibr" target="#b38">[39]</ref>). Note that we employ the publicly available codes or results provided by the authors for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Comparison with hand-crafted based trackers</head><p>We compare the proposed STRCF with other state-of-theart trackers using hand-crafted features. <ref type="table" target="#tab_2">Table 1</ref> gives the results of the mean OP and FPS on OTB-2015. As shown in <ref type="table" target="#tab_2">Table 1</ref>, STRCF performs significantly better than most of the competing trackers except ECO-HC and surpasses its counterpart SRDCF by 6.9%. We owe these significant improvements to the introduction of the temporal regularization. STRCF is also superior to the SRDCFDecon tracker which follows the SRDCF work and addresses the corrupted sample problem by re-weighting the samples in the training set. It indicates that the introduction of temporal regularization is more helpful than multiple samples training with explicit sample re-weighting. Besides, our method also outperforms the recent CF-based trackers: BACF <ref type="bibr" target="#b16">[17]</ref>, SAMF+AT <ref type="bibr" target="#b2">[3]</ref> and Staple+CA <ref type="bibr" target="#b29">[30]</ref>. Overall, the only tracker performing comparably with STRCF on OTB-2015 is the ECO-HC <ref type="bibr" target="#b8">[9]</ref>. It is worth noting that ECO-HC adopts the Gaussian Mixture Model (GMM)-based generative sample space method to reduce the number of samples for training, and employs continuous convolution and factorized convolution for boosting the performance. In contrast, even our STRCF does not consider continuous convolution and factorized convolution techniques, it still yields favorable performance against the competing trackers.</p><p>In addition, we also report the tracking speed (FPS) comparison on OTB-2015 dataset in <ref type="table" target="#tab_2">Table 1</ref>. One can see that STRCF (HOGCN) runs at 24.3 FPS and is nearly 4.2× than its counterpart SRDCF (5.8 FPS), validating the high efficiency of the proposed ADMM over the SRDCF solver (i.e. the Gauss-Seidel algorithm). STRCF (HOG) using HOG feature performs even faster and obtains a real-time speed of 31.5 FPS, which is 1.2× faster than recent BACF tracker. Next, we provide the overlap success curves of the competing trackers with the hand-crafted features on OTB-2015 dataset, which is ranked using the Area-Under-the-Curve (AUC) score. As shown in <ref type="figure" target="#fig_4">Fig. 3a</ref>, our STRCF achieves an AUC score of 65.1% and ranks the second best performance among all the trackers. Similar to the mean OP results, STRCF also outperforms its counterparts SRDCF and SRDCFDecon by a gain of 5.4% and 2.3%, respectively.</p><p>Finally, we perform qualitative evaluation of different trackers on several video sequences. For clearer visualization, we show the results of STRCF and 4 state-of-theart trackers based on hand-crafted features, including ECO-HC <ref type="bibr" target="#b8">[9]</ref>, BACF <ref type="bibr" target="#b19">[20]</ref>, SRDCF <ref type="bibr" target="#b12">[13]</ref> and SRDCFDecon <ref type="bibr" target="#b9">[10]</ref>. The tracking results on on 6 video sequences are shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. One can note that, with the introduction of temporal regularization, the proposed STRCF performs favorably against the state-of-the-art hand-crafted trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Video Attribute Based Comparison</head><p>In this section, we perform quantitative analysis of the total 11 video attributes on the OTB-2015 dataset. Our STRCF outperforms most of the competing trackers except ECO-HC on all the attributes. Due to the page limits, here we only provide the overlap success plots of 4 attributes in <ref type="figure">Fig. 5</ref> and the remaining results can be found in the supplementary material. In the case of out of view (OV) and occlusion (OCC), the target always encounters with partial or fully disappearance from the camera, which leads to an adverse impact on model updating. Trackers using multiple samples training with naive sample weighting strategy (i.e. SRDCF) or linear interpolation updating (i.e. Staple and SAMF+AT) suffer from significant degradation because of over-fitting to the recent samples. Benefited from the temporal regularization, our STRCF can adaptively make the balance between updating the CFs with the latest samples and keeping close to the previously learned CFs, and thus is robust to such kinds of variations. In particular, STRCF achieves remarkable improvements over its baseline SRDCF, i.e., 14.5% and 5.7% gains on these two attributes, respectively. And it also outperforms the SRDCFDecon tracker by an AUC score of 9.3% and 2.1% on these two attributes. As for the In-plane/Out-of-plane rotation attributes, STRCF also per-forms better than most of the trackers and is superior to the baseline SRDCF by 5.8% and 7.6%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparison with deep feature-based trackers</head><p>To further assess STRCF, we follow the settings in C-COT <ref type="bibr" target="#b14">[15]</ref>, and combine the outputs of conv3 layer from VGG-M network <ref type="bibr" target="#b34">[35]</ref> with HOGCN features for STRCF training (we name it as DeepSTRCF for simplicity). Using mean OP and speed as performance metrics, <ref type="table" target="#tab_3">Table 2</ref> compares Deep-STRCF with the state-of-the-art trackers based on deep features on OTB-2015. One can see that DeepSTRCF achieves a mean OP of 84.2% and performs much better than the SRDCF with CNN features (i.e. DeepSRDCF) by a gain of 7.4%, demonstrating the effectiveness of the temporal regularization. It even outperforms than C-COT with both spatial regularization and continuous convolution by a gain of  1.2% on OTB-2015. In terms of the tracking speed, the best performance belongs to SiameseFC (83.7 FPS), followed by CF-Net (78.4 FPS) and MSDAT (25 FPS), while Deep-STRCF runs at 5.3 FPS. The higher speed of these trackers, however, comes at the cost of much lower accuracy in comparison to STRCF. Furthermore, We also provide the overlap success curves of the competing trackers in <ref type="figure" target="#fig_4">Fig. 3b</ref>. One can see that DeepSTRCF ranks the second and outperforms DeepSRDCF with a margin of 5.2% on OTB-2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Internal Analysis of the proposed approach 4.2.1 Impacts of the Temporal regularization</head><p>In this section, we investigate the impacts of the temporal regularization on the proposed STRCF approach using the OTB-2015 dataset. <ref type="figure">Fig. 6a</ref> gives the overlap success plot of different SRDCF variants (discussed in Section 1) and our STRCF. Compared with the KCF method, we can see that the introduction of scale estimation (i.e. SRDCF(-MS)) and spatial regularization (i.e. SRDCF(-M)) can boost the performance by 3.5% and 6.7%, respectively. Besides, SRDCF also outperforms SRDCF(-M) by 1.6% with the coupling of DCF learning and model updating. However, when incorporating the temporal regularization into SRDCF(-M) formulation, our STRCF can bring notable improvements over both SRDCF(-M) and SRDCF with a gain of 6% and 4.4%, respectively. This can be explained by the merits of online PA on adaptively balancing the tradeoff between aggressive and passive model updating.</p><p>To further illustrate the differences of STRCF and SRDCF on model learning, we visualize the temporal CF variation (i.e. ft−ft−1 2 z , where z is the normalization factor) against frames on sequence Shaking in <ref type="figure">Fig. 6b</ref>. From it we can draw the following conclusions: (1) Compared with the SRDCF tracker, our STRCF passively updates the CFs in most frames with small appearance variations, thus leading to more robust DCF variations. (2) While SRDCF suffers from slow appearance variations (i.e. occlusion in the 3∼20-th frames), our STRCF is dominated by the passive model learning and thus insensitive to these variations. <ref type="formula">(3)</ref> In the case of sudden appearance variations (i.e. the illumination changes in the 58∼68-th frames), STRCF can benefit from the aggressive model learning and better adapt to these situations than SRDCF. It should be noted that these phenomena are ubiquitous in various video attributes, and the visualizations of the temporal CF variations on more videos are given in <ref type="figure" target="#fig_9">Fig. 7</ref>. In summary, with the introduction of the temporal regularization, our STRCF can provide a more robust appearance model than SRDCF, thereby leading to superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Effect of regularization parameter µ</head><p>We further analyze the effect of regularization parameter µ on the tracking performance of STRCF with hand-crafted features. The regularization parameter µ determines the rate at which to replace the learned CF f t−1 from previous frames with the new sample x in the current frame. The lower the parameter µ, the higher relevance of filter f given to the sample x. In <ref type="figure" target="#fig_10">Fig. 8</ref>, it is shown that the accuracy of STRCF tracker is significantly affected by the choice of µ. <ref type="figure" target="#fig_10">From Fig. 8</ref>, the best performance is achieved around µ = 16. Note that when µ = 0, STRCF is trained only with the current frame and ignores all historical information, thus it even performs worse than KCF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The Temple-Color Benchmark</head><p>We perform comparative experiments on Temple-Color dataset <ref type="bibr" target="#b24">[25]</ref> which consists of 128 color sequences. We compare STRCF and DeepSTRCF with the state-of-the-art trackers mentioned above except CF-Net <ref type="bibr" target="#b35">[36]</ref> which trained the network on Temple-Color . <ref type="figure" target="#fig_11">Fig. 9</ref> shows the comparison of overlap success plots for different trackers. We note DSST <ref type="bibr" target="#b10">[11]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The VOT-2016 Benchmark</head><p>We also report the results on Visual Object Tracking 2016 benchmark (VOT-2016) <ref type="bibr" target="#b21">[22]</ref>, which consists of 60 challenging videos. We evaluate the trackers in terms of accuracy, robustness and expected average overlap (EAO) <ref type="bibr" target="#b4">[5]</ref>. The accuracy measures the average overlap ratio between the predicted bounding box and the ground-truth. The robustness computes the average number of tracking failures over the sequence. And EAO averages the no-reset overlap of a tracker on several short-term sequences.</p><p>We compare STRCF and DeepSTRCF with state-of-theart trackers, including MDNet <ref type="bibr" target="#b31">[32]</ref> (VOT-2015 winner) and TCNN <ref type="bibr" target="#b30">[31]</ref> (VOT-2016 winner). <ref type="table" target="#tab_6">Table 3</ref> lists the results of different trackers on VOT-2016 dataset. We can see from <ref type="table" target="#tab_6">Table 3</ref> that STRCF performs significantly better than the BACF and SRDCF methods in terms of the EAO metric. In addition, DeepSTRCF also performs favorably against its counterpart DeepSRDCF by a gain of 3.7% in EAO metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose the spatial-temporal regularized correlation filters (STRCF) to address the inefficiency problem of SRDCF. By introducing the temporal regularizer to SRDCF formulation with single sample, STRCF serves as an approximation of SRDCF with multiple training samples. Moreover, as an extension of online PA, STRCF can adaptively balance the tradeoff between aggressive and passive model learning, thus leading to more robust models in the case of large appearance variations. An ADMM algorithm is developed to solve the STRCF model. We perform experiments on three benchmarks, and the results show that STRCF with hand-crafted features is superior than the baseline SRDCF by accuracy and speed. Moreover, STRCF with deep features also performs favorably against stateof-the-art trackers in terms of accuracy and robustness. In future, we will further improve our STRCF by investigating whether the temporal regularizer can be compatible to SAMF+AT <ref type="bibr" target="#b2">[3]</ref>, Staple+CA <ref type="bibr" target="#b29">[30]</ref>, and the GMM and continuous convolution in ECO <ref type="bibr" target="#b8">[9]</ref>. ÿ ÿ 9ÿ <ref type="bibr">9 9</ref> !" #$%ÿ '( !" #$%ÿ ') !" #$%ÿ )( !" #$%ÿ )* 9 ÿ ÿ ÿ 9ÿ    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A comparison of SRDCF and STRCF on model learning. SRDCF learns the CFs with multiple samples from historical tracking results and emphasizes more to the recent samples. Thus it may suffer from over-fitting to the recent inaccurate samples and results in tracking failure in the case of occlusion. In contrast, our STRCF trains the CF ft with the sample from current frame and the learned CF ft−1. Benefited from online PA, STRCF can successfully follow the targets by passively updating the CFs in the case of occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 , ( 6 )</head><label>26</label><figDesc>wheref denotes the discrete Fourier transform (DFT) of the filter f . From Eqn.<ref type="bibr" target="#b5">(6)</ref>, we can see that the j-th element of the labelŷ only depends on the j-th element of the filter f and samplex t across all D channels. Denote by V j (f ) ∈ R D the vector consisting of the j-th elements of f along all D channels. Eqn.<ref type="bibr" target="#b5">(6)</ref> can be further decomposed into M N subproblems, where each of them is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Complexity Analysis. Since Eqn. (6) is separable in each pixel location, we should solve M N subproblems and each is a system of linear equations with D variables. With Sherman-Morrison formula, each system can be solved in O(D). 1 Thus, the complexity of solvingf is O(DM N ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>A comparison of the overlap success plots with the state-of-the-art trackers on OTB-2015 dataset. (a) Trackers with hand-crafted features. (b) Trackers with deep features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative evaluation on 6 video sequences (i.e. CarScale, Dog, Girl2, Human3, Panda and Trans). We show the results of STRCF, ECO-HC, BACF, SRDCF and SRDCFDecon with different colors, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>The overlap success plots of the competing trackers with 4 video attributes on the OTB-2015 dataset. $ !"ÿ %&amp; !"ÿ '( !"ÿ &amp;) !"ÿ &amp;* !"ÿ +, (b) Ablative study on the STRCF method. (a) The overlap success plot of the SRDCF variants and our STRCF on OTB-2015. (b) The visualization results of temporal CF variation against frames on sequence Shaking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>!!</head><label></label><figDesc>$ÿ &amp; ! "#$ÿ ' ! "#$ÿ ( ! "#$ÿ &amp;) "ÿ $%&amp; !"ÿ $'( !"ÿ $') !"ÿ $)"ÿ $% !"ÿ $&amp; !"ÿ $' !"ÿ %(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of the temporal CF variation against frames between SRDCF and our STRCF on 6 video sequences (i.e. Bolt, Bolt2, Panda, Football1, DragonBaby and Jogging).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>μFigure 8 :</head><label>8</label><figDesc>Impacts of the temporal regularization parameter µ on OTB-2015 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>The overlap success plot of different trackers on Temple-Color. Only the top 10 trackers are displayed for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>from the periodic assumption of training samples, * Corresponding author. This work is supported by NSFC grants (No.s 61671182 and 61471146) and HK RGC GRF grant (PolyU 152240/15E).</figDesc><table><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SRDCF</cell><cell>SRDCF(-M)</cell><cell>SRDCF(-MS)</cell><cell>KCF</cell><cell>STRCF (HOG)</cell></row><row><cell>OTB-2015</cell><cell>72.7</cell><cell>69.3</cell><cell>61.5</cell><cell>55.5</cell><cell>79.2</cell></row><row><cell>Temple-Color</cell><cell>62.8</cell><cell>59.1</cell><cell>51.6</cell><cell>47.1</cell><cell>67.8</cell></row><row><cell>Avg.OP</cell><cell>67.8</cell><cell>64.2</cell><cell>56.6</cell><cell>51.3</cell><cell>73.5</cell></row><row><cell>Avg.FPS</cell><cell>5.3</cell><cell>7.6</cell><cell>32.3</cell><cell>167.4</cell><cell>28.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ECO-HC [9] SRDCFDecon [10] Staple [1] Staple+CA[30] SAMF+AT [3] SAMF [24] MEEM [41] DSST [11] KCF [19] STRCF (HOG) STRCF (HOGCN)</figDesc><table><row><cell>Mean OP FPS</cell><cell>72.7 5.8</cell><cell>77.5 26.7</cell><cell>79.6 42</cell><cell>77 2.0</cell><cell>71 76.6</cell><cell>73.8 35.3</cell><cell>68 2.2</cell><cell>64.4 23.2</cell><cell>62.3 22.4</cell><cell>62.2 20.4</cell><cell>55.5 171.8</cell><cell>79.2 31.5</cell><cell>79.6 24.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The mean OP (in %) and FPS results of trackers with hand-crafted features on OTB-2015. The best three results are shown in red, blue and green fonts, respectively.</figDesc><table><row><cell></cell><cell cols="10">ECO [9] DeepSRDCF [13] SiameseFC [2] FCNT [37] HDT [34] MSDAT [39] HCF [28] C-COT [15] CF-Net [36] DeepSTRCF</cell></row><row><cell>Mean OP FPS</cell><cell>85.5 9.8</cell><cell>76.8 0.2</cell><cell>71 83.7</cell><cell>67.1 1.2</cell><cell>65.8 2.7</cell><cell>65.6 25</cell><cell>65.6 10.2</cell><cell>82.7 0.8</cell><cell>73 78.4</cell><cell>84.2 5.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The OP metric (in %) and FPS results of trackers with deep features on OTB-2015. The best three results are shown in red, blue and green fonts, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>ECO [9] Staple [1] MDNet N [32] TCNN [31] SRDCF [13] BACF [20] SRDCFDecon [10] DeepSRDCF [12] ECO-HC [9] STRCF DeepSTRCF</figDesc><table><row><cell>EAO</cell><cell>0.181</cell><cell>0.375</cell><cell>0.295</cell><cell>0.257</cell><cell>0.325</cell><cell>0.247</cell><cell>0.223</cell><cell>0.262</cell><cell>0.276</cell><cell>0.322</cell><cell>0.279</cell><cell>0.313</cell></row><row><cell>Accuracy</cell><cell>0.5</cell><cell>0.53</cell><cell>0.54</cell><cell>0.53</cell><cell>0.54</cell><cell>0.52</cell><cell>0.56</cell><cell>0.53</cell><cell>0.51</cell><cell>0.54</cell><cell>0.53</cell><cell>0.55</cell></row><row><cell>Robustness</cell><cell>2.72</cell><cell>0.73</cell><cell>1.35</cell><cell>1.2</cell><cell>0.96</cell><cell>1.5</cell><cell>1.88</cell><cell>1.42</cell><cell>1.17</cell><cell>1.08</cell><cell>1.32</cell><cell>0.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>A comparison with the state-of-the-art trackers on VOT-2016 dataset. that STRCF is on par with ECO-HC and surpasses its counterparts SRDCF, DeepSRDCF and SRDCFDecon by 3.6%, 1.4% and 1.1%, respectively. Meanwhile, DeepSTRCF performs the best among the competing trackers and achieves an AUC score of 60.1%, further demonstrating the effectiveness of STRCF on deep features.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Target response adaptation for correlation filter tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual object tracking performance measures revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1261" to="1274" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Online passive-aggressive algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrently targetattending tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive decontamination of the training set: A unified formulation for discriminative visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative scale space tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive color attributes for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V D</forename><surname>Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the douglas-rachford splitting method and the proximal point algorithm for maximal monotone operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="293" to="318" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Correlation filters with limited boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-channel correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2016 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integrating boundary and center correlation filters for visual tracking with aspect ratio variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Encoding color information for visual tracking: Algorithms and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structural correlation filter for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time part-based visual tracking via adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long-term correlation tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context-aware correlation filter tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Modeling and propagating cnns in a tree structure for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The matrix cookbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Theobald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
		<respStmt>
			<orgName>Technical University of Denmark</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hedged deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large margin object tracking with circulant feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Robust and real-time deep tracking via multi-scale domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00561</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Meem: robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-task correlation particle filter for robust object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming-Hsuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning support correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06032</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
