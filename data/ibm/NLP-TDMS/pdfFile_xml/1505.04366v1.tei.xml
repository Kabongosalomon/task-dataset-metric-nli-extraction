<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Deconvolution Network for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
							<email>hyeonwoonoh@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
							<email>bhhan@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Deconvolution Network for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained with no external data through ensemble with the fully convolutional network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNN) have shown excellent performance in various visual recognition problems such as image classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>, semantic segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>, and action recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>. The representation power of CNNs leads to successful results; a combination of feature descriptors extracted from CNNs and simple off-the-shelf classifiers works very well in practice. Encouraged by the success in classification problems, researchers start to apply CNNs to structured prediction problems, i.e., semantic segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1]</ref>, human pose estimation <ref type="bibr" target="#b15">[16]</ref>, and so on.</p><p>Recent semantic segmentation algorithms are often formulated to solve structured pixel-wise labeling problems based on CNN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>. They convert an existing CNN architecture constructed for classification to a fully convolutional network (FCN). They obtain a coarse label map from the network by classifying every local region in image, and perform a simple deconvolution, which is implemented as  <ref type="bibr" target="#b16">[17]</ref> bilinear interpolation, for pixel-level labeling. Conditional random field (CRF) is optionally applied to the output map for fine segmentation <ref type="bibr" target="#b13">[14]</ref>. The main advantage of the methods based on FCN is that the network accepts a whole image as an input and performs fast and accurate inference. Semantic segmentation based on FCNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref> have a couple of critical limitations. First, the network can handle only a single scale semantics within image due to the fixed-size receptive field. Therefore, the object that is substantially larger or smaller than the receptive field may be fragmented or mislabeled. In other words, label prediction is done with only local information for large objects and the pixels that belong to the same object may have inconsistent labels as shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>. Also, small objects are often ignored and classified as background, which is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(b). Although <ref type="bibr" target="#b16">[17]</ref> attempts to sidestep this limitation using skip architecture, this is not a fundamental solution and performance gain is not significant. Second, the detailed structures of an object are often lost or smoothed because the label map, input to the deconvolutional layer, is too coarse and deconvolution procedure is overly simple. Note that, in the original FCN <ref type="bibr" target="#b16">[17]</ref>, the label map is only 16 × 16 in size and is deconvolved to generate segmentation result in the original input size through bilinear interpolation. The absence of real deconvolution in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref> makes it difficult to achieve good performance. However, recent methods ameliorate this problem using CRF <ref type="bibr" target="#b13">[14]</ref>.</p><p>To overcome such limitations, we employ a completely different strategy to perform semantic segmentation based on CNN. Our main contributions are summarized below:</p><p>• We learn a multi-layer deconvolution network, which is composed of deconvolution, unpooling, and rectified linear unit (ReLU) layers. Learning deconvolution network for semantic segmentation is meaningful but no one has attempted to do it yet to our knowledge.</p><p>• The trained network is applied to individual object proposals to obtain instance-wise segmentations, which are combined for the final semantic segmentation; it is free from scale issues found in FCN-based methods and identifies finer details of an object.</p><p>• We achieve outstanding performance using the deconvolution network trained only on PASCAL VOC 2012 dataset, and obtain the best accuracy through the ensemble with <ref type="bibr" target="#b16">[17]</ref> by exploiting the heterogeneous and complementary characteristics of our algorithm with respect to FCN-based methods.</p><p>We believe that all of these three contributions help achieve the state-of-the-art performance in PASCAL VOC 2012 benchmark. The rest of this paper is organized as follows. We first review related work in Section 2 and describe the architecture of our network in Section 3. The detailed procedure to learn a supervised deconvolution network is discussed in Section 4. Section 5 presents how to utilize the learned deconvolution network for semantic segmentation. Experimental results are demonstrated in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>CNNs are very popular in many visual recognition problems and have also been applied to semantic segmentation actively. We first summarize the existing algorithms based on supervised learning for semantic segmentation.</p><p>There are several semantic segmentation methods based on classification. Mostajabi et al. <ref type="bibr" target="#b17">[18]</ref> and Farabet et al. <ref type="bibr" target="#b5">[6]</ref> classify multi-scale superpixels into predefined categories and combine the classification results for pixel-wise labeling. Some algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> classify region proposals and refine the labels in the image-level segmentation map to obtain the final segmentation.</p><p>Fully convolutional network (FCN) <ref type="bibr" target="#b16">[17]</ref> has driven recent breakthrough on deep learning based semantic segmentation. In this approach, fully connected layers in the standard CNNs are interpreted as convolutions with large receptive fields, and segmentation is achieved using coarse class score maps obtained by feedforwarding an input image. An interesting idea in this work is that a simple interpolation filter is employed for deconvolution and only the CNN part of the network is fine-tuned to learn deconvolution indirectly. Surprisingly, the output network illustrates impressive performance on the PASCAL VOC benchmark. Chen et al. <ref type="bibr" target="#b0">[1]</ref> obtain denser score maps within the FCN framework to predict pixel-wise labels and refine the label map using the fully connected CRF <ref type="bibr" target="#b13">[14]</ref>.</p><p>In addition to the methods based on supervised learning, several semantic segmentation techniques in weakly supervised settings have been proposed. When only bounding box annotations are given for input images, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref> refine the annotations through iterative procedures and obtain accurate segmentation outputs. On the other hand, <ref type="bibr" target="#b19">[20]</ref> performs semantic segmentation based only on image-level annotations in a multiple instance learning framework.</p><p>Semantic segmentation involves deconvolution conceptually, but learning deconvolution network is not very common. Deconvolution network is introduced in [25] to reconstruct input images. As the reconstruction of an input image is non-trivial due to max pooling layers, it proposes the unpooling operation by storing the pooled location. Using the deconvoluton network, the input image can be reconstructed from its feature representation. This approach is also employed to visualize activated features in a trained CNN <ref type="bibr" target="#b23">[24]</ref> and update network architecture for performance enhancement. This visualization is useful for understanding the behavior of a trained CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System Architecture</head><p>This section discusses the architecture of our deconvolution network, and describes the overall semantic segmentation algorithm. <ref type="figure">Figure 2</ref> illustrates the detailed configuration of the entire deep network. Our trained network is composed of two parts-convolution and deconvolution networks. The convolution network corresponds to feature extractor that transforms the input image to multidimensional feature representation, whereas the deconvolution network is a shape generator that produces object segmentation from the feature extracted from the convolution network. The final output of the network is a probability map in the same size to input image, indicating probability of each pixel that belongs to one of the predefined classes. <ref type="figure">Figure 2</ref>. Overall architecture of the proposed network. On top of the convolution network based on VGG 16-layer net, we put a multilayer deconvolution network to generate the accurate segmentation map of an input proposal. Given a feature representation obtained from the convolution network, dense pixel-wise class prediction map is constructed through multiple series of unpooling, deconvolution and rectification operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>We employ VGG 16-layer net <ref type="bibr" target="#b21">[22]</ref> for convolutional part with its last classification layer removed. Our convolution network has 13 convolutional layers altogether, rectification and pooling operations are sometimes performed between convolutions, and 2 fully connected layers are augmented at the end to impose class-specific projection. Our deconvolution network is a mirrored version of the convolution network, and has multiple series of unpooing, deconvolution, and rectification layers. Contrary to convolution network that reduces the size of activations through feedforwarding, deconvolution network enlarges the activations through the combination of unpooling and deconvolution operations. More details of the proposed deconvolution network is described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deconvolution Network for Segmentation</head><p>We now discuss two main operations, unpooling and deconvolution, in our deconvolution network in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Unpooling</head><p>Pooling in convolution network is designed to filter noisy activations in a lower layer by abstracting activations in a receptive field with a single representative value. Although it helps classification by retaining only robust activations in upper layers, spatial information within a receptive field is lost during pooling, which may be critical for precise localization that is required for semantic segmentation.</p><p>To resolve such issue, we employ unpooling layers in deconvolution network, which perform the reverse operation of pooling and reconstruct the original size of activations as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. To implement the unpooling operation, we follow the similar approach proposed in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. It records the locations of maximum activations selected during pooling operation in switch variables, which are employed to place each activation back to its original pooled location. This unpooling strategy is particularly useful to reconstruct the structure of input object as described in <ref type="bibr" target="#b23">[24]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Deconvolution</head><p>The output of an unpooling layer is an enlarged, yet sparse activation map. The deconvolution layers densify the sparse activations obtained by unpooling through convolution-like operations with multiple learned filters. However, contrary to convolutional layers, which connect multiple input activations within a filter window to a single activation, deconvolutional layers associate a single input activation with multiple outputs, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. The output of the deconvolutional layer is an enlarged and dense activation map. We crop the boundary of the enlarged activation map to keep the size of the output map identical to the one from the preceding unpooling layer.</p><p>The learned filters in deconvolutional layers correspond to bases to reconstruct shape of an input object. Therefore, similar to the convolution network, a hierarchical structure of deconvolutional layers are used to capture different level of shape details. The filters in lower layers tend to capture overall shape of an object while the class-specific finedetails are encoded in the filters in higher layers. In this way, the network directly takes class-specific shape infor- mation into account for semantic segmentation, which is often ignored in other approaches based only on convolutional layers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Analysis of Deconvolution Network</head><p>In the proposed algorithm, the deconvolution network is a key component for precise object segmentation. Contrary to the simple deconvolution in <ref type="bibr" target="#b16">[17]</ref> performed on coarse activation maps, our algorithm generates object segmentation masks using deep deconvolution network, where a dense pixel-wise class probability map is obtained by successive operations of unpooling, deconvolution, and rectification. <ref type="figure" target="#fig_2">Figure 4</ref> visualizes the outputs from the network layer by layer, which is helpful to understand internal operations of our deconvolution network. We can observe that coarse-tofine object structures are reconstructed through the propagation in the deconvolutional layers; lower layers tend to capture overall coarse configuration of an object (e.g. location, shape and region), while more complex patterns are discovered in higher layers. Note that unpooling and deconvolution play different roles for the construction of segmentation masks. Unpooling captures example-specific structures by tracing the original locations with strong activations back to image space. As a result, it effectively reconstructs the detailed structure of an object in finer resolutions. On the other hand, learned filters in deconvolutional layers tend to capture class-specific shapes. Through deconvolutions, the activations closely related to the target classes are amplified while noisy activations from other regions are suppressed effectively. By the combination of unpooling and deconvolution, our network generates accurate segmentation maps. <ref type="figure" target="#fig_3">Figure 5</ref> illustrates examples of outputs from FCN-8s and the proposed network. Compared to the coarse activation map of FCN-8s, our network constructs dense and precise activations using the deconvolution network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">System Overview</head><p>Our algorithm poses semantic segmentation as instancewise segmentation problem. That is, the network takes a sub-image potentially containing objects-which we refer to as instance(s) afterwards-as an input and produces pixel-wise class prediction as an output. Given our network, semantic segmentation on a whole image is obtained by applying the network to each candidate proposals extracted from the image and aggregating outputs of all proposals to the original image space.</p><p>Instance-wise segmentation has a few advantages over image-level prediction. It handles objects in various scales effectively and identifies fine details of objects while the approaches with fixed-size receptive fields have troubles with these issues. Also, it alleviates training complexity by reducing search space for prediction and reduces memory requirement for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training</head><p>The entire network described in the previous section is very deep (twice deeper than <ref type="bibr" target="#b21">[22]</ref>) and contains a lot of associated parameters. In addition, the number of training examples for semantic segmentation is relatively small compared to the size of the network-12031 PASCAL training and validation images in total. Training a deep network with a limited number of examples is not trivial and we train the network successfully using the following ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Batch Normalization</head><p>It is well-known that a deep neural network is very hard to optimize due to the internal-covariate-shift problem <ref type="bibr" target="#b10">[11]</ref>; input distributions in each layer change over iteration during training as the parameters of its previous layers are updated. This is problematic in optimizing very deep networks since the changes in distribution are amplified through propagation across layers.</p><p>We perform the batch normalization <ref type="bibr" target="#b10">[11]</ref> to reduce the internal-covariate-shift by normalizing input distributions of every layer to the standard Gaussian distribution. For the purpose, a batch normalization layer is added to the output of every convolutional and deconvolutional layer. We observe that the batch normalization is critical to optimize our network; it ends up with a poor local optimum without batch normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Two-stage Training</head><p>Although batch normalization helps escape local optima, the space of semantic segmentation is still very large com-pared to the number of training examples and the benefit to use a deconvolution network for instance-wise segmentation would be cancelled. Then, we employ a two-stage training method to address this issue, where we train the network with easy examples first and fine-tune the trained network with more challenging examples later.</p><p>To construct training examples for the first stage training, we crop object instances using ground-truth annotations so that an object is centered at the cropped bounding box. By limiting the variations in object location and size, we reduce search space for semantic segmentation significantly and train the network with much less training examples successfully. In the second stage, we utilize object proposals to construct more challenging examples. Specifically, candidate proposals sufficiently overlapped with ground-truth segmentations are selected for training. Using the proposals to construct training data makes the network more robust to the misalignment of proposals in testing, but makes training more challenging since the location and scale of an object may be significantly different across training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Inference</head><p>The proposed network is trained to perform semantic segmentation for individual instances. Given an input image, we first generate a sufficient number of candidate proposals, and apply the trained network to obtain semantic segmentation maps of individual proposals. Then we aggregate the outputs of all proposals to produce semantic segmentation on a whole image. Optionally, we take ensemble of our method with FCN <ref type="bibr" target="#b16">[17]</ref> to further improve performance. We describe detailed procedure in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Aggregating Instance-wise Segmentation Maps</head><p>Since some proposals may result in incorrect predictions due to misalignment to object or cluttered background, we should suppress such noises during aggregation. The pixelwise maximum or average of the score maps corresponding all classes turns out to be sufficiently effective to obtain robust results.</p><p>Let g i ∈ R W ×H×C be the output score maps of the ith proposal, where W × H and C denote the size of proposal and the number of classes, respectively. We first put it on image space with zero padding outside g i ; we denote the segmentation map corresponding to g i in the original image size by G i hereafter. Then we construct the pixel-wise class score map of an image by aggregating the outputs of all proposals by</p><formula xml:id="formula_1">P (x, y, c) = max i G i (x, y, c), ∀i,<label>(1)</label></formula><p>or</p><formula xml:id="formula_2">P (x, y, c) = i G i (x, y, c), ∀i.<label>(2)</label></formula><p>Class conditional probability maps in the original image space are obtained by applying softmax function to the aggregated maps obtained by Eq. (1) or <ref type="bibr" target="#b1">(2)</ref>. Finally, we apply the fully-connected CRF <ref type="bibr" target="#b13">[14]</ref> to the output maps for the final pixel-wise labeling, where unary potential are obtained from the pixel-wise class conditional probability maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ensemble with FCN</head><p>Our algorithm based on the deconvolution network has complementary characteristics to the approaches relying on FCN; our deconvolution network is appropriate to capture the fine-details of an object, whereas FCN is typically good at extracting the overall shape of an object. In addition, instance-wise prediction is useful for handling objects with various scales, while fully convolutional network with a coarse scale may be advantageous to capture context within image. Exploiting these heterogeneous properties may lead to better results, and we take advantage of the benefit of both algorithms through ensemble.</p><p>We develop a simple method to combine the outputs of both algorithms. Given two sets of class conditional probability maps of an input image computed independently by the proposed method and FCN, we compute the mean of both output maps and apply the CRF to obtain the final semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>This section first describes our implementation details and experiment setup. Then, we analyze and evaluate the proposed network in various aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation Details</head><p>Network Configuration <ref type="table">Table 2</ref> summarizes the detailed configuration of the proposed network presented in <ref type="figure">Figure 2</ref>. Our network has symmetrical configuration of convolution and deconvolution network centered around the 2nd fully-connected layer (fc7). The input and output layers correspond to input image and class conditional probability maps, respectively. The network contains approximately 252M parameters in total.</p><p>Dataset We employ PASCAL VOC 2012 segmentation dataset <ref type="bibr" target="#b4">[5]</ref> for training and testing the proposed deep network. For training, we use augmented segmentation annotations from <ref type="bibr" target="#b7">[8]</ref>, where all training and validation images are used to train our network. The performance of our network is evaluated on test images. Note that only the images in PASCAL VOC 2012 datasets are used for training in our experiment, whereas some state-of-the-art algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref> employ additional data to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Data Construction</head><p>We employ a two-stage training strategy and use a separate training dataset in each stage. To construct training examples for the first stage, we draw a tight bounding box corresponding to each annotated object in training images, and extend the box 1.2 times larger to include local context around the object. Then we crop the window using the extended bounding box to obtain a training example. The class label for each cropped region is provided based only on the object located at the center while all other pixels are labeled as background. In the second stage, each training example is extracted from object proposal <ref type="bibr" target="#b25">[26]</ref>, where all relevant class labels are used for annotation. We employ the same post-processing as the one used in the first stage to include context. For both datasets, we maintain the balance for the number of examples across classes by adding redundant examples for the classes with limited number of examples. To augment training data, we transform an input example to a 250 × 250 image and randomly crop the image to 224 × 224 with optional horizontal flipping in a similar way to <ref type="bibr" target="#b21">[22]</ref>. The number of training examples is 0.2M and 2.7M in the first and the second stage, respectively, which is sufficiently large to train the deconvolution network from scratch.</p><p>Optmization We implement the proposed network based on Caffe <ref type="bibr" target="#b12">[13]</ref> framework. The standard stochastic gradient descent with momentum is employed for optimization, where initial learning rate, momentum and weight decay are set to 0.01, 0.9 and 0,0005, respectively. We initialize the weights in the convolution network using VGG 16-layer net pre-trained on ILSVRC <ref type="bibr" target="#b3">[4]</ref> dataset, while the weights in the deconvolution network are initialized with zero-mean Gaussians. We remove the drop-out layers due to batch normalization, and reduce learning rate in an order of magnitude whenever validation accuracy does not improve. Although our final network is learned with both train and validation datasets, learning rate adjustment based on validation accuracy still works well according to our experience. The network converges after approximately 20K and 40K SGD iterations with mini-batch of 64 samples in the first and second stage training, respectively. Training takes 6 days (2 days for the first stage and 4 days for the second stage) in a single Nvidia GTX Titan X GPU with 12G memory.</p><p>Inference We employ edge-box <ref type="bibr" target="#b25">[26]</ref> to generate object proposals. For each testing image, we generate approximately 2000 object proposals, and select top 50 proposals based on their objectness scores. We observe that this number is sufficient to obtain accurate segmentation in practice. To obtain pixel-wise class conditional probability maps for a whole image, we compute pixel-wise maximum to aggregate proposal-wise predictions as in Eq. (1).  <ref type="figure">Figure 6</ref>. Benefit of instance-wise prediction. We aggregate the proposals in a decreasing order of their sizes. The algorithm identifies finer object structures through iterations by handling multi-scale objects effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation on Pascal VOC</head><p>We evaluate our network on PASCAL VOC 2012 benchmark <ref type="bibr" target="#b4">[5]</ref>, which contains 1456 test images and involves 20 object categories. We adopt comp6 evaluation protocol that measures scores based on Intersection over Union (IoU) between ground truth and predicted segmentations.</p><p>The quantitative results of the proposed algorithm and the competitors are presented in <ref type="table" target="#tab_0">Table 1</ref>  <ref type="bibr" target="#b0">1</ref> , where our method is denoted by DeconvNet. The performance of DeconvNet is competitive to the state-of-the-art methods. The CRF <ref type="bibr" target="#b13">[14]</ref> as post-processing enhances accuracy by approximately 1% point. We further improve performance through an ensemble with FCN-8s. It improves mean IoU about 10.3% and 3.1% point with respect to FCN-8s and our DeconvNet, respectively, which is notable considering relatively low accuracy of FCN-8s. We believe that this is because our method and FCN have complementary characteristics as discussed in Section 5.2; this property differentiates our algorithm from the existing ones based on FCN. Our ensemble method with FCN-8s denoted by EDeconvNet achieves the best accuracy among methods trained only on PASCAL VOC data. <ref type="figure">Figure 6</ref> demonstrates effectiveness of instance-wise prediction for accurate segmentation. We aggregate the proposals in a decreasing order of their sizes and observe the progress of segmentation. As the number of aggregated proposals increases, the algorithm identifies finer object structures, which are typically captured by small proposals. The qualitative results of DeconvNet, FCN and their ensemble are presented in <ref type="figure">Figure 7</ref>. Overall, DeconvNet produces fine segmentations compared to FCN, and handles multi-scale objects effectively through instance-wise prediction. FCN tends to fail in labeling too large or small objects <ref type="figure">(Figure 7(a)</ref>) due to its fixed-size receptive field. Our network sometimes returns noisy predictions <ref type="figure">(Figure 7(b)</ref>), when the proposals are misaligned or located at background regions. The ensemble with FCN-8s produces much better results as observed in <ref type="figure">Figure 7</ref>(a) and 7(b). Note that inaccurate predictions from both FCN and DeconvNet are sometimes corrected by ensemble as shown in <ref type="figure">Figure 7</ref>(c). Adding CRF to ensemble improves quantitative performance, although the improvement is not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We proposed a novel semantic segmentation algorithm by learning a deconvolution network. The proposed deconvolution network is suitable to generate dense and pre-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Inconsistent labels due to large object size person person (b) Missing labels due to small object size Limitations of semantic segmentation algorithms based on fully convolutional network. (Left) original image. (Center) ground-truth annotation. (Right) segmentations by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of deconvolution and unpooling operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of activations in our deconvolution network. The activation maps from top left to bottom right correspond to the output maps from lower to higher layers in the deconvolution network. We select the most representative activation in each layer for effective visualization. The image in (a) is an input, and the rest are the outputs from (b) the last 14 × 14 deconvolutional layer, (c) the 28 × 28 unpooling layer, (d) the last 28 × 28 deconvolutional layer, (e) the 56 × 56 unpooling layer, (f) the last 56 × 56 deconvolutional layer, (g) the 112 × 112 unpooling layer, (h) the last 112 × 112 deconvolutional layer, (i) the 224 × 224 unpooling layer and (j) the last 224 × 224 deconvolutional layer. The finer details of the object are revealed, as the features are forward-propagated through the layers in the deconvolution network. Note that noisy activations from background are suppressed through propagation while the activations closely related to the target classes are amplified. It shows that the learned filters in higher deconvolutional layers tend to capture class-specific shape information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Comparison of class conditional probability maps from FCN and our network (top: dog, bottom: bicycle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results on PASCAL VOC 2012 test set. (Asterisk ( * ) denotes the algorithms trained with additional data.) Method bkg areo bike bird boat bottle bus car cat chair cow table dog horse mbk person plant sheep sofa train tv mean Hypercolumn [10] 88.9 68.4 27.2 68.2 47.6 61.7 76.9 72.1 71.1 24.3 59.3 44.8 62.7 59.4 73.5 70.6 52.0 63.0 38.1 60.0 54.1 59.2 MSRA-CFM [3] 87.7 75.7 26.7 69.5 48.8 65.6 81.0 69.2 73.3 30.0 68.7 51.5 69.1 68.1 71.7 67.5 50.4 66.5 44.4 58.9 53.5 61.8 FCN8s [17] 91.2 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2 TTI-Zoomout-16 [18] 89.8 81.9 35.1 78.2 57.4 56.5 80.5 74.0 79.8 22.4 69.6 53.7 74.0 76.0 76.6 68.8 44.3 70.2 40.2 68.9 55.3 64.4 DeepLab-CRF [1] 93.1 84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1 83.2 80.8 59.7 82.2 50.4 73.1 63.7 71.6 DeconvNet 92.7 85.9 42.6 78.9 62.5 66.6 87.4 77.8 79.5 26.3 73.4 60.2 70.8 76.5 79.6 77.7 58.2 77.4 52.9 75.2 59.8 69.6 DeconvNet+CRF 92.9 87.8 41.9 80.6 63.9 67.3 88.1 78.4 81.3 25.9 73.7 61.2 72.0 77.0 79.9 78.7 59.5 78.3 55.0 75.2 61.5 70.5 EDeconvNet 92.9 88.4 39.7 79.0 63.0 67.7 87.1 81.5 84.4 27.8 76.1 61.2 78.0 79.3 83.1 79.3 58.0 82.5 52.3 80.1 64.0 71.7 EDeconvNet+CRF 93.1 89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 83.6 80.2 58.8 83.4 54.3 80.7 65.0 72.5 * WSSL [19] 93.2 85.3 36.2 84.8 61.2 67.5 84.7 81.4 81.0 30.8 73.8 53.8 77.5 76.5 82.3 81.6 56.3 78.9 52.3 76.6 63.3 70.4 * BoxSup [2] 93.6 86.4 35.5 79.7 65.2 65.2 84.3 78.5 83.7 30.5 76.2 62.6 79.3 76.1 82.1 81.3 57.0 78.2 55.0 72.5 68.1 71.0</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">All numbers in this table are from the officially published papers, not from the leaderboard, including the ones in arXiv.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head><p>Ground-truth FCN DeconvNet EDeconvNet EDeconvNet+CRF (a) Examples that our method produces better results than FCN <ref type="bibr" target="#b16">[17]</ref>.</p><p>(b) Examples that FCN produces better results than our method.</p><p>(c) Examples that inaccurate predictions from our method and FCN are improved by ensemble. <ref type="figure">Figure 7</ref>. Example of semantic segmentation results on PASCAL VOC 2012 validation images. Note that the proposed method and FCN have complementary characteristics for semantic segmentation, and the combination of both methods improves accuracy through ensemble. Although CRF removes some noises, it does not improve quantitative performance of our algorithm significantly. <ref type="table">Table 2</ref>. Detailed configuration of the proposed network. "conv" and "deconv" denote layers in convolution and deconvolution network, respectively, while numbers next to each layer name mean the order of the corresponding layer in the network. ReLU layers are omitted from the table for brevity.</p><p>cise object segmentation masks since coarse-to-fine structures of an object is reconstructed progressively through a sequence of deconvolution operations. Our algorithm based on instance-wise prediction is advantageous to handle object scale variations by eliminating the limitation of fixed-size receptive field in the fully convolutional network. We further proposed an ensemble approach, which combines the outputs of the proposed algorithm and FCNbased method, and achieved substantially better performance thanks to complementary characteristics of both algorithms. Our network demonstrated the state-of-the-art performance in PASCAL VOC 2012 segmentation benchmark among the methods trained with no external data.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01640</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0774</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a DCNN for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02734</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
