<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sampling Bias in Deep Active Classification: An Empirical Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameya</forename><surname>Prabhu</surname></persName>
							<email>2ameya.prabhu@eng.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>1 Verisk | AI</addrLine>
									<country>Verisk Analytics</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Dognin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>1 Verisk | AI</addrLine>
									<country>Verisk Analytics</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
							<email>maneesh.singh@verisk.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>1 Verisk | AI</addrLine>
									<country>Verisk Analytics</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sampling Bias in Deep Active Classification: An Empirical Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The exploding cost and time needed for data labeling and model training are bottlenecks for training DNN models on large datasets. Identifying smaller representative data samples with strategies like active learning can help mitigate such bottlenecks. Previous works on active learning in NLP identify the problem of sampling bias in the samples acquired by uncertainty-based querying and develop costly approaches to address it. Using a large empirical study, we demonstrate that active set selection using the posterior entropy of deep models like FastText.zip (FTZ) is robust to sampling biases and to various algorithmic choices (query size and strategies) unlike that suggested by traditional literature. We also show that FTZ based query strategy produces sample sets similar to those from more sophisticated approaches (e.g ensemble networks). Finally, we show the effectiveness of the selected samples by creating tiny highquality datasets, and utilizing them for fast and cheap training of large models. Based on the above, we propose a simple baseline for deep active text classification that outperforms the state-of-the-art. We expect the presented work to be useful and informative for dataset compression and for problems involving active, semi-supervised or online learning scenarios. Code and models are available at: https://github.com/drimpossible/Sampling-Bias-Active-Learning</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) trained on large datasets provide state-of-the-art results on various NLP problems <ref type="bibr">(Devlin et al., 2019)</ref> including text classification <ref type="bibr">(Howard and Ruder, 2018)</ref>. However, the cost and time needed to get labeled data and to train models is a serious impediment to creating new and/or better models. This problem can be mitigated by creating smaller representative datasets with active learning which can be used for training DNNs to achieve similar test accuracy as * indicates equal contribution â€  Work done at Verisk | AI that using the full training dataset . In other words, the smaller sample can be considered a surrogate for the full data. However, there is lack of clarity in the active learning literature regarding sampling bias in such surrogate datasets created using active learning <ref type="bibr">(Settles, 2009</ref>): its dependence on models, functions and parameters used to acquire the sample. Indeed, what constitutes a good sample? In this paper, we perform an empirical investigation using active text classification as the application.</p><p>Early work in active text classification <ref type="bibr">(Lewis and Gale, 1994)</ref> suggests that greedy query generation using label uncertainty may lead to efficient representative samples (Nonetheless, the same test accuracy). Subsequent concerns regarding sampling bias has lead to explicit use of expensive diversity measures <ref type="bibr">(Brinker, 2003;</ref><ref type="bibr">Hoi et al., 2006)</ref> in acquisition functions or using ensemble approaches <ref type="bibr">(Liere and Tadepalli, 1997;</ref><ref type="bibr">McCallum and Nigam, 1998)</ref> to improve diversity implicitly.</p><p>Deep active learning approaches adapt the discussed framework above to train DNNs on large data. However, it is not clear if the properties of deep approaches mirror those of their shallow counterparts and if the theory and the empirical evidence regarding sampling efficiency and bias translates from shallow to deep models. For example, <ref type="bibr">(Sener and Savarese, 2018)</ref> and <ref type="bibr">(Ducoffe and Precioso, 2018)</ref> find that uncertainty based strategies perform no better than random sampling even if ensembles are used and using diversity measures outperform both. On the other hand, <ref type="bibr" target="#b1">(Beluch et al., 2018;</ref><ref type="bibr">Gissin and Shalev-Shwartz, 2019)</ref> find that uncertainty measures computed with ensembles outperform diversity based approaches while <ref type="bibr">(Gal et al., 2017;</ref><ref type="bibr" target="#b1">Beluch et al., 2018;</ref><ref type="bibr">Siddhant and Lipton, 2018)</ref> find them to outperform uncertainty measures computed using single models. A recent empirical study <ref type="bibr">(Siddhant and Lipton, 2018)</ref> investigating active learning in NLP suggests that Bayesian active learning outperforms classical uncertainty sampling across all settings. However, the approaches have been limited to rel-arXiv:1909.09389v1 [cs.CL] 20 Sep 2019 atively small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Sampling Bias in Active Classification</head><p>In this paper, we investigate the issues of sampling bias and sample efficiency, the stability of the actively collected query and train sets and the impact of algorithmic factors -i.e. the setup chosen while training the algorithm, in the context of deep active text classification on large datasets. In particular, we consider two sampling biases: label and distributional bias, three algorithmic factors: initial set selection, query size and query strategy along with two trained models and four acquisition functions on eight large datasets.</p><p>To isolate and evaluate the impact of the above (combinatorial) factors, a large experimental study was necessary. Consequently, we conducted over 2.3K experiments on 8 popular, large, datasets of sizes ranging from 120K-3.6M. Note that the current trend in deep learning is to train large models on very large datasets. However, the aforementioned issues have not yet been investigated in the literature in such a setup. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the datasets used in latest such analysis on active text classification by <ref type="bibr">(Siddhant and Lipton, 2018)</ref> are quite small in comparison. The datasets used by us are two orders of magnitude larger, our query samples often being the size of the entire datasets used by previous works, and the presented empirical study is more extensive (20x experiments).</p><p>Our findings are as follows:</p><p>(i) We find that utilizing the uncertainty query strategy using a deep model like FastText.zip (FTZ) 1 to actively construct a representative sample provides query and train sets with remarkably good sampling properties.</p><p>(ii) We finds that a single deep model (FTZ) used for querying provides a sample set similar to more expensive approaches using ensemble of models. Additionally, the sample set has a large overlap with support vectors of an SVM trained on the entire dataset largely invariant to a variety of algorithmic factors, thus indicating the robustness of the acquired sample set.</p><p>(iii) We demonstrate that the actively acquired training datasets can be utilized as small, surrogate training sets with a 5x-40x compression for training large, deep text classification models. In particular, we can train the <ref type="bibr">ULMFiT (Howard and Ruder, 2018)</ref> model to state of the art accuracy at 25x-200x speedups.</p><p>(iv) Finally, we create a novel, state-of-the-art baseline for active text classification which outperforms recent work (Siddhant and Lipton, 2018), using Bayesian dropout, utilizing 4x less training data. We also outperform (Sener and Savarese, 2018) at all training data sizes. The latter uses an expensive diversity based query strategy (coreset sampling).</p><p>The rest of the paper is organized as follows: in Section 2, the experimental methodology and setup are described. Section 3 presents the experimental study on sampling biases as well as the impact of various algorithmic factors. In Section 4, we compare with prior literature in active text classification. Section 5 presents a downstream use case -fast bootstrapping of the training of very large models like ULMFiT. Finally, we discuss the current literature in light of our work in Section 6 and summarize the conclusions in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>This section describes the experimental approach and the setup used to empirically investigate the issues of (i) sampling bias and (ii) sampling efficiency in creating small samples to train deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Approach</head><p>A labelled training set is incrementally built from a pool of unlabeled data by selecting &amp; acquiring labels from an oracle in sequential increments. In this, we follow the standard approach found in the active learning literature. We use the following terminology:</p><p>Queries &amp; Query Strategy: We refer to the (incremental) set of points selected to be labeled and added to the training as the query and the (acquisition) function used to select the samples as the query strategy.</p><p>Pool &amp; Train Sets: The pool is the unlabeled data from which queries are iteratively selected, labeled and added to the (labeled) train set.</p><p>Let D S = (x i , y i ) denote a dataset consisting of |S| = n i.i.d samples of data/label pairs, where |.| denotes the cardinality. Let S 0 âŠ‚ S denote an initial randomly drawn sample from the initial pool. At each iteration, we train the model on the current train set and use a model-dependent query strategy to acquire new samples from the pool, get them labeled by an oracle and add them to the train set. Thus, a sequence of training sets: [S 1 , S 2 . . . , S b ] is created by sampling b queries from the pool set, each of size K. The b queries are given by</p><formula xml:id="formula_0">[S 1 âˆ’ S 0 , S 2 âˆ’ S 1 . . . , S b âˆ’ S bâˆ’1 ]. Note that |S i | = (|S 0 | + i Ã— K) and S 1 âŠ‚ S 2 . . . âŠ‚ S b âŠ‚ S.</formula><p>In this paper, we investigate the efficiency and bias of sample sets S 1 b , S 2 b , . . . , S t b obtained by different query strategies Q 1 , Q 2 , . . . Q t . We exclude the randomly acquired initial set and perform comparisons on the actively acquired sample sets defined asÅœ i j = (S i j âˆ’ S i 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experimental Setup</head><p>In this section, we share details of the experimental setup, and present and explain the choice of the datasets, models and query strategies used. Datasets: We used eight, large, representative datasets widely used for text classification: AG-News (AGN), DBPedia (DBP), Amazon Review Polarity (AMZP), Amazon Review Full (AMZF), Yelp Review Polarity (YRP), Yelp Review Full (YRF), Yahoo Answers (YHA) and Sogou News (SGN). Please refer to Section 4 of <ref type="bibr">(Zhang et al., 2015)</ref> for details regarding the collection and characteristics of these datasets. <ref type="table" target="#tab_1">Table 1</ref> provides a comparison regarding the choice of datasets, models and number of experiments between our study and (Siddhant and Lipton, 2018) which investigates a variety of NLP tasks including text classification while we focus only on the latter.</p><p>Models: We reported two text classification models as representatives of classical and deep learning approaches respectively which were fast to train and also had good performance on text classification: Multinomial Naive Bayes (MNB) with TF-IDF (Wang and Manning, 2012) and Fast-Text.zip (FTZ) <ref type="bibr">(Joulin et al., 2016)</ref>. The FTZ model provides results competitive with VDC-NNs (a 29 layer CNN) (Conneau et al., 2017) but with over 15,000Ã— speedup <ref type="bibr">(Joulin et al., 2017)</ref>. This allowed us to conduct a thorough empirical study on large datasets. Multinomial Naive Bayes (MNB) with TF-IDF features is a popularly claimed baseline for text classification <ref type="bibr">(Wang and Manning, 2012)</ref>.</p><p>Query Strategies: Uncertainty based query strategies are widely used and well studied in the active learning literature. Those strategies typi-   <ref type="bibr">(Siddhant and Lipton, 2018)</ref> and our work. We use significantly larger datasets (two orders larger), perform 20x more experiments, and use more efficient and accurate models.</p><p>cally use a scoring function on the (softmax) output of a single model. We evaluate the following ones: Least Confidence (LC) and Entropy (Ent). Independently training ensembles of models <ref type="bibr">(Lakshminarayanan et al., 2017)</ref> is another principled approach to obtain uncertainties associated with the output estimate.Then, we tried four query strategies -LC and Ent computed using single and ensemble models and evaluated them against random sampling (chance) as a baseline. For ensembles, we used five FTZ ensembles <ref type="bibr">(Lakshminarayanan et al., 2017)</ref>. In contrast, <ref type="bibr">(Siddhant and Lipton, 2018)</ref> used Bayesian ensembles using Dropout, proposed in <ref type="bibr">(Gal et al., 2017)</ref>. Please refer to Section 4 for a comparison.</p><p>Implementation Details: We performed 2304 active learning experiments. We obtained our results on three random initial sets and three runs per seed (to account for stochasticity in FTZ) for each of the eight datasets. The query sizes were 0.5% of the dataset for AGN, AMZF, YRF and YHA and 0.25% for SGN, DBP, YRP and AMZP respectively for b = 39 sequential, active queries. We also experimented with different query sizes keeping the size of the final training data b Ã— K constant. The default query strategy uses a single model with output Entropy (Ent) unless explicitly stated otherwise. Results in the chance column are obtained using random query strategy.</p><p>We used Scikit-Learn (Pedregosa et al., 2011) implementation for MNB and original implementation for FastText.zip (FTZ) 2 . We required 3 weeks of running time for all FTZ experiments on a x1.16xlarge AWS instance with Intel Xeon E7-8880 v3 processors and 1TB RAM to obtain results presented in this work. The experiments are deterministic beyond the stochasticity involved in Dsets Limit FTZ (âˆ©Q) MNB (âˆ©Q) FTZ (âˆ©S) MNB (âˆ©S) SGN 1.61 1.56 Â± 0.03 1.15 Â± 0.32 1.59 Â± 0.01 1.57 Â± 0.01 DBP 2.64 2.50 Â± 0.02 2.27 Â± 0.11 2.51 Â± 0.0 2.58 Â± 0.01 YHA 2.30 2.25 Â± 0.01 2.22 Â± 0.02 2.25 Â± 0.0 2.28 Â± 0.0 YRP 0.69 0.69 Â± 0.0 0.56 Â± 0.13 0.69 Â± 0.0 0.69 Â± 0.01 YRF 1.61 1.56 Â± 0.02 1.42 Â± 0.21 1.56 Â± 0.0 1.57 Â± 0.01 AGN 1.39 1.33 Â± 0.04 1.13 Â± 0.17 1.33 Â± 0.0 1.35 Â± 0.01 AMZP 0.69 0.69 Â± 0.0 0.69 Â± 0.0 0.69 Â± 0.0 0.69 Â± 0.0 AMZF 1.61 1.58 Â± 0.02 1.6 Â± 0.01 1.59 Â± 0.0 1.61 Â± 0.0 <ref type="table">Table 2</ref>: Label entropy with a large query size (b = 9 queries). âˆ©Q denotes averaging across queries of a single run, âˆ©S denotes the label entropy of the final collected samples, averaged across seeds. Naive Bayes (âˆ©Q) has biased (inefficient) queries while FastText (âˆ©Q) shows stable, high label entropy showing a rich diversity in classes despite the large query size. Overall, the resultant sample (âˆ©S) becomes balanced in both cases. training the FTZ model, random initialization and SGD updates. The entire list of hyperparameters and metrics affecting uncertainty such as calibration error <ref type="bibr">(Guo et al., 2017)</ref> is given in the supplementary material. The experimental logs and models are available on our github link 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In this section, we study several aspects of sampling bias (class bias, feature bias) and the impact of relevant algorithmic factors (initial set selection, query size and query strategy.</p><p>We evaluated the actively acquired queries and sample set for sampling bias, and for the stability as measured by %intersection of collected sets across a critical influencing factor. Higher sample intersections indicate more stability increase to the chosen influencing factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Aspects of Sampling Bias</head><p>We study two types of sampling biases: (a) Class Bias and (b) Feature Bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Class Bias</head><p>Greedy uncertainty based query strategies are said to pick disproportionately from a subset of classes per query <ref type="bibr">(Sener and Savarese, 2018;</ref><ref type="bibr">Ebert et al., 2012)</ref>, developing a lopsided representation in each query. However, its effect on the resulting sample set is not clear. We test this by measuring the Kullback-Leibler (KL) divergence between the ground-truth label distribution and the distribution obtained per query as one experiment (âˆ©Q), and over the resulting sample (âˆ©S) as the second. Let us denote P as the true distribution of labels,P the sample distribution and C the total number of classes. Since P follows a uniform distribution, we can use Label entropy instead (L = âˆ’KL(P ||P ) + log(C)). Label entropy L is an intuitive measure. The maximum label entropy is reached when sampling is uniform,</p><formula xml:id="formula_1">P (x) = P (x), i.e. L = log(C).</formula><p>We present our results in <ref type="table" target="#tab_1">Table 15</ref>. We observe that across queries (âˆ©Q), FTZ with entropy strategy has a balanced representation from all classes (high mean) with a high probability (low std) while Multinomial Naive Bayes (MNB) results in more biased queries (lower mean) with high probability (high std) as studied previously. However, we did not find evidence of class bias in the resulting sample (âˆ©S) in both models: Fast-Text and Naive Bayes (column 5 and 6 from <ref type="table" target="#tab_1">Table  15</ref>).</p><p>We conclude that entropy as a query strategy can be robust to class bias even with large query sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Feature Bias</head><p>Uncertainty sampling can lead to undesirable sampling bias in feature space (Settles, 2009) by repeating redundant samples and picking outliers <ref type="bibr">(Zhu et al., 2008)</ref>. Diversity-based query strategies (Sener and Savarese, 2018) are used to address this issue, by selecting a representative subset of the data. In the context of active classification, it is good to pick the most informative samples to be the ones closer to class boundaries 4 .</p><p>Indeed, recent work suggests that the learning in deep classification networks may focus on small part of the data closer to class boundaries, thus resembling support vectors <ref type="bibr">(Xu et al., 2018;</ref><ref type="bibr">Toneva et al., 2019)</ref>. To investigate whether uncertainty sampling also exhibits this behavior, we perform below a direct comparison with support vectors from a SVM. For this, we train a FTZ model on the full training data and train a SVM on the resulting features (sentence embeddings) to obtain the support vectors and compute the intersection of support vectors with each selected set. The percentage intersections are shown in <ref type="table" target="#tab_2">Table 3</ref>. The high percentage overlap is a surprising result which shows that the sampling is indeed biased but in FastText is robust to increase in query size and significantly outperforms random in all cases. Naive Bayes: (Left) All including b=39 perform worse than random, (Center) All including b=9 eventually perform better than random (Right) b = 39 performs better than random but larger query sizes perform worse than random. Uncertainty sampling with Naive Bayes suffers from sampling size bias.</p><p>Dsets Common% Chance% #SV SGN 71.3 Â± 0.5 9.3 Â± 0.5 13184 DBP 86.3 Â± 0.5 9.7 Â± 0.5 1479 YRP 57.3 Â± 0.5 9.7 Â± 0.5 31750 AGN 45.0 Â± 0.8 21.0 Â± 1.6 1032  <ref type="table">Table 4</ref>: % Intersection of samples obtained with different seeds (ModelD) compared to same seeds (Mod-elS) and chance intersection for b = 39 queries. We see that FastText is initialization independent (FTZD â‰ˆ FTZS Chance). NaiveBayes shows significant dependency on the initial set sometimes, while other times performs comparable to FastText.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithmic Factors</head><p>We analyze three algorithmic factors of relevance to sampling bias: (a) Initial set selection (b) Query size, and, (c) Query strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Initial Set Selection</head><p>To investigate the dependence of the actively acquired train set on the initial set, we compare the overlap (intersection) of the incrementally constructed sets from different random initial sets versus the same initial set. The results are shown in <ref type="table">Table 4</ref>. We first observe that chance overlaps (column 2) are very low -less than 4%. Columns   3 and 5 present overlaps from different initial sets while 4 and 6 from same initial sets. We note from column 4 and 6 that due to the stochasticity of training in FTZ, we expect non-identical final sets even with same initial samples as well. The results demonstrate that samples obtained using FastText are largely initialization independent (low variation between columns 3 and 4) consistently across datasets while the samples obtained with Naive Bayes can be vastly different showing relatively heavy dependence on the initial seed. This indicates the relative stability of train set obtained with the posterior uncertainty of the actively trained FTZ as an acquisition function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Query size</head><p>Since the sampled data is sequentially constructed by training models on previously sampled data, large query sizes were expected to impact samples collected by uncertainty sampling and the performance thereof <ref type="bibr">(Hoi et al., 2006)</ref>. We experiment with various query sizes -(0.25%, 0.5%, 1%) for DBP, SGN, YRP and AMZP and (0.5%, 1%, 2%) for the rest corresponding to 9, 19 and 39 iterations. <ref type="figure" target="#fig_0">Figure 1</ref> shows that FastText (top row) has very stable performance across sample sizes while MNB (bottom row) show more erratic performance. <ref type="table" target="#tab_3">Table 5</ref> presents the intersection of samples obtained with different query sizes across multiple runs. We observe a high overlap of the acquired samples across different query sizes indicating that the performance is independent of the query size (compare column 3 to column 4 where the size is held constant) while MNB results in lower overlap with more erratic behavior due to change in the query size (compare column 5 compared to column 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Query strategy</head><p>We now investigate the impact of various query strategies using FastText by evaluating and comparing the correlation between the respective actively selected sample sets. Acquisition Functions: We compare four uncertainty query strategies: Least Confidence (LC) and Entropy (Ent), with and without deletion of least uncertain samples from the training set. Deletion of least uncertain samples reduces the dependence on the initial randomly selected set. The results are presented in <ref type="table" target="#tab_1">Table 14</ref>. We present five  <ref type="table">Table 7</ref>: Intersection of query strategies across single and ensemble of 5FTZ models. We observe that the % intersection of samples selected by ensembles and single models is comparable to intersection among either. The 5-model committee does not seem to add any additional value over selection by a single model. of the ten possible combinations and again observe the high degree of overlap in the collected samples. It can be concluded that the approach is fairly robust to these variations in the query strategy. Ensembles versus Single Models: A similar experiment was conducted to investigate the overlap between a single FTZ model and a probabilistic committee of models (5-model ensemble with FTZ (Lakshminarayanan et al., 2017)) to identify comparative advantages of using ensemble methods. The results are presented in <ref type="table">Table 7</ref> showing little to no difference in sample overlaps. <ref type="bibr">5</ref> We conclude that more expensive sampling strategies commonly used, like ensembling, may offer little benefit compared to using a single FTZ model with posterior uncertainty as a query function.</p><p>The experiments in this section demonstrate that uncertainty based sampling using deep models like FTZ show no class bias or an undesirable feature bias (and favorable bias to class boundaries). There is also a high degree of robustness to algorithmic factors, especially query size, a surprisingly high degree of overlap in the resulting training samples and stable performances (classification accuracy). Additionally, all uncertainty query strategies perform well, and expensive sampling strategies like ensembling offer little benefit. We conclude that sampling biases demonstrated in active learning literature do hold well with traditional models, however, they do not seem to translate to deep models like FTZ using (posterior) uncertainty. <ref type="figure">Figure 2</ref>: Active text classification: Comparison with K-Center Coreset, BALD and SVM algorithms. Accuracy is plotted against percentage data sampled. We reach full-train accuracy using 12% of the data, compared to BALD which requires 50% data and perform significantly worse in terms of accuracy. We also outperform K-center greedy Coreset at all sampling percentages without utilizing additional diversity-based augmentation.</p><p>model would be a good baseline for active text classification. We compare our baseline with the latest work in deep active learning for text classification -BALD <ref type="bibr">(Siddhant and Lipton, 2018)</ref> and with the recent diversity based Coreset query function (Sener and Savarese, 2018) which uses a costly K-center algorithm to build the query. Experiments are performed on TREC-QA for a fair comparison <ref type="bibr">(used by (Siddhant and Lipton, 2018)</ref>). <ref type="table" target="#tab_6">Table 8</ref> shows that the results of our study generalize to small datasets like TREC-QA.</p><p>The results are shown in <ref type="figure">Figure 2</ref> using the baseline with the query size of 2% of the full dataset (b=9 queries). Note that uncertainty sampling converges to full accuracy using just 12% of the data, whereas (Siddhant and Lipton, 2018) required 50% of the data. There is also a remarkable accuracy improvement over <ref type="bibr">(Siddhant and Lipton, 2018)</ref> which can be largely attributed to the models used (FastText versus 1layer CNN/BiLSTM). Also, uncertainty sampling outperforms diversity-based augmentations like Coreset Sampling (Sener and Savarese, 2018) before convergence. Thus, we establish a new stateof-the-art baseline for further research in deep active text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Application: Training of Large Models</head><p>The cost and time needed to get and label vast amounts of data to train large DNNs is a serious Dsets Chance FTZ-Ent-Ent FTZ Ent-LC SV Chce% SV Com% TQA 15.1 Â± 0.0 59.7 Â± 0.5 56.3 Â± 1.4 18.7 Â± 6.1 79.0 Â± 3.6    <ref type="bibr">(Howard and Ruder, 2018)</ref> (%dataset in brackets). We observe that using our cheaply obtained compressed datasets, we can achieve similar accuracies with 25x-200x speedup (5x less epochs, 5x-40x less data). Transferability to other models is evidence of the generalizability of the subset collected using FTZ to other deep models.</p><p>impediment to creating new and/or better models.</p><p>Our study suggests that the training samples collected with uncertainty sampling (entropy) on a single model FTZ may provide a good representation (surrogate) for the entire dataset. Buoyed by this, we investigate if we can speedup training of ULMFiT (Howard and Ruder, 2018) using the surrogate dataset. We show these results in <ref type="table" target="#tab_1">Table 10</ref>. We achieve 25x-200x speedup 6 (5x fewer epochs, 5x-40x smaller training size). We also benchmark the performance against the state-ofthe-art on text classification as shown in <ref type="table" target="#tab_7">Table 9</ref>. We conclude that we can significantly compress the training datasets and speedup classifier training time with little tradeoff in accuracy. Implementation Details: We use the official github repository for ULMFiT 7 , use default hyperparameters and train on one NVIDIA Tesla V100 16GB GPU. Further details are provided in sup- <ref type="bibr">6</ref> The cost of acquiring the training data using FTZ-Ent is negligible in comparison. 7 https://github.com/fastai/fastai/ tree/master/courses/dl2/imdb_scripts plementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>We now expand on the brief literature review in Section 1 to better contextualize our work. We divide the past works into (i) Traditional Models and (ii) Deep Models.</p><p>Sampling Bias in Classical AL in NLP: Active learning (AL) in text classification started with greedy uncertainty query strategy from a pool using decision trees <ref type="bibr">(Lewis and Gale, 1994)</ref>, which was shown to be effective and led to widespread adoption with classifiers like SVMs <ref type="bibr">(Tong and Koller, 2001)</ref>, Naive Bayes (Roy and <ref type="bibr">McCallum, 2001)</ref> and <ref type="bibr">KNN (Fujii et al., 1998)</ref>. This strategy was also applied to other NLP tasks like parse selection <ref type="bibr" target="#b0">(Baldridge and Osborne, 2004)</ref>, sequence labeling (Settles and Craven, 2008) and information extraction <ref type="bibr">(Thompson and Mooney, 1999)</ref>. These early papers popularized two greedy uncertainty query methods: Least Confident and Entropy.</p><p>Issues of lack of diversity (large reduduncy in sampling) <ref type="bibr">(Zhang and Oles, 2000)</ref> and lack of robustness (high variance in sample quality)(Krogh and Vedelsby, 1994) guided subsequent efforts. The two most popular directions were: (i) augmenting uncertainty with diversity measures <ref type="bibr">(Hoi et al., 2006;</ref><ref type="bibr">Brinker, 2003;</ref><ref type="bibr">Tang et al., 2002)</ref> and (ii) using query-by-committee <ref type="bibr">(McCallum and Nigam, 1998;</ref><ref type="bibr">Liere and Tadepalli, 1997)</ref>. For a comprehensive survey of classical AL methods for NLP, please refer to <ref type="bibr">(Settles, 2009)</ref>.</p><p>Sampling Bias in Deep AL: Deep active learning approach adapt the above framework to the training of DNNs on large data. Two main query strategies are used: (i) ensemble based greedy uncertainty, which represents a probabilistic queryby-committee paradigm <ref type="bibr">(Gal et al., 2017;</ref><ref type="bibr" target="#b1">Beluch et al., 2018)</ref>, and (ii) diversity based measures <ref type="bibr">(Sener and Savarese, 2018;</ref><ref type="bibr">Ducoffe and Precioso, 2018)</ref>. Papers proposing diversity based approaches find that greedy uncertainty based sampling (using ensemble and single model) perform significantly worse than random <ref type="bibr">(See Figures 4 and 2 respectively in (Sener and Savarese, 2018;</ref><ref type="bibr">Ducoffe and Precioso, 2018)</ref>). They attribute the poor performance to redundant, highly correlated sampling selected using uncertainty based methods and justify the need for prohibitively expensive diversity-based approaches (Refer section 2 of (Sener and Savarese, 2018) for details on the expensiveness of various diversity sampling methods). However, K-center greedy coreset sampling scales poorly: we were only able to use it on TREC-QA (a small dataset). On the other hand, ensemble-based greedy uncertainty methods find that probabilistic averaging from a committee <ref type="bibr">(Gal et al., 2017;</ref><ref type="bibr" target="#b1">Beluch et al., 2018)</ref> performs better than single model as with on diversity based methods like coreset <ref type="bibr">(Gissin and Shalev-Shwartz, 2019;</ref><ref type="bibr" target="#b1">Beluch et al., 2018)</ref>. Current approaches in text classification literature mostly adopt the ensemble based greedy uncertainty framework <ref type="bibr">(Siddhant and Lipton, 2018;</ref><ref type="bibr">Lowell et al., 2018;</ref><ref type="bibr">Zhang et al., 2017)</ref>.</p><p>However, our work demonstrates the problems of sampling bias and efficiency may not translate from shallow to deep approaches. Recent evidence from image domain <ref type="bibr">(Gissin and Shalev-Shwartz, 2019)</ref> demonstrates atleast a subset of our findings generalize to other DNNs (class bias and query functions). Uncertainty sampling using a deep model like FTZ demonstrates surprisingly good sampling properties without using ensembles or bayesian methods. Ensembles do not seem to significantly affect sampling. Whether this behavior generalizes to other deep models and tasks is yet to be seen.</p><p>Other Related Works: An interesting set of papers <ref type="bibr">(Soudry et al., 2018;</ref><ref type="bibr">Xu et al., 2018)</ref> show that deep neural networks trained with SGD converge to the maximum margin solution in the linearly separable case. Several works investigate the possibility that deep networks give high importance to a subset of the training dataset <ref type="bibr">(Toneva et al., 2019;</ref><ref type="bibr">Vodrahalli et al., 2018;</ref><ref type="bibr" target="#b2">Birodkar et al., 2019)</ref>, resembling supports in support vector machines. In our experiments, we find that active learning with uncertainty sampling with deep models like FTZ has a (surprisingly) large overlap with the support vectors of an SVM. Thus, it seems to have a inductive bias for class boundaries, similar to the above works. Whether this property generalizes to other deep models is yet to be seen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We conducted a large empirical study of sampling bias and efficiency, along with algorithmic factors which impacting active text classification. We conclude that uncertainty sampling with deep models like FastText.zip exhibits negligible class bias, seems to be favorably biased to sampling data points near class boundaries, is robust to various algorithmic factors and expensive sampling strategies like ensembling offer little benefit. Also, we find a surprisingly large overlap of actively acquired points with supports of a SVM. We additionally show that uncertainty sampling can be effectively used to bootstrap the training of large DNN models by generating compact surrogate datasets (5x-40x compression). Finally, FTZ-Ent provides a strong baseline for deep active text classification, outperforming previous results by a margin of 4x less data.</p><p>The current work opens up several directions for future investigations. To list a few: (a) a deeper look into the nature of sampled data -their distribution in the feature space, as well as their importance for the task at hand; (b) the creation of surrogate datasets for a variety of applications, including hyperparameter and architecture search, etc; (c) an extension to other deep models (beyond FTZ) and beyond classification models; and, (d) an extension to semi-supervised, online and continual learning. AGN SGN DBP YHA YRP YRF AMZP AZMF #Class 4 5 14 10 2 5 2 5 #Train 120k 450k 560k 1.4M 560k 650k 3.6M 3.0M #Test 7.6k 60k 70k 60k 38k 50k 400k 650k  In this document, we present statistics, additional tables and hyperparameters left out of the main work due to lack of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Details</head><p>Details of the train, test sizes and number of classes for each dataset can be found in <ref type="table" target="#tab_1">Table 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment Hyperparameters</head><p>In this section, we detail the complete list of hyperparameters, for reproducibility. We will release our code on Github.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Models</head><p>We describe the model hyperparameters used for 4 models: (i) FastText (ii) SVM (iii) ULMFiT (iv) Multinomial Naive Bayes for reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 FastText</head><p>We use the original implementation 8 . The hyperparameters used for each dataset can be found in <ref type="table" target="#tab_1">Table 12</ref>. We chose to use the zipped version of FastText for optimized memory usage without loss of accuracy, or speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 ULMFiT</head><p>For ULMFiT, we used the default hyperparameters from the author's implementation 9 , except the  batch size which we set to 32. We recall that ULMFiT has two steps: the fine-tuning of the language model and the fine-tuning of the classifier. We initialized the language model with the pre-trained weights released by the authors. Results of a pre-training on Wikitext-103 consisting of 28,595 pre-processed Wikipedia articles and 103 million words. For each compressed datasets (small and very small), we fine-tuned the language model and the classifier for 10 epochs. For finetuning both language model and classifier, we used a NVIDIA Tesla V100 16GB.</p><p>The hyperparameters for the language model are: batch size of 32, learning rate of 4e-3, bptt of 70, embedding size of 400, 1150 hidden units per hidden layer and 3 hidden layers. Adam Optimizer with Î² 1 = 0.8 and Î² 2 = 0.99. The dropout rates are: 0.15 between LSTM layers, 0.25 for the input layer, 0.02 for the embedding layer, 0.2 for the internal LSTM recurrent weights.</p><p>The hyperparameters for the classifier are: batch size of 32, learning rate of 0.01, embedding size of 400, 1150 hidden units per hidden layer and 3 hidden layers. Adam Optimizer with Î² 1 = 0.8 and Î² 2 = 0.99. The dropout rates are: 0.3 between LSTM layers, 0.4 for the input layer, 0.05 for the embedding layer, 0.5 for the internal LSTM recurrent weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.3 Multinomial Naive Bayes (MNB)</head><p>We use the scikit-learn implementation of Multinomial Naive Bayes 10 with default hyperparameters: smoothing parameter Î± = 1.0, fit prior set to True and class prior set to None. As input to our <ref type="table" target="#tab_1">Table 14</ref>: Intersection across query strategies using 19 and 9 iterations (mean Â± std across runs) and different seeds Datasets Limit FTZ (âˆ©Q) MNB (âˆ©Q) FTZ (âˆ©Q) MNB (âˆ©Q) FTZ (âˆ©Q) MNB (âˆ©Q) SGN 1.6 1.5 Â± 0.1 1.4 Â± 0.2 1.6 Â± 0.0 1.3 Â± 0.2 1.6 Â± 0.0 1.2 Â± 0.2 DBP 2.6 2.5 Â± 0.1 2.3 Â± 0.1 2.5 Â± 0.1 2.3 Â± 0.2 2.5 Â± 0.1 2.3 Â± 0.1 YA 2.3 2.3 Â± 0.0 2.3 Â± 0.0 2.3 Â± 0.0 2.2 Â± 0.0 2.3 Â± 0.0 2.2 Â± 0.0 YRP 0.7 0.7 Â± 0.0 0.7 Â± 0.1 0.7 Â± 0.0 0.6 Â± 0.2 0.7 Â± 0.0 0.7 Â± 0.0 YRF 1.6 1.6 Â± 0.0 1.5 Â± 0.1 1.6 Â± 0.0 1.4 Â± 0.2 1.6 Â± 0.0 1.3 Â± 0.2 AGN 1.4 1.3 Â± 0.0 1.3 Â± 0.1 1.3 Â± 0.1 1.1 Â± 0.2 1.3 Â± 0.0 1.1 Â± 0.1 AMZP 0.7 0.7 Â± 0.0 0.7 Â± 0.1 0.7 Â± 0.0 0.7 Â± 0.0 0.7 Â± 0.0 0.7 Â± 0.0 AMZF 1.6 1.6 Â± 0.0 1.6 Â± 0.0 1.6 Â± 0.0 1.6 Â± 0.1 1.6 Â± 0.0 1.6 Â± 0.0 MNB, we use the scikit-learn implementation of the TFIDF Vectorizer 11 . All default hyperparameters remain unchanged except that we use a maximum feature threshold of 50000, we remove all stop words contained in the default list 'english' and we set sublinear tf to True.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.4 SVM</head><p>To compute the support vectors of the datasets we used ThuderSVM, a Fast SVM library running on a V100 GPU. 12 . We used the SVC with a linear kernel, degree = 3, gamma = auto, coef0 = 0.0, C = 1.0, tol = 0.001, probability = False, classweight = None, shrinking = False, cachesize = None, verbose = False, max iter = -1, gpuid=0, maximum memory size = -1, random state = None and decision function = 'ovo'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Class Bias</head><p>We provide in <ref type="table" target="#tab_1">Table 15</ref> the complete results of our class bias experiments with 39, 19 and 4 iterations using entropy query strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 Results Across Iterations</head><p>We provide in <ref type="table" target="#tab_1">Table 14</ref> the results of our intersection experiments for 19 and 9 iterations using en-11 https://scikit-learn.org/stable/ modules/generated/sklearn.feature_ extraction.text.TfidfVectorizer.html 12 https://github.com/Xtra-Computing/ thundersvm tropy query strategy for FastText (FTZ) and Multinomial Naive Bayes (MNB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Metrics Affecting Uncertainty</head><p>We provide in <ref type="table" target="#tab_1">Table 13</ref> several metrics measured on the resulting samples of each dataset after 39 queries and using the entropy query strategy. NLL denotes the negative log-likelihood, BrierL denotes the Brier Score Loss, ECE denotes the expected calibration error, VarR denotes the variation ratio, ENT denotes the entropy, STD denotes the standard deviation. We measure these properties of the predicted sample and compute their average over the dataset. We observe that the Fast-Text model is well calibrated except for YRF and AMZF. Similar trends are observed in the average uncertainty measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Accuracy Plots for Remaining Datasets</head><p>We show in <ref type="figure">Figure 3</ref> the accuracy curves for Fast-Text and NaiveBayes, for 4, 9, 19 and 39 iterations using entropy query strategy vs random.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Accuracy across different number of queries b for FastText and Naive Bayes, with b Ã— K constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>DsetsChance FTZ 9 âˆ© 19 âˆ© 39 FTZ 39 âˆ© 39 âˆ© 39 MNB 9 âˆ© 19 âˆ© 39 MNB 39 âˆ© 39 âˆ© 39 SGN 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of active text classification datasets and models (Acc on Trec-QA) used in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Proportion of Support Vectors intersecting with our actively selected set calculated by |S SV âˆ©Åœ b | |S SV | . Actively selected sets share large overlap with supports of an SVM (critical for classification). a desirable way. Since the support vectors represent the class boundaries, a large percentage of selected data consists of samples around the class boundaries. This overlap indicates that the actively acquired training sample covers the support vectors well which are important for good classification performance. The overlap with the support vectors of an SVM (a fixed algorithm) also suggests that uncertainty sampling using deep models might generalize beyond FastText, to other learning algorithms. We used a fast GPU implementation for training an SVM with a linear kernel(Wen et al., 2018)  with default hyperparameters. Please refer to supplementary material for additional details. We ensured the SVM achieves similar accuracies as original FTZ model.</figDesc><table><row><cell>Experimental Details:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Intersection of samples obtained with different values of b. We see the intersection of samples selected with different number of intersections comparable to highest possible (different seeds) in FastText, far higher compared to chance intersection. This indicates similar samples are selected regardless of sample size. NaiveBayes does not show clear trends but occasionally the queried percentage drops significantly when increasing iterations, occasionally it remains unaffected.</figDesc><table><row><cell>Dsets</cell><cell>Chance</cell><cell cols="5">FTZ Ent-Ent FTZ Ent-LC FTZ Ent-DelEnt FTZ DelEnt-DelLC FTZ DelEnt-DelEnt</cell></row><row><cell>SGN</cell><cell>9.4 Â± 0.0</cell><cell>84.6 Â± 0.2</cell><cell>83.1 Â± 0.3</cell><cell>81.7 Â± 0.1</cell><cell>82.6 Â± 0.1</cell><cell>84.2 Â± 0.1</cell></row><row><cell>DBP</cell><cell>9.3 Â± 0.0</cell><cell>85.7 Â± 0.2</cell><cell>85.5 Â± 0.3</cell><cell>83.3 Â± 0.1</cell><cell>83.0 Â± 0.4</cell><cell>83.2 Â± 0.2</cell></row><row><cell>YHA</cell><cell>19.0 Â± 0.0</cell><cell>79.0 Â± 0.0</cell><cell>71.6 Â± 0.2</cell><cell>76.3 Â± 0.1</cell><cell>69.6 Â± 0.7</cell><cell>75.6 Â± 3.9</cell></row><row><cell>YRP</cell><cell>9.3 Â± 0.0</cell><cell>58.4 Â± 0.6</cell><cell>59.0 Â± 0.3</cell><cell>59.0 Â± 0.6</cell><cell>61.6 Â± 0.7</cell><cell>62.1 Â± 0.1</cell></row><row><cell>YRF</cell><cell>19.0 Â± 0.0</cell><cell>77.8 Â± 0.2</cell><cell>66.6 Â± 0.3</cell><cell>75.8 Â± 0.1</cell><cell>65.4 Â± 0.3</cell><cell>80.1 Â± 0.2</cell></row><row><cell>AGN</cell><cell>19.1 Â± 0.0</cell><cell>78.3 Â± 0.1</cell><cell>77.3 Â± 0.1</cell><cell>77.1 Â± 0.3</cell><cell>78.2 Â± 0.4</cell><cell>79.0 Â± 0.3</cell></row><row><cell cols="2">AMZP 9.5 Â± 0.0</cell><cell>63.5 Â± 0.2</cell><cell>63.5 Â± 0.3</cell><cell>66.1 Â± 0.4</cell><cell>70.0 Â± 0.1</cell><cell>70.0 Â± 0.1</cell></row><row><cell cols="2">AMZF 19.0 Â± 0.0</cell><cell>70.3 Â± 0.1</cell><cell>64.3 Â± 0.2</cell><cell>69.6 Â± 0.1</cell><cell>65.6 Â± 0.2</cell><cell>72.6 Â± 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Intersection of query strategies across acquisition functions. We observe that the % intersection among samples in the Ent-LC is comparable to those Ent-Ent. Similarly, the Ent-DelEnt (entropy with deletion) is compa- rable to both DelEnt-DelLC and DelEnt-DelEnt showing robustness of FastText to query functions (beyond minor variation). DelEnt-DelEnt obtains similar intersections as compared to Ent-Ent, showing the robustness of the acquired samples to deletion.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Â± 0.0 85.7 Â± 0.2 86.6 Â± 0.3 86.78 Â± 0.1 87.8 Â± 0.2 YRP 9.3 Â± 0.0 58.4 Â± 0.6 58.1 Â± 0.7 58.3 Â± 0.3 58.2 Â± 0.2 YRF 19.0 Â± 0.0 77.8 Â± 0.2 79.0 Â± 0.3 68.5 Â± 1.1 77.6 Â± 0.3 AGN 19.1 Â± 0.0 78.3 Â± 0.1 79.0 Â± 0.2 79.1 Â± 0.2 77.9 Â± 0.2</figDesc><table><row><cell>Dsets</cell><cell>Chance</cell><cell>FTZ-FTZ Ent</cell><cell>FTZ-5F TZ Ent</cell><cell>5FTZ-5FTZ Ent-LC</cell><cell>5FTZ-5FTZ Ent-Ent</cell></row><row><cell>SGN</cell><cell cols="4">9.4 Â± 0.0 84.6 Â± 0.2 86.3 Â± 0.2 85.4 Â± 0.4</cell><cell>85.8 Â± 0.0</cell></row><row><cell>DBP</cell><cell>9.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Results of sample selection from previous investigations on small datasets (Trec-QA).</figDesc><table><row><cell>Model</cell><cell>AGN</cell><cell>DBP</cell><cell>SGN</cell><cell>YRF</cell><cell>YRP</cell><cell>YHA</cell><cell>AMZP</cell><cell>AMZF</cell></row><row><cell>VDCNN (Conneau et al., 2017)</cell><cell>91.3</cell><cell>98.7</cell><cell>96.8</cell><cell>64.7</cell><cell>95.7</cell><cell>73.4</cell><cell>95.7</cell><cell>63.0</cell></row><row><cell>DPCNN (Johnson and Zhang, 2017)</cell><cell>93.1</cell><cell>99.1</cell><cell>98.1</cell><cell>69.4</cell><cell>97.3</cell><cell>76.1</cell><cell>96.7</cell><cell>65.2</cell></row><row><cell>WC-Reg (Qiao et al., 2018)</cell><cell>92.8</cell><cell>98.9</cell><cell>97.6</cell><cell>64.9</cell><cell>96.4</cell><cell>73.7</cell><cell>95.1</cell><cell>60.9</cell></row><row><cell>DC+MFA (Wang et al., 2018)</cell><cell>93.6</cell><cell>99.2</cell><cell>-</cell><cell>66.0</cell><cell>96.5</cell><cell>-</cell><cell>-</cell><cell>63.0</cell></row><row><cell>DRNN (Wang, 2018)</cell><cell>94.5</cell><cell>99.2</cell><cell>-</cell><cell>69.1</cell><cell>97.3</cell><cell>70.3</cell><cell>96.5</cell><cell>64.4</cell></row><row><cell>ULMFiT (Howard and Ruder, 2018)</cell><cell>95.0</cell><cell>99.2</cell><cell>-</cell><cell>70.0</cell><cell>97.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EXAM (Du et al., 2019)</cell><cell>93.0</cell><cell>99.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.8</cell><cell>95.5</cell><cell>61.9</cell></row><row><cell>Ours: ULMFiT (Small data)</cell><cell cols="8">93.7 (20) 99.2 (10) 97.0 (10) 67.6 (20) 97.1 (10) 74.3 (20) 96.1 (10) 64.1 (20)</cell></row><row><cell>Ours: ULMFiT (Tiny data)</cell><cell cols="4">91.7 (8) 98.6 (2.3) 97.4 (6.3) 66.3 (8)</cell><cell>96.7 (4)</cell><cell>73.3 (8)</cell><cell>95.8 (4)</cell><cell>62.9 (8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Comparison of accuracies with state-of-the-art approaches (earliest-latest) for text classification (%dataset in brackets). We are competitive with state-of-the-art models while using 5x-40x compressed datasets.</figDesc><table><row><cell>ULMFiT</cell><cell>AGN</cell><cell>DBP</cell><cell>YRP</cell><cell>YRF</cell></row><row><cell>Full</cell><cell>95.0</cell><cell>99.2</cell><cell>97.8</cell><cell>70.0</cell></row><row><cell cols="5">Ours-Small 93.7 (20) 99.2 (10) 97.1 (10) 67.6 (20)</cell></row><row><cell>Ours-Tiny</cell><cell cols="3">91.7 (8) 98.6 (2.3) 96.7 (4)</cell><cell>66.3(8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table /><note>ULMFiT: Resulting sampleÅœ b compared to reported accuracies in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Details about the dataset sizes (both train and test) along with the number of classes.</figDesc><table><row><cell cols="6">Dsets Emb Dim NGrams Epochs LR Acc Full</cell></row><row><cell>SGN</cell><cell>25</cell><cell>2</cell><cell>10</cell><cell>0.25</cell><cell>96.9</cell></row><row><cell>TQA</cell><cell>25</cell><cell>2</cell><cell>20</cell><cell>0.75</cell><cell>97.2</cell></row><row><cell>DBP</cell><cell>25</cell><cell>2</cell><cell>10</cell><cell>1</cell><cell>98.6</cell></row><row><cell>YHA</cell><cell>25</cell><cell>2</cell><cell>10</cell><cell>0.02</cell><cell>72.1</cell></row><row><cell>YRP</cell><cell>25</cell><cell>2</cell><cell>10</cell><cell>0.05</cell><cell>95.6</cell></row><row><cell>YRF</cell><cell>25</cell><cell>2</cell><cell>10</cell><cell>0.05</cell><cell>63.6</cell></row><row><cell>AGN</cell><cell>25</cell><cell>2</cell><cell>10</cell><cell>0.25</cell><cell>92.1</cell></row><row><cell>AMZP</cell><cell>25</cell><cell>2</cell><cell>10</cell><cell>0.01</cell><cell>94.2</cell></row><row><cell>AMZF</cell><cell>25</cell><cell>2</cell><cell>10</cell><cell>0.01</cell><cell>59.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>: Hyperparameters Used for FastText: Em-</cell></row><row><cell>bedding dimension, Number of n-grams, number of</cell></row><row><cell>epochs, learning rate, accuracy obtained using the full</cell></row><row><cell>train set</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>.37 0.05 0.16 0.12 0.5 0.27 YRP 0.16 0.04 0.02 0.03 0.11 0.47 YRF 1.15 0.11 0.17 0.21 0.73 0.31 AGN 0.46 0.03 0.04 0.02 0.08 0.42 AMZP 0.26 0.05 0.04 0.02 0.08 0.48 AMZF 1.32 0.12 0.21 0.22 0.77 0.31</figDesc><table><row><cell cols="3">Dsets NLL BrierL ECE VarR ENT STD</cell></row><row><cell cols="3">SGN 0.14 0.01 0.01 0.02 0.07 0.39</cell></row><row><cell>DBP 0.07</cell><cell>0.0</cell><cell>0.01 0.0 0.02 0.26</cell></row><row><cell>YHA 1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>Metrics measured after training FastText (FTZ-Ent) model on the resulting sample, with 39 queries, using entropy query strategy. We observe that NLL and Multiclass Brier Score remains low. The model is also well calibrated, i.e. gives calibrated uncertainty estimates.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 15 :</head><label>15</label><figDesc>Class Bias Experiments: Average Label entropy (mean Â± std ) across query iterations, for 39, 19 and 4 query iterations each.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use FastText.zip (FTZ) to optimize the time and resources needed for this study.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebookresearch/ fastText</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/drimpossible/Sampling-Bias-Active-Learning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In this work, we assume ergodicity in the setup. We do not consider incremental, online modeling scenarios where new modes or new classes are sequentially encountered.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Application: Active Text ClassificationExperimental results from the previous sections suggest that entropy function with a single FTZ 5 The ensembles were too costly to run on larger datasets, so the results for YHA, AMZP and AMZF could not be obtained.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/facebookresearch/ fastText/ 9 https://github.com/fastai/fastai/ tree/master/courses/dl2/imdb_scripts</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://scikit-learn.org/stable/ modules/generated/sklearn.naive_bayes. MultinomialNB.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 3: Accuracy across different number of queries b for FastText and Naive Bayes, with b Ã— K constant. FastText is robust to increase in query size and significantly outperforms random in all cases</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We thank Prof. Vineeth Balasubramium, IIT Hyderabad, India for the many helpful suggestions and discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.</p><p>Character-level convolutional networks for text classification. In NeurIPS.</p><p>Ye Zhang, Matthew Lease, and Byron C Wallace. 2017. Active discriminative text representation learning. In AAAI.</p><p>Jingbo Zhu, Huizhen Wang, Tianshun Yao, and Benjamin K Tsou. 2008. Active learning with sampling by uncertainty and density for word sense disambiguation and text classification. In ACL.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active learning and the total cost of annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The power of ensembles for active learning in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Beluch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">M</forename><surname>Nurnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11409</idno>
		<title level="m">Semantic redundancies in imageclassification datasets: The 10% you don&apos;t need</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
