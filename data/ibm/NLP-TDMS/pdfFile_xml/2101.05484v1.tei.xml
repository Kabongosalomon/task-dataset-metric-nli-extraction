<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Journal Title Journal XX (XXXX) XXXXXX https://doi.org/XXXX/XXXX 4D Attention-based Neural Network for EEG Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guowen</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengwen</forename><surname>Ye</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Bowen Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quansheng</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Journal Title Journal XX (XXXX) XXXXXX https://doi.org/XXXX/XXXX 4D Attention-based Neural Network for EEG Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>EEG</term>
					<term>emotion recognition</term>
					<term>attention mechanism</term>
					<term>convolutional recurrent neural network 2</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Electroencephalograph (EEG) emotion recognition is a significant task in the brain-computer interface field. Although many deep learning methods are proposed recently, it is still challenging to make full use of the information contained in different domains of EEG signals.</p><p>In this paper, we present a novel method, called four-dimensional attention-based neural network (4D-aNN) for EEG emotion recognition. First, raw EEG signals are transformed into 4D spatial-spectral-temporal representations. Then, the proposed 4D-aNN adopts spectral and spatial attention mechanisms to adaptively assign the weights of different brain regions and frequency bands, and a convolutional neural network (CNN) is utilized to deal with the spectral and spatial information of the 4D representations. Moreover, a temporal attention mechanism is integrated into a bidirectional Long Short-Term Memory (LSTM) to explore temporal dependencies of the 4D representations. Our model achieves state-of-the-art performance on the SEED dataset under intra-subject splitting. The experimental results have shown the effectiveness of the attention mechanisms in different domains for EEG emotion recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Emotion plays an important role in daily life and is closely related to human behavior and cognition <ref type="bibr" target="#b7">(Dolan 2002)</ref>. As one of the most significant research topics of affective computing, emotion recognition has received increasing attention in recent years for its applications of disease detection <ref type="bibr" target="#b2">(Bamdad et al. 2015;</ref><ref type="bibr" target="#b9">Figueiredo et al. 2019</ref>), human-computer interaction <ref type="bibr" target="#b10">(Fiorinia et al. 2020;</ref><ref type="bibr" target="#b14">Katsigiannis and Ramzan 2017)</ref>, and workload estimation <ref type="bibr" target="#b3">(Blankertz et al. 2016)</ref>. In general, emotion recognition methods can be divided into two categories <ref type="bibr" target="#b21">(Mühl et al. 2014)</ref>. One is based on external emotion responses including facial expressions and gestures <ref type="bibr" target="#b28">(Yan et al. 2016)</ref>, and the other is based on internal emotion responses including electroencephalograph (EEG) and electrocardiography (ECG) <ref type="bibr" target="#b32">(Zheng et al. 2017)</ref>. Neuroscientific researches have shown that some major brain cortex regions are closely related to emotions, making it possible to decode emotions based on EEG <ref type="bibr" target="#b4">(Brittona et al. 2006;</ref><ref type="bibr" target="#b20">Lotfia and Akbarzadeh-T 2014)</ref>. EEG is non-invasive, portable, and inexpensive so that it has been widely used in the field of brain-computer interfaces (BCIs) <ref type="bibr" target="#b22">(Pfurtscheller et al. 2010)</ref>. Besides, EEG signals contain various spatial, spectral, and temporal information about emotions evoked by specific stimulation patterns. Therefore, more and more researchers concentrate on EEG emotion recognition recently <ref type="bibr" target="#b1">(Alhagry et al. 2017;</ref><ref type="bibr" target="#b18">Li and Lu 2009)</ref>.</p><p>Traditional EEG emotion recognition methods usually extract hand-crafted features from EEG signals first and then adopt shallow models to classify the emotion features. EEG emotion features can be extracted from the time domain, frequency domain, and time-frequency domain. Jenke et al. conduct a comprehensive survey on EEG feature extraction methods by using machine learning techniques on a selfrecorded dataset <ref type="bibr" target="#b11">(Jenke et al. 2014)</ref>. For classifying the extracted emotion features, many researchers have adopted machine learning methods over the past few years <ref type="bibr" target="#b15">(Kim et al. 2013</ref>). Li et al. apply a linear support vector machine (SVM) to classify emotion features extracted from the gamma frequency band <ref type="bibr" target="#b18">(Li and Lu 2009</ref>). Duan et al. extract differential entropy (DE) features, which are superior to representing emotion states in EEG signals <ref type="bibr" target="#b24">(Shi et al. 2013)</ref>, from multichannel EEG data and combine a k-Nearest Neighbor (KNN) with SVM to classify the DE features <ref type="bibr" target="#b8">(Duan et al. 2013</ref>). However, shallow models require lots of expert knowledge to design and select emotion features, limiting their performance on EEG emotion classification.</p><p>Deep learning methods have been demonstrated to outperform traditional machine learning methods in many fields such as computer vision, natural language processing, and biomedical signal processing <ref type="bibr" target="#b0">(Abbass et al. 2018;</ref><ref type="bibr" target="#b6">Craik et al. 2019)</ref> for the ability to learn high-level features from data automatically <ref type="bibr" target="#b16">(Krizhevsky et al. 2012</ref>  . All those deep learning methods outperform the shallow models.</p><p>Although deep learning emotion recognition models have achieved higher accuracy than shallow models, it is still challenging to fuse more important information on different domains and capture discriminative local patterns in EEG signals. In the past decades, many researchers have investigated the critical frequency bands and channels for EEG emotion recognition. Zheng et al. demonstrate that β[14~31 Hz] and γ[31~51 Hz] bands are more related to emotion recognition than other bands, and their model achieves the best performance when combining all frequency bands. They also conduct experiments to select critical channels and propose the minimum pools of electrode sets for emotion recognition <ref type="bibr" target="#b31">(Zheng and Lu 2015)</ref>. To utilize the spatial information of EEG signals, Li et al. propose a 2D sparse map to maintain the information hidden in the electrode placement ). Zhong et al. introduce a regularized graph neural network (RGNN) to capture both local and global relations among different EEG channels for emotion recognition <ref type="bibr" target="#b33">(Zhong et al. 2020)</ref>. The temporal dependencies in EEG signals are also important to emotion recognition. For example, <ref type="bibr" target="#b13">Ma et al. (Jiaxin Ma et al. 2019)</ref> apply LSTMs in their models to extract temporal features for emotion recognition. Shen et al. transform the DE features of different channels into 4D structures to integrate the spectral, spatial, and temporal information simultaneously and then use a fourdimensional convolutional recurrent neural network (4D-CRNN) to recognize different emotions <ref type="bibr" target="#b23">(Shen et al. 2020</ref>). However, the differences among brain regions and frequency bands are not fully utilized in their work. To adaptively capture discriminative patterns in EEG signals, attention mechanisms have been applied to EEG emotion recognition. For instance, Tao et al. introduce a channel-wise attention mechanism, assigning the weights of different channels adaptively, along with an extended self-attention to explore the temporal dependencies of EEG signals <ref type="bibr" target="#b26">(Tao et al. 2020</ref>). Jia et al. propose a two-stream network with attention mechanisms to adaptively focus on important patterns <ref type="bibr" target="#b12">(Jia et al. 2020</ref>). From the above, it can be observed that it is critical 3 to integrate information on different domains and adaptively capture important brain regions, frequency bands, and timestamps in a unified network for EEG emotion recognition.</p><p>In this paper, we propose a four-dimensional attentionbased neural network named 4D-aNN for EEG emotion recognition. First, we transform raw EEG signals into 4D spatial-spectral-temporal representations which consist of several temporal slices. Different brain regions and frequency bands vary in the contributions to EEG emotion recognition, and the temporal dependencies of 4D representations should also be considered. Therefore, we employ attention mechanisms on both a CNN and a bidirectional LSTM network to adaptively capture discriminative patterns. For the CNN model, the attention mechanism is applied to the spatial and spectral dimensions of each temporal slice so that the important brain regions and frequency bands could be captured. As for the bidirectional LSTM model, the attention mechanism is applied to utilize long-range temporal dependencies so that the importance of different temporal slices in one 4D representation could be fully explored.</p><p>The primary contribution of this paper are summarized as follows: a) We propose a four-dimensional attention-based neural network, which fuses information on different domains and captures discriminative patterns in EEG signals based on the 4D spatial-spectral-temporal representation. b) We conduct experiments on the SEED dataset, and the experimental results indicate that our model achieves state-ofthe-art performance under intra-subject splitting.</p><p>The remainder of this paper is organized as follows. We describe our proposed method in the Method section. Dataset, experiment settings, results, ablation studies, and discussion are presented in the Experiment section. Finally, conclusions are given in the Conclusion section. <ref type="figure">Figure 1</ref> illustrates the overall structure of 4D-aNN for EEG emotion recognition. It consists of the 4D spatial-spectraltemporal representation, the attention-based CNN, the attention-based bidirectional LSTM, and the classifier. We will describe the details of each part in sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1</head><p>The overall structure of 4D-aNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4D spatial-spectral-temporal representation</head><p>The process of generating 4D representation is depicted in <ref type="figure">Fig. 2</ref>  <ref type="bibr" target="#b32">(Zheng et al. 2017)</ref>, are extracted from five frequency bands respectively with a 0.5s window for each segment.</p><p>PSD is defined as</p><formula xml:id="formula_0">ℎ ( ) = [ 2 ]<label>(1)</label></formula><p>where is formally a random variable and in this context, the signal acquired from a certain frequency band on a certain EEG channel.</p><p>DE feature is capable of discriminating EEG patterns between low and high frequency energy, which is defined as</p><formula xml:id="formula_1">ℎ ( ) = − ∫ ( ) log( ( )) (2)</formula><p>where ( ) is the probability density function of . If obeys the Gaussian distribution ( , 2 ), DE can simply be calculated by the following formulation:</p><formula xml:id="formula_2">ℎ ( ) = − ∫ 1 √2 2 ( − ) 2 2 2  − log 1 √2 2 ( − ) 2 2 2 = 1 2 log 2 2 (3)</formula><p>where and are Euler's constant and standard deviation of , respectively.</p><p>Thus, We extract a 3D feature tensor  2 2 , = 1, 2, . . . , from each segment, where is the number of total segments, is the number of EEG channels, 2 represents DE and PSD features of frequency bands, and 2 is 4 s atial s ectral te oral re resentation ttention ased ttention ased idirectional lassi ier 4 derived by the 0.5s window without overlapping. To utilize the spatial information of electrodes, we organize all the channels as a 2D sparse map so that the 3D feature tensor is transformed into a 4D representation  ℎ 2 2 , where ℎ and are the height and width of the 2D sparse map, respectively. The 2D sparse map of all the c channels with zero-padding is shown in <ref type="figure">Fig. 3</ref>, which preserves the topology of different electrodes. In this paper, we set ℎ = 19, = 19, and = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2</head><p>The generation of 4D spatial-spectral-temporal representation. For each Ts EEG signal segment, we extract DE and PSD features from different channels and frequency bands with a 0.5s window. Then, the features are transformed into a 4D representation which consists of 2T temporal slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3</head><p>The 2D sparse map with zero-padding of 62 channels. The purpose of the organization is to preserve the positional relationships among different electrodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-based CNN</head><p>For a 4D spatial-spectral-temporal representation , we extract the spatial and spectral information from each temporal slice  ℎ 2 , = 1, 2, . . . , 2 with a CNN, explore the discriminative local patterns in spatial and spectral domains with a convolutional attention module, and finally get its spatial and spectral representation. The attention module here is similar to what Woo et al. propose <ref type="bibr" target="#b27">(Woo et al. 2018)</ref>, which is originally used to improve the representation power of CNN networks.</p><p>The structure of the attention-based CNN is shown in <ref type="figure" target="#fig_0">Fig.  4</ref>. It contains four convolutional layers, four convolutional attention modules, one max-pooling layer, and one fullyconnected layer. The four convolutional layers have 64, 128, 256, and 64 feature maps with the filter size of 5  5, 5  5, 5  5, and 3  3, respectively. Specifically, a convolutional attention module is used after each convolutional layer to utilize the spatial and spectral attention mechanisms, and the details will be given later. We only use one max-pooling layer with a filter size of 2  2 after the last convolutional attention module to preserve more information and enhance the robustness of the network. Finally, outputs of the max-pooling layer are flattened and fed to the fully-connected layer with 150 units. Thus, for each temporal slice , we take the final output  150 as its spatial and spectral representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional attention module</head><p>The convolutional attention module is applied after each convolutional layer to adaptively capture important brain regions and frequency bands. The structure of the convolutional attention module is shown in <ref type="figure" target="#fig_1">Fig. 5</ref>. It consists of two sub-modules, i.e. the spatial attention module and the spectral attention module.</p><p>For each convolutional layer above, its output is a 3D feature tensor  ℎ × × , where ℎ , , and are the height of the 2D feature maps of , the width of the 2D feature maps of , and the number of the 2D feature maps of , respectively. We take as the input of the convolutional attention module.</p><p>The spectral attention module is applied to identify valuable frequency bands for emotion recognition. The average pooling has been widely used to aggregate spatial information and the maximum pooling has been commonly adopted to gather distinctive features. Therefore, we shrink the spatial dimension of by a spatial-wise average pooling and a spatial-wise maximum pooling, which are defined as:</p><formula xml:id="formula_3">, = 1 ℎ × ∑ ∑ =1 (ℎ, ) ℎ ℎ=1 , = 1, 2, … ,<label>(4)</label></formula><formula xml:id="formula_4">, = ( ), = 1, 2, . . . ,<label>(5)</label></formula><p>where  ℎ × denotes the 2D feature map in the i-th channel of , , represents the element in the i-th channel of the spatial average representation  ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( )</head><p>returns the largest element in , and , is the element in the i-th channel of the spatial maximum representation  . Subsequently, we implement the spectral attention by two fully-connected layers, a Relu activation function and a sigmoid activation function, which is defined as:</p><formula xml:id="formula_5">, = 2 ( ( 1 ) (6) , = 2 ( ( 1 ) (7) = ( ,  , ) (8) ( ) = max( , 0)<label>(9)</label></formula><formula xml:id="formula_6">( ) = 1 1 + −<label>(10)</label></formula><p>where 1 and 2 are learnable parameters,  denotes the element-wise addition, and  1×1× is the spectral attention. The elements of represent the importance of the corresponding 2D feature maps of the spectral domain. After generating the spectral attention , the output of the spectral attention module can be defined as:</p><formula xml:id="formula_7">′ = <label>(11)</label></formula><p>where ′ denotes the refined 3D feature tensor, and  represents the element-wise multiplication.</p><p>The spatial attention module is applied to identify valuable brain regions for emotion recognition. Firstly, we shrink the spectral dimension of ′ by spectral-wise average pooling and spectral-wise maximum pooling, which is defined as:</p><formula xml:id="formula_8">,(ℎ, ) = 1 ∑ ℎ,<label>′ ( ) =1</label></formula><p>, ℎ = 1, 2, . . . , ℎ ;</p><p>= 1, 2, . . . ,</p><p>,(ℎ, ) = ( ℎ, ′ ), ℎ = 1, 2, . . . , ℎ ;</p><p>= 1, 2, . . . ,</p><p>where ℎ, ′  denotes the channel in the h-th row and w-th column of ′ , ,(ℎ, ) represents the element in the h-th row and w-th column of the spectral average representation  ℎ × ×1 and ,(ℎ, ) is the element in the hth row and w-th column of the spectral maximum representation  ℎ × ×1 . In the following, we implement the spatial attention with a convolutional layer and a sigmoid activation function, which is defined as:</p><formula xml:id="formula_11">= ( , ) (14) = ( ( ))<label>(15)</label></formula><p>where ( , ) denotes the concatenation of and along the spectral dimension, ( ) represents the convolutional layer for , and  ℎ × ×1 is the spatial attention. The elements of represent the importance of the corresponding regions of the spatial domain. Subsequently, the output of the spatial attention module can be defined as:</p><formula xml:id="formula_12">′′ =  ′<label>(16)</label></formula><p>where ′′  ℎ × × denotes the final output 3D feature tensor of the convolutional attention module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-based bidirectional LSTM</head><p>For each temporal slice  ℎ 2 , = 1, 2, . . . , 2 , the final output of the attention-based CNN is  150 . Since the variation between different temporal slices contains temporal information for emotion recognition, we utilize an attentionbased bidirectional LSTM to explore the importance of different slices, as shown in <ref type="figure">Fig. 6</ref>.</p><p>A bidirectional LSTM connects two unidirectional LSTMs with opposite directions to the same output. Comparing with a unidirectional LSTM, a bidirectional LSTM preserves information from both past and future, making it understand the context better. In this paper, the bidirectional LSTM comprises two unidirectional LSTMs with 36 memory cells. The unidirectional LSTM for positive time direction, LSTMP takes the output sequence of the attention-based CNN = ( 1 , 2 , . . ., 2 ) as the input sequence, while the other for negative time direction, LSTMN takes the reverse sequence = ( 2 , 2 −1 , . . ., 1 ) as the input sequence. The outputs of the i-th node of the unidirectional LSTMs are  36 and  36 , = 1, 2, . . . , 2 , respectively. Then, we concatenate and 2 + 1 − as the output of the i-th node of the bidirectional LSTM  72 . Different from traditional ways that only use the output of the last node of an LSTM for classification or other applications, we take the outputs of all the bidirectional LSTM nodes  2 ×72 into consideration and explore the importance of different temporal slices by the temporal attention mechanism.</p><p>The temporal attention mechanism is implemented with two fully-connected layers, a Relu activation function, and a softmax activation function, which is defined as:</p><formula xml:id="formula_13">= 2 ( ( 1 Y i + 1 )) + 2 (17) = ( ) (18) ( ) = ( ) ∑ ( )<label>(19)</label></formula><p>where 1 , 2 , 1 , and 2 are learnable parameters, represents the i-th element of  2 ×1 which projects  2 ×72 to a lower dimension, and  2 ×1 is the temporal attention. The elements of represent the importance of the corresponding temporal slices. Subsequently, the high-level representation of the 4D sample can be defined as:</p><formula xml:id="formula_14">( ) = ∑  , = 1, 2, … , 72<label>(20)</label></formula><p>where  R 2T×1 denotes the e-th column of  R 2T×72 and ( ) is the e-th element of the high-level representation  72 , which integrates spatial, spectral, and temporal information of . <ref type="figure">Fig. 6</ref> The top block is the structure of the bidirectional LSTM. We concatenate the outputs of LSTMP and LSTMP as the output of the bidirectional LSTM,  2 ×72 . The middle block represents the projection of the outputs of the bidirectional LSTM. The bottom block denotes the generation of temporal attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier</head><p>Based on the high-level representation of EEG signals, we apply a fully-connected layer and a softmax activation function to predict the label of the 4D sample , which can be defined as follows:</p><formula xml:id="formula_15">= ( + ) (21)</formula><p>where , are learnable parameters and  denotes the probability of belonging to all the classes. Specifically, the class of the largest probability is the predicted label of 4D-aNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>In this section, we firstly introduce a widely used dataset. Then, the experiment settings are described. Finally, the results on the dataset are reported and discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEED Dataset</head><p>SEED dataset <ref type="bibr" target="#b31">(Zheng and Lu 2015)</ref> contains 3 different categories of emotion data: positive, neutral, and negative. For each kind of emotion, 5 film clips that are about 4 minutes long and can elicit the desired target emotion are selected. 15 healthy subjects (7 males and 8 females, with age (23.27  2.37)) take part in the EEG signals collection experiment. 3 groups of experiments are conducted for each subject, and each experiment consists of 15 clips viewing processes. Each clip viewing process can be divided into four stages, including a 5 seconds hint of start, a 4 minutes clip period, a 45 seconds self-assessment, and a 15 seconds rest period. The order of the 15 clips is arranged so that two clips eliciting the same emotion are not shown consecutively. The EEG signals in the experiments are recorded by a 62-channel's E I euro can system and down-sampled to 200 Hz. Besides, the EEG signals seriously contaminated by electromyography (EMG) and electrooculography (EOG) are removed manually. Then, a bandpass filter between 0.3 to 50 Hz is applied to filter the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>The proposed 4D-aNN takes a 4D segment  ℎ 2 2 as the input. In this paper, we adopt the 2D sparse map with ℎ = 19 and = 19 to maintain the positional relationship of electrodes. As shown in previous works, the combination of all the 5 bands can contribute to better results so that we set = 5. For each experiment, we set the length of segments as 3, obtaining about 1128 samples per experiment. Then, we conduct a fivefold cross-validation on each experiment and calculate the average classification accuracy (ACC) and standard deviation (STD) of 3 experiments for each subject. The average ACC and STD of all subjects are taken as the final performances of our method. We train the 4D-aNN on an NVIDIA GTX 1080 GPU. The Adam optimization is applied to minimize the loss function. We set the learning rate as 0.0003, the batch size as 12, and the maximum of epochs as 150.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Models</head><p>• HCNN : It uses a hierarchical CNN architecture for EEG emotion recognition, taking 2D DE feature maps extracted from γ band as inputs. HCNN only considers the spatial information of EEG signals. • BiHDM : It considers the asymmetric differences between two hemispheres for EEG emotion recognition. • RGNN <ref type="bibr" target="#b33">(Zhong et al. 2020)</ref>: It takes the biological topology among different brain regions into consideration to capture both global and local relations among different EEG channels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare our model with 5 baseline models on SEED dataset. <ref type="table">Table 1</ref> presents the average ACC and STD of these models for EEG emotion recognition. HCNN uses the hierarchical CNN architecture to classify emotion, but only considers the spatial information of EEG signals, reaching 88.60% on classification accuracy. BiHDM  applies four directed RNNs to obtain the deep representation o all the EEG electrodes' signals, reaching 93.12% on classification accuracy. RGNN considers the biological topology among different brain regions, reaching 94.24% on classification accuracy. 4D-CRNN takes 4D DE feature maps containing spatial, spectral, and temporal information as inputs, reaching 94.74% on classification accuracy. SST-EmotionNet uses a two-stream network with the attention mechanisms, reaching 96.02% on classification accuracy. However, the data size of each input sample of SST-EmotionNet is about 4 times larger than 4D-aNN. Comparing with the baseline models, the proposed 4D-aNN achieves the state-of-the-art performance on the SEED dataset under intrasubject splitting. The average ACC of all subjects is 96.10%. The performances on each subject are shown as <ref type="figure">Fig. 7</ref>, and there are 9 subjects (#5, #6, #8, #9, #10, #11, #12, #13, and #15) whose performances are better than the average ACC. Specifically, to make a fair comparison with 4D-CRNN, we conduct experiments on 4D-aNN (DE) and 4D-aNN (PSD), which represents the 4D-aNN only takes DE features as inputs and only takes PSD features as inputs, respectively. The accuracy of 4D-aNN (DE) exceeds that of 4D-CRNN by 0.65%, indicating the superiority of the proposed 4D-aNN. When compared with 4D-aNN (DE) and 4D-aNN (PSD), 4D-aNN displays the best performance, which indicates the effectiveness of the combination of different features. <ref type="table">Table 1</ref> The performance (average ACC and STD (%)) of the compared models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>SEED ACC (%) STD (%) HCNN  88.60 2.60 BiHDM  93.12 6.06 RGNN <ref type="bibr" target="#b33">(Zhong et al. 2020)</ref> 94.24 5.95 4D-CRNN <ref type="bibr" target="#b23">(Shen et al. 2020)</ref> 94.74 2.32 SST-EmotionNet <ref type="bibr" target="#b12">(Jia et al. 2020)</ref> 96  <ref type="figure">. 7</ref> The performance of 4D-aNN on each subject. In the SEED dataset, 3 experiments are conducted for each subject. We evaluate the performance of each experiment and also present the average classification accuracy for each subject.</p><p>To verify the importance of the attention mechanisms in our model, we conduct an additional experiment for ablation studies on SEED dataset. The experiment is ablation on spatial, spectral, and temporal attention mechanisms. We evaluate the performances of 4D-aNN when spatial, spectral, temporal, and all the attention mechanisms are ablated respectively. As shown in <ref type="figure" target="#fig_3">Fig. 8</ref>, when one of the attention mechanisms is ablated, the classification accuracy decreases. 4D-aNN without the spectral attention mechanism decreases by 0.63%, 4D-aNN without the spatial attention mechanism decreases by 0.47%, and 4D-aNN without the temporal attention mechanism decrease by 1.19%. Specifically, 4D-aNN without all the attention mechanisms decreases by 2.17%, which is the worst among the models used for comparison. In conclusion, the results indicate that the attention mechanisms make contributions to EEG emotion recognition for the ability to capture the discriminative local patterns in spatial, spectral, and temporal domains. In particular, to explore the critical brain regions for different emotions, we separately depict the electrode activity heatmaps in <ref type="figure">Fig. 9</ref>. We draw the heatmaps using Grad-CAM++ <ref type="bibr" target="#b5">(Chattopadhay et al. 2018)</ref>, based on the experimental results of subject #15. Grad-CAM++ uses the last convolutional layer feature maps and the class scores of the classifier to generate heatmaps. The heatmaps are able to explain which input regions are important for predictions. In this work, the size of each heatmap is 1919, which is the same as the 2D sparse map. The elements in the heatmaps represent the contributions of the corresponding brain regions to the recognition of the target emotions. From <ref type="figure">Fig. 9</ref>, We can observe the distinct distributions of important brain regions with regard to different emotions: channels FC5, FC3, and C5 are important for recognition of positive emotions, channels CP5, CP3, and CP1 are important for recognition of neutral emotions, and channels PO7, PO5, and P3 are important for recognition of negative emotions for subject #15. In particular, the critical brain regions could vary with different subjects, time, and emotions so that the attention mechanisms that enable 4D-aNN to adaptively capture discriminative patterns make sense for EEG emotion recognition. <ref type="figure">Fig. 9</ref> The electrode activity heatmaps based on the experimental results of subject #15. Parts (a), (b), and (c) correspond to positive, neutral, and negative emotions, respectively. Dark red regions denote more significant contributions to the recognition of the corresponding emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We conduct several experiments to investigate the use of 4D-aNN which fuses the spatial-spectral-temporal information and the effectiveness of the attention mechanisms on different domains for EEG emotion classification. In this section, we discuss three noteworthy points.</p><p>First, to deal with the spatial-spectral information, we apply an attention-based CNN which consists of a CNN network, a spectral attention module, and a spatial attention module. The CNN network extracts the spatial-spectral representation from inputs first. Then, the spectral attention mechanism is applied to each spectral feature to explore the importance of different frequency bands and features. Besides, the spatial attention mechanism is applied to each 2D feature map to adaptively capture the critical brain regions. The critical brain regions and frequency bands could vary with different individuals, emotions, and time so that the ability to capture discriminative patterns of the attention modules improves the performance of 4D-aNN.</p><p>Second, to explore the temporal dependencies in 4D spatialspectral-temporal representations, we utilize an attentionbased bidirectional LSTM. The bidirectional LSTM extracts high-level representations from the outputs of the attentionbased CNN. Different from traditional ways that only use the output of the last node of an LSTM for classifications or other applications, we consider outputs of all the nodes with the temporal attention mechanism. The temporal attention mechanism adaptively assigns weights of different temporal slices so that the dynamic content of emotions in 4D representations could be captured better.</p><p>Third, to address the importance of the attention mechanisms, we conduct ablation studies on different attention modules. 4D-aNN without the spatial, spectral, and temporal attention mechanism decreases by 0.47%, 0.63%, and 1.19% on classification accuracy, respectively. In particular, 4D-aNN without all the attention mechanisms decreases by 2.17%, which is the worst among the models in comparison. The experimental results demonstrate the effectiveness of the attention mechanisms to adaptively capture discriminative patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose the 4D-aNN model for EEG emotion recognition. The 4D-aNN takes 4D spatial-spectraltemporal representations containing spatial, spectral, and temporal information of EEG signals as inputs. We integrate the attention mechanisms into the CNN module and the bidirectional LSTM module. The CNN module deals with the spatial and spectral information of EEG signals while the spatial and spectral attention mechanisms capture critical brain regions and frequency bands adaptively. The bidirectional LSTM module extracts temporal dependencies on the outputs of the CNN module while the temporal attention mechanism explores the importance of different temporal slices. The experiments on SEED dataset demonstrate better performance than all baselines. In particular, the ablation studies on different attention modules show the effectiveness of the attention mechanisms in our model for EEG emotion recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4</head><label>4</label><figDesc>The structure of the attention-based CNN. The upper half of the blocks in the figure is the type of layers and the lower denotes the shape of its output tensors.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5</head><label>5</label><figDesc>The top block is the overall structure of the convolutional attention block, it consists of the spectral attention module and the spatial attention module. The middle block represents the generation of spectral attention. The bottom block denotes the generation of spatial attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>• 4D-CRNN<ref type="bibr" target="#b23">(Shen et al. 2020</ref>): It builds DE features extracted from EEG signals into 4D feature structures and uses a convolutional recurrent neural network to extract spatial features, spectral features, and temporal features for EEG emotion recognition.• SST-EmotionNet<ref type="bibr" target="#b12">(Jia et al. 2020)</ref>: It uses a two-stream network to extract spatial, spectral, and temporal features. Besides, SST-EmotionNet adopts the attention mechanisms to improve its performance on EEG emotion recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8</head><label>8</label><figDesc>Ablation studies on different input features and attention modules of 4D-a . "−" denotes the a lation on certain attention modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Recently, some researchers have applied deep learning to EEG emotion recognition. Zheng et al. introduce a deep belief network (DBN) to investigate the critical frequency bands and EEG signal channels for EEG emotion recognition (Zheng and Lu 2015). Yang et al. propose a hierarchical network to classify the DE features extracted from different frequency bands (Yang et al. 2018b). Song et al. use a graph convolutional neural network to classify the DE features.</figDesc><table><row><cell>Ma et al. propose a multimodal residual Long Short-Term</cell></row><row><cell>Memory model (MMResLSTM) for emotion recognition,</cell></row><row><cell>which shares temporal weights across the multiple modalities</cell></row><row><cell>(Jiaxin Ma et al. 2019). To learn the bi-hemispheric</cell></row><row><cell>discrepancy for EEG emotion recognition, Yang et al. propose</cell></row><row><cell>a novel bi-hemispheric discrepancy model (BiHDM)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. As previous works do<ref type="bibr" target="#b23">(Shen et al. 2020;</ref><ref type="bibr" target="#b29">Yang et al. 2018a</ref>), we split original EEG signals into seconds long segments without overlapping. Each segment is assigned with the same label as the original EEG signals. Then we decompose each segment into five frequency bands (i.e. δ[1~4 Hz], θ[4~8 Hz], α[8~14 HZ], β[14~31 Hz], and γ[31~51 Hz]) with five-order Butterworth filters. The Differential Entropy (DE) features and Power Spectral Density (PSD) features of all EEG channels, which have been proven to be effective for emotion recognition</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skgha</forename><surname>Abbass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Al-Mamun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thakor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bezerianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNSRE.2018.2864119</idno>
		<title level="m">Spatio-Spectral Representation Learning for Electroencephalographic Gait-Pattern Classification Ieee T Neur Sys Reh</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1858" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emotion recognition based on EEG using LSTM recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alhagry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Fahmy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>El-Khoribi</surname></persName>
		</author>
		<idno type="DOI">10.14569/IJACSA.2017.081046</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="335" to="358" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Application of BCI systems in neurorehabilitation: a scoping review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bamdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zarshenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Auais</surname></persName>
		</author>
		<idno type="DOI">10.3109/17483107.2014.961569</idno>
	</analytic>
	<monogr>
		<title level="j">Disability and Rehabilitation: Assistive Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="355" to="364" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Berlin brain-computer interface: progress beyond communication and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Blankertz</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2016.00530</idno>
	</analytic>
	<monogr>
		<title level="j">Front Neurosci-Switz</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">530</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural correlates of social and nonsocial emotions: An fMRI study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Brittona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Berridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Liberzon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2005.11.027</idno>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="397" to="409" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chattopadhay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting><address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-03" />
			<biblScope unit="page" from="12" to="15" />
		</imprint>
	</monogr>
	<note>Paper presented at</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning for electroencephalogram (EEG) classification tasks: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Craik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Contreras-Vidal</surname></persName>
		</author>
		<idno type="DOI">10.1088/1741-2552/ab0ab5</idno>
	</analytic>
	<monogr>
		<title level="j">J Neural Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">31001</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1076358</idno>
	</analytic>
	<monogr>
		<title level="j">Emotion, cognition, and behavior Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page" from="1191" to="1194" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Differential entropy feature for eeg-based emotion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International IEEE/EMBS Conference on Neural Engineering (NER)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-11" />
			<biblScope unit="page" from="6" to="8" />
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attentional bias for emotional faces in depressed and nondepressed individuals: an eye-tracking study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Ripka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efr</forename><surname>Romaneli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ulbricht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<meeting><address><addrLine>Berlin, Germany, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-27" />
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised emotional state classification through physiological parameters for social robotics applications Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fiorinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mancioppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Semeraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cavallo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2019.105217</idno>
		<imprint>
			<date type="published" when="0190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature extraction and selection for emotion recognition from eeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buss</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2014.2339834</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="327" to="339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial-Spectral-Temporal based Attention 3D Dense Network for EEG Emotion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sst-Emotionnet</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413724</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2909" to="2917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emotion recognition using multimodal residual LSTM network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B-L</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3343031.3350871</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia<address><addrLine>Nice, France; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="176" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Katsigiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramzan</surname></persName>
		</author>
		<idno type="DOI">10.1109/JBHI.2017.2688239</idno>
	</analytic>
	<monogr>
		<title level="j">Ieee J Biomed Health</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="98" to="107" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A review on the computational methods for emotional state estimation from the human eeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-P</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1155/2013/573734</idno>
	</analytic>
	<monogr>
		<title level="j">Comput Math Method M</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional neural networks for EEG-based emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12559-017-9533-x</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="368" to="380" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Emotion classification based on gammaband EEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<meeting><address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="3" to="6" />
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition Ieee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCDS.2020.2999337</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn Dev Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Practical emotional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lotfia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-T M-R</forename><surname>Akbarzadeh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2014.06.012</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="61" to="72" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baa</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanel</surname></persName>
		</author>
		<idno type="DOI">10.1080/2326263X.2014.912881</idno>
	</analytic>
	<monogr>
		<title level="j">Brain-Computer Interfaces</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="84" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pfurtscheller</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnpro.2010.00003</idno>
	</analytic>
	<monogr>
		<title level="j">The hybrid BCI Front Neurosci-Switz</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">EEGbased emotion recognition using 4D convolutional recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11571-020-09634-1</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn Neurodynamics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Differential entropy feature for eeg-based vigilance estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-07" />
			<biblScope unit="page" from="3" to="7" />
		</imprint>
	</monogr>
	<note>Paper presented at the</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2018.2817622</idno>
		<title level="m">EEG Emotion Recognition Using Dynamical Graph Convolutional Neural Networks IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="532" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">EEG-based Emotion Recognition via Channel-wise Attention and Self Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2020.3025777</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Cbam: Convolutional block attention module. Computer Vision -ECCV 2018</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_1</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sparse kernel reduced-rank regression for bimodal emotion recognition from facial expression and speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2016.2557721</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1319" to="1329" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Continuous Convolutional Neural Network with 3D Input for EEG-Based Emotion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-04239-4_39</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<editor>Cheng L, Leung ACS, Ozawa S</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="433" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition using hierarchical network with subnetwork nodes Ieee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qmj</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B-L</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCDS.2017.2685338</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn Dev Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="408" to="419" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B-L</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAMD.2015.2431497</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="162" to="175" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Identifying stable patterns over time for emotion recognition from EEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B-L</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2017.2712143</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="417" to="429" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">EEG-Based Emotion Recognition Using Regularized Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAFFC.2020.2994159</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Networks IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
