<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
							<email>maningning@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc (Face++)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<email>zhangxiangyu@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
							<email>zheng.haitao@sz.tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc (Face++)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNN architecture design</term>
					<term>efficiency</term>
					<term>practical</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Currently, the neural network architecture design is mostly guided by the indirect metric of computation complexity, i.e., FLOPs. However, the direct metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical guidelines for efficient network design. Accordingly, a new architecture is presented, called ShuffleNet V2.</p><p>Comprehensive ablation experiments verify that our model is the stateof-the-art in terms of speed and accuracy tradeoff.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The architecture of deep convolutional neutral networks (CNNs) has evolved for years, becoming more accurate and faster. Since the milestone work of AlexNet <ref type="bibr" target="#b0">[1]</ref>, the ImageNet classification accuracy has been significantly improved by novel structures, including VGG <ref type="bibr" target="#b1">[2]</ref>, GoogLeNet <ref type="bibr" target="#b2">[3]</ref>, ResNet <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, DenseNet <ref type="bibr" target="#b5">[6]</ref>, ResNeXt <ref type="bibr" target="#b6">[7]</ref>, SE-Net <ref type="bibr" target="#b7">[8]</ref>, and automatic neutral architecture search <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, to name a few.</p><p>Besides accuracy, computation complexity is another important consideration. Real world tasks often aim at obtaining best accuracy under a limited computational budget, given by target platform (e.g., hardware) and application scenarios (e.g., auto driving requires low latency). This motivates a series of works towards light-weight architecture design and better speed-accuracy tradeoff, including Xception <ref type="bibr" target="#b11">[12]</ref>, MobileNet <ref type="bibr" target="#b12">[13]</ref>, MobileNet V2 <ref type="bibr" target="#b13">[14]</ref>, ShuffleNet <ref type="bibr" target="#b14">[15]</ref>, and CondenseNet <ref type="bibr" target="#b15">[16]</ref>, to name a few. Group convolution and depth-wise convolution are crucial in these works.</p><p>To measure the computation complexity, a widely used metric is the number of float-point operations, or FLOPs 1 . However, FLOPs is an indirect metric. It is an approximation of, but usually not equivalent to the direct metric that we really care about, such as speed or latency. Such discrepancy has been noticed Equal contribution. <ref type="bibr" target="#b0">1</ref> In this paper, the definition of FLOPs follows <ref type="bibr" target="#b14">[15]</ref>, i.e. the number of multiply-adds. in previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>. For example, MobileNet v2 <ref type="bibr" target="#b13">[14]</ref> is much faster than NASNET-A <ref type="bibr" target="#b8">[9]</ref> but they have comparable FLOPs. This phenomenon is further exmplified in <ref type="figure" target="#fig_0">Figure 1</ref>(c)(d), which show that networks with similar FLOPs have different speeds. Therefore, using FLOPs as the only metric for computation complexity is insufficient and could lead to sub-optimal design.</p><p>The discrepancy between the indirect (FLOPs) and direct (speed) metrics can be attributed to two main reasons. First, several important factors that have considerable affection on speed are not taken into account by FLOPs. One such factor is memory access cost (MAC). Such cost constitutes a large portion of runtime in certain operations like group convolution. It could be bottleneck on devices with strong computing power, e.g., GPUs. This cost should not be simply ignored during network architecture design. Another one is degree of parallelism. A model with high degree of parallelism could be much faster than another one with low degree of parallelism, under the same FLOPs.</p><p>Second, operations with the same FLOPs could have different running time, depending on the platform. For example, tensor decomposition is widely used in early works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> to accelerate the matrix multiplication. However, the recent work <ref type="bibr" target="#b18">[19]</ref> finds that the decomposition in <ref type="bibr" target="#b21">[22]</ref> is even slower on GPU although it reduces FLOPs by 75%. We investigated this issue and found that this is because the latest CUDNN <ref type="bibr" target="#b22">[23]</ref> library is specially optimized for 3 × 3 conv. We cannot certainly think that 3 × 3 conv is 9 times slower than 1 × 1 conv. With these observations, we propose that two principles should be considered for effective network architecture design. First, the direct metric (e.g., speed) should be used instead of the indirect ones (e.g., FLOPs). Second, such metric should be evaluated on the target platform.</p><p>In this work, we follow the two principles and propose a more effective network architecture. In Section 2, we firstly analyze the runtime performance of two representative state-of-the-art networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>. Then, we derive four guidelines for efficient network design, which are beyond only considering FLOPs. While these guidelines are platform independent, we perform a series of controlled experiments to validate them on two different platforms (GPU and ARM) with dedicated code optimization, ensuring that our conclusions are state-of-the-art.</p><p>In Section 3, according to the guidelines, we design a new network structure. As it is inspired by ShuffleNet <ref type="bibr" target="#b14">[15]</ref>, it is called ShuffleNet V2. It is demonstrated much faster and more accurate than the previous networks on both platforms, via comprehensive validation experiments in Section 4. <ref type="figure" target="#fig_0">Figure 1</ref>(a)(b) gives an overview of comparison. For example, given the computation complexity budget of 40M FLOPs, ShuffleNet v2 is 3.5% and 3.7% more accurate than ShuffleNet v1 and MobileNet v2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Practical Guidelines for Efficient Network Design</head><p>Our study is performed on two widely adopted hardwares with industry-level optimization of CNN library. We note that our CNN library is more efficient than most open source libraries. Thus, we ensure that our observations and conclusions are solid and of significance for practice in industry.</p><p>-GPU. A single NVIDIA GeForce GTX 1080Ti is used. The convolution library is CUDNN 7.0 <ref type="bibr" target="#b22">[23]</ref>. We also activate the benchmarking function of CUDNN to select the fastest algorithms for different convolutions respectively. -ARM. A Qualcomm Snapdragon 810. We use a highly-optimized Neon-based implementation. A single thread is used for evaluation. Other settings include: full optimization options (e.g. tensor fusion, which is used to reduce the overhead of small operations) are switched on. The input image size is 224 × 224. Each network is randomly initialized and evaluated for 100 times. The average runtime is used.</p><p>To initiate our study, we analyze the runtime performance of two stateof-the-art networks, ShuffleNet v1 <ref type="bibr" target="#b14">[15]</ref> and MobileNet v2 <ref type="bibr" target="#b13">[14]</ref>. They are both highly efficient and accurate on ImageNet classification task. They are both widely used on low end devices such as mobiles. Although we only analyze these two networks, we note that they are representative for the current trend. At their core are group convolution and depth-wise convolution, which are also crucial components for other state-of-the-art networks, such as ResNeXt <ref type="bibr" target="#b6">[7]</ref>, Xception <ref type="bibr" target="#b11">[12]</ref>, MobileNet <ref type="bibr" target="#b12">[13]</ref>, and CondenseNet <ref type="bibr" target="#b15">[16]</ref>.</p><p>The overall runtime is decomposed for different operations, as shown in <ref type="figure">Figure</ref> 2. We note that the FLOPs metric only account for the convolution part. Although this part consumes most time, the other operations including data I/O, data shuffle and element-wise operations (AddTensor, ReLU, etc) also occupy considerable amount of time. Therefore, FLOPs is not an accurate enough estimation of actual runtime.</p><p>Based on this observation, we perform a detailed analysis of runtime (or speed) from several different aspects and derive several practical guidelines for efficient network architecture design. G1) Equal channel width minimizes memory access cost (MAC). The modern networks usually adopt depthwise separable convolutions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref>, where the pointwise convolution (i.e., 1 × 1 convolution) accounts for most of the complexity <ref type="bibr" target="#b14">[15]</ref>. We study the kernel shape of the 1 × 1 convolution. The shape is specified by two parameters: the number of input channels c 1 and output channels c 2 . Let h and w be the spatial size of the feature map, the FLOPs of the 1 × 1 convolution is B = hwc 1 c 2 .</p><p>For simplicity, we assume the cache in the computing device is large enough to store the entire feature maps and parameters. Thus, the memory access cost (MAC), or the number of memory access operations, is MAC = hw(c 1 +c 2 )+c 1 c 2 . Note that the two terms correspond to the memory access for input/output feature maps and kernel weights, respectively.</p><p>From mean value inequality, we have </p><formula xml:id="formula_0">MAC ≥ 2 √ hwB + B hw .<label>(1)</label></formula><p>Therefore, MAC has a lower bound given by FLOPs. It reaches the lower bound when the numbers of input and output channels are equal.</p><p>The conclusion is theoretical. In practice, the cache on many devices is not large enough. Also, modern computation libraries usually adopt complex blocking strategies to make full use of the cache mechanism <ref type="bibr" target="#b23">[24]</ref>. Therefore, the real MAC may deviate from the theoretical one. To validate the above conclusion, an experiment is performed as follows. A benchmark network is built by stacking 10 building blocks repeatedly. Each block contains two convolution layers. The first contains c 1 input channels and c 2 output channels, and the second otherwise. <ref type="table" target="#tab_0">Table 1</ref> reports the running speed by varying the ratio c 1 : c 2 while fixing the total FLOPs. It is clear that when c 1 : c 2 is approaching 1 : 1, the MAC becomes smaller and the network evaluation speed is faster.</p><p>G2) Excessive group convolution increases MAC. Group convolution is at the core of modern network architectures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. It reduces the computational complexity (FLOPs) by changing the dense convolution between all channels to be sparse (only within groups of channels). On one hand, it allows usage of more channels given a fixed FLOPs and increases the network capacity (thus better accuracy). On the other hand, however, the increased number of channels results in more MAC.</p><p>Formally, following the notations in G1 and Eq. 1, the relation between MAC and FLOPs for 1 × 1 group convolution is</p><formula xml:id="formula_1">MAC = hw(c 1 + c 2 ) + c 1 c 2 g = hwc 1 + Bg c 1 + B hw ,<label>(2)</label></formula><p>where g is the number of groups and B = hwc 1 c 2 /g is the FLOPs. It is easy to see that, given the fixed input shape c 1 × h × w and the computational cost B, MAC increases with the growth of g.</p><p>To study the affection in practice, a benchmark network is built by stacking 10 pointwise group convolution layers. <ref type="table" target="#tab_1">Table 2</ref> reports the running speed of using  <ref type="table">Table 4</ref>: Validation experiment for Guideline 4. The ReLU and shortcut operations are removed from the "bottleneck" unit <ref type="bibr" target="#b3">[4]</ref>, separately. c is the number of channels in unit. The unit is stacked repeatedly for 10 times to benchmark the speed. different group numbers while fixing the total FLOPs. It is clear that using a large group number decreases running speed significantly. For example, using 8 groups is more than two times slower than using 1 group (standard dense convolution) on GPU and up to 30% slower on ARM. This is mostly due to increased MAC. We note that our implementation has been specially optimized and is much faster than trivially computing convolutions group by group.</p><p>Therefore, we suggest that the group number should be carefully chosen based on the target platform and task. It is unwise to use a large group number simply because this may enable using more channels, because the benefit of accuracy increase can easily be outweighed by the rapidly increasing computational cost.</p><p>G3) Network fragmentation reduces degree of parallelism. In the GoogLeNet series <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31]</ref> and auto-generated architectures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref>), a "multipath" structure is widely adopted in each network block. A lot of small operators (called "fragmented operators" here) are used instead of a few large ones. For example, in NASNET-A [9] the number of fragmented operators (i.e. the number of individual convolution or pooling operations in one building block) is 13. In contrast, in regular structures like ResNet <ref type="bibr" target="#b3">[4]</ref>, this number is 2 or 3.</p><p>Though such fragmented structure has been shown beneficial for accuracy, it could decrease efficiency because it is unfriendly for devices with strong parallel computing powers like GPU. It also introduces extra overheads such as kernel launching and synchronization.</p><p>To quantify how network fragmentation affects efficiency, we evaluate a series of network blocks with different degrees of fragmentation. Specifically, each  building block consists of from 1 to 4 1 × 1 convolutions, which are arranged in sequence or in parallel. The block structures are illustrated in appendix. Each block is repeatedly stacked for 10 times. Results in <ref type="table" target="#tab_2">Table 3</ref> show that fragmentation reduces the speed significantly on GPU, e.g. 4-fragment structure is 3× slower than 1-fragment. On ARM, the speed reduction is relatively small. G4) Element-wise operations are non-negligible. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, in light-weight models like <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>, element-wise operations occupy considerable amount of time, especially on GPU. Here, the element-wise operators include ReLU, AddTensor, AddBias, etc. They have small FLOPs but relatively heavy MAC. Specially, we also consider depthwise convolution <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> as an element-wise operator as it also has a high MAC/FLOPs ratio.</p><p>For validation, we experimented with the "bottleneck" unit (1 × 1 conv followed by 3 × 3 conv followed by 1 × 1 conv, with ReLU and shortcut connection) in ResNet <ref type="bibr" target="#b3">[4]</ref>. The ReLU and shortcut operations are removed, separately. Runtime of different variants is reported in <ref type="table">Table 4</ref>. We observe around 20% speedup is obtained on both GPU and ARM, after ReLU and shortcut are removed.</p><p>Conclusion and Discussions Based on the above guidelines and empirical studies, we conclude that an efficient network architecture should 1) use "balanced" convolutions (equal channel width); 2) be aware of the cost of using group convolution; 3) reduce the degree of fragmentation; and 4) reduce element-wise operations. These desirable properties depend on platform characterics (such as memory manipulation and code optimization) that are beyond theoretical FLOPs. They should be taken into accout for practical network design.</p><p>Recent advances in light-weight neural network architectures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref> are mostly based on the metric of FLOPs and do not consider these properties above. For example, ShuffleNet v1 <ref type="bibr" target="#b14">[15]</ref> heavily depends group convolutions (against G2) and bottleneck-like building blocks (against G1). MobileNet v2 <ref type="bibr" target="#b13">[14]</ref> uses an inverted bottleneck structure that violates G1. It uses depthwise convolutions and ReLUs on "thick" feature maps. This violates G4. The auto-generated structures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref> are highly fragmented and violate G3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ShuffleNet V2: an Efficient Architecture</head><p>Review of ShuffleNet v1 <ref type="bibr" target="#b14">[15]</ref>. ShuffleNet is a state-of-the-art network architecture. It is widely adopted in low end devices such as mobiles. It inspires our work. Thus, it is reviewed and analyzed at first.</p><p>According to <ref type="bibr" target="#b14">[15]</ref>, the main challenge for light-weight networks is that only a limited number of feature channels is affordable under a given computation budget (FLOPs). To increase the number of channels without significantly increasing FLOPs, two techniques are adopted in <ref type="bibr" target="#b14">[15]</ref>: pointwise group convolutions and bottleneck-like structures. A "channel shuffle" operation is then introduced to enable information communication between different groups of channels and improve accuracy. The building blocks are illustrated in <ref type="figure" target="#fig_2">Figure 3</ref> As discussed in Section 2, both pointwise group convolutions and bottleneck structures increase MAC (G1 and G2). This cost is non-negligible, especially for light-weight models. Also, using too many groups violates G3. The element-wise "Add" operation in the shortcut connection is also undesirable (G4). Therefore, in order to achieve high model capacity and efficiency, the key issue is how to maintain a large number and equally wide channels with neither dense convolution nor too many groups.</p><p>Channel Split and ShuffleNet V2 Towards above purpose, we introduce a simple operator called channel split. It is illustrated in <ref type="figure" target="#fig_2">Figure 3(c)</ref>. At the beginning of each unit, the input of c feature channels are split into two branches with c − c and c channels, respectively. Following G3, one branch remains as identity. The other branch consists of three convolutions with the same input and output channels to satisfy G1. The two 1 × 1 convolutions are no longer group-wise, unlike <ref type="bibr" target="#b14">[15]</ref>. This is partially to follow G2, and partially because the split operation already produces two groups.</p><p>After convolution, the two branches are concatenated. So, the number of channels keeps the same (G1). The same "channel shuffle" operation as in <ref type="bibr" target="#b14">[15]</ref> is then used to enable information communication between the two branches.</p><p>After the shuffling, the next unit begins. Note that the "Add" operation in ShuffleNet v1 <ref type="bibr" target="#b14">[15]</ref> no longer exists. Element-wise operations like ReLU and depthwise convolutions exist only in one branch. Also, the three successive elementwise operations, "Concat", "Channel Shuffle" and "Channel Split", are merged into a single element-wise operation. These changes are beneficial according to G4.</p><p>For spatial down sampling, the unit is slightly modified and illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>(d). The channel split operator is removed. Thus, the number of output channels is doubled.</p><p>The proposed building blocks (c)(d), as well as the resulting networks, are called ShuffleNet V2. Based the above analysis, we conclude that this architecture design is highly efficient as it follows all the guidelines.</p><p>The building blocks are repeatedly stacked to construct the whole network. For simplicity, we set c = c/2. The overall network structure is similar to Shuf-fleNet v1 <ref type="bibr" target="#b14">[15]</ref> and summarized in <ref type="table">Table 5</ref>. There is only one difference: an additional 1 × 1 convolution layer is added right before global averaged pooling to mix up features, which is absent in ShuffleNet v1. Similar to <ref type="bibr" target="#b14">[15]</ref>, the number of channels in each block is scaled to generate networks of different complexities, marked as 0.5×, 1×, etc.</p><p>Analysis of Network Accuracy ShuffleNet v2 is not only efficient, but also accurate. There are two main reasons. First, the high efficiency in each building block enables using more feature channels and larger network capacity.</p><p>Second, in each block, half of feature channels (when c = c/2) directly go through the block and join the next block. This can be regarded as a kind of feature reuse, in a similar spirit as in DenseNet <ref type="bibr" target="#b5">[6]</ref> and CondenseNet <ref type="bibr" target="#b15">[16]</ref>.</p><p>In DenseNet <ref type="bibr" target="#b5">[6]</ref>, to analyze the feature reuse pattern, the l1-norm of the weights between layers are plotted, as in <ref type="figure" target="#fig_4">Figure 4(a)</ref>. It is clear that the connections between the adjacent layers are stronger than the others. This implies that the dense connection between all layers could introduce redundancy. The recent CondenseNet <ref type="bibr" target="#b15">[16]</ref> also supports the viewpoint.</p><p>In ShuffleNet V2, it is easy to prove that the number of "directly-connected" channels between i-th and (i+j)-th building block is r j c, where r = (1−c )/c. In other words, the amount of feature reuse decays exponentially with the distance between two blocks. Between distant blocks, the feature reuse becomes much weaker. <ref type="figure" target="#fig_4">Figure 4(b)</ref> plots the similar visualization as in (a), for r = 0.5. Note that the pattern in (b) is similar to (a). Thus, the structure of ShuffleNet V2 realizes this type of feature re-use pattern by design. It shares the similar benefit of feature re-use for high accuracy as in DenseNet <ref type="bibr" target="#b5">[6]</ref>, but it is much more efficient as analyzed earlier. This is verified in experiments, <ref type="table" target="#tab_7">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>Our ablation experiments are performed on ImageNet 2012 classification dataset <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Following the common practice <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, all networks in comparison have four levels of computational complexity, i.e. about 40, 140, 300 and 500+ MFLOPs. Such complexity is typical for mobile scenarios. Other hyper-parameters and protocols are exactly the same as ShuffleNet v1 <ref type="bibr" target="#b14">[15]</ref>.</p><p>We compare with following network architectures <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref>:</p><p>-ShuffleNet v1 <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b14">[15]</ref>, a series of group numbers g is compared. It is suggested that the g = 3 has better trade-off between accuracy and speed. This also agrees with our observation. In this work we mainly use g = 3. -MobileNet v2 <ref type="bibr" target="#b13">[14]</ref>. It is better than MobileNet v1 <ref type="bibr" target="#b12">[13]</ref>. For comprehensive comparison, we report accuracy in both original paper <ref type="bibr" target="#b13">[14]</ref> and our reimplemention, as some results in <ref type="bibr" target="#b13">[14]</ref> are not available. -Xception <ref type="bibr" target="#b11">[12]</ref>. The original Xception model <ref type="bibr" target="#b11">[12]</ref> is very large (FLOPs &gt;2G), which is out of our range of comparison. The recent work <ref type="bibr" target="#b33">[34]</ref> proposes a modified light weight Xception structure that shows better trade-offs between accuracy and efficiency. So, we compare with this variant. -DenseNet <ref type="bibr" target="#b5">[6]</ref>. The original work <ref type="bibr" target="#b5">[6]</ref> only reports results of large models (FLOPs &gt;2G). For direct comparison, we reimplement it following the architecture settings in <ref type="table">Table 5</ref>, where the building blocks in Stage 2-4 consist of DenseNet blocks. We adjust the number of channels to meet different target complexities. <ref type="table" target="#tab_7">Table 8</ref> summarizes all the results. We analyze these results from different aspects.</p><p>Accuracy vs. FLOPs. It is clear that the proposed ShuffleNet v2 models outperform all other networks by a large margin 2 , especially under smaller computational budgets. Also, we note that MobileNet v2 performs pooly at 40 MFLOPs level with 224 × 224 image size. This is probably caused by too few channels.</p><p>In contrast, our model do not suffer from this drawback as our efficient design allows using more channels. Also, while both of our model and DenseNet <ref type="bibr" target="#b5">[6]</ref> reuse features, our model is much more efficient, as discussed in Sec. 3. <ref type="table" target="#tab_7">Table 8</ref> also compares our model with other state-of-the-art networks including CondenseNet <ref type="bibr" target="#b15">[16]</ref>, IGCV2 <ref type="bibr" target="#b26">[27]</ref>, and IGCV3 <ref type="bibr" target="#b27">[28]</ref> where appropriate. Our model performs better consistently at various complexity levels.</p><p>Inference Speed vs. FLOPs/Accuracy. For four architectures with good accuracy, ShuffleNet v2, MobileNet v2, ShuffleNet v1 and Xception, we compare their actual speed vs. FLOPs, as shown in <ref type="figure" target="#fig_0">Figure 1(c)(d)</ref>. More results on different resolutions are provided in Appendix <ref type="table" target="#tab_0">Table 1</ref>.</p><p>ShuffleNet v2 is clearly faster than the other three networks, especially on GPU. For example, at 500MFLOPs ShuffleNet v2 is 58% faster than MobileNet v2, 63% faster than ShuffleNet v1 and 25% faster than Xception. On ARM, the speeds of ShuffleNet v1, Xception and ShuffleNet v2 are comparable; however, MobileNet v2 is much slower, especially on smaller FLOPs. We believe this is because MobileNet v2 has higher MAC (see G1 and G4 in Sec. 2), which is significant on mobile devices.</p><p>Compared with MobileNet v1 <ref type="bibr" target="#b12">[13]</ref>, IGCV2 <ref type="bibr" target="#b26">[27]</ref>, and IGCV3 [28], we have two observations. First, although the accuracy of MobileNet v1 is not as good, its speed on GPU is faster than all the counterparts, including ShuffleNet v2. We believe this is because its structure satisfies most of proposed guidelines (e.g. for G3, the fragments of MobileNet v1 are even fewer than ShuffleNet v2). Second, IGCV2 and IGCV3 are slow. This is due to usage of too many convolution groups (4 or 8 in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>). Both observations are consistent with our proposed guidelines.</p><p>Recently, automatic model search <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> has become a promising trend for CNN architecture design. The bottom section in <ref type="table" target="#tab_7">Table 8</ref> evaluates some auto-generated models. We find that their speeds are relatively slow. We believe this is mainly due to the usage of too many fragments (see G3). Nevertheless, this research direction is still promising. Better models may be obtained, for example, if model search algorithms are combined with our proposed guidelines, and the direct metric (speed) is evaluated on the target platform.</p><p>Finally, <ref type="figure" target="#fig_0">Figure 1</ref>(a)(b) summarizes the results of accuracy vs. speed, the direct metric. We conclude that ShuffeNet v2 is best on both GPU and ARM.</p><p>Compatibility with other methods. ShuffeNet v2 can be combined with other techniques to further advance the performance. When equipped with Squeezeand-excitation (SE) module <ref type="bibr" target="#b7">[8]</ref>, the classification accuracy of ShuffleNet v2 is improved by 0.5% at the cost of certain loss in speed. The block structure is illustrated in Appendix <ref type="figure" target="#fig_1">Figure 2(b)</ref>. Results are shown in <ref type="table" target="#tab_7">Table 8</ref> (bottom section).</p><p>Generalization to Large Models. Although our main ablation is performed for light weight scenarios, ShuffleNet v2 can be used for large models (e.g, FLOPs ≥ 2G). <ref type="table">Table 6</ref> compares a 50-layer ShuffleNet v2 (details in Appendix) with the counterpart of ShuffleNet v1 <ref type="bibr" target="#b14">[15]</ref> and ResNet-50 <ref type="bibr" target="#b3">[4]</ref>. ShuffleNet v2 still outperforms ShuffleNet v1 at 2.3GFLOPs and surpasses ResNet-50 with 40% fewer FLOPs.</p><p>For very deep ShuffleNet v2 (e.g. over 100 layers), for the training to converge faster, we slightly modify the basic ShuffleNet v2 unit by adding a residual path (details in Appendix). <ref type="table">Table 6</ref> presents a ShuffleNet v2 model of 164 layers equipped with SE <ref type="bibr" target="#b7">[8]</ref> components (details in Appendix). It obtains superior accuracy over the previous state-of-the-art models <ref type="bibr" target="#b7">[8]</ref> with much fewer FLOPs.</p><p>Object Detection To evaluate the generalization ability, we also tested COCO object detection <ref type="bibr" target="#b37">[38]</ref> task. We use the state-of-the-art light-weight detector -Light-Head RCNN [34] -as our framework and follow the same training and test protocols. Only backbone networks are replaced with ours. Models are pretrained on ImageNet and then finetuned on detection task. For training we use train+val set in COCO except for 5000 images from minival set, and use the minival set to test. The accuracy metric is COCO standard mmAP, i.e. the averaged mAPs at the box IoU thresholds from 0.5 to 0.95.</p><p>ShuffleNet v2 is compared with other three light-weight models: Xception <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref>, ShuffleNet v1 <ref type="bibr" target="#b14">[15]</ref> and MobileNet v2 <ref type="bibr" target="#b13">[14]</ref> on four levels of complexities. Results in <ref type="table">Table 7</ref> show that ShuffleNet v2 performs the best.</p><p>Compared the detection result <ref type="table">(Table 7)</ref> with classification result <ref type="table" target="#tab_7">(Table 8)</ref>, it is interesting that, on classification the accuracy rank is ShuffleNet v2 ≥ MobileNet v2 &gt; ShuffeNet v1 &gt; Xception, while on detection the rank becomes ShuffleNet v2 &gt; Xception ≥ ShuffleNet v1 ≥ MobileNet v2. This reveals that Xception is good on detection task. This is probably due to the larger receptive field of Xception building blocks than the other counterparts (7 vs. 3). Inspired by this, we also enlarge the receptive field of ShuffleNet v2 by introducing an additional 3 × 3 depthwise convolution before the first pointwise convolution in each building block. This variant is denoted as ShuffleNet v2*. With only a few additional FLOPs, it further improves accuracy.</p><p>We also benchmark the runtime time on GPU. For fair comparison the batch size is set to 4 to ensure full GPU utilization. Due to the overheads of data copying (the resolution is as high as 800 × 1200) and other detection-specific operations (like PSRoI Pooling <ref type="bibr" target="#b33">[34]</ref>), the speed gap between different models is smaller than that of classification. Still, ShuffleNet v2 outperforms others, e.g. around 40% faster than ShuffleNet v1 and 16% faster than MobileNet v2.</p><p>Furthermore, the variant ShuffleNet v2* has best accuracy and is still faster than other methods. This motivates a practical question: how to increase the size of receptive field? This is critical for object detection in high-resolution images <ref type="bibr" target="#b38">[39]</ref>. We will study the topic in the future.  <ref type="table">Table 7</ref>: Performance on COCO object detection. The input image size is 800 × 1200. FLOPs row lists the complexity levels at 224 × 224 input size. For GPU speed evaluation, the batch size is 4. We do not test ARM because the PSRoI Pooling operation needed in <ref type="bibr" target="#b33">[34]</ref> is unavailable on ARM currently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose that network architecture design should consider the direct metric such as speed, instead of the indirect metric like FLOPs. We present practical guidelines and a novel architecture, ShuffleNet v2. Comprehensive experiments verify the effectiveness of our new model. We hope this work could inspire future work of network architecture design that is platform aware and more practical.    <ref type="table" target="#tab_1">Table 2</ref>: Architectures for large models. Building blocks are shown in brackets, with the convolution kernel shapes and the numbers of blocks stacked. Downsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2. For ShuffleNet v1-50 and ResNet-50, the bottleneck ratio is set to 1:4. For SE-ShuffleNet v2-164, we add the SE modules right before the residual add-ReLUs (details in Appendix <ref type="figure" target="#fig_1">Figure  2)</ref>; we set the neural numbers in SE modules to the 1/2 of the channel numbers in the corresponding building blocks. See Section 4 for details.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>arXiv:1807.11164v1 [cs.CV] 30 Jul 2018 Measurement of accuracy (ImageNet classification on validation set), speed and FLOPs of four network architectures on two hardware platforms with four different level of computation complexities (see text for details). (a, c) GPU results, batchsize = 8. (b, d) ARM results, batchsize = 1. The best performing algorithm, our proposed ShuffleNet v2, is on the top right region, under all cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Run time decomposition on two representative state-of-the-art network architectures, ShuffeNet v1 [15] (1×, g = 3) and MobileNet v2 [14] (1×).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Building blocks of ShuffleNet v1<ref type="bibr" target="#b14">[15]</ref> and this work. (a): the basic ShuffleNet unit; (b) the ShuffleNet unit for spatial down sampling (2×); (c) our basic unit; (d) our unit for spatial down sampling (2×). DWConv: depthwise convolution. GConv: group convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a)(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Illustration of the patterns in feature reuse for DenseNet [6] and ShuffleNet V2. (a) (courtesy of [6]) the average absolute filter weight of convolutional layers in a model. The color of pixel (s, l) encodes the average l1-norm of weights connecting layer s to l. (b) The color of pixel (s, l) means the number of channels directly connecting block s to block l in ShuffleNet v2. All pixel values are normalized to [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 : 2 :</head><label>12</label><figDesc>Building blocks used in experiments for guideline 3. (a) 1-fragment. (b) 2-fragment-series. (c) 4-fragment-series. (d) 2-fragment-parallel. (e) 4-fragmentparallel. Building blocks of ShuffleNet v2 with SE/residual. (a) ShuffleNet v2 with residual. (b) ShuffleNet v2 with SE. (c) ShuffleNet v2 with SE and residual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Validation experiment for Guideline 1. Four different ratios of number of input/output channels (c1 and c2) are tested, while the total FLOPs under the four ratios is fixed by varying the number of channels. Input image size is 56 × 56.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">GPU (Batches/sec.)</cell><cell></cell><cell cols="2">ARM (Images/sec.)</cell></row><row><cell cols="3">c1:c2 (c1,c2) for ×1 ×1 ×2</cell><cell>×4</cell><cell cols="2">(c1,c2) for ×1 ×1 ×2</cell><cell>×4</cell></row><row><cell>1:1</cell><cell cols="2">(128,128) 1480 723</cell><cell>232</cell><cell>(32,32)</cell><cell>76.2 21.7</cell><cell>5.3</cell></row><row><cell>1:2</cell><cell>(90,180)</cell><cell>1296 586</cell><cell>206</cell><cell>(22,44)</cell><cell>72.9 20.5</cell><cell>5.1</cell></row><row><cell>1:6</cell><cell>(52,312)</cell><cell>876 489</cell><cell>189</cell><cell>(13,78)</cell><cell>69.1 17.9</cell><cell>4.6</cell></row><row><cell>1:12</cell><cell>(36,432)</cell><cell>748 392</cell><cell>163</cell><cell>(9,108)</cell><cell>57.6 15.1</cell><cell>4.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Validation experiment for Guideline 2. Four values of group number g are tested, while the total FLOPs under the four values is fixed by varying the total channel number c. Input image size is 56 × 56.</figDesc><table><row><cell></cell><cell cols="2">GPU (Batches/sec.)</cell><cell></cell><cell cols="2">CPU (Images/sec.)</cell></row><row><cell cols="2">g c for ×1 ×1 ×2</cell><cell>×4</cell><cell cols="2">c for ×1 ×1 ×2</cell><cell>×4</cell></row><row><cell cols="2">1 128 2451 1289</cell><cell>437</cell><cell>64</cell><cell>40.0 10.2</cell><cell>2.3</cell></row><row><cell cols="2">2 180 1725 873</cell><cell>341</cell><cell>90</cell><cell>35.0 9.5</cell><cell>2.2</cell></row><row><cell cols="2">4 256 1026 644</cell><cell>338</cell><cell cols="2">128 32.9 8.7</cell><cell>2.1</cell></row><row><cell>8 360</cell><cell>634 445</cell><cell>230</cell><cell cols="2">180 27.8 7.5</cell><cell>1.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Validation experiment for Guideline 3. c denotes the number of channels for 1-fragment. The channel number in other fragmented structures is adjusted so that the FLOPs is the same as 1-fragment. Input image size is 56 × 56.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">GPU (Batches/sec.) CPU (Images/sec.)</cell></row><row><cell></cell><cell></cell><cell cols="4">c=128 c=256 c=512 c=64 c=128 c=256</cell></row><row><cell>1-fragment</cell><cell></cell><cell cols="3">2446 1274 434 40.2 10.1</cell><cell>2.3</cell></row><row><cell cols="2">2-fragment-series</cell><cell cols="2">1790 909</cell><cell>336 38.6 10.1</cell><cell>2.2</cell></row><row><cell cols="2">4-fragment-series</cell><cell>752</cell><cell>745</cell><cell>349 38.4 10.1</cell><cell>2.3</cell></row><row><cell cols="4">2-fragment-parallel 1537 803</cell><cell>320 33.4 9.1</cell><cell>2.2</cell></row><row><cell cols="3">4-fragment-parallel 691</cell><cell>572</cell><cell>292 35.0 8.4</cell><cell>2.1</cell></row><row><cell></cell><cell></cell><cell cols="4">GPU (Batches/sec.) CPU (Images/sec.)</cell></row><row><cell cols="6">ReLU short-cut c=32 c=64 c=128 c=32 c=64 c=128</cell></row><row><cell>yes</cell><cell>yes</cell><cell cols="3">2427 2066 1436 56.7 16.9</cell><cell>5.0</cell></row><row><cell>yes</cell><cell>no</cell><cell cols="3">2647 2256 1735 61.9 18.8</cell><cell>5.2</cell></row><row><cell>no</cell><cell>yes</cell><cell cols="3">2672 2121 1458 57.3 18.2</cell><cell>5.1</cell></row><row><cell>no</cell><cell>no</cell><cell cols="3">2842 2376 1782 66.3 20.2</cell><cell>5.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>:</head><label></label><figDesc>Overall architecture of ShuffleNet v2, for four different levels of complexities.</figDesc><table><row><cell>Layer</cell><cell cols="4">Output size KSize Stride Repeat</cell><cell cols="4">Output channels 0.5× 1× 1.5× 2×</cell></row><row><cell>Image</cell><cell>224×224</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>Conv1 MaxPool</cell><cell>112×112 56×56</cell><cell>3×3 3×3</cell><cell>2 2</cell><cell>1</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>24</cell></row><row><cell>Stage2</cell><cell>28×28 28×28</cell><cell></cell><cell>2 1</cell><cell>1 3</cell><cell cols="4">48 116 176 244</cell></row><row><cell>Stage3</cell><cell>14×14 14×14</cell><cell></cell><cell>2 1</cell><cell>1 7</cell><cell cols="4">96 232 352 488</cell></row><row><cell>Stage4</cell><cell>7×7 7×7</cell><cell></cell><cell>2 1</cell><cell>1 3</cell><cell cols="4">192 464 704 976</cell></row><row><cell>Conv5</cell><cell>7×7</cell><cell>1×1</cell><cell>1</cell><cell>1</cell><cell cols="4">1024 1024 1024 2048</cell></row><row><cell>GlobalPool</cell><cell>1×1</cell><cell>7×7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">1000 1000 1000 1000</cell></row><row><cell>FLOPs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">41M 146M 299M 591M</cell></row><row><cell># of Weights</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">1.4M 2.3M 3.5M 7.4M</cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>:</head><label></label><figDesc>Results of large models. See text for details.</figDesc><table><row><cell>Model</cell><cell cols="2">FLOPs Top-1 err. (%)</cell></row><row><cell>ShuffleNet v2-50 (ours)</cell><cell>2.3G</cell><cell>22.8</cell></row><row><cell>ShuffleNet v1-50 [15] (our impl.)</cell><cell>2.3G</cell><cell>25.2</cell></row><row><cell>ResNet-50 [4]</cell><cell>3.8G</cell><cell>24.0</cell></row><row><cell cols="2">SE-ShuffleNet v2-164 (ours, with residual) 12.7G</cell><cell>18.56</cell></row><row><cell>SENet [8]</cell><cell>20.7G</cell><cell>18.68</cell></row><row><cell>Table 6</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparison of several network architectures over classification error (on validation set, single center crop) and speed, on two platforms and four levels of computation complexity. Results are grouped by complexity levels for better comparison. The batch size is 8 for GPU and 1 for ARM. The image size is 224 × 224 except: [*] 160 × 160 and [**] 192 × 192.We do not provide speed measurements for CondenseNets<ref type="bibr" target="#b15">[16]</ref> due to lack of efficient implementation currently.</figDesc><table><row><cell>Model</cell><cell>Complexity (MFLOPs)</cell><cell>Top-1 err. (%)</cell><cell>GPU Speed (Batches/sec.)</cell><cell>ARM Speed (Images/sec.)</cell></row><row><cell>ShuffleNet v2 0.5× (ours)</cell><cell>41</cell><cell>39.7</cell><cell>417</cell><cell>57.0</cell></row><row><cell>0.25 MobileNet v1 [13]</cell><cell>41</cell><cell>49.4</cell><cell>502</cell><cell>36.4</cell></row><row><cell>0.4 MobileNet v2 [14] (our impl.) *</cell><cell>43</cell><cell>43.4</cell><cell>333</cell><cell>33.2</cell></row><row><cell>0.15 MobileNet v2 [14] (our impl.)</cell><cell>39</cell><cell>55.1</cell><cell>351</cell><cell>33.6</cell></row><row><cell>ShuffleNet v1 0.5× (g=3) [15]</cell><cell>38</cell><cell>43.2</cell><cell>347</cell><cell>56.8</cell></row><row><cell>DenseNet 0.5× [6] (our impl.)</cell><cell>42</cell><cell>58.6</cell><cell>366</cell><cell>39.7</cell></row><row><cell>Xception 0.5× [12] (our impl.)</cell><cell>40</cell><cell>44.9</cell><cell>384</cell><cell>52.9</cell></row><row><cell>IGCV2-0.25 [27]</cell><cell>46</cell><cell>45.1</cell><cell>183</cell><cell>31.5</cell></row><row><cell>ShuffleNet v2 1× (ours)</cell><cell>146</cell><cell>30.6</cell><cell>341</cell><cell>24.4</cell></row><row><cell>0.5 MobileNet v1 [13]</cell><cell>149</cell><cell>36.3</cell><cell>382</cell><cell>16.5</cell></row><row><cell>0.75 MobileNet v2 [14] (our impl.) **</cell><cell>145</cell><cell>32.1</cell><cell>235</cell><cell>15.9</cell></row><row><cell>0.6 MobileNet v2 [14] (our impl.)</cell><cell>141</cell><cell>33.3</cell><cell>249</cell><cell>14.9</cell></row><row><cell>ShuffleNet v1 1× (g=3) [15]</cell><cell>140</cell><cell>32.6</cell><cell>213</cell><cell>21.8</cell></row><row><cell>DenseNet 1× [6] (our impl.)</cell><cell>142</cell><cell>45.2</cell><cell>279</cell><cell>15.8</cell></row><row><cell>Xception 1× [12] (our impl.)</cell><cell>145</cell><cell>34.1</cell><cell>278</cell><cell>19.5</cell></row><row><cell>IGCV2-0.5 [27]</cell><cell>156</cell><cell>34.5</cell><cell>132</cell><cell>15.5</cell></row><row><cell>IGCV3-D (0.7) [28]</cell><cell>210</cell><cell>31.5</cell><cell>143</cell><cell>11.7</cell></row><row><cell>ShuffleNet v2 1.5× (ours)</cell><cell>299</cell><cell>27.4</cell><cell>255</cell><cell>11.8</cell></row><row><cell>0.75 MobileNet v1 [13]</cell><cell>325</cell><cell>31.6</cell><cell>314</cell><cell>10.6</cell></row><row><cell>1.0 MobileNet v2 [14]</cell><cell>300</cell><cell>28.0</cell><cell>180</cell><cell>8.9</cell></row><row><cell>1.0 MobileNet v2 [14] (our impl.)</cell><cell>301</cell><cell>28.3</cell><cell>180</cell><cell>8.9</cell></row><row><cell>ShuffleNet v1 1.5× (g=3) [15]</cell><cell>292</cell><cell>28.5</cell><cell>164</cell><cell>10.3</cell></row><row><cell>DenseNet 1.5× [6] (our impl.)</cell><cell>295</cell><cell>39.9</cell><cell>274</cell><cell>9.7</cell></row><row><cell>CondenseNet (G=C=8) [16]</cell><cell>274</cell><cell>29.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Xception 1.5× [12] (our impl.)</cell><cell>305</cell><cell>29.4</cell><cell>219</cell><cell>10.5</cell></row><row><cell>IGCV3-D [28]</cell><cell>318</cell><cell>27.8</cell><cell>102</cell><cell>6.3</cell></row><row><cell>ShuffleNet v2 2× (ours)</cell><cell>591</cell><cell>25.1</cell><cell>217</cell><cell>6.7</cell></row><row><cell>1.0 MobileNet v1 [13]</cell><cell>569</cell><cell>29.4</cell><cell>247</cell><cell>6.5</cell></row><row><cell>1.4 MobileNet v2 [14]</cell><cell>585</cell><cell>25.3</cell><cell>137</cell><cell>5.4</cell></row><row><cell>1.4 MobileNet v2 [14] (our impl.)</cell><cell>587</cell><cell>26.7</cell><cell>137</cell><cell>5.4</cell></row><row><cell>ShuffleNet v1 2× (g=3) [15]</cell><cell>524</cell><cell>26.3</cell><cell>133</cell><cell>6.4</cell></row><row><cell>DenseNet 2× [6] (our impl.)</cell><cell>519</cell><cell>34.6</cell><cell>197</cell><cell>6.1</cell></row><row><cell>CondenseNet (G=C=4) [16]</cell><cell>529</cell><cell>26.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Xception 2× [12] (our impl.)</cell><cell>525</cell><cell>27.6</cell><cell>174</cell><cell>6.7</cell></row><row><cell>IGCV2-1.0 [27]</cell><cell>564</cell><cell>29.3</cell><cell>81</cell><cell>4.9</cell></row><row><cell>IGCV3-D (1.4) [28]</cell><cell>610</cell><cell>25.5</cell><cell>82</cell><cell>4.5</cell></row><row><cell>ShuffleNet v2 2x (ours, with SE [8])</cell><cell>597</cell><cell>24.6</cell><cell>161</cell><cell>5.6</cell></row><row><cell>NASNet-A [9] ( 4 @ 1056, our impl.)</cell><cell>564</cell><cell>26.0</cell><cell>130</cell><cell>4.6</cell></row><row><cell>PNASNet-5 [10] (our impl.)</cell><cell>588</cell><cell>25.8</cell><cell>115</cell><cell>4.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">As reported in<ref type="bibr" target="#b13">[14]</ref>, MobileNet v2 of 500+ MFLOPs has comparable accuracy with the counterpart ShuffleNet v2 (25.3% vs. 25.1% top-1 error); however, our reimplemented version is not as good (26.7% error, seeTable 8).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements Thanks Yichen Wei for his help on paper writing. This research is partially supported by National Natural Science Foundation of China (Grant No. 61773229).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<publisher>Cvpr</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<title level="m">Squeeze-and-excitation networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00559</idno>
		<title level="m">Progressive neural architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<title level="m">Regularized evolution for image classifier architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<title level="m">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09224</idno>
		<title level="m">Condensenet: An efficient densenet using learned group convolutions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<title level="m">Speeding up convolutional neural networks with low rank expansions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient and accurate approximations of nonlinear convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1984" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Accelerating very deep convolutional networks for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1943" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">cudnn: Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vaidynathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalamkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06709</idno>
		<title level="m">Distributed deep learning using synchronous stochastic gradient descent</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06489</idno>
		<title level="m">Deep roots: Improving cnn efficiency with hierarchical filter groups</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interleaved group convolutions for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06202</idno>
		<title level="m">Igcv 2: Interleaved structured sparse convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00178</idno>
		<title level="m">Igcv3: Interleaved low-rank group convolutions for efficient deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Light-head r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07264</idno>
	</analytic>
	<monogr>
		<title level="m">defense of two-stage object detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01513</idno>
		<title level="m">Genetic cnn</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01041</idno>
		<title level="m">Large-scale evolution of image classifiers</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02719</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">GPU (Batches/sec</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">GPU (Batches/sec.) CPU (Images/sec</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Appendix Table 1: Table (a) compares the speed of each network</title>
		<imprint/>
	</monogr>
	<note>whole architecture</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Table (b) compares the speed of each network&apos;s unit, we stack 10 network units of each network; the value of c means the number of channels for ShuffleNet v2, we adjust the number of channels to keep the FLOPs unchanged for other network units. Please refer to Section 4 for details</title>
		<imprint/>
	</monogr>
	<note>* ] For the models of 40M FLOPs with input size of 320 × 320, the batchsize is set to 8 to ensure the GPU utilization rate, and we set batchsize = 1 otherwise</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
