<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comparative Review of Recent Kinect-based Action Recognition Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019">2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Du</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
						</author>
						<title level="a" type="main">A Comparative Review of Recent Kinect-based Action Recognition Algorithms</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING</title>
						<imprint>
							<biblScope unit="issue">1</biblScope>
							<date type="published" when="2019">2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Human action recognition</term>
					<term>Kinect-based algo- rithms</term>
					<term>cross-view action recognition</term>
					<term>3D action analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video-based human action recognition is currently one of the most active research areas in computer vision. Various research studies indicate that the performance of action recognition is highly dependent on the type of features being extracted and how the actions are represented. Since the release of the Kinect camera, a large number of Kinect-based human action recognition techniques have been proposed in the literature. However, there still does not exist a thorough comparison of these Kinect-based techniques under the grouping of feature types, such as handcrafted versus deep learning features and depthbased versus skeleton-based features. In this paper, we analyze and compare ten recent Kinect-based algorithms for both crosssubject action recognition and cross-view action recognition using six benchmark datasets. In addition, we have implemented and improved some of these techniques and included their variants in the comparison. Our experiments show that the majority of methods perform better on cross-subject action recognition than cross-view action recognition, that skeleton-based features are more robust for cross-view recognition than depth-based features, and that deep learning features are suitable for large datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H UMAN action recognition has many useful applications such as human computer interaction, smart video surveillance, sports and health care. These applications are one of motivations behind much research work devoted to this area in the last few years. However, even with a large number of research papers found in the literature, many challenging problems, such as different viewpoints, visual appearances, human body sizes, lighting conditions, and speeds of action execution, still affect the performance of these algorithms. Further problems include partial occlusion of the human subjects by other objects in the scene and self-occlusion of human subjects themselves. Among the human action recognition papers presented in the literature, some of the early techniques focus on using conventional RGB videos <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. While these video-based techniques gave promising results, their recognition accuracy is still relatively low, even when the scene is free of clutter.</p><p>The Kinect camera introduced by Microsoft in 2001 was an attempt to broaden the 3D gaming experience of the Xbox 360's audience. However, as the Kinect camera can capture real-time RGB and depth videos, and there is a publicly available toolkit for computing the human skeleton model from each frame of a depth video, many research papers on 3D human action recognition using the Kinect camera have emerged. One advantage of using depth videos than the conventional RGB videos is that it is easier to segment the foreground human subject even when the scene is cluttered. As depth videos do not have colour information, the colour of the clothes worn by the human subject has no effect on the segmentation process. This allows action recognition researchers to focus their effort more on getting robust feature descriptors to describe the actions rather than on low level segmentation. Numerous representative methods for 3D action analysis using depth videos include <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. These methods employ advanced machine learning techniques for which good results have been reported. Of course, depth images are also vulnerable to noise due to various factors <ref type="bibr" target="#b14">[15]</ref>. Thus, using depth images does not always guarantee good action recognition performance <ref type="bibr" target="#b8">[9]</ref>. The algorithm used for computing the 3D joint positions of the human skeletal model by the Kinect toolkit is based on the human skeleton tracking framework (OpenNI) of Shotton et al. <ref type="bibr" target="#b15">[16]</ref>. In addition to the availability of the real-time depth video stream, this tracking framework also opens up the research area of skeleton-based human action recognition <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>.</p><p>Human action recognition methods using the Kinect data can be classified into two categories, based on how the feature descriptors are extracted to represent the human actions. The first category is handcrafted features. Action recognition methods using handcrafted features require two complex handdesign stages, namely feature extraction and feature representation, to build the final descriptor. Both the feature extraction stage and feature representation stage differ from one method to another. The feature extraction stage may involve computing the depth (and/or colour) gradients, histogram, and other more complex transformations of the video data. The feature representation stage may involve simple concatenation of the feature components extracted from the previous stage, a more complex fusion step of these feature components, or even using a machine learning technique, to get the final feature descriptor. These methods usually involve a number of operations that require researchers to carry out careful feature engineering and tuning. Kinect-based human action recognition algorithms using handcrafted features reported in arXiv:1906.09955v1 [cs.CV] 24 Jun 2019 the literature include <ref type="bibr">[9-13, 16, 17, 19, 23, 24]</ref>.</p><p>The second category is deep learning features. With the huge advance in neural network research in the last decade, deep neural networks have been used to extract high-level features from video sequences for many different applications, including 3D human action analysis. Deep learning methods reduce the need for feature engineering; however, they require a huge amount of labelled training data, which may not be available, and a long time to train. For small human action recognition datasets, deep learning methods may not give the best performance. Recent Kinect-based human action recognition algorithms are: <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>. Research contributions. Although both handcrafted and deep learning features have been used in human action recognition, to the best of our knowledge, a thorough comparison of recent action recognition methods for these two categories is not found in the literature. Our contributions in this paper are twofold:</p><p>• We evaluate the performance of 10 recent state-of-art human action recognition algorithms, with specific focus on comparing the effectiveness of using handcrafted features versus deep learning features and skeleton-based features versus depth-based features. We believe that there is a lack of such a comparison in the literature on human action recognition. • Furthermore, we evaluate the cross-view versus crosssubject performance of these algorithms and, for the multiview datasets, the impact of the camera view for both small and large datasets on human action recognition with respect to whether the features being used are depthbased, skeleton-based, or depth+skeleton-based. To the best of our knowledge, such evaluation has not been performed before. The paper is organized as follows. Section II gives a brief review on recent human action recognition techniques. Section III covers the details of the 10 algorithms being compared in this paper. In Section IV, we describe our experimental setting and the benchmark datasets. Sections V and VI summarize our experimental results, comparison, and discussions. The last section concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Action recognition methods can be classified into three categories based on the type of input data: colour-based <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref>, depth-based <ref type="bibr">[9-14, 22, 24, 25, 47, 55-65]</ref>, and skeleton-based <ref type="bibr">[16-24, 28-38, 55, 66-76]</ref>. In this section, we will focus on reviewing recent methods using the last two types of features. Depth-based action recognition. Action recognition from depth videos <ref type="bibr">[9-11, 13, 56-60, 63]</ref> has become more popular because of the availability of real-time cost-effective sensors. Most existing depth-based action recognition methods use global features such as space-time volume and silhouette information. For example, Oreifej and Liu <ref type="bibr" target="#b10">[11]</ref> captured the discriminative features by projecting the 4D surface normals obtained from the depth sequence onto a 4D regular space to build the Histogram of Oriented 4D Normals (HON4D).</p><p>Yang and Tian <ref type="bibr" target="#b59">[60]</ref> extended HON4D by concatenating local neighbouring hypersurface normals from the depth video to jointly characterize local shape and motion information. More precisely, they introduced an adaptive spatio-temporal pyramid to subdivide the depth video into a set of space-time cells for more discriminative features. Xia and Aggarwal <ref type="bibr" target="#b56">[57]</ref> proposed to filter out the noise from the depth sensor so as to get more reliable spatio-temporal interest points for action recognition. Although these methods have achieved impressive performance for frontal action recognition, they are sensitive to changes of viewpoint. One way to alleviate this viewpoint issue is to directly process the pointclouds, as reported in the paper by Rahmani et al. <ref type="bibr" target="#b12">[13]</ref>.</p><p>Apart from the methods mentioned above which use handcrafted features, the use of deep learning features <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref> in human action recognition is on the rise. For example, Wang et al. <ref type="bibr" target="#b60">[61]</ref> used a Hierarchical Depth Motion Maps (HDMMs) to extract the body shape and motion information and then trained a 3-channel deep Convolutional Neural Network (CNN) on the HDMMs for human action recognition. In the following years, Rahmani and Mian <ref type="bibr" target="#b13">[14]</ref> proposed to train a single Human Pose Model (HPM) from real motion capture data to transfer the human pose from different unknown views to a view-invariant feature space, and Zhang et al. <ref type="bibr" target="#b63">[64]</ref> used a multi-stream deep neural networks to jointly learn the semantic relations among action attributes.</p><p>Skeleton-based action recognition. Existing skeleton-based action recognition methods can be grouped into two categories: joint-based methods and body part based methods. Joint-based methods model the positions and motion of the joints (either individual or a combination) using the coordinates of the joints extracted by the OpenNI tracking framework. For instance, a reference joint may be used and the coordinates of other joints are defined relative to the reference joint <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, or the joint orientations may be computed relative to a fixed coordinate system and used to represent the human pose <ref type="bibr" target="#b65">[66]</ref>, etc. For the body part based methods, the human body parts are used to model the human's articulated system. These body parts are usually modelled as rigid cylinders connected by joints. Information such as joint angles <ref type="bibr" target="#b18">[19]</ref>, temporal evolution of body parts <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b70">71]</ref>, and 3D relative geometric relationships between rigid body parts <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref> has all been used to represent the human pose for action recognition.</p><p>The method proposed by Vemulapalli et al. <ref type="bibr" target="#b16">[17]</ref> falls into the body part based category. They represent the relative geometry between a pair of body parts, which may or may not be directly connected by a joint, as a point in SE(3). Thus, a human skeleton is a point of the Lie group SE(3) × · · · × SE(3) where each action corresponds to a unique evolution of such a point in time. The approach of Ke et al. <ref type="bibr" target="#b19">[20]</ref> relies on both body parts and body joints. The human skeleton model was divided into 5 body parts. A specific joint was selected for each body part as the reference joint and the coordinates of other joints were expressed as vectors relative to that reference joint. Various distance measures were computed from these vectors to yield a feature vector for each video frame. The features vectors from all video frames were finally appended together and scaled to form a handcrafted greyscale image descriptor fed into a CNN. Somewhat related approach <ref type="bibr" target="#b76">[77]</ref> uses kernels formed over body joints to obtain feature maps fed into a CNN for simultaneous action recognition and domain adaptation.</p><p>Recent human action recognition papers favour deep learning techniques to perform human action recognition. Apart from the CNN-based approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b76">77]</ref>, Recurrent Neural Networks (RNNs) have also been popular <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref>. Since Long Short-term Memory (LSTM) <ref type="bibr" target="#b77">[78]</ref> can model temporal dependencies as RNNs and even capture the co-occurrences of human joints, LSTM networks have also been a popular choice in human action recognition <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b72">73]</ref>. For instance, Zhu et al. <ref type="bibr" target="#b68">[69]</ref> presented an end-to-end deep LSTM network with a dropout step, Shahroudy et al. <ref type="bibr" target="#b17">[18]</ref> proposed a Part-aware Long Short-term Memory (P-LSTM) network to learn the long-term patterns of the 3D trajectories for each grouped body part, and Liu et al. <ref type="bibr" target="#b35">[36]</ref> introduced the use of trust gates in their spatio-temporal LSTM architecture.</p><p>Action recognition via a combination of skeleton and depth features. Combining skeleton and depth features together helps overcome situations when there are interactions between human subject and other objects or when the actions have very similar motion trajectories. Various action recognition algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b71">72]</ref> that use both depth and skeleton features for robust human action recognition have been reported in recent years. For example, Rahmani et al. <ref type="bibr" target="#b11">[12]</ref> proposed to combine 4 types of local features extracted from both depth images and 3D joint positions to deal with local occlusions and to increase the recognition accuracy. We refer to their method as HDG from hereon. Another example is the approach of Shahroudy et al. <ref type="bibr" target="#b46">[47]</ref> where Local Occupancy Patterns (LOP), HON4D, and skeleton-based features are combined with hierarchical mixed norms which regularize the weights in each modality group of each body part. Recently, Rahmani and Bennamoun <ref type="bibr" target="#b21">[22]</ref> used an end-to-end deep learning model to learn the body part representation from both skeletal and depth images. To improve the performance of the model, they adopted a bilinear compact pooling <ref type="bibr" target="#b78">[79]</ref> layer for the generated depth and skeletal features. Elmadany et al. <ref type="bibr" target="#b71">[72]</ref>, on the other hand, proposed to use canonical correlation analysis to maximize the correlation of features extracted from different sensors. The features investigated in their paper include bag of angles extracted from skeleton data, depth motion map from depth video, and optical flow from RGB video. The subspace shared by all the features was learned and average pooling was used to get the final feature descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ANALYZED AND EVALUATED ALGORITHMS</head><p>We chose ten action recognition algorithms shown in Table I for our comparison and evaluation as they are recent action recognition methods (from 2013 onward) and they use skeleton-based, depth-based, handcrafted, and/or deep learning features. The technical details of these algorithms are summarized below. HON4D. Oreifej and Liu <ref type="bibr" target="#b10">[11]</ref> presented a global feature descriptor that captures the geometry and motion of human action in the 4D space of spatial coordinates, depth and time. To form the HON4D descriptor, the space was quantized using a 600-cell polychoron with 120 vertices. The vectors stretching from the origin to these vertices were used as projection axes to obtain the distribution of normals for each video sequence. To improve the classification performance, random perturbations were added to those projectors. The dimensions of HON4D features <ref type="table" target="#tab_0">(Table I)</ref> vary across different datasets. HDG. In this algorithm <ref type="bibr" target="#b11">[12]</ref>, each depth sequence was firstly divided into small subvolumes; the histograms of depth and depth derivatives were computed for each subvolume. For each skeleton sequence, the torso joint was used as a stable reference joint for computing the histograms of joint position differences. In addition, the variations of each joint movement volume were incorporated into the global feature vector to form spatio-temporal joint features. Two Random Decision Forests (RDFs) were trained in this algorithm, one for feature pruning and one for classification. More details about feature dimensions of HDG and the feature pruning applied by us will be given in Section IV-D. HOPC. Approach <ref type="bibr" target="#b12">[13]</ref> models depth images as 3D pointclouds. The authors used two types of support volume, namely, so-called spatial support volume and spatio-temporal support volume. The HOPC descriptor was extracted from the pointcloud falling inside the support volume around each point, which may be classified as a spatio-temporal Keypoint (STK) if the eigenvalue ratios of the pointcloud around it are larger than some predefined threshold. For each STK, the algorithm further projected eigenvectors onto the axes of the 20 vertices of a regular dodecahedron. The final HOPC descriptor for each STK is a concatenation of 3 small histograms, each of which captures the distribution of an eigenvector of the pointcloud within the support volume. LARP-SO. Vemulapalli and Chellappa <ref type="bibr" target="#b18">[19]</ref> extended their earlier work <ref type="bibr" target="#b16">[17]</ref> by the use of Lie Algebra Relative Pairs via SO(3) for action recognition. We follow the convention adopted in <ref type="bibr" target="#b21">[22]</ref> and name this algorithm as LARP-SO. In this algorithm, the rolling map, which describes how a Riemannian manifold rolls over another one along a smooth rolling curve, was used for 3D action recognition. Each skeleton sequence was firstly represented by the relative 3D rotations between various human body parts, and each action was then modelled as a curve in the Lie Group. Since it is difficult to perform the classification of action curves in a non-Euclidean space, the curves were unwrapped by the logarithm map at a single point while a rolling map was used to reduce distortions. The Fourier Temporal Pyramid (FTP) representation <ref type="bibr" target="#b55">[56]</ref> was used in the algorithm to make the descriptor more robust to noise and less sensitive to temporal misalignments. SCK+DCK. Koniusz et al. <ref type="bibr" target="#b22">[23]</ref> used tensor representations to capture the higher-order relationships between 3D human body joints for action recognition. They applied two different RBF kernels which they referred to as Sequence Compatibility Kernel (SCK) and Dynamics Compatibility Kernel (DCK). The former kernel captures the spatio-temporal compatibility of joints while the latter models the action dynamics of a sequence. An SVM was then trained on linearized feature maps of such kernels for action classification. HPM+TM. Approach <ref type="bibr" target="#b13">[14]</ref> employs a dictionary containing representative human poses from a motion capture database. A deep CNN architecture which is a modification of <ref type="bibr" target="#b44">[45]</ref> was then used to train a view-invariant human pose model. Real depth sequences were passed to the learned model frame-byframe to extract high-level view-invariant features. Similarly to the LARP-SO algorithm above, the FTP was used to capture the temporal structure of the action videos. The final descriptor for each video sequence is a collection of the Fourier coefficients from all the segments. P-LSTM. Approach <ref type="bibr" target="#b17">[18]</ref> proceeds by transforming 3D coordinates of the body joints from the camera coordinate system to the body coordinate system with the origin set at the spine. The 3D coordinates of all other body joints were then scaled based on the distance between the 'hip centre' joint and the 'spine' joint. A P-LSTM model was built by splitting the memory cells from the LSTM model into body part based sub-cells. For each video sequence, the pre-processed human joints were grouped into 5 parts (torso, two hands, and two legs) and the video was divided into 8 equal-sized video segments. Then, for a randomly selected frame per video segment, 3D coordinates of the joints inside each grouped part were concatenated and passed as input to the P-LSTM network to learn common temporal patterns of the parts and combine them into a global representation. Clips+CNN+MTLN. Ke et al. <ref type="bibr" target="#b20">[21]</ref> presented a skeletal representation referred to as clips. The method proceeds by transforming the Cartesian coordinates of human joints (per skeleton sequence) into the cylindrical coordinates to generate 3 clips, with each clip corresponding to one channel of the cylindrical coordinates. To encode the temporal information for the whole video sequence, four stable joints (left shoulder, right shoulder, left hip and right hip) were selected as reference joints to produce 4 coordinate frames. The pre-trained VGG19 network <ref type="bibr" target="#b79">[80]</ref> was used as a feature extractor to learn the long-term spatio-temporal features from intermediate images formed from the 4 coordinate frames. Moreover, approach <ref type="bibr" target="#b20">[21]</ref> also employs the Multi-task Learning Network (MTLN) proposed by <ref type="bibr" target="#b80">[81]</ref> to incorporate the spatial structural information from the CNN features. IndRNN. Li et al. <ref type="bibr" target="#b26">[27]</ref> proposed a new RNN method, an Independently Recurrent Neural Network, for which neurons per layer are independent of each other but they are reused across layers. Finally, multiple IndRNNs were stacked to build a deeper network than the traditional RNN. ST-GCN. The spatio-temporal graph representation for skeleton sequences proposed by Yan et al. <ref type="bibr" target="#b25">[26]</ref> is an extension of Graph Convolutional Networks (GCN) <ref type="bibr" target="#b81">[82]</ref><ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref> tailored to perform human action recognition. Firstly, the spatio-temporal graph is constructed by inserting edges between neighbouring body joints (nodes) of the human body skeleton as well as along the temporal direction. Subsequently, GCN and a classifier are applied to infer dependencies in the graphs (a single graph corresponds to a single action sequence) and perform classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETTING</head><p>To perform experiments, we obtained off-the-shelf codes for HON4D <ref type="bibr" target="#b10">[11]</ref>, HOPC <ref type="bibr" target="#b12">[13]</ref>, LARP-SO <ref type="bibr" target="#b18">[19]</ref>, HPM+TM <ref type="bibr" target="#b13">[14]</ref>, IndRNN <ref type="bibr" target="#b26">[27]</ref> and ST-GCN <ref type="bibr" target="#b25">[26]</ref> from the respective authors' websites. For SCK+DCK <ref type="bibr" target="#b22">[23]</ref>, HDG <ref type="bibr" target="#b11">[12]</ref>, P-LSTM <ref type="bibr" target="#b17">[18]</ref> and Clips+CNN+MTLN <ref type="bibr" target="#b20">[21]</ref>, we used our own Matlab implementations given that codes for these methods are not publicly available. Moreover, we employed ten variants of the HDG <ref type="bibr" target="#b11">[12]</ref> representation so as to evaluate the performance with respect to different combinations of its individual descriptor types. We also implemented the traditional RNN and LSTM as baseline methods, and added four variants of P-LSTM to evaluate the impact of using different numbers of video segments for skeletal representation as well as different numbers of hidden neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Benchmark Datasets</head><p>Listed in <ref type="table" target="#tab_0">Table II</ref> are six benchmark datasets used in our evaluation, each of which is detailed below. MSRAction3D <ref type="bibr" target="#b8">[9]</ref> is one of the earliest action datasets captured with the Kinect depth camera. It contains 20 human sport-related activities such as jogging, golf swing and side boxing. Each action in this dataset was performed 2 or 3 times by 10 people. This dataset is challenging because of high interaction similarities. 3D Action Pairs <ref type="bibr" target="#b10">[11]</ref> contains 6 selected pairs of actions that have very similar motion trajectories, e.g., put on a hat and take off a hat; pick up a box and put down a box; stick a poster and remove a poster. Each action was performed 3 times by 10 people. There are two challenging aspects of this dataset:  (i) the actions in each pair have similar motion trajectories;</p><p>(ii) the object that is interacted by the subject in each video is only present in the RGB-D data but not the skeleton data.</p><p>Cornell Activity Dataset (CAD) <ref type="bibr" target="#b41">[42]</ref> comprises two subdatasets, CAD-60 and CAD-120. Both sub-datasets contain RGB-D and tracked skeleton video sequences of human activities captured by a Kinect sensor. In this paper, only CAD-60 was used in the experiments. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates depth images from the CAD-60 dataset and demonstrates that this dataset exhibits high levels of noise in its depth videos. UWA3D Activity Dataset <ref type="bibr" target="#b57">[58]</ref> contains 30 actions performed by 10 people of various height at different speeds in cluttered scenes. This dataset has high between-class similarity and contains frequent self-occlusions. UWA3D Multiview Activity II <ref type="bibr" target="#b12">[13]</ref> contains 30 actions performed by 9 people in a cluttered environment. In this dataset, the Kinect camera was moved to different positions to capture the actions from 4 different views (see <ref type="figure" target="#fig_1">Fig. 2</ref></p><formula xml:id="formula_0">(a)- (c)): front view (V 1 ), left view (V 2 ), right view (V 3 ), and top view (V 4 )</formula><p>. This dataset is therefore more challenging than the previous four datasets. <ref type="figure" target="#fig_1">Fig. 2</ref>  experiments. For every dataset, we used half of the subjects' data for training and the remaining half for testing. We tested all the possible combinations of subjects for the training and testing splits to obtain the average recognition accuracy of each algorithm. For example, for 10 subjects in the MSRAction3D dataset, <ref type="bibr">10 5</ref> = 252 experiments were carried out. The UWA3D Multiview Activity II dataset was used for cross-view experiments, with two views of the samples being used for training and the remaining views for testing. There were 12 different view combinations in the experiments.</p><formula xml:id="formula_1">Front view ( " ) Left view ( # ) Right view ( $ ) Top view ( % ) (d) (a) (b) (c) 50 - 50 - 50 -</formula><p>The NTU RGB+D dataset was used in both cross-subject and cross-view experiments. Despite indications that this dataset has 80 views of human action recognition, the data samples were grouped according to three camera sets. For cross-view action recognition, we used the video sequences captured by two cameras as our training data and the remaining sequences for testing. A total of 3 different camera combinations were experimented with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Settings</head><p>Below we detail the experimental settings of the algorithms. HON4D. According to <ref type="bibr" target="#b10">[11]</ref>, HON4D has the frame size of 320 × 240 and each video is divided into 4×3×3 (width× height×#frames) spatio-temporal cells. In our evaluations, we used these same settings for all the datasets. HOPC. Rahmani et al. <ref type="bibr" target="#b12">[13]</ref> used different spatial and temporal scales for different datasets. In this paper, a constant temporal scale and spatial scale were used for all the datasets. For the MSRAction3D and 3D Action Pairs datasets, the temporal and spatial scales were set to 2 and 19, respectively. For the remaining datasets, we used 2 for the temporal scale and 140 as the spatial scale. Moreover, we divided each depth video into 6×5×3 spatio-temporal cells (along the X, Y and time axes) to extract features. LARP-SO. The desired number of frames <ref type="bibr" target="#b18">[19]</ref> used for computing skeletal representation varies depending on the datasets used in the experiments. The desired frame numbers for the UWA3D Activity, UWA3D Multiview Activity II, and NTU RGB+D datasets were all set to 100. For the MSRAction3D, 3D Action Pairs datasets, and CAD-60, they were set to 76, 111, and 1,000, respectively. SCK+DCK. We followed the experimental settings described in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b84">85]</ref> for all the datasets and we used authors' newest model which aggregates over subsequences (not just sequences). For SCK, we normalized all human body joints with respect to the hip joints across frames as well as the lengths of all body parts. For DCK, we used the unnormalized body joints, and assumed that the displacements of body joint coordinates across frames captured their temporal evolution. HDG. According to <ref type="bibr" target="#b11">[12]</ref>), the number of used subvolumes has no significant effect to the discriminative features, we divided each video sequence into 10×10×5 subvolumes (along X, Y and time) for computing the histograms of depth as well as the depth gradients. For the joint movement volume features, each joint volume was divided into 1×1×5 cells (along X, Y and time). There are four individual feature representations encapsulated by HDG:</p><p>(i) histogram of depth (hod), (ii) histogram of depth gradients (hodg), (iii) joint position differences (jpd), (iv) joint movement volume features (jmv). We follow <ref type="bibr" target="#b85">[86]</ref> and evaluate the performance of the 10 variants of HDG in our experiments. HPM+TM. We followed <ref type="bibr" target="#b13">[14]</ref> and set the number of Fourier Pyramid levels to 3 and the number of low frequency Fourier coefficients to 4 for all datasets. We used the human pose model trained by Rahmani and Mian <ref type="bibr" target="#b13">[14]</ref> to extract viewinvariant features from each depth sequence. We also compared the recognition accuracies of this algorithm given Average Pooling (AP) versus Temporal Modelling (TM) used for extraction of CNN features. P-LSTM. We applied the same normalization preprocessing step as in <ref type="bibr" target="#b17">[18]</ref> for the skeletal representation. In our experiments, the number of video segments and the number of hidden neurons for 1-layer RNN, 2-layer RNN, 1-layer LSTM, 2-layer LSTM, 1-layer P-LSTM and 2-layer P-LSTM were all set to 8 and 50, respectively. We also evaluated the performance of different numbers of video segments and different numbers of hidden neurons in P-LSTM. The learning rate and the number of epochs in our experiments were set to 0.01 and 300, respectively. Clips+CNN+MTLN. The learning rate was set to 0.001 and the batch size was set to 100 for MTLN. We selected four different experimental settings from <ref type="bibr" target="#b20">[21]</ref> to compare the performance of recognition: Frames+CNN, Clips+CNN+Pooling, Clips+CNN+Concatenation, and Clips+CNN+MTLN.</p><p>IndRNN. We used the Adam optimizer with the initial learning rate 2 × 10 −4 and applied the decay by 10 once the evaluation accuracy did not increase. For cross-subject and cross-view experiments, the dropout rates were set to 0.25 and 0.1, respectively. ST-GCN. For the convolution operations, we used the optimal partition strategy according to the ablation study in <ref type="bibr" target="#b25">[26]</ref>. As different datasets have different numbers of body joints (see <ref type="table" target="#tab_0">Table II</ref>), we reconstructed the spatio-temporal skeleton graphs. For NTU RGB+D dataset, we used the same experimental settings as described in <ref type="bibr" target="#b25">[26]</ref> (e.g., we work with up to two human subjects per sequence). For the remaining 5 datasets, we used a different setting as only one performing subject was present per video. Moreover, we performed extra experiments for IndRNN and ST-GCN: instead of using 3D skeleton sequences as inputs, we used jpd features which redefine the 3D skeleton joint positions by translating them to be centred at the torso (or 'spine') joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Measure</head><p>The recognition accuracy A c of an algorithm for any given action class c is defined as the proportion of correct class c labels returned by the algorithm:</p><p>A c = #correct class c labels / #actual class c labels . <ref type="formula">(1)</ref> To show the recognition accuracies of an algorithm for all the action classes, a confusion matrix is often used. The overall performance of an algorithm on a given dataset is evaluated using the average recognition accuracyĀ defined as:Ā = 1 C C c=1 A c , where C is the total number of action classes in a given dataset.</p><p>To show the overall performance of each algorithm on M datasets, we first rank the performance of each algorithm from 1 to 5 (a lower rank value represents a better performance) based on the recognition accuracy so each algorithm has its own rank value r i given the i th dataset. We then compute the Average Rank (AVRank) as follows:</p><formula xml:id="formula_2">AVRank = 1 M M i=1 r i .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimisation of Hyperparameters for HDG</head><p>There are 3 hyperparameters in the HDG algorithm. The first hyperparameter is the number of subvolumes, which we set to the same value as in <ref type="bibr" target="#b11">[12]</ref>. The second and third hyperparameters, which are the number of trees N trees used in training and the threshold θ used in feature pruning, were optimised during our experiments <ref type="table" target="#tab_0">(Table III)</ref>. As the length of the combined features in the HDG algorithm is large (e.g., the length of the HDG-all features for the MSRAction3D dataset is 13,250), we trained one RDF to select the feature components that have high importance values. This helped to increase the processing speed without compromising the recognition accuracy.</p><p>We evaluated the effect of hyperparameters N trees and θ on the HDG algorithm for different combinations of individual HDG features using the MSRAction3D dataset (for singleview) and the UWA3D Multiview Activity II dataset (for crossview). <ref type="table" target="#tab_0">Table III</ref> shows the optimal values for N trees and θ obtained from the grid search. The corresponding dimensions of different HDG combined features before and after pruning are also indicated. Compared to other individual features of HDG, jpd and jmv are small-sized features, thus when used alone in both datasets, their dimensions are not reduced by much during feature pruning. However, when either or both of them are combined with other individual features in cross-view action recognition, their importance values are significantly higher. This allows a large reduction in feature dimension after pruning (see the last 4 rows of the table). Our experiments in the next section also confirm that skeleton-based features deal better with the view-invariance than depth-based features.</p><p>The optimal values of N trees and θ shown in <ref type="table" target="#tab_0">Table III</ref> were used to prune the HDG features for all datasets. As different datasets have different numbers of body joints (see <ref type="table" target="#tab_0">Table II</ref>), the dimensions of these HDG features after pruning across datasets are not the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>A. MSRAction3D, 3D Action Pairs, CAD-60, and UWA3D Activity Datasets <ref type="table" target="#tab_0">Table IV</ref> summarizes the results for the single-view action recognition on these datasets. The algorithms with the highest recognition accuracy for the handcrafted feature and deep learning feature categories are highlighted in bold.</p><p>Among the methods that use handcrafted features, SCK+DCK outperformed all other methods on all the datasets. The use of RBF kernels to capture higher-order statistics of the data and complexity of action dynamics demonstrates its effectiveness for action recognition. For the 3D Action Pairs and UWA3D Activity datasets, HON4D and HOPC are, respectively, the second top performers. The poorer performance of HDG was due to noise in the depth sequences which affected the HDG-hod and HDG-hodg features, even though the human subjects were successfully segmented. In general, HDG-hodg outperformed HDG-hod, and HDG-jmv outperformed HDG-jpd. We also found that concatenating more individual features in HDG (see <ref type="table" target="#tab_0">Table IV</ref>, row HDG-hod+hodg+jpd to row HDG-all features) helped improve action recognition. We note that our results for HDG are different from those reported in <ref type="bibr" target="#b11">[12]</ref> because we used 1×1×5 = 5 cells [86] instead of 2 × 2 × 5 = 20 cells to store the joint motion features.</p><p>For deep learning methods, the 1-layer P-LSTM method (8 video segments, 50 hidden neurons) outperformed others on the UWA3D Activity dataset. In general, a 1-layer LSTM with more hidden neurons performed better than a 1-layer LSTM with fewer neurons, and P-LSTM performed better than the traditional LSTM and RNN (see the last column in <ref type="table" target="#tab_0">Table IV</ref>). The 2-layer P-LSTM (20 video segments, 50 hidden neurons) achieved the best recognition accuracy on the MSRAction3D dataset and HPM+TM outperformed on the 3D Action Pairs dataset. Comparing the last 5 rows of <ref type="table" target="#tab_0">Table IV</ref> for different variants of 2-layer P-LSTM shows that having more video segments and/or hidden neurons does not guarantee better performance. The reasons are: (i) more video segments have less averaging effect and so it is likely that noisy video frames with unreliable skeletal information would be used for feature representation; (ii) having too many hidden neurons would cause overfitting in the training process.</p><p>Using the jpd features instead of the raw 3D joint coordinates boosts the recognition accuracies for both IndRNN and ST-GCN on almost all the datasets. For the CAD-60 and MSRAction3D datasets, the 4-layer IndRNN and 6-layer IndRNN using jpd features are the top two performers. Compared to using the raw 3D joint coordinates, the improvement due to the use of jpd is 4.56% for IndRNN (6 layers) on the MSRAction3D dataset and 32.97% for IndRNN (4 layers) on the CAD-60 dataset.</p><p>The last column of Table IV computed using Eq. (2) shows that SCK+DCK obtains average rank 1 score, followed closely by the 6-layer IndRNN with jpd with average rank 2 score. The ST-GCN method uses a more complex architecture having 9 layers of spatio-temporal graph convolutional operators, which require a large dataset for training. As all the datasets in <ref type="table" target="#tab_0">Table IV</ref> are quite small, its poor performance (average rank 4.75 score) is not unexpected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. NTU RGB+D Dataset</head><p>Table V summarizes the evaluation results for cross-subject and cross-view action recognition on the NTU RGB+D dataset, where methods are grouped into the handcrafted feature and deep learning feature categories. Top performing methods are highlighted in bold.</p><p>Among the methods using handcrafted features, SCK+DCK performed the best for both cross-subject action recognition and cross-view action recognition. Similarly to results on the four datasets shown in <ref type="table" target="#tab_0">Table IV</ref>, combining more individual features in HDG resulted in higher recognition accuracy for both the cross-subject and cross-view action recognition.</p><p>Among deep learning methods, ST-GCN and the 6-layer IndRNN combined with the jpd features outperformed other deep learning methods in both cross-subject and cross-view action recognition. In particular, with jpd, ST-GCN became the top performer (achieving 83.36%) for cross-subject action recognition and IndRNN (6 layers) achieved the highest accuracy (89.0%) for cross-view action recognition. Compared to using the raw 3D skeleton joint coordinates, using the jpd features helps improve the recognition accuracies of both IndRNN and ST-GCN. For example, with jpd features, both the top-1 and top-5 accuracies of ST-GCN increased by 1.79% and 0.61% in the cross-subject experiment.</p><p>For the other deep learning methods, the recognition accuracies of 2-layer RNN, 2-layer LSTM and 2-layer P-LSTM are higher than those of 1-layer RNN, 1-layer LSTM and 1-layer P-LSTM. Similar to the results in <ref type="table" target="#tab_0">Table IV</ref>, P-LSTM performed better than the traditional LSTM and RNN. HPM+AP and HPM+TM, on the other hand, did not perform so well. The reason is that both of these methods were trained given only 339 representative human poses from a human pose dictionary whereas the 60 action classes of NTU RGB+D dataset include many more human poses of higher complexity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. UWA3D Multiview Activity II Dataset</head><p>The ten algorithms were compared on the UWA3D Multiview Activity II dataset using cross-view action recognition. <ref type="table" target="#tab_0">Table VI</ref> summarizes the results. The top 2 action recognition algorithms are highlighted in bold in each column for the handcrafted feature and deep learning feature categories.</p><p>For the methods using handcrafted features, the HDG-all features performed the best for cross-view action recognition (the last column in <ref type="table" target="#tab_0">Table VI</ref>) followed by other HDG variants that use skeletons and depth. Among skeleton-only methods, HDG-jpd+jmv was the second best performer followed by SCK+DCK, HDG-jmv, and LARP-SO-FTP, which all performed better than depth-based features such as HON4D, HDG-hod, HDG-hodg and HDG-hod+hodg. According to the table, using one or both skeleton-based features (HDGjpd and/or HDG-jmv) in HDG improved its results. For deep learning methods, HPM+TM and HPM+AP achieved the highest results. Although Clips+CNN+MTLN performed better than other 'Clips' variants, it did not perform as good as HPM+TM and HPM+AP, due to the limited number of video samples in the dataset. P-LSTM performed better than the traditional LSTM and RNN. We noticed that stacking more layers for IndRNN or using jpd features for both IndRNN and ST-GCN did not help increase the recognition accuracy. This is due to the lack of representative training videos (compared to the results on the NTU RGB+D dataset in cross-view action recognition). <ref type="figure" target="#fig_2">Fig. 3</ref> shows a confusion matrix for the HDG-all representation on the UWA3D Multiview Activity II dataset when V 3 and V 4 views were used for training and V 1 was used for testing. According the figure, the algorithm was confused by a few actions which have similar motion trajectories. For example, one hand waving is similar to two hand waving and two hand punching; walking is very similar to irregular walking; bending and putting down have very similar appearance; sneezing and coughing are very similar actions.  Multiview Activity II dataset when V3 and V4 were used for training and V1 was used for testing. For each action class along the diagonal, the darker is the colour, the higher is the recognition accuracy. P-LSTM, and SCK+DCK performed better for single-view action recognition but performed slightly worse in crossview action recognition. For cross-subject action recognition, SCK+DCK outperformed all other algorithms; for cross-view action recognition, HDG-all, SCK+DCK and our improved IndRNN with jpd features all performed well (having the same average rank values). <ref type="figure" target="#fig_3">Fig. 4</ref> summarizes the average accuracy of all the algorithms grouped under the handcrafted and deep learning feature categories. On average, we found that the recognition accuracy for cross-view action recognition is lower than crosssubject action recognition using handcrafted features, with the poorest performance from depth-based features. Combining both depth-and skeleton-based features together helps improve cross-view action recognition and gives a similar performance for cross-subject action recognition. Overall, the performance on cross-view recognition is lower than crosssubject recognition, except for the deep learning depth-based feature methods. It should be noted that there are only two methods (HPM+AP and HPM+TM) falling into this category and they performed well on the UWA3D Multiview Activity II dataset and reasonably well on the NTU RGB+D dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSIONS A. Single-view versus cross-view</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Influence of camera views in cross-view evaluation</head><p>To analyze the influence of using different combinations of camera views in training and testing, we classified all the algorithms in <ref type="table" target="#tab_0">Table VI</ref> into 3 feature types: depth-based, skeletonbased, and depth+skeleton-based representations. Scatter plots showing the performance of all the algorithms on these three feature types are given in <ref type="figure" target="#fig_4">Fig. 5</ref>, where the blue dots and red crosses denote, respectively, algorithms using handcrafted features and deep learning features.  According to plots in <ref type="figure" target="#fig_4">Fig. 5</ref> the recognition accuracy was high when the left view V 2 and right view V 3 were used for training and front view V 1 for testing (see axis tick mark V 2 V 3 -V 1 ). This is obvious as the viewing angle of the front view V 1 is between V 2 and V 3 , as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. However, it is surprising that the recognition accuracy was even slightly higher for V 2 V 4 -V 1 and V 3 V 4 -V 1 (see Figs. 5(b) and 5(c)).  main reason for this behavior is that the visual appearances of an action in V 2 and V 3 are different, and it is very difficult to find the same view-invariant features from these views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other combinations of views, such as</head><formula xml:id="formula_3">V 1 V 3 -V 4 , V 1 V 3 -V 2 , and V 1 V 2 -V 4</formula><p>, also led to a lower performance (see the distribution of the blue dots in <ref type="figure" target="#fig_4">Fig. 5(a)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Depth-based features versus skeleton-based features</head><p>Cross-subject action recognition. It seems that for the crosssubject action recognition, skeleton-based features outperformed depth-based features for both the handcrafted and deep learning feature categories ( <ref type="figure" target="#fig_3">Fig. 4(a)</ref>). However, it is surprising that adding the depth-based features to skeleton-based features actually led to a slight decrease in the action recognition accuracy. The main reason is the background clutter and noise (for instance, see <ref type="figure" target="#fig_0">Fig. 1</ref>) in the depth sequences, making the depth-based features less representative for robust action recognition.</p><p>On the other hand, some skeleton-based algorithms such as LARP-SO-FTP and its variants performed well in human action recognition because they only extracted reliable features from those human body joints which have high confidence values. The only exception is the HOPC algorithm which uses depth-based features and which performed better than skeleton-based features such as HDG-jpd+jmv in the crosssubject action recognition (see the average performance in the last column of <ref type="table" target="#tab_0">Table IV</ref>). One must note that the HOPC algorithm is different from other depth-based algorithms as it treats the depth images as a 3D pointcloud. Such an approach allows the HOPC algorithm to estimate the orientations of the local surface patches on the body of the human subject to more robustly deal with viewpoint changes. Cross-view action recognition. In <ref type="figure" target="#fig_4">Fig. 5(b)</ref>, for the crossview action recognition, methods using handcrafted features (blue dots) performed better than those using deep learning features (red crosses) for skeleton-based features. For depthbased features ( <ref type="figure" target="#fig_4">Fig. 5(a)</ref>), no clear conclusion can be drawn for the handcrafted versus deep learning features.</p><p>Comparing the handcrafted features (blue dots) in both Figs. 5(a) and 5(b), we can see that algorithms using skeletonbased features produced better results than depth-based fea-tures. This is evident from <ref type="figure" target="#fig_4">Fig. 5(b)</ref> as the blue dots are well above the red crosses, whereas in <ref type="figure" target="#fig_4">Fig. 5(a)</ref> the blue dots are more spread out and they occupy mainly the lower region of the plot. Our pruning process of the HDG algorithm also proved that skeleton-based features are more robust than the depth-based algorithms in the cross-view action recognition.</p><p>For the plot in <ref type="figure" target="#fig_4">Fig. 5(c)</ref>, the algorithms in our experiments did not use deep learning to compute depth+skeleton features, thus no comparison on the performance between using handcrafted and deep learning features is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Handcrafted features versus deep learning features</head><p>As expected, our experiments confirm that deep learning methods (e.g., Clips+CNN+MTLN, IndRNN, and ST-GCN) performed better on large datasets such as the NTU RGB+D dataset, which has more than 56,000 video sequences, but achieved a lower recognition accuracy on the other smaller datasets. The main reason for this behavior is the reliance of deep learning methods on large amount of data for training to avoid overfitting (in contrast to handcrafted methods). However, most existing action datasets have small numbers of performing subjects and action classes, and very few camera views due to high cost of collecting/annotating data.</p><p>We also found that the performance of most handcrafted methods is highly dependent on the datasets. For example, HON4D and HOPC performed better only on some specific datasets, such as MSRAction3D and 3D Action Pairs, but achieved lower performance on the NTU RGB+D dataset. This means that features that have been handcrafted to work on one dataset may not be transferable to another dataset. Moreover, as handcrafted methods prevent overfitting well, they may lack the capacity to learn from big data. With larger benchmark datasets becoming available to the research community, future research trend is more likely to shift to using deep learning features with efforts being put into tuning their hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. 'Quo Vadis, action recognition?'</head><p>Based on the methods evaluated in this paper, we see the following trends in human action recognition. Handcrafted features progressed from global descriptors (e.g., HON4D</p><p>in 2013) to local descriptors (e.g., HOPC in 2016) and combinations of global and local feature representation (e.g., HDG in 2014). More recent works focus on designing robust 3D human joint-based representations (e.g., LARP-SO-FTP and SCK+DCK in 2016). Moreover, researchers have adopted features extracted from skeleton sequences as they are easier to process and analyze than depth videos. We also note that deep learning representations are evolving from basic neural networks (e.g., traditional RNN and LSTM) to adapted and/or dedicated networks that rely on a pre-trained network (e.g., HPM+AP, HPM+TM, and Clips+CNN+MTLN). Researchers also modify basic neural network models to improve their learning capability for action recognition (e.g., P-LSTM and IndRNN). Recently, new models such as ST-GCN were devised to robustly model the human actions in large datasets. Given the surge of large datasets and powerful GPU machines, the newest methods learn representations in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We have presented, analyzed and compared ten state-ofthe-art algorithms in 3D action recognition on six benchmark datasets. These algorithms cover the use of handcrafted and deep learning features computed from depth and skeleton video sequences. We believe that our comparison results will be useful for future researchers interested in designing robust representations for recognizing human actions from videos captured by the Kinect camera and/or similar action sequence capturing sensors.</p><p>In our study, we found that skeleton-based features are more robust than depth-based features for both cross-subject action recognition and cross-view action recognition. Handcrafted features performed better than deep learning features given smaller datasets. However, deep learning methods achieved very good results if trained on large datasets. While accuracy as high as 90% has been achieved by some algorithms on cross-subject action recognition, the average accuracy on cross-view action recognition is much lower.</p><p>From our literature review and comprehensive experiments, we see that many research papers on human action recognition have already attempted to tackle the challenging issues mentioned in the introduction of the paper. Examples include using a skeleton model or treating depth maps as 3D pointcloud to overcome the changes of viewpoint; using Fourier temporal pyramid or dynamic time warping to deal with different speeds of execution of actions; using depth videos to overcome changes in lighting conditions and eliminate visual appearances that may be irrelevant to the actions performed. While these papers all report promising action recognition results, new and more robust action recognition algorithms are still required, given that there is a pressing demand for using these algorithms in real and new environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Sample depth images from the CAD-60 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>(a) A perspective view of the camera setup in the UWA3D Multiview Activity II dataset. The views V1, V2 and V3 are at the same height. (b) and (c) show the top and side views of the setup. The angles between V1 and V2, between V1 and V3, and between V1 and V4 are all approximately 50 degrees [58]. (d) An example video frame of the depth and skeleton data for the bending action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Confusion matrix for HDG-all features on the UWA3D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The average recognition accuracy (in percentage) of methods using handcrafted and deep learning features for cross-subject and cross-view recognition. Numbers of methods using handcrafted (i) depth-based features: 7; (ii) skeleton-based features: 7; (iii) depth+skeleton-based features: 4. Numbers of methods using deep learning (i) depth-based features: 2; (ii) skeleton-based features: 20; (iii) depth+skeleton-based features: 0 (see Tables IV-VI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Scatter plots showing the performance of cross-view action recognition on the UWA3D Multiview Activity II dataset. The blue dots and red crosses, respectively, represent methods using handcrafted features and deep learning features. On the horizontal axis of each plot, we use the notation ViVj-V k to denote views i and j being used for training and view k being used for testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Ten state-of-the-art action recognition methods evaluated in this paper. STK stands for spatio-temporal keypoint. ‡ The P-LSTM features include 8 video segments, each of which is composed of a number of 3D joints.</figDesc><table><row><cell>Algorithms</cell><cell>Year</cell><cell>Short descriptions</cell><cell>Kinect data used</cell><cell>Feature dimension</cell></row><row><cell>HON4D (Oreifej &amp; Liu) [11]</cell><cell>CVPR 2013</cell><cell>handcrafted (global descriptor)</cell><cell>depth</cell><cell>[17880, 151200]</cell></row><row><cell>HDG (Rahmani et al.) [12]</cell><cell>WACV 2014</cell><cell>handcrafted (local + global descriptor)</cell><cell>depth+skeleton</cell><cell>[1662, 1819]</cell></row><row><cell>LARP-SO (Vemulapalli &amp; Chellappa) [19]</cell><cell>CVPR 2016</cell><cell>handcrafted (Lie Group)</cell><cell>skeleton</cell><cell>3 × 3× #frames</cell></row><row><cell>HOPC (Rahmani et al.) [13]</cell><cell>TPAMI 2016</cell><cell>handcrafted (local descriptor)</cell><cell>depth → pointcloud</cell><cell>depending on #STKs  †</cell></row><row><cell>SCK+DCK (Koniusz et al.) [23]</cell><cell>ECCV 2016</cell><cell>handcrafted (tensor representations)</cell><cell>skeleton</cell><cell>∼ 40k</cell></row><row><cell>P-LSTM (Shahroudy et al.) [18]</cell><cell>CVPR 2016</cell><cell>deep learning (LSTM)</cell><cell>skeleton</cell><cell>#joints ×3 × 8  ‡</cell></row><row><cell>HPM+TM (Rahmani &amp; Mian) [14]</cell><cell>CVPR 2016</cell><cell>deep learning (CNN)</cell><cell>depth</cell><cell>4096</cell></row><row><cell>Clips+CNN+MTLN (Ke et al.) [21]</cell><cell>CVPR 2017</cell><cell>deep learning (pre-trained VGG19, MTLN)</cell><cell>skeleton</cell><cell>7168</cell></row><row><cell>IndRNN (Li et al.) [27]</cell><cell>CVPR 2018</cell><cell>deep learning (independently RNN)</cell><cell>skeleton</cell><cell>512</cell></row><row><cell>ST-GCN (Yan et al.) [26]</cell><cell>AAAI 2018</cell><cell>deep learning (Graph ConvNet)</cell><cell>skeleton</cell><cell>256</cell></row><row><cell>†</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Six publicly available benchmark datasets used in our experiments for 3D action recognition.</figDesc><table><row><cell>Datasets</cell><cell></cell><cell>Year</cell><cell>Classes</cell><cell>Subjects</cell><cell>#Views</cell><cell>#videos</cell><cell>Sensor</cell><cell>Modalities</cell><cell>#joints</cell></row><row><cell>MSRAction3D [9]</cell><cell></cell><cell>2010</cell><cell>20</cell><cell>10</cell><cell>1</cell><cell>567</cell><cell>Kinect v1</cell><cell>Depth + 3DJoints</cell><cell>20</cell></row><row><cell>3D Action Pairs [11]</cell><cell></cell><cell>2013</cell><cell>12</cell><cell>10</cell><cell>1</cell><cell>360</cell><cell>Kinect v1</cell><cell>RGB + Depth + 3DJoints</cell><cell>20</cell></row><row><cell>CAD-60 [42]</cell><cell></cell><cell>2011</cell><cell>14</cell><cell>4</cell><cell>-</cell><cell>68</cell><cell>Kinect v1</cell><cell>RGB + Depth + 3DJoints</cell><cell>15</cell></row><row><cell cols="2">UWA3D Activity Dataset [58]</cell><cell>2014</cell><cell>30</cell><cell>10</cell><cell>1</cell><cell>701</cell><cell>Kinect v1</cell><cell>RGB + Depth + 3DJoints</cell><cell>15</cell></row><row><cell cols="2">UWA3D Multiview Activity II [13]</cell><cell>2015</cell><cell>30</cell><cell>9</cell><cell>4</cell><cell>1070</cell><cell>Kinect v1</cell><cell>RGB + Depth + 3DJoints</cell><cell>15</cell></row><row><cell>NTU RGB+D Dataset [18]</cell><cell></cell><cell>2016</cell><cell>60</cell><cell>40</cell><cell>80</cell><cell>56880</cell><cell>Kinect v2</cell><cell>RGB + Depth + IR + 3DJoints</cell><cell>25</cell></row><row><cell cols="4">(The number of views is not stated in the CAD-60 dataset.)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a) talking (phone)</cell><cell cols="2">(b) writing</cell><cell cols="2">(c) brushing teeth</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(d) talking (couch) (e) relaxing (couch) (f) cooking (stirring)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>(d) shows sample video frames from this dataset. NTU RGB+D Dataset [18] is so far the largest Kinect-based action dataset which contains 56,880 video sequences and over 4 million frames. There are 60 action classes performed by 40 subjects captured from 80 views with 3 Kinect v.2 cameras.</figDesc><table /><note>This dataset has variable sequence lengths for different se- quences and exhibits high intra-class variations. Dataset usage. Below we detail how the above six datasets were used in our experiments. The MSRAction3D, 3D Action Pairs, CAD-60 and UWA3D Activity datasets were used for cross-subject (single-view)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Optimal hyperparameter values and feature dimensions before and after pruning for the HDG combined features.</figDesc><table><row><cell>Combination</cell><cell></cell><cell cols="2">MSRAction3D</cell><cell></cell><cell></cell><cell cols="2">UWA3D Multiview Activity II</cell><cell></cell></row><row><cell>of individual</cell><cell>dimensions</cell><cell>optimal θ</cell><cell>optimal Ntrees</cell><cell>dimensions</cell><cell>dimensions</cell><cell>optimal θ</cell><cell>optimal Ntrees</cell><cell>dimensions</cell></row><row><cell>features</cell><cell>before pruning</cell><cell>×10 −3</cell><cell></cell><cell>after pruning</cell><cell>before pruning</cell><cell>×10 −3</cell><cell></cell><cell>after pruning</cell></row><row><cell>HDG-hod</cell><cell>2,500</cell><cell>1.5</cell><cell>100</cell><cell>1,442</cell><cell>2,500</cell><cell>2.7</cell><cell>60</cell><cell>1,231</cell></row><row><cell>HDG-hodg</cell><cell>10,000</cell><cell>20.9</cell><cell>200</cell><cell>550</cell><cell>10,000</cell><cell>2.4</cell><cell>60</cell><cell>3,891</cell></row><row><cell>HDG-jpd</cell><cell>150</cell><cell>11.5</cell><cell>80</cell><cell>148</cell><cell>150</cell><cell>48.1</cell><cell>120</cell><cell>142</cell></row><row><cell>HDG-jmv</cell><cell>600</cell><cell>3.4</cell><cell>140</cell><cell>571</cell><cell>450</cell><cell>3.1</cell><cell>60</cell><cell>449</cell></row><row><cell>HDG-hod+hodg</cell><cell>12,500</cell><cell>17.0</cell><cell>180</cell><cell>786</cell><cell>12,500</cell><cell>4.7</cell><cell>140</cell><cell>3,987</cell></row><row><cell>HDG-jpd+jmv</cell><cell>750</cell><cell>2.0</cell><cell>80</cell><cell>690</cell><cell>600</cell><cell>6.8</cell><cell>100</cell><cell>581</cell></row><row><cell>HDG-hod+hodg+jpd</cell><cell>12,650</cell><cell>8.2</cell><cell>160</cell><cell>2,221</cell><cell>12,650</cell><cell>29.4</cell><cell>80</cell><cell>239</cell></row><row><cell>HDG-hod+hodg+jmv</cell><cell>13,100</cell><cell>7.4</cell><cell>180</cell><cell>2,189</cell><cell>12,950</cell><cell>25.0</cell><cell>100</cell><cell>135</cell></row><row><cell>HDG-hodg+jpd+jmv</cell><cell>10,750</cell><cell>8.3</cell><cell>180</cell><cell>1,711</cell><cell>10,600</cell><cell>15.2</cell><cell>140</cell><cell>456</cell></row><row><cell>HDG-all features</cell><cell>13,250</cell><cell>13.3</cell><cell>120</cell><cell>1,013</cell><cell>13,100</cell><cell>19.0</cell><cell>100</cell><cell>300</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison of average cross-subject action recognition accuracies (percentage) for the four single-view datasets (i.e., M = 4 in Eq. (2)). Each block of rows shows the performance of one method and its variants. The best algorithm for each dataset is highlighted in bold. The last column of the table shows the average rank of the best performing algorithm in each block. The final rank values are computed using Eq. (2), where top performing methods have smaller rank values. Other poorer performing methods in the same block are not considered for their rank values, so their final ranks are marked as '-'.</figDesc><table><row><cell></cell><cell>Method</cell><cell>MSRAction3D</cell><cell>3D Action Pairs</cell><cell>CAD-60</cell><cell>UWA3D Activity</cell><cell>AVRank</cell></row><row><cell></cell><cell>HON4D [11] (Depth)</cell><cell>82.15</cell><cell>96.00</cell><cell>72.70</cell><cell>48.89</cell><cell>3.25</cell></row><row><cell></cell><cell>HOPC [13] (Depth)</cell><cell>85.49</cell><cell>92.44</cell><cell>47.55</cell><cell>60.58</cell><cell>3.25</cell></row><row><cell></cell><cell>LARP-SO-logarithm map [19] (Skel.)</cell><cell>88.69</cell><cell>92.96</cell><cell>69.12</cell><cell>51.96</cell><cell>-</cell></row><row><cell></cell><cell>LARP-SO-unwrapping while rolling [19] (Skel.)</cell><cell>88.47</cell><cell>94.09</cell><cell>69.12</cell><cell>53.05</cell><cell>-</cell></row><row><cell></cell><cell>LARP-SO-FTP [19] (Skel.)</cell><cell>89.40</cell><cell>94.67</cell><cell>76.96</cell><cell>50.41</cell><cell>2.50</cell></row><row><cell></cell><cell>HDG-hod [86] (Depth)</cell><cell>66.22</cell><cell>81.20</cell><cell>26.47</cell><cell>44.35</cell><cell>-</cell></row><row><cell>Hand-crafted features</cell><cell>HDG-hodg [86] (Depth) HDG-jpd [86] (Skel.) HDG-jmv [86] (Skel.) HDG-hod+hodg [86] (Depth)</cell><cell>70.34 55.54 62.40 71.81</cell><cell>90.98 53.78 84.87 90.96</cell><cell>50.98 46.08 41.18 51.96</cell><cell>54.23 40.88 51.02 55.17</cell><cell>----</cell></row><row><cell></cell><cell>HDG-jpd+jmv [86] (Skel.)</cell><cell>65.57</cell><cell>84.93</cell><cell>49.02</cell><cell>55.57</cell><cell>-</cell></row><row><cell></cell><cell>HDG-hod+hodg+jpd [86] (Depth + Skel.)</cell><cell>72.06</cell><cell>90.72</cell><cell>51.47</cell><cell>56.41</cell><cell>-</cell></row><row><cell></cell><cell>HDG-hod+hodg+jmv [86] (Depth + Skel.)</cell><cell>75.41</cell><cell>92.27</cell><cell>49.51</cell><cell>58.80</cell><cell>-</cell></row><row><cell></cell><cell>HDG-hodg+jpd+jmv [86] (Depth + Skel.)</cell><cell>75.00</cell><cell>92.28</cell><cell>52.94</cell><cell>59.82</cell><cell>-</cell></row><row><cell></cell><cell>HDG-all features [86] (Depth + Skel.)</cell><cell>75.45</cell><cell>92.13</cell><cell>51.96</cell><cell>60.33</cell><cell>4.00</cell></row><row><cell></cell><cell>SCK+DCK [23] (Skel.)</cell><cell>89.47</cell><cell>96.00</cell><cell>89.22</cell><cell>61.52</cell><cell>1.00</cell></row><row><cell></cell><cell>Frames+CNN [21] (Skel.)</cell><cell>60.73</cell><cell>73.71</cell><cell>58.82</cell><cell>46.47</cell><cell>-</cell></row><row><cell></cell><cell>Clips+CNN+Pooling [21] (Skel.)</cell><cell>67.64</cell><cell>74.86</cell><cell>58.82</cell><cell>46.47</cell><cell>-</cell></row><row><cell></cell><cell>Clips+CNN+Concatenation [21] (Skel.)</cell><cell>71.27</cell><cell>78.29</cell><cell>61.76</cell><cell>53.85</cell><cell>-</cell></row><row><cell></cell><cell>Clips+CNN+MTLN [21] (Skel.)</cell><cell>73.82</cell><cell>79.43</cell><cell>67.65</cell><cell>54.81</cell><cell>2.25</cell></row><row><cell></cell><cell>HPM+AP [14] (Depth)</cell><cell>56.73</cell><cell>56.11</cell><cell>44.12</cell><cell>42.32</cell><cell>-</cell></row><row><cell></cell><cell>HPM+TM [14] (Depth)</cell><cell>72.00</cell><cell>98.33</cell><cell>44.12</cell><cell>54.78</cell><cell>2.50</cell></row><row><cell></cell><cell>1-layer RNN [18] (Skel.)</cell><cell>18.02</cell><cell>32.76</cell><cell>54.90</cell><cell>14.27</cell><cell>-</cell></row><row><cell></cell><cell>2-layer RNN [18] (Skel.)</cell><cell>27.80</cell><cell>56.13</cell><cell>54.91</cell><cell>35.36</cell><cell>-</cell></row><row><cell></cell><cell>1-layer LSTM [18] (Skel.)</cell><cell>62.26</cell><cell>67.14</cell><cell>61.77</cell><cell>50.81</cell><cell>-</cell></row><row><cell></cell><cell>2-layer LSTM [18] (Skel.)</cell><cell>65.33</cell><cell>73.72</cell><cell>63.24</cell><cell>46.78</cell><cell>-</cell></row><row><cell></cell><cell>1-layer P-LSTM [18] (Skel.)</cell><cell>70.50</cell><cell>70.86</cell><cell>61.76</cell><cell>55.16</cell><cell>-</cell></row><row><cell>Deep learning features</cell><cell>2-layer P-LSTM [18] (Skel.) Our implementation with modified hyperparam. values: 1-layer LSTM (8 segments, 100 hidden neurons) (Skel.) 2-layer P-LSTM (10 segments, 50 hidden neurons) (Skel.)</cell><cell>69.35 64.75 66.09</cell><cell>72.00 73.14 75.43</cell><cell>67.65 58.82 67.65</cell><cell>50.81 52.58 50.00</cell><cell>2.75 --</cell></row><row><cell></cell><cell>2-layer P-LSTM (10 segments, 100 hidden neurons) (Skel.)</cell><cell>67.43</cell><cell>71.43</cell><cell>54.41</cell><cell>50.32</cell><cell>-</cell></row><row><cell></cell><cell>2-layer P-LSTM (20 segments, 50 hidden neurons) (Skel.)</cell><cell>73.18</cell><cell>71.43</cell><cell>52.94</cell><cell>49.68</cell><cell>-</cell></row><row><cell></cell><cell>2-layer P-LSTM (20 segments, 100 hidden neurons) (Skel.)</cell><cell>70.50</cell><cell>71.43</cell><cell>58.82</cell><cell>53.55</cell><cell>-</cell></row><row><cell></cell><cell>IndRNN (4 layers) [27] (Skel.)</cell><cell>71.50</cell><cell>90.05</cell><cell>51.72</cell><cell>44.63</cell><cell>-</cell></row><row><cell></cell><cell>IndRNN (6 layers) [27] (Skel.)</cell><cell>72.91</cell><cell>89.53</cell><cell>57.03</cell><cell>42.66</cell><cell>-</cell></row><row><cell></cell><cell>Our improved results:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>IndRNN (4 layers, with jpd) (Skel.)</cell><cell>76.34</cell><cell>82.66</cell><cell>84.69</cell><cell>52.09</cell><cell>-</cell></row><row><cell></cell><cell>IndRNN (6 layers, withjpd) (Skel.)</cell><cell>77.47</cell><cell>86.88</cell><cell>80.16</cell><cell>51.34</cell><cell>2.00</cell></row><row><cell></cell><cell>ST-GCN  *  [26] (Skel.)</cell><cell>27.64 (69.09)</cell><cell>20.00 (77.14)</cell><cell>23.53 (70.59)</cell><cell>22.12 (45.83)</cell><cell>-</cell></row><row><cell></cell><cell>Our improved results:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ST-GCN  *  (with jpd) (Skel.)</cell><cell>18.18 (64.00)</cell><cell>54.16 (96.57)</cell><cell>26.47 (67.65)</cell><cell>36.54 (70.51)</cell><cell>4.75</cell></row></table><note>* For ST-GCN, the numbers inside the parentheses denote the top-5 accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Comparison of average recognition accuracies (percentage) for both cross-subject and cross-view action recognition on the NTU RGB+D Dataset.</figDesc><table><row><cell></cell><cell>Method</cell><cell>Cross-subject</cell><cell>Cross-view</cell></row><row><cell></cell><cell>HON4D [11] (Depth)</cell><cell>30.6</cell><cell>7.3</cell></row><row><cell></cell><cell>HOPC [13] (Depth)</cell><cell>40.3</cell><cell>30.6</cell></row><row><cell></cell><cell>LARP-SO-FTP [19] (Skel.)</cell><cell>52.1</cell><cell>53.4</cell></row><row><cell></cell><cell>HDG-hod [86] (Depth)</cell><cell>20.1</cell><cell>13.5</cell></row><row><cell></cell><cell>HDG-hodg [86] (Depth)</cell><cell>23.0</cell><cell>25.2</cell></row><row><cell>Hand-crafted features</cell><cell>HDG-jpd [86] (Skel.) HDG-jmv [86] (Skel.) HDG-hod+hodg [86] (Depth) HDG-jpd+jmv [86] (Skel.)</cell><cell>27.8 38.1 24.6 39.7</cell><cell>35.9 50.0 26.5 51.9</cell></row><row><cell></cell><cell>HDG-hod+hodg+jpd [86] (Depth + Skel.)</cell><cell>29.4</cell><cell>38.8</cell></row><row><cell></cell><cell>HDG-hod+hodg+jmv [86] (Depth + Skel.)</cell><cell>39.0</cell><cell>57.0</cell></row><row><cell></cell><cell>HDG-hodg+jpd+jmv [86] (Depth + Skel.)</cell><cell>41.2</cell><cell>57.2</cell></row><row><cell></cell><cell>HDG-all features [86] (Depth + Skel.)</cell><cell>43.3</cell><cell>58.2</cell></row><row><cell></cell><cell>SCK+DCK [23] (Skel.)</cell><cell>72.8</cell><cell>74.1</cell></row><row><cell></cell><cell>Frames+CNN [21] (Skel.)</cell><cell>75.7</cell><cell>79.6</cell></row><row><cell></cell><cell>Clips+CNN+Concatenation [21] (Skel.)</cell><cell>77.1</cell><cell>81.1</cell></row><row><cell></cell><cell>Clips+CNN+Pooling [21] (Skel.)</cell><cell>76.4</cell><cell>80.5</cell></row><row><cell></cell><cell>Clips+CNN+MTLN [21] (Skel.)</cell><cell>79.6</cell><cell>84.8</cell></row><row><cell></cell><cell>Clips+CNN+MTLN  ‡ [21] (Skel.)</cell><cell>79.54</cell><cell>84.70</cell></row><row><cell></cell><cell>HPM+AP [13] (Depth)</cell><cell>40.2</cell><cell>42.2</cell></row><row><cell></cell><cell>HPM+TM [13] (Depth)</cell><cell>50.1</cell><cell>53.4</cell></row><row><cell></cell><cell>1-layer RNN [18] (Skel.)</cell><cell>56.0</cell><cell>60.2</cell></row><row><cell></cell><cell>2-layer RNN [18] (Skel.)</cell><cell>56.3</cell><cell>64.1</cell></row><row><cell>Deep learning features</cell><cell>1-layer LSTM [18] (Skel.) 2-layer LSTM [18] (Skel.) 1-layer P-LSTM [18] (Skel.) 2-layer P-LSTM [18] (Skel.)</cell><cell>59.1 60.7 62.1 62.9</cell><cell>66.8 67.3 69.4 70.3</cell></row><row><cell></cell><cell>2-layer P-LSTM  ‡ [18] (Skel.)</cell><cell>63.02</cell><cell>70.39</cell></row><row><cell></cell><cell>IndRNN (4 layers) [27] (Skel.)</cell><cell>78.6</cell><cell>83.8</cell></row><row><cell></cell><cell>IndRNN (6 layers) [27] (Skel.)</cell><cell>81.8</cell><cell>88.0</cell></row><row><cell></cell><cell>Our improved results:</cell><cell></cell><cell></cell></row><row><cell></cell><cell>IndRNN (4 layers, with jpd)(Skel.)</cell><cell>79.5</cell><cell>84.5</cell></row><row><cell></cell><cell>IndRNN (6 layers, with jpd)(Skel.)</cell><cell>83.0</cell><cell>89.0</cell></row><row><cell></cell><cell>ST-GCN  *  [26] (Skel.)</cell><cell>81.57 (96.85)</cell><cell>88.76 (98.83)</cell></row><row><cell></cell><cell>Our improved results:</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ST-GCN  *  (with jpd) (Skel.)</cell><cell>83.36 (97.46)</cell><cell>88.84 (98.87)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>‡ Our implementations for reproducing original authors' experiment results.* For ST-GCN, the numbers inside the parentheses denote the top-5 accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table VIIsummarizes the performance of each algorithm for both single-view action recognition and cross-view action recognition. One can observe from the table that some algorithms such as HON4D, LARP-SO-FTP, Clips+CNN+MTLN,</figDesc><table><row><cell>33.3</cell><cell>33.3</cell><cell>33.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>77.8</cell><cell></cell><cell></cell><cell></cell><cell>11.1</cell><cell></cell><cell>11.1</cell></row><row><cell>11.1</cell><cell>88.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>11.1</cell><cell>22.2</cell><cell>33.3</cell><cell></cell><cell>11.1</cell><cell>22.2</cell><cell></cell></row><row><cell></cell><cell></cell><cell>66.7</cell><cell>11.1</cell><cell></cell><cell></cell><cell></cell><cell>11.1</cell><cell>11.1</cell></row><row><cell></cell><cell></cell><cell>66.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>44.4</cell><cell>22.2</cell><cell></cell><cell>22.2</cell><cell>11.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>88.9</cell><cell></cell><cell></cell><cell></cell><cell>11.1</cell></row><row><cell></cell><cell>22.2</cell><cell></cell><cell></cell><cell>66.7</cell><cell></cell><cell>11.1</cell></row><row><cell></cell><cell>11.1</cell><cell></cell><cell></cell><cell></cell><cell>77.8</cell><cell></cell><cell>11.1</cell></row><row><cell></cell><cell></cell><cell>11.1</cell><cell>11.1</cell><cell></cell><cell></cell><cell>66.7</cell><cell>11.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.4</cell><cell>33.3</cell><cell>22.2</cell></row><row><cell></cell><cell></cell><cell>11.1</cell><cell></cell><cell></cell><cell></cell><cell>11.1</cell><cell>77.8</cell></row><row><cell></cell><cell></cell><cell>22.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>11.1</cell><cell>11.1</cell><cell></cell><cell>11.1</cell><cell>66.7</cell></row><row><cell>11.1</cell><cell>11.1</cell><cell></cell><cell>11.1</cell><cell>22.2</cell><cell>11.1</cell><cell>11.1</cell><cell>22.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>11.1</cell><cell></cell><cell></cell><cell></cell><cell>22.2</cell><cell>44.4</cell><cell>11.1</cell><cell>11.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88.9</cell><cell>11.1</cell></row><row><cell>11.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>22.2</cell><cell>66.7</cell></row><row><cell>11.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.8</cell><cell>11.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.1</cell><cell>88.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88.9</cell><cell>11.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100.0</cell></row><row><cell></cell><cell></cell><cell>11.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.1</cell><cell>11.1 66.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50.0</cell><cell>12.5</cell><cell>37.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.1</cell><cell>22.2</cell><cell>66.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.1</cell><cell>88.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>11.1</cell><cell>11.1</cell><cell></cell><cell>11.1</cell><cell>11.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Comparison of average recognition accuracies (percentage) for cross-view action recognition on the UWA3D Multiview Activity II dataset.</figDesc><table><row><cell></cell><cell>Training view</cell><cell cols="2">V1 &amp; V2</cell><cell cols="2">V1 &amp; V3</cell><cell cols="2">V1 &amp; V4</cell><cell cols="2">V2 &amp; V3</cell><cell cols="2">V2 &amp; V4</cell><cell cols="2">V3 &amp; V4</cell><cell>Average</cell></row><row><cell></cell><cell>Testing view</cell><cell>V3</cell><cell>V4</cell><cell>V2</cell><cell>V4</cell><cell>V2</cell><cell>V3</cell><cell>V1</cell><cell>V4</cell><cell>V1</cell><cell>V3</cell><cell>V1</cell><cell>V2</cell></row><row><cell>Hand-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>crafted</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Frames+CNN [21] (Skel.)</cell><cell cols="12">30.4 29.8 27.8 23.8 31.7 27.2 39.2 27.8 31.4 23.2 38.8 28.6</cell><cell>30.0</cell></row><row><cell></cell><cell>Clips+CNN+Pooling [21] (Skel.)</cell><cell cols="12">31.6 33.3 30.6 27.8 35.3 29.6 44.3 31.3 35.3 23.2 43.1 31.0</cell><cell>33.0</cell></row><row><cell></cell><cell>Clips+CNN+Concatenation [21] (Skel.)</cell><cell cols="12">33.2 36.9 32.5 29.8 36.9 31.2 46.3 31.0 39.2 23.6 45.5 31.7</cell><cell>34.8</cell></row><row><cell></cell><cell>Clips+CNN+MTLN [21] (Skel.)</cell><cell cols="12">36.4 38.9 34.1 30.6 37.7 33.2 46.7 31.3 38.8 25.6 49.8 33.7</cell><cell>36.4</cell></row><row><cell></cell><cell>HPM+AP [14] (Depth)</cell><cell>68.3</cell><cell>51.7</cell><cell cols="2">60.2 62.2</cell><cell cols="3">38.7 50.0 58.7</cell><cell>37.8</cell><cell cols="4">70.6 61.6 74.0 55.6</cell><cell>57.5</cell></row><row><cell></cell><cell>HPM+TM [14] (Depth)</cell><cell cols="12">81.7 76.4 74.1 78.7 57.9 69.4 75.8 62.9 81.4 79.9 83.3 73.7</cell><cell>74.6</cell></row><row><cell></cell><cell>1-layer RNN [18] (Skel.)</cell><cell cols="10">11.4 11.1 10.0 14.9 11.9 13.3 11.1 15.2 10.3 12.2</cell><cell>9.8</cell><cell>12.4</cell><cell>12.0</cell></row><row><cell></cell><cell>2-layer RNN [18] (Skel.)</cell><cell cols="12">23.4 21.6 27.3 22.7 26.3 20.3 21.7 19.5 20.7 24.8 27.0 21.5</cell><cell>23.1</cell></row><row><cell></cell><cell>1-layer LSTM [18] (Skel.)</cell><cell cols="12">28.9 12.8 19.2 15.5 20.0 33.1 48.2 16.5 43.3 15.5 38.7 16.4</cell><cell>25.7</cell></row><row><cell>Deep learning features</cell><cell>2-layer LSTM [18] (Skel.) 1-layer P-LSTM [18] (Skel.) 2-layer P-LSTM [18] (Skel.) Our implementation with modified hyperparam. values:</cell><cell cols="12">29.7 16.3 20.4 12.8 26.8 30.5 53.0 11.0 37.0 17.1 47.4 20.0 26.8 23.5 22.4 22.7 24.4 31.3 49.6 20.3 38.3 19.5 45.9 17.6 30.1 24.7 25.2 21.5 24.4 37.4 51.0 22.7 43.1 21.1 47.8 17.2</cell><cell>26.8 28.5 30.5</cell></row><row><cell></cell><cell>2-layer P-LSTM (10 segments, 50 hidden neurons) (Skel.)</cell><cell cols="12">23.5 24.3 22.4 25.5 25.6 30.9 48.6 21.1 41.5 19.5 43.9 15.6</cell><cell>28.5</cell></row><row><cell></cell><cell>2-layer P-LSTM (10 segments, 100 hidden neurons) (Skel.)</cell><cell cols="12">21.1 22.7 24.8 21.1 22.4 33.7 47.8 21.1 45.1 22.0 49.4 17.6</cell><cell>29.1</cell></row><row><cell></cell><cell>2-layer P-LSTM (20 segments, 50 hidden neurons) (Skel.)</cell><cell cols="12">25.2 18.3 24.8 21.5 26.4 30.5 49.0 19.1 41.5 24.4 43.5 14.8</cell><cell>28.3</cell></row><row><cell></cell><cell>2-layer P-LSTM (20 segments, 100 hidden neurons) (Skel.)</cell><cell cols="12">27.6 24.3 24.8 21.9 28.4 32.9 44.3 24.3 44.3 20.3 45.9 15.6</cell><cell>29.6</cell></row><row><cell></cell><cell>IndRNN (4 layers) [27] (Skel.)</cell><cell>34.3</cell><cell>53.8</cell><cell cols="2">35.2 42.5</cell><cell cols="3">39.1 38.9 49.2</cell><cell>42.5</cell><cell cols="4">46.0 27.1 48.6 30.9</cell><cell>40.7</cell></row><row><cell></cell><cell>IndRNN (6 layers) [27] (Skel.)</cell><cell cols="12">30.7 47.2 32.2 36.0 38.8 35.4 44.5 37.9 40.6 23.9 39.2 25.2</cell><cell>36.0</cell></row><row><cell></cell><cell>IndRNN (4 layers, with jpd) (Skel.)</cell><cell cols="12">33.5 40.2 26.9 40.9 30.4 41.1 50.7 36.7 46.1 24.6 49.0 23.0</cell><cell>36.9</cell></row><row><cell></cell><cell>IndRNN (6 layers, with jpd) (Skel.)</cell><cell cols="12">29.4 36.3 23.9 38.2 26.0 36.8 45.4 33.0 41.2 20.5 44.7 18.6</cell><cell>32.8</cell></row><row><cell></cell><cell>ST-GCN [26] (Skel.)</cell><cell cols="12">36.4 26.2 20.6 30.2 33.7 22.4 43.1 26.6 16.9 12.8 26.3 36.5</cell><cell>27.6</cell></row><row><cell></cell><cell>ST-GCN (with jpd) (Skel.)</cell><cell cols="12">29.6 21.8 15.5 19.1 17.1 18.8 35.3 14.3 31.0 14.8 13.3 15.5</cell><cell>20.5</cell></row></table><note>* This result is obtained from [13] for comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII :</head><label>VII</label><figDesc>The average recognition accuracies / [AVRank] of all the ten algorithms for both cross-subject and cross-view action recognition.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">† For cross-subject action recognition, the performance of each algorithm is computed by averaging all the recognition accuracies / rank values on the MSRAction3D, 3D Action Pairs, CAD-60, UWA3D Activity and NTU RGB+D datasets. ‡ For cross-view action recognition, they are computed over the UWA3D Multiview Activity II and NTU RGB+D datasets.One can also notice that for depth-based methods which use handcrafted features, the recognition accuracy dropped the most when the left view V 2 and top view V 4 were used for training, and the right view V 3 was used for testing. The</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the authors of HON4D <ref type="bibr" target="#b10">[11]</ref>, HOPC <ref type="bibr" target="#b12">[13]</ref>, LARP-SO <ref type="bibr" target="#b18">[19]</ref>, HPM+TM <ref type="bibr" target="#b13">[14]</ref>, IndRNN <ref type="bibr" target="#b26">[27]</ref> and ST-GCN <ref type="bibr" target="#b25">[26]</ref> for making their codes publicly available.</p><p>We thank the ROSE Lab of Nanyang Technological University (NTU), Singapore, for making the NTU RGB+D dataset freely accessible. We would like to thank Data61/CSIRO for providing the computing cluster for running our experiments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Recognition of Human Movement Using Temporal Templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="267" />
			<date type="published" when="2001-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Behavior Recognition via Sparse Spatio-Temporal Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Actions as Space-Time Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatio-temporal Shape and Flow Correlation for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Workshop on Visual Surveillance</title>
		<meeting>7th Int. Workshop on Visual Surveillance</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Realistic Human Actions from Movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Human Actions via Information Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Recognising Actions as Clouds of Space-Time Interest Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bregonzio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1948" to="1955" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognizing Realistic Actions from Videos in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action Recognition Based on A Bag of 3D Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">EigenJoints-based Action Recognition Using Naive-Bayes-Nearest-Neighbor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="14" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real Time Action Recognition Using Histograms of Depth Gradients and Random Decision Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="626" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Histogram of Oriented Principal Components for Cross-View Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="2430" to="2443" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3D Action Recognition from Novel Viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Characterizations of Noise in Kinect Depth Images: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mallick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Majumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SEN</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1731" to="1740" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Real-Time Human Pose Recognition in Parts from Single Depth Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="588" to="595" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rolling Rotations for Recognizing Human Actions from 3D Skeletal Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4471" to="4479" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SkeletonNet: Mining Deep Part Features for 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="731" to="735" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A New Representation of Skeleton Sequences for 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning Action Recognition Model From Depth and Skeleton Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5832" to="5841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tensor Representations via Kernel Linearization for Action Recognition from 3D Skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action Recognition From Arbitrary Views Using Transferable Dictionary Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P H</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in TIP</title>
		<imprint>
			<biblScope unit="page" from="4709" to="4723" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning a Deep Model for Human Action Recognition from Novel Viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<biblScope unit="page" from="3482" to="3489" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Global Context-Aware Attention LSTM Networks for 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Learning on Lie Groups for Skeleton-based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6099" to="6108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ensemble Deep Learning for Skeleton-based Action Recognition using Temporal Sliding LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond Joints: Learning Representations From Primitive Geometries for Skeleton-Based Action Recognition and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in TIP</title>
		<imprint>
			<biblScope unit="page" from="4382" to="4394" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in TPAMI</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coding Kendall&apos;s Shape Trajectories for 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tanfous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2840" to="2849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning with Long-Term Dynamics for Skeleton Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<biblScope unit="page" from="2644" to="2651" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hands-On RFID: Wireless Wearables for Detecting Use of Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Fishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ISWC</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">An &apos;Object-Use Fingerprint&apos;: The Use of Electronic Sensors for Human Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Pollack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>UbiComp</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Recognizing Daily Activities with RFID-Based Sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buettner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wetherall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>UbiComp</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Human Activity Detection from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">PAIR</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Unstructured Human Activity Detection from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning Human Activities and Object Affordances from RGB-D Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
	<note type="report_type">IJRR</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Rich Features from RGB-D Images for Object Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic Image Networks for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="3034" to="3042" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multimodal Multipart Learning for Action Recognition in Depth Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2123" to="2129" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Spatiotemporal Multiplier Networks for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="3376" to="3385" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Higher-order Pooling of CNN Features via Kernel Linearization for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV</title>
		<imprint>
			<biblScope unit="page" from="130" to="138" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Generalized Rank Pooling for Activity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3222" to="3231" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page" from="1045" to="1058" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Quo Vadis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Action Recognition? A New Model and the Kinetics Dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Convolutional Two-Stream Network Fusion for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep Bilinear Learning for RGB-D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mining Actionlet Ensemble for Action Recognition with Depth Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1290" to="1297" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2834" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">HOPC: Histogram of Oriented Principal Components of 3D Pointclouds for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="742" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Action Classification with Locality-constrained Linear Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3511" to="3516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Super Normal Vector for Activity Recognition using Depth Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="804" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for Action Recognition Using Depth Map Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Action Recognition From Depth Maps Using Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE T HUM-MACH SYST</publisher>
			<biblScope unit="page" from="498" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Discriminative Human Action Classification using Locality-constrained Linear Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="page" from="62" to="71" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">DAAL: Deep Activation-based Attribute Learning for Action Recognition in Depth Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="page" from="37" to="49" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning and Refining of Privileged Information-based RNNs for Action Recognition from Depth Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3461" to="3470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<title level="m">View Invariant Human Action Recognition Using Histograms of 3D Joints</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Differential Recurrent Neural Networks for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Co-occurrence Feature Learning for Skeleton based Action Recognition using Regularized Deep LSTM Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="816" to="833" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Action Recognition Using Rate-Invariant Analysis of Skeletal Shape Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Information Fusion for Human Action Recognition via Biset/Multiset Globality Locality Preserving Canonical Correlation Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E D</forename><surname>Elmadany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in TIP</title>
		<imprint>
			<biblScope unit="page" from="5275" to="5287" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition with Directed Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">CNN-based Action Recognition and Supervised Domain Adaptation on 3D Body Skeletons via Kernel Feature Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Compact Bilinear Pooling,&quot; CVPR</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Largescale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Multitask Learning, ser. 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Spectral Networks and Deep Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Tensor Representations for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Analysis and Evaluation of Kinect-based Action Recognition Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
		<respStmt>
			<orgName>School of the Computer Science and Software Engineering, The University of Western Australia</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
