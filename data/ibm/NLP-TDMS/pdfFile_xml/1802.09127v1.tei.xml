<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2018 DEEP BAYESIAN BANDITS SHOWDOWN AN EMPIRICAL COMPARISON OF BAYESIAN DEEP NETWORKS FOR THOMPSON SAMPLING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Tucker</forename><surname>Google Brain</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
							<email>jsnoek@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2018 DEEP BAYESIAN BANDITS SHOWDOWN AN EMPIRICAL COMPARISON OF BAYESIAN DEEP NETWORKS FOR THOMPSON SAMPLING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting. * Google AI Resident</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent advances in reinforcement learning have sparked renewed interest in sequential decision making with deep neural networks. Neural networks have proven to be powerful and flexible function approximators, allowing one to learn mappings directly from complex states (e.g., pixels) to estimates of expected return. While such models can be accurate on data they have been trained on, quantifying model uncertainty on new data remains challenging. However, having an understanding of what is not yet known or well understood is critical to some central tasks of machine intelligence, such as effective exploration for decision making.</p><p>A fundamental aspect of sequential decision making is the exploration-exploitation dilemma: in order to maximize cumulative reward, agents need to trade-off what is expected to be best at the moment, (i.e., exploitation), with potentially sub-optimal exploratory actions. Solving this trade-off in an efficient manner to maximize cumulative reward is a significant challenge as it requires uncertainty estimates. Furthermore, exploratory actions should be coordinated throughout the entire decision making process, known as deep exploration, rather than performed independently at each state. <ref type="bibr">Thompson Sampling (Thompson, 1933)</ref> and its extension to reinforcement learning, known as Posterior Sampling, provide an elegant approach that tackles the exploration-exploitation dilemma by maintaining a posterior over models and choosing actions in proportion to the probability that they are optimal. Unfortunately, maintaining such a posterior is intractable for all but the simplest models. As such, significant effort has been dedicated to approximate Bayesian methods for deep neural networks. These range from variational methods <ref type="bibr" target="#b17">(Graves, 2011;</ref><ref type="bibr" target="#b6">Blundell et al., 2015;</ref><ref type="bibr" target="#b23">Kingma et al., 2015)</ref> to stochastic minibatch Markov Chain Monte Carlo <ref type="bibr" target="#b32">(Neal, 1994;</ref><ref type="bibr" target="#b50">Welling &amp; Teh, 2011;</ref><ref type="bibr" target="#b25">Li et al., 2016;</ref><ref type="bibr" target="#b1">Ahn et al., 2012;</ref><ref type="bibr" target="#b26">Mandt et al., 2016)</ref>, among others. Because the exact posterior is intractable, evaluating these approaches is hard. Furthermore, these methods are rarely compared on benchmarks that measure the quality of their estimates of uncertainty for downstream tasks.</p><p>To address this challenge, we develop a benchmark for exploration methods using deep neural networks. We compare a variety of well-established and recent Bayesian approximations under the lens of Thompson Sampling for contextual bandits, a classical task in sequential decision making. All code and implementations to reproduce the experiments will be available open-source, to provide a reproducible benchmark for future <ref type="bibr">development. 1</ref> Exploration in the context of reinforcement learning is a highly active area of research. Simple strategies such as epsilon-greedy remain extremely competitive <ref type="bibr" target="#b31">(Mnih et al., 2015;</ref><ref type="bibr" target="#b42">Schaul et al., 2016)</ref>. However, a number of promising techniques have recently emerged that encourage exploration though carefully adding random noise to the parameters <ref type="bibr" target="#b37">(Plappert et al., 2017;</ref><ref type="bibr" target="#b12">Fortunato et al., 2017;</ref><ref type="bibr" target="#b13">Gal &amp; Ghahramani, 2016)</ref> or bootstrap sampling <ref type="bibr" target="#b34">(Osband et al., 2016)</ref> before making decisions. These methods rely explicitly or implicitly on posterior sampling for exploration.</p><p>In this paper, we investigate how different posterior approximations affect the performance of Thompson Sampling from an empirical standpoint. For simplicity, we restrict ourselves to one of the most basic sequential decision making scenarios: that of contextual bandits.</p><p>No single algorithm bested the others in every bandit problem, however, we observed some general trends. We found that dropout, injecting random noise, and bootstrapping did provide a strong boost in performance on some tasks, but was not able to solve challenging synthetic exploration tasks. Other algorithms, like Variational Inference, Black Box α-divergence, and minibatch Markov Chain Monte Carlo approaches, strongly couple their complex representation and uncertainty estimates. This proves problematic when decisions are made based on partial optimization of both, as online scenarios usually require. On the other hand, making decisions according to a Bayesian linear regression on the representation provided by the last layer of a deep network offers a robust and easy-to-tune approach. It would be interesting to try this approach on more complex reinforcement learning domains.</p><p>In Section 2 we discuss Thompson Sampling, and present the contextual bandit problem. The different algorithmic approaches that approximate the posterior distribution fed to Thompson Sampling are introduced in Section 3, while the linear case is described in Section 4. The main experimental results are presented in Section 5, and discussed in Section 6. Finally, Section 7 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DECISION-MAKING VIA THOMPSON SAMPLING</head><p>The contextual bandit problem works as follows. At time t = 1, . . . , n a new context X t ∈ R d arrives and is presented to algorithm A. The algorithm -based on its internal model and X t -selects one of the k available actions, a t . Some reward r t = r t (X t , a t ) is then generated and returned to the algorithm, that may update its internal model with the new data. At the end of the process, the reward for the algorithm is given by r = n t=1 r t , and cumulative regret is defined as R A = E[r * − r], where r * is the cumulative reward of the optimal policy (i.e., the policy that always selects the action with highest expected reward given the context). The goal is to minimize R A .</p><p>The main research question we address in this paper is how approximated model posteriors affect the performance of decision making via Thompson Sampling (Algorithm 1) in contextual bandits. We study a variety of algorithmic approaches to approximate a posterior distribution, together with different empirical and synthetic data problems that highlight several aspects of decision making. We consider distributions π over the space of parameters that completely define a problem instance θ ∈ Θ. For example, θ could encode the reward distributions of a set of arms in the multi-armed bandit scenario, or -more generally-all the parameters of an MDP in reinforcement learning.</p><p>Thompson Sampling is a classic algorithm <ref type="bibr" target="#b47">(Thompson, 1933)</ref> which requires only that one can sample from the posterior distribution over plausible problem instances (for example, values or rewards). At each round, it draws a sample and takes a greedy action under the optimal policy for the sample. The posterior distribution is then updated after the result of the action is observed. Thompson Sampling has been shown to be extremely effective for bandit problems both in practice <ref type="bibr" target="#b9">(Chapelle &amp; Li, 2011;</ref><ref type="bibr" target="#b16">Granmo, 2010)</ref> and theory <ref type="bibr" target="#b0">(Agrawal &amp; Goyal, 2012)</ref>. It is especially appealing for deep neural networks as one rarely has access to the full posterior but can often approximately sample from it.</p><p>Algorithm 1 Thompson Sampling 1: Input: Prior distribution over models, π 0 : θ ∈ Θ → [0, 1]. 2: for time t = 0, . . . , N do 3:</p><p>Observe context X t ∈ R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Sample model θ t ∼ π t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Compute a t = BestAction(X t , θ t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Select action a t and observe reward r t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Update posterior distribution π t+1 with (X t , a t , r t ).</p><p>In the following sections we rely on the idea that, if we had access to the actual posterior π t given the observed data at all times t, then choosing actions using Thompson Sampling would lead to near-optimal cumulative regret or, more informally, to good performance. It is important to remark that in some problems this is not necessarily the case; for example, when actions that have no chance of being optimal still convey useful information about other actions. <ref type="bibr">Thompson Sampling (or UCB approaches)</ref> would never select such actions, even if they are worth their cost <ref type="bibr" target="#b40">(Russo &amp; Van Roy, 2014)</ref>. In addition, Thompson Sampling does not take into account the time horizon where the process ends, and if known, exploration efforts should be tuned accordingly <ref type="bibr" target="#b41">(Russo et al., 2017)</ref>. Nonetheless, under the assumption that very accurate posterior approximations lead to efficient decisions, the question is: what happens when the approximations are not so accurate? In some cases, the mismatch in posteriors may not hurt in terms of decision making, and we will still end up with good decisions. Unfortunately, in other cases, this mismatch together with its induced feedback loop will degenerate in a significant loss of performance. We would like to understand the main aspects that determine which way it goes. This is an important practical question as, in large and complex systems, computational sacrifices and statistical assumptions are made to favor simplicity and tractability. But, what is their impact?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHMS</head><p>In this section, we describe the different algorithmic design principles that we considered in our simulations of Section 5. These algorithms include linear methods, Neural Linear and Neural Greedy, variational inference, expectation-propagation, dropout, Monte Carlo methods, bootstrapping, direct noise injection, and Gaussian Processes. In <ref type="figure" target="#fig_5">Figure 6</ref> in the appendix, we visualize the posteriors of the nonlinear algorithms on a synthetic one dimensional problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Methods</head><p>We apply well-known closed-form updates for Bayesian linear regression for exact posterior inference in linear models <ref type="bibr" target="#b5">(Bishop, 2006)</ref>. We provide the specific formulas below, and note that they admit a computationally-efficient online version. We consider exact linear posteriors as a baseline; i.e., these formulas compute the posterior when the data was generated according to Y = X T β + where ∼ N (0, σ 2 ), and Y represents the reward. Importantly, we model the joint distribution of β and σ 2 for each action. Sequentially estimating the noise level σ 2 for each action allows the algorithm to adaptively improve its understanding of the volume of the hyperellipsoid of plausible β's; in general, this leads to a more aggressive initial exploration phase (in both β and σ 2 ).</p><p>The posterior at time t for action i, after observing X, Y , is π t (β, σ 2 ) = π t (β | σ 2 ) π t (σ 2 ), where we assume σ 2 ∼ IG(a t , b t ), and β | σ 2 ∼ N (µ t , σ 2 Σ t ), an Inverse Gamma and Gaussian distribution, respectively. Their parameters are given by</p><formula xml:id="formula_0">Σ t = X T X + Λ 0 −1 , µ t = Σ t Λ 0 µ 0 + X T Y ,<label>(1)</label></formula><formula xml:id="formula_1">a t = a 0 + t/2, b t = b 0 + 1 2 Y T Y + µ T 0 Σ 0 µ 0 − µ T t Σ −1 t µ t .<label>(2)</label></formula><p>We set the prior hyperparameters to µ 0 = 0, and Λ 0 = λ Id, while a 0 = b 0 = η &gt; 1. It follows that initially, for σ 2 0 ∼ IG(η, η), we have the prior β | σ 2 0 ∼ N (0, σ 2 0 /λ Id), where E[σ 2 0 ] = η/(η − 1). Note that we independently model and regress each action's parameters, β i , σ 2 i for i = 1, . . . , k. We consider two approximations to (1) motivated by function approximators where d is large. While posterior distributions or confidence ellipsoids should capture dependencies across parameters as shown above (say, a dense Σ t ), in practice, computing the correlations across all pairs of parameters is too expensive, and diagonal covariance approximations are common. For linear models it may still be feasible to exactly compute (1), whereas in the case of Bayesian neural networks, unfortunately, this may no longer be possible. Accordingly, we study two linear approximations where Σ t is diagonal. Our goal is to understand the impact of such approximations in the simplest case, to properly set our expectations for the loss in performance of equivalent approximations in more complex approaches, like mean-field variational inference or Stochastic Gradient Langevin Dynamics.</p><p>Assume for simplicity the noise standard deviation is known. In <ref type="figure" target="#fig_2">Figure 2a</ref>, for d = 2, we see the posterior distribution β t ∼ N (µ t , Σ t ) of a linear model based on (1), in green, together with two diagonal approximations. Each approximation tries to minimize a different objective. In blue, the PrecisionDiag posterior approximation finds the diagonalΣ ∈ R d×d minimizing KL(N (µ t ,Σ) || N (µ t , Σ t )), like in mean-field variational inference. In particular,Σ = Diag(Σ −1 t ) −1 . On the other hand, in orange, the Diag posterior approximation finds the diagonal matrixΣ minimizing KL(N (µ t , Σ t ) || N (µ t ,Σ)) instead. In this case, the solution is simplyΣ = Diag(Σ t ).</p><p>We add linear baselines that do not model the uncertainty in the action noise σ 2 . In addition, we also consider simple greedy and epsilon greedy linear baselines (i.e., not based on Thompson Sampling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Linear</head><p>The main problem linear algorithms face is their lack of representational power, which they complement with accurate uncertainty estimates. A natural attempt at getting the best of both worlds consists in performing a Bayesian linear regression on top of the representation of the last layer of a neural network, similarly to <ref type="bibr" target="#b45">Snoek et al. (2015)</ref>. The predicted value v i for each action a i is given by</p><formula xml:id="formula_2">v i = β T i z x ,</formula><p>where z x is the output of the last hidden layer of the network for context x. While linear methods directly try to regress values v on x, we can independently train a deep net to learn a representation z, and then use a Bayesian linear regression to regress v on z, obtain uncertainty estimates on the β's, and make decisions accordingly via Thompson Sampling. Note that we do not explicitly consider the weights of the linear output layer of the network to make decisions; further, the network is only used to find good representations z. In addition, we can update the network and the linear regression at different time-scales. It makes sense to keep an exact linear regression (as in <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref>) at all times, adding each new data point as soon as it arrives. However, we only update the network after a number of points have been collected. In our experiments, after updating the network, we perform a forward pass on all the training data to obtain z x , which is then fed to the Bayesian regression. In practice this may be too expensive, and z could be updated periodically with online updates on the regression. We call this algorithm Neural Linear.</p><p>Neural Greedy We refer to the algorithm that simply trains a neural network and acts greedily (i.e., takes the action whose predicted score for the current context is highest) as RMS, as we train it using the RMSProp optimizer. This is our non-linear baseline, and we tested several versions of it (based on whether the training step was decayed, reset to its initial value for each re-training or not, and how long the network was trained for). We also tried the -greedy version of the algorithm, where a random action was selected with probability for some decaying schedule of .</p><p>Variational Inference Variational approaches approximate the posterior by finding a distribution within a tractable family that minimizes the KL divergence to the posterior <ref type="bibr" target="#b20">(Hinton &amp; Van Camp, 1993)</ref>. These approaches formulate and solve an optimization problem, as opposed, for example, to sampling methods like MCMC <ref type="bibr" target="#b22">(Jordan et al., 1999;</ref><ref type="bibr" target="#b49">Wainwright et al., 2008)</ref>. Typically (and in our experiments), the posterior is approximated by a mean-field or factorized distribution where strong independence assumptions are made. For instance, each neural network weight can be modeled via a -conditionally independent-Gaussian distribution whose mean and variance are estimated from data. Recent advances have scaled these approaches to estimate the posterior of neural networks with millions of parameters <ref type="bibr" target="#b6">(Blundell et al., 2015)</ref>. A common criticism of variational inference is that it underestimates uncertainty (e.g., <ref type="bibr" target="#b5">(Bishop, 2006)</ref>), which could lead to under-exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expectation-Propagation</head><p>The family of expectation-propagation algorithms <ref type="bibr" target="#b33">(Opper &amp; Winther, 2000;</ref><ref type="bibr" target="#b30">Minka, 2001b;</ref><ref type="bibr">a)</ref> is based on the message passing framework <ref type="bibr" target="#b36">(Pearl, 1986)</ref>. They iteratively approximate the posterior by updating a single approximation factor (or site) at a time, which usually corresponds to the likelihood of one data point. The algorithm sequentially minimizes a set of local KL divergences, one for each site. Most often, and for computational reasons, likelihoods are chosen to lie in the exponential family. In this case, the minimization corresponds to moment matching. See <ref type="bibr" target="#b14">Gelman et al. (2014)</ref> for further details. We focus on methods that directly optimize the global EP objective via stochastic gradient descent, as, for instance, Power EP <ref type="bibr" target="#b28">(Minka, 2004)</ref>. In particular, in this work, we implement the black-box α-divergence minimization algorithm <ref type="bibr" target="#b19">(Hernández-Lobato et al., 2016)</ref>, where local parameter sharing is applied to the Power EP energy function. Note that different values of α ∈ R\{0} correspond to common algorithms: α = 1 to EP, and α → 0 to Variational Bayes. The optimal α value is problem-dependent <ref type="bibr" target="#b19">(Hernández-Lobato et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dropout</head><p>Dropout is a training technique where the output of each neuron is independently zeroed out with probability p at each forward pass <ref type="bibr" target="#b46">(Srivastava et al., 2014)</ref>. Once the network has been trained, dropout can still be used to obtain a distribution of predictions for a specific input. Following the best action with respect to the random dropout prediction can be interpreted as an implicit form of Thompson sampling. Dropout can be seen as optimizing a variational objective <ref type="bibr" target="#b23">(Kingma et al., 2015;</ref><ref type="bibr" target="#b13">Gal &amp; Ghahramani, 2016;</ref><ref type="bibr" target="#b21">Hron et al., 2017)</ref>.</p><p>Monte Carlo Monte Carlo sampling remains one of the simplest and reliable tools in the Bayesian toolbox. Rather than parameterizing the full posterior, Monte Carlo methods estimate the posterior through drawing samples. This is naturally appealing for highly parameterized deep neural networks for which the posterior is intractable in general and even simple approximations such as multivariate Gaussian are too expensive (i.e. require computing and inverting a covariance matrix over all parameters). Among Monte Carlo methods, Hamiltonian Monte Carlo <ref type="bibr" target="#b32">(Neal, 1994</ref>) (HMC) is often regarded as a gold standard algorithm for neural networks as it takes advantage of gradient information and momentum to more effectively draw samples. However, it remains unfeasible for larger datasets as it involves a Metropolis accept-reject step that requires computing the log likelihood over the whole data set. A variety of methods have been developed to approximate HMC using mini-batch stochastic gradients. These Stochastic Gradient Langevin Dynamics (SGLD) methods <ref type="bibr" target="#b32">(Neal, 1994;</ref><ref type="bibr" target="#b50">Welling &amp; Teh, 2011)</ref> add Gaussian noise to the model gradients during stochastic gradient updates in such a manner that each update results in an approximate sample from the posterior. Different strategies have been developed for augmenting the gradients and noise according to a preconditioning matrix. <ref type="bibr" target="#b25">Li et al. (2016)</ref> show that a preconditioner based on the RMSprop algorithm performs well on deep neural networks. <ref type="bibr" target="#b35">Patterson &amp; Teh (2013)</ref> suggested using the Fisher information matrix as a preconditioner in SGLD. Unfortunately the approximations of SGLD hold only if the learning rate is asymptotically annealed to zero. <ref type="bibr" target="#b1">Ahn et al. (2012)</ref> introduced Stochastic Gradient Fisher Scoring to elegantly remove this requirement by preconditioning according to the Fisher information (or a diagonal approximation thereof). <ref type="bibr" target="#b26">Mandt et al. (2016)</ref> develop methods for approximately sampling from the posterior using a constant learning rate in stochastic gradient descent and develop a prescription for a stable version of SGFS. We evaluate the diagonal-SGFS and constant-SGD algorithms from <ref type="bibr" target="#b26">Mandt et al. (2016)</ref> in this work. Specifically for constant-SGD we use a constant learning rate for stochastic gradient descent, where the learning rate is given by = 2 S N BB T where S is the batch size, N the number of data points and BB T is an online average of the diagonal empirical Fisher information matrix. For Stochastic Gradient Fisher Scoring we use the following stochastic gradient update for the model parameters θ at step t:</p><formula xml:id="formula_3">θ t+1 = θ t − H g(θ t ) + √ H E ν, ν ∼ N (0, I)<label>(3)</label></formula><p>where we take the noise covariance EE T to also be BB T and H = 2 N ( BB T + EE T ) −1 . Bootstrap A simple empirical approach to approximate the sampling distribution of any estimator is the Bootstrap <ref type="bibr" target="#b10">(Efron, 1982)</ref>. The main idea is to simultaneously train q models, where each model i is based on a different dataset D i . When all the data D is available in advance, D i is typically created by sampling |D| elements from D at random with replacement. In our case, however, the data grows one example at a time. Accordingly, we set a parameter p ∈ (0, 1], and append the new datapoint to each D i independently at random with probability p. In order to emulate Thompson Sampling, we sample a model uniformly at random (i.e., with probability p i = 1/q.) and take the action predicted to be best by the sampled model. We mainly tested cases q = 5, 10 and p = 0.8, 1.0, with neural network models. Note that even when p = 1 and the datasets are identical, the random initialization of each network, together with the randomness from SGD, lead to different predictions.</p><p>Direct Noise Injection Parameter-Noise <ref type="bibr" target="#b37">(Plappert et al., 2017)</ref> is a recently proposed approach for exploration in deep RL that has shown promising results. The training updates for the network are unchanged, but when selecting actions, the network weights θ are perturbed with isotropic Gaussian noise. Crucially, the network uses layer normalization <ref type="bibr" target="#b3">(Ba et al., 2016)</ref>, which ensures that all weights are on the same scale. The magnitude of the Gaussian noise is adjusted so that the overall effect of the perturbations is similar in scale to -greedy with a linearly decaying schedule (see <ref type="bibr" target="#b37">(Plappert et al., 2017)</ref> for details). Because the perturbations are done on the model parameters, we might hope that the actions produced by the perturbations are more sensible than -greedy.</p><p>Bayesian Non-parametric Gaussian processes <ref type="bibr" target="#b38">(Rasmussen &amp; Williams, 2005</ref>) are a gold-standard method for modeling distributions over non-linear continuous functions. It can be shown that, in the limit of infinite hidden units and under a Gaussian prior, a Bayesian neural network converges to a Gaussian process <ref type="bibr" target="#b32">(Neal, 1994)</ref>. As such, GPs would appear to be a natural baseline. Unfortunately, standard GPs computationally scale cubically in the number of observations, limiting their applicability to relatively small datasets. There are a wide variety of methods to approximate Gaussian processes using, for example, pseudo-observations <ref type="bibr" target="#b44">(Snelson &amp; Ghahramani, 2006)</ref> or variational inference <ref type="bibr" target="#b48">(Titsias, 2009</ref>). We implemented both standard and sparse GPs but only report the former due to similar performance. For the standard GP, due to the scaling issue, we stop adding inputs to the GP after 1000 observations. This performed significantly better than randomly sampling inputs. Our implementation is a multi-task Gaussian process <ref type="bibr" target="#b7">(Bonilla et al., 2008)</ref> with a linear and Matern 3/2 product kernel over the inputs and an exponentiated quadratic kernel over latent vectors for the different tasks. The hyperparameters of this model and the latent task vectors are optimized over the GP marginal likelihood. This allows the model to learn correlations between the outputs of the model. Specifically, the covariance function K(·) of the GP is given by:</p><formula xml:id="formula_4">K(x k ,x l ) = k matern (x k ,x l ) · k lin (x k ,x l ) · k task (v k , v l ) (4) k matern (x k ,x l ) = α(1 + √ 3r λm (x,x)) exp(− √ 3r λm (x,x))) (5) k lin (x k ,x l ) = β(x λ l )(x λ l ) T<label>(6)</label></formula><p>and the task kernel between tasks t and l are k t (</p><formula xml:id="formula_5">x k ,x l ) = exp(−r λm (v k ,v l )) 2 )</formula><p>where v l indexes the latent vector for task l and r λ (</p><formula xml:id="formula_6">x,x) = |(x λ) − (x λ)|.</formula><p>The length-scales, λ m and λ l , and amplitude parameters α, β are optimized via the log marginal likelihood. For the sparse version we used a Sparse Variational GP <ref type="bibr" target="#b18">(Hensman et al., 2015)</ref> with the same kernel and with 300 inducing points, trained via minibatch stochastic gradient descent .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FEEDBACK LOOP IN THE LINEAR CASE</head><p>In this section, we illustrate some of the subtleties that arise when uncertainty estimates drive sequential decision-making using simple linear examples.</p><p>There is a fundamental difference between static and dynamic scenarios. In a static scenario, e.g. supervised learning, we are given a model family Θ (like the set of linear models, trees, or neural networks with specific dimensions), a prior distribution π 0 over Θ, and some observed data D that -importantly-is assumed i.i.d. Our goal is to return an approximate posterior distribution: π ≈ π = P(θ | D). We define the quality of our approximation by means of some distance d(π, π).</p><p>On the other hand, in dynamic settings, our estimate at time t, sayπ t , will be used via some mechanism M, in this case Thompson sampling, to collect the next data-point, which is then appended to D t . In this case, the data-points in D t are no longer independent. D t will now determine two distributions: the posterior given the data that was actually observed, π t+1 = P(θ | D t ), and our new estimateπ t+1 . When the goal is to make good sequential decisions in terms of cumulative regret, the distance d(π t , π t ) is in general no longer a definitive proxy for performance. For instance, a poorly-approximated decision boundary could lead an algorithm, based onπ, to get stuck repeatedly selecting a single sub-optimal action a. After collecting lots of data for that action,π t and π t could start to agree (to their capacity) on the models that explain what was observed for a, while both would stick to something close to the prior regarding the other actions. At that point, d(π t , π t ) may show relatively little disagreement, but the regret would already be terrible.</p><p>Let π * t be the posterior distribution P(θ | D t ) under Thompson Sampling's assumption, that is, data was always collected according to π * j for j &lt; t. We follow the idea thatπ t being close to π * t for all t leads to strong performance. However, this concept is difficult to formalize: once different decisions are made, data for different actions is collected and it is hard to compare posterior distributions.</p><p>We illustrate the previous points with a simple example, see <ref type="figure" target="#fig_1">Figure 1</ref>. Data is generated according to a bandit with k = 6 arms. For a given context X ∼ N (µ, Σ), the reward obtained by pulling arm i follows a linear model r i,X = X T β i + with ∼ N (0, σ 2 i ). The posterior distribution over   β i ∈ R d can be exactly computed using the standard Bayesian linear regression formulas presented in Section 3. We set the contextual dimension d = 20, and the prior to be β ∼ N (0, λ I d ), for λ &gt; 0.</p><p>In <ref type="figure" target="#fig_1">Figure 1</ref>, we show the posterior distribution for two dimensions of β i for each arm i after n = 500 pulls. In particular, in <ref type="figure" target="#fig_1">Figure 1a</ref>, two independent runs of Thompson Sampling with their posterior distribution are displayed in red and green. While strongly aligned, the estimates for some arms disagree (especially for arms that are best only for a small fraction of the contexts, like Arm 2 and 3, where fewer data-points are available). In <ref type="figure" target="#fig_1">Figure 1b</ref>, we also consider Thompson Sampling with an approximate posterior with diagonal covariance matrix, Diag in red, as defined in Section 3. Each algorithm collects its own data based on its current posterior (or approximation). In this case, the posterior disagreement after n = 500 decisions is certainly stronger. However, as shown in <ref type="figure" target="#fig_1">Figure 1c</ref>, if we computed the approximate posterior with a diagonal covariance matrix based on the data collected by the actual posterior, the disagreement would be reduced as much as possible within the approximation capacity (i.e., it still cannot capture correlations in this case). <ref type="figure" target="#fig_1">Figure 1b</ref> shows then the effect of the feedback loop. We look next at the impact that this mismatch has on regret.</p><p>We illustrate with a similar example how inaccurate posteriors sometimes lead to quite different behaviors in terms of regret. In <ref type="figure" target="#fig_2">Figure 2a</ref>, we see the posterior distribution β ∼ N (µ, Σ) of a linear model in green, together with the two diagonal linear approximations introduced in Section 3: the Diag (in orange) and the PrecisionDiag (in blue) approximations, respectively. We now assume there are k linear arms, β i ∈ R d for i = 1, . . . , k, and decisions are made according to the posteriors in <ref type="figure" target="#fig_2">Figure 2a</ref>. In <ref type="figure" target="#fig_2">Figures 2b and 2c</ref> we plot the regret of Thompson Sampling when there are k = 20 arms, for both d = 15 and d = 30. We see that, while the PrecisionDiag approximation does even outperform the actual posterior, the diagonal covariance approximation truly suffers poor regret when we increase the dimension d, as it is heavily penalized by simultaneously over-exploring in a large number of dimensions and repeateadly acting according to implausible models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EMPIRICAL EVALUATION</head><p>In this section, we present the simulations and outcomes of several synthetic and real-world data bandit problems with each of the algorithms introduced in Section 3. In particular, we first explain how the simulations were set up and run, and the metrics we report. We then split the experiments according to how data was generated, and the underlying models fit by the algorithms from Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">THE EXPERIMENTAL FRAMEWORK</head><p>We run the contextual bandit experiments as described at the beginning of Section 2, and discuss below some implementation details of both experiments and algorithms. A detailed summary of the key parameters used for each algorithm can be found in <ref type="table" target="#tab_2">Table 2</ref> in the appendix.</p><p>Neural Network Architectures All algorithms based on neural networks as function approximators share the same architecture. In particular, we fit a simple fully-connected feedforward network with two hidden layers with 100 units each and ReLu activations. The input of the network has dimension d (same as the contexts), and there are k outputs, one per action. Note that for each training point (X t , a t , r t ) only one action was observed (and algorithms usually only take into account the loss corresponding to the prediction for the observed action).</p><p>Updating Models A key question is how often and for how long models are updated. Ideally, we would like to train after each new observation and for as long as possible. However, this may limit the applicability of our algorithms in online scenarios where decisions must be made immediately.</p><p>We update linear algorithms after each time-step by means of (1) and <ref type="formula" target="#formula_1">(2)</ref>. For neural networks, the default behavior was to train for t s = 20 or 100 mini-batches every t f = 20 timesteps. <ref type="bibr">2</ref> The size of each mini-batch was 512. We experimented with increasing values of t s , and it proved essential for some algorithms like variational inference approaches. See the details in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Metrics We report two metrics: cumulative regret and simple regret. We approximate the latter as the mean cumulative regret in the last 500 time-steps, a proxy for the quality of the final policy (see further discussion on pure exploration settings, <ref type="bibr" target="#b8">Bubeck et al. (2009)</ref>). Cumulative regret is computed based on the best expected reward, as is standard. For most real datasets (Statlog, Covertype, Jester, Adult, Census, and Song), the rewards were deterministic, in which case, the definition of regret also corresponds to the highest realized reward (i.e., possibly leading to a hard task, which helps to understand why in some cases all regrets look linear). We reshuffle the order of the contexts, and rerun the experiment 50 times to obtain the cumulative regret distribution and report its statistics.</p><p>Hyper-Parameter Tuning Deep learning methods are known to be very sensitive to the selection of a wide variety of hyperparameters, and many of the algorithms presented are no exception. Moreover, that choice is known to be highly dataset dependent. Unfortunately, in the bandits scenario, we commonly do not have access to each problem a-priori to perform tuning. For the vast majority of algorithms, we report the outcome for three versions of the algorithm defined as follows. First, we use one version where hyper-parameters take values we guessed to be reasonable a-priori. Then, we add two additional instances whose hyper-parameters were optimized on two different datasets via Bayesian Optimization. For example, in the case of Dropout, the former version is named Dropout, while the optimized versions are named Dropout-MR (using the Mushroom dataset) and Dropout-SL (using the Statlog dataset) respectively. Some algorithms truly benefit from hyper-parameter (a) δ = 0.5 (b) δ = 0.7 (c) δ = 0.9 (d) δ = 0.95 (e) δ = 0.99 <ref type="figure">Figure 3</ref>: Wheel bandits for increasing values of δ ∈ (0, 1). Optimal action for blue, red, green, black, and yellow regions, are actions 1, 2, 3, 4, and 5, respectively.</p><p>optimization, while others do not show remarkable differences in performance; the latter are more appropriate in settings where access to the real environment for tuning is not possible in advance.</p><p>Buffer After some experimentation, we decided not to use a data buffer as evidence of catastrophic forgetting was observed, and datasets are relatively small. Accordingly, all observations are sampled with equal probability to be part of a mini-batch. In addition, as is standard in bandit algorithms, each action was initially selected s = 3 times using round-robin independently of the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">REAL-WORLD DATA PROBLEMS WITH NON-LINEAR MODELS</head><p>We evaluated the algorithms on a range of bandit problems created from real-world data. In particular, we test on the Mushroom, Statlog, Covertype, Financial, Jester, Adult, Census, and Song datasets (see Appendix Section A for details on each dataset and bandit problem). They exhibit a broad range of properties: small and large sizes, one dominating action versus more homogeneous optimality, learnable or little signal, stochastic or deterministic rewards, etc. For space reasons, the outcome of some simulations are presented in the Appendix. The Statlog, Covertype, Adult, and Census datasets were originally tested in <ref type="bibr" target="#b11">Elmachtoub et al. (2017)</ref>. We summarize the final cumulative regret for Mushroom, Statlog, Covertype, Financial, and Jester datasets in <ref type="table" target="#tab_0">Table 1</ref>. In <ref type="figure">Figure 5</ref> at the appendix, we show a box plot of the ranks achieved by each algorithm across the suite of bandit problems (see Appendix <ref type="table">Table 6</ref> and 7 for the full results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">REAL-WORLD DATA PROBLEMS WITH LINEAR MODELS</head><p>As most of the algorithms from Section 3 can be implemented for any model architecture, in this subsection we use linear models as a baseline comparison across algorithms (i.e., neural networks that contain a single linear layer). This allows us to directly compare the approximate methods against methods that can compute the exact posterior. The specific hyper-parameter configurations used in the experiments are described in <ref type="table" target="#tab_3">Table 3</ref> in the appendix. Datasets are the same as in the previous subsection. The cumulative and simple regret results are provided in appendix Tables 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">THE WHEEL BANDIT</head><p>Some of the real-data problems presented above do not require significant exploration. We design an artificial problem where the need for exploration is smoothly parameterized. The wheel bandit is defined as follows (see <ref type="figure">Figure 3</ref>). Set d = 2, and δ ∈ (0, 1), the exploration parameter. Contexts are sampled uniformly at random in the unit circle in R 2 , X ∼ U (D). There are k = 5 possible actions. The first action a 1 always offers reward r ∼ N (µ 1 , σ 2 ), independently of the context. On the other hand, for contexts such that X ≤ δ, i.e. inside the blue circle in <ref type="figure">Figure 3</ref>, the other four actions are equally distributed and sub-optimal, with r ∼ N (µ 2 , σ 2 ) for µ 2 &lt; µ 1 . When X &gt; δ, we are outside the blue circle, and only one of the actions a 2 , . . . , a 5 is optimal depending on the sign of context components X = (X 1 , X 2 ). If X 1 , X 2 &gt; 0, action 2 is optimal. If X 1 &gt; 0, X 2 &lt; 0, action 3 is optimal, and so on. Non-optimal actions still deliver r ∼ N (µ 2 , σ 2 ) in this region, except a 1 whose mean reward is always µ 1 , while the optimal action provides r ∼ N (µ 3 , σ 2 ), with µ 3 µ 1 . We set µ 1 = 1.2, µ 2 = 1.0, µ 3 = 50.0, and σ = 0.01. Note that the probability of a context randomly falling in the high-reward region is 1 − δ 2 (not blue). The difficulty of the problem increases with δ, and we expect algorithms to get stuck repeatedly selecting action a 1 for large δ. The problem can be easily generalized for d &gt; 2. Results are shown in <ref type="table">Table 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Overall, we found that there is significant room for improvement in uncertainty estimation for neural networks in sequential decision-making problems. First, unlike in supervised learning, sequential decision-making requires the model to be frequently updated as data is accumulated. As a result, methods that converge slowly are at a disadvantage because we must truncate optimization to make the method practical for the online setting. In these cases, we found that partially optimized uncertainty estimates can lead to catastrophic decisions and poor performance. Second, and while it deserves further investigation, it seems that decoupling representation learning and uncertainty estimation improves performance. The NeuralLinear algorithm is an example of this decoupling. With such a model, the uncertainty estimates can be solved for in closed form (but may be erroneous due to the simplistic model), so there is no issue with partial optimization. We suspect that this may be the reason for the improved performance. In addition, we observed that many algorithms are sensitive to their hyperparameters, so that best configurations are problem-dependent.</p><p>Finally, we found that in many cases, the inherit randomness in Stochastic Gradient Descent provided sufficient exploration. Accordingly, in some scenarios it may be hard to justify the use of complicated (and less transparent) variations of simple methods. However, Stochastic Gradient Descent is by no  We emphasize that in this evaluation, all algorithms use the same family of models (i.e., linear). While PrecisionDiag exactly solves the mean field problem, BBB relies on partial optimization via SGD. As the number of training epochs increases, BBB improves performance, but is always outperformed by PrecisionDiag. means always enough: in our synthetic exploration-oriented problem (the Wheel bandit) additional exploration was necessary.</p><p>Next, we discuss our main findings for each class of algorithms.</p><p>Linear Methods. Linear methods offer a reasonable baseline, surprisingly strong in many cases. While their representation power is certainly a limiting factor, their ability to compute informative uncertainty measures seems to payoff and balance their initial disadvantage. They do well in several datasets, and are able to react fast to unexpected or extreme rewards (maybe as single points can have a heavy impact in fitted models, and their updates are immediate, deterministic, and exact). Some datasets clearly need more complex non-linear representations, and linear methods are unable to efficiently solve those. In addition, linear methods obviously offer computational advantages, and it would be interesting to investigate how their performance degrades when a finite data buffer feeds the estimates as various real-world online applications may require (instead of all collected data).</p><p>In terms of the diagonal linear approximations described in Section 3, we found that diagonalizing the precision matrix (as in mean-field Variational Inference) performs dramatically better than diagonalizing the covariance matrix.</p><p>NeuralLinear. The NeuralLinear algorithm sits near a sweet spot that is worth further studying. In general it seems to improve the RMS neural network it is based on, suggesting its exploration mechanisms add concrete value. We believe its main strength is that it is able to simultaneously learn a data representation that greatly simplifies the task at hand, and to accurately quantify the uncertainty over linear models that explain the observed rewards in terms of the proposed representation. While the former process may be noisier and heavily dependent on the amount of training steps that were taken and available data, the latter always offers the exact solution to its approximate parent problem. This, together with the partial success of linear methods with poor representations, may explain its promising results. In some sense, it knows what it knows. In the Wheel problem, which requires increasingly good exploration mechanisms, NeuralLinear is probably the best algorithm. Its performance is almost an order of magnitude better than any RMS algorithm (and its spinoffs, like Bootstrapped NN, Dropout, or Parameter Noise), and all greedy linear approaches. On the other hand, it is able to successfully solve problems that require non-linear representations (as Statlog or Covertype) where linear approaches fail. In addition, the algorithm is remarkably easy to tune, and robust in terms of hyper-parameter configurations. While conceptually simple, its deployment to large scale systems may involve some technical difficulties; mainly, to update the Bayesian estimates when the network is re-trained. We believe, however, standard solutions to similar problems (like running averages) could greatly mitigate these issues. In our experiments and compared to other algorithms, as shown in <ref type="table">Table 8</ref>, NeuralLinear is fast from a computational standpoint.</p><p>Variational Inference. Overall, Bayes By Backprop performed poorly, ranking in the bottom half of algorithms across datasets <ref type="table" target="#tab_0">(Table 1)</ref>. To investigate if this was due to underestimating uncertainty (as variational methods are known to <ref type="bibr" target="#b5">(Bishop, 2006)</ref>), to the mean field approximation, or to stochastic optimization, we applied BBB to a linear model, where the mean field optimization problem can be solved in closed form <ref type="figure" target="#fig_4">(Figure 4)</ref>. We found that the performance of BBB slowly improved as the number of training epochs increased, but underperformed compared to the exact mean field solution. Moreover, the difference in performance due to the number of training steps dwarfed the difference between the mean field solution and the exact posterior. This suggests that it is not sufficient to partially optimize the variational parameters when the uncertainty estimates directly affect the data being collected. In supervised learning, optimizing to convergence is acceptable, however in the online setting, optimizing to convergence at every step incurs unreasonable computational cost.</p><p>Expectation-Propagation. The performance of Black Box α-divergence algorithms was poor. Because this class of algorithms is similar to BBB (in fact, as α → 0, it converges to the BBB objective), we suspect that partial convergence was also the cause of their poor performance. We found these algorithms to be sensitive to the number of training steps between actions, requiring a large number to achieve marginal performance. Their terrible performance in the Mushroom bandit is remarkable, while in the other datasets they perform slightly worse than their variational inference counterpart. Given the successes of Black Box α-divergence in other domains <ref type="bibr" target="#b19">(Hernández-Lobato et al., 2016)</ref>, investigating approaches to sidestep the slow convergence of the uncertainty estimates is a promising direction for future work.</p><p>Monte Carlo. Constant-SGD comes out as the winner on Covertype, which requires non-linearity and exploration as evidenced by performance of the linear baseline approaches <ref type="table" target="#tab_0">(Table 1</ref>). The method is especially appealing as it does not require tuning learning rates or exploration parameters. SGFS, however, performs better on average. The additional injected noise in SGFS may cause the model to explore more and thus perform better, as shown in the Wheel Bandit problem where SGFS strongly outperforms Constant-SGD.</p><p>Bootstrap. The bootstrap offers significant gains with respect to its parent algorithm (RMS) in several datasets. Note that in Statlog one of the actions is optimal around 80% of the time, and the bootstrapped predictions may help to avoid getting stuck, something from which RMS methods may suffer. In other scenarios, the randomness from SGD may be enough for exploration, and the bootstrap may not offer important benefits. In those cases, it might not justify the heavy computational overhead of the method. We found it surprising that the optimized versions of BootstrappedNN decided to use only q = 2 and q = 3 networks respectively (while we set its value to q = 10 in the manually tuned version, and the extra networks did not improve performance significantly). Unfortunately, Bootstrapped NNs were not able to solve the Wheel problem, and its performance was fairly similar to that of RMS. One possible explanation is that -given the sparsity of the rewardall the bootstrapped networks agreed for the most part, and the algorithm simply got stuck selecting action a 1 . As opposed to linear models, reacting to unusual rewards could take Bootstrapped NNs some time as good predictions could be randomly overlooked (and useful data discarded if p 1).</p><p>Direct Noise Injection. When properly tuned, Parameter-Noise provided an important boost in performance across datasets over the learner that it was based on (RMS), average rank of ParamNoise-SL is 20.9 compared to RMS at 28.7 <ref type="table" target="#tab_0">(Table 1)</ref>. However, we found the algorithm hard to tune and sensitive to the heuristic controlling the injected noise-level. On the synthetic Wheel problem -where exploration is necessary-both parameter-noise and RMS suffer from underexploration and perform similarly, except ParamNoise-MR which does a good job. In addition, developing an intuition for the heuristic is not straightforward as it lacks transparency and a principled grounding, and thus may require repeated access to the decision-making process for tuning.</p><p>Dropout. We initially experimented with two dropout versions: fixed p = 0.5, and p = 0.8. The latter consistently delivered better results, and it is the one we manually picked. The optimized versions of the algorithm provided decent improvements over its base RMS (specially Dropout-MR).</p><p>In the Wheel problem, dropout performance is somewhat poor: Dropout is outperformed by RMS, while Dropout-MR offers gains with respect to all versions of RMS but it is not competitive with the best algorithms. Overall, the algorithm seems to heavily depend on its hyper-parameters (see cum-regret performance of the raw Dropout, for example). Dropout was used both for training and for decision-making; unfortunately, we did not add a baseline where dropout only applies during training. Consequently, it is not obvious how to disentangle the contribution of better training from that of better exploration. This remains as future work.</p><p>Bayesian Non-parametrics. Perhaps unsurprisingly, Gaussian processes perform reasonably well on problems with little data but struggle on larger problems. While this motivated the use of sparse GP, the latter was not able to perform similarly to stronger (and definitively simpler) methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>In this work, we empirically studied the impact on performance of approximate model posteriors for decision making via Thompson Sampling in contextual bandits. We found that the most robust methods exactly measured uncertainty (possibly under the wrong model assumptions) on top of complex representations learned in parallel. More complicated approaches that learn the representation and its uncertainty together seemed to require heavier training, an important drawback in online scenarios, and exhibited stronger hyper-parameter dependence. Further exploring and developing the promising approaches is an exciting avenue for future work.  <ref type="figure">Figure 5</ref>: A boxplot of the ranks achieved by each algorithm across the suite of benchmarks. The red and black solid lines respectively indicate the median and mean rank across problems. , and t f = 20 (one training period every t f contexts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm Description</head><p>AlphaDivergence <ref type="formula" target="#formula_0">(1)</ref> BB α-divergence with α = 0.5, noise σ = 0.1, K = 10, prior var σ 2 0 = 0.1. (ts = 100, first 100 times linear decay from ts = 10000). AlphaDivergence BB α-divergence with α = 0.1, noise σ = 0.1, K = 10, prior var σ 2 0 = 0.1. (ts = 100, first 100 times linear decay from ts = 10000). AlphaDivergence-SL BB α-divergence with α = 0.01, noise σ = 2.79, K = 1, prior var σ 2 0 = 0.18. (ts = 200, first 100 times linear decay from ts = 10000). , and t f = 20 (one training period every t f contexts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm Description</head><p>Alpha Divergences BB α-divergence with α = 0.1, noise σ = 0.1, K = 10, prior var σ 2 0 = 0.1. (ts = 100, first 100 times linear decay from ts = 10000). Alpha Divergences <ref type="formula" target="#formula_0">(1)</ref> BB α-divergence with α = 0.5, noise σ = 0.1, K = 10, prior var σ 2 0 = 0.1. (ts = 100, first 100 times linear decay from ts = 10000). Alpha Divergences <ref type="formula" target="#formula_1">(2)</ref> BB α-divergence with α = 1.0, noise σ = 0.1, K = 10, prior var σ 2 0 = 0.1. (ts = 100, first 100 times linear decay from ts = 10000). Alpha Divergences <ref type="formula" target="#formula_3">(3)</ref> BB α-divergence with α = 0.5, noise σ = 0.1, K = 10, prior var σ 2 0 = 1.0. (ts = 100, first 100 times linear decay from ts = 10000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BBBN</head><p>BayesByBackprop with noise σ = 0.1. (ts = 100, first 100 times linear decay from ts = 10000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BBBN2</head><p>BayesByBackprop with noise σ = 0.5. (ts = 100, first 100 times linear decay from ts = 10000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BBBN3</head><p>BayesByBackprop with noise σ = 0.75. (ts = 100, first 100 times linear decay from ts = 10000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BBBN4</head><p>BayesByBackprop with noise σ = 1.0. (ts = 100, first 100 times linear decay from ts = 10000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bootstrapped NN</head><p>Bootstrapped with q = 5 models, and p = 0.85. Based on RMS3 net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bootstrapped NN2</head><p>Bootstrapped with q = 5 models, and p = 1.0. Based on RMS3 net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bootstrapped NN3</head><p>Bootstrapped with q = 10 models, and p = 1.0. Based on RMS3 net. Dropout (RMS3) Dropout with probability p = 0.8. Based on RMS3 net. Dropout (RMS2) Dropout with probability p = 0.8. Based on RMS2 net. RMS1</p><p>Greedy NN approach, fixed learning rate (γ = 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMS2</head><p>Learning rate decays, and it is reset every training period. RMS2b</p><p>Similar to RMS2, but training for longer (ts = 800).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMS3</head><p>Learning rate decays, and it is not reset at all. Starts at γ = 1. SGFS Burning = 500, learning rate γ = 0.014, EMA decay = 0.9, noise σ = 0.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ConstSGD</head><p>Burning = 500, EMA decay = 0.9, noise σ = 0.5. EpsGreedy (RMS1) Initial = 0.01. Multiplied by 0.999 after every context. Based on RMS1 net. EpsGreedy (RMS2) Initial = 0.01. Multiplied by 0.999 after every context. Based on RMS2 net. EpsGreedy (RMS3) Initial = 0.01. Multiplied by 0.999 after every context. Based on RMS3 net. LinDiagPost Σ in Eq. 1 is diagonalized. Ridge prior λ = 0.25. Assumed noise level σ 2 = 0.25. LinDiagPrecPost Σ −1 in Eq. 1 is diagonalized. Ridge prior λ = 0.25. Assumed noise level σ 2 = 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LinGreedy</head><p>Takes action with highest predicted reward for Ridge regression, λ = 0.25. Noise level σ 2 = 0.25. LinGreedy (eps = 0.01) linGreedy that selects action uniformly at random with prob p = 0.01. LinGreedy (eps = 0.05) linGreedy that selects action uniformly at random with prob p = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LinPost</head><p>Ridge prior λ = 0.25. Assumed noise level σ 2 = 0.25. LinFullDiagPost Σ in Eq. 1 is diagonalized. Noise prior a 0 = 6, b 0 = 6. Ridge prior λ = 0.25. LinFullDiagPrecPost Σ −1 in Eq. 1 is diagonalized. Noise prior a 0 = 6, b 0 = 6. Ridge prior λ = 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LinFullPost</head><p>Noise prior a 0 = 6, b 0 = 6. Ridge prior λ = 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Param-Noise</head><p>Initial noise σ = 0.01, and level = 0.01. Based on RMS3 net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Param-Noise2</head><p>Initial noise σ = 0.01, and level = 0.01. Based on RMS3 net. Trained for longer: ts = 800.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uniform</head><p>Takes each action at random with equal probability.</p><p>Published as a conference paper at ICLR 2018 <ref type="table">Table 4</ref>: Cumulative regret incurred by linear models using algorithms in Section 3 on the bandits described in Section A. Values reported are the mean over 50 independent trials with standard error of the mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mushroom</head><p>Statlog Covertype Financial Jester Adult</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cumulative regret</head><p>Published as a conference paper at ICLR 2018 <ref type="table">Table 5</ref>: Simple regret incurred by linear models using algorithms in Section 3 on the bandits described in Section A. Simple regret was approximated by averaging the regret over the final 500 steps. Values reported are the mean over 50 independent trials with standard error of the mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mushroom</head><p>Statlog Covertype Financial Jester Adult</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple regret</head><p>Alpha Divergences 0.68 ± 0.04 0.07 ± 0.00 0.31 ± 0.00 0.00 ± 0.00 2.91 ± 0.04 0.75 ± 0.00 Alpha Divergences <ref type="formula" target="#formula_0">(1)</ref> 1.50 ± 0.05 0.08 ± 0.00 0.31 ± 0.00 0.00 ± 0.00 2.98 ± 0.03 0.75 ± 0.00 Alpha Divergences <ref type="formula" target="#formula_1">(2)</ref> 1.51 ± 0.05 0.13 ± 0.00 0.32 ± 0.00 0.00 ± 0.00 3.42 ± 0.05 0.77 ± 0.00 Alpha Divergences <ref type="formula">(</ref> 0.37 ± 0.05 0.05 ± 0.00 0.28 ± 0.00 0.01 ± 0.00 2.77 ± 0.04 0.69 ± 0.00 LinGreedy 0.51 ± 0.08 0.07 ± 0.01 0.30 ± 0.00 0.01 ± 0.00 2.73 ± 0.03 0.74 ± 0.01 LinGreedy (eps=0.01) 0.07 ± 0.01 0.07 ± 0.00 0.29 ± 0.00 0.01 ± 0.00 2.80 ± 0.03 0.67 ± 0.00 LinGreedy (eps=0.05) 0.24 ± 0.02 0.10 ± 0.00 0.31 ± 0.00 0.06 ± 0.00 2.86 ± 0.03 0.68 ± 0.00 LinPost 0.29 ± 0.03 0.06 ± 0.00 0.28 ± 0.00 0.01 ± 0.00 2.74 ± 0.04 0.69 ± 0.00 LinfullDiagPost 4.10 ± 0.07 0.18 ± 0.00 0.63 ± 0.00 0.00 ± 0.00 2.86 ± 0.03 0.89 ± 0.00 LinfullDiagPrecPost 0.19 ± 0.02 0.05 ± 0.00 0.28 ± 0.00 0.00 ± 0.00 2.82 ± 0.03 0.67 ± 0.00 LinfullPost 0.08 ± 0.01 0.05 ± 0.00 0.28 ± 0.00 0.00 ± 0.00 2.86 ± 0.03 0.67 ± 0.00 Param-Noise 0.49 ± 0.07 0.05 ± 0.00 0.32 ± 0.00 0.01 ± 0.00 2.87 ± 0.04 0.69 ± 0.00 Param-Noise2 0.36 ± 0.05 0.05 ± 0.00 0.33 ± 0.00 0.01 ± 0.00 2.83 ± 0.04 0.69 ± 0.00 Uniform 4.88 ± 0.07 0.86 ± 0.00 0.86 ± 0.00 1.25 ± 0.02 5.03 ± 0.07 0.93 ± 0.00  <ref type="bibr">(2016)</ref>. We plot the mean and standard deviation of 100 samples drawn from each method conditioned on a small set of observations with three outputs (two are from the same underlying function and thus strongly correlated while the third (bottom) is independent). The true underlying functions are plotted in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A REAL-WORLD DATASETS</head><p>Mushroom. The Mushroom Dataset <ref type="bibr" target="#b43">(Schlimmer, 1981)</ref> contains 22 attributes per mushroom, and two classes: poisonous and safe. As in <ref type="bibr" target="#b6">Blundell et al. (2015)</ref>, we create a bandit problem where the agent must decide whether to eat or not a given mushroom. Eating a safe mushroom provides reward +5. Eating a poisonous mushroom delivers reward +5 with probability 1/2 and reward -35 otherwise. If the agent does not eat a mushroom, then the reward is 0. We set n = 50000.</p><p>Statlog. The Shuttle Statlog Dataset <ref type="bibr" target="#b2">(Asuncion &amp; Newman, 2007)</ref> provides the value of d = 9 indicators during a space shuttle flight, and the goal is to predict the state of the radiator subsystem of the shuttle. There are k = 7 possible states, and if the agent selects the right state, then reward 1 is generated. Otherwise, the agent obtains no reward (r = 0). The most interesting aspect of the dataset is that one action is the optimal one in 80% of the cases, and some algorithms may commit to this action instead of further exploring. In this case, n = 43500.</p><p>Covertype. The Covertype Dataset <ref type="bibr" target="#b2">(Asuncion &amp; Newman, 2007)</ref> classifies the cover type of northern Colorado forest areas in k = 7 classes, based on d = 54 features, including elevation, slope, aspect, and soil type. Again, the agent obtains reward 1 if the correct class is selected, and 0 otherwise. We run the bandit for n = 150000.</p><p>Financial. We created the Financial Dataset by pulling the stock prices of d = 21 publicly traded companies in NYSE and Nasdaq, for the last 14 years (n = 3713). For each day, the context was the price difference between the beginning and end of the session for each stock. We synthetically created the arms, to be a linear combination of the contexts, representing k = 8 different potential portfolios. By far, this was the smallest dataset, and many algorithms over-explored at the beginning with no time to amortize their investment (Thompson Sampling does not account for the horizon).</p><p>Jester. We create a recommendation system bandit problem as follows. The Jester Dataset <ref type="bibr" target="#b15">(Goldberg et al., 2001)</ref> provides continuous ratings in [−10, 10] for 100 jokes from 73421 users. We find a complete subset of n = 19181 users rating all 40 jokes. Following <ref type="bibr" target="#b39">Riquelme et al. (2017)</ref>, we take d = 32 of the ratings as the context of the user, and k = 8 as the arms. The agent recommends one joke, and obtains the reward corresponding to the rating of the user for the selected joke.</p><p>Adult. The Adult Dataset <ref type="bibr" target="#b24">(Kohavi, 1996;</ref><ref type="bibr" target="#b2">Asuncion &amp; Newman, 2007)</ref> comprises personal information from the US Census Bureau database, and the standard prediction task is to determine if a person makes over $50K a year or not. However, we consider the k = 14 different occupations as feasible actions, based on d = 94 covariates (many of them binarized). As in previous datasets, the agent obtains reward 1 for making the right prediction, and 0 otherwise. We set n = 45222.</p><p>Census. The US Census (1990) Dataset <ref type="bibr" target="#b2">(Asuncion &amp; Newman, 2007</ref>) contains a number of personal features (age, native language, education...) which we summarize in d = 389 covariates, including binary dummy variables for categorical features. Our goal again is to predict the occupation of the individual among k = 9 classes. The agent obtains reward 1 for making the right prediction, and 0 otherwise, for each of the n = 250000 randomly selected data points.</p><p>Song. The YearPredictionMSD Dataset is a subset of the Million Song Dataset <ref type="bibr" target="#b4">(Bertin-Mahieux et al., 2011)</ref>. The goal is to predict the year a given song was released (1922-2011) based on d = 90 technical audio features. We divided the years in k = 10 contiguous year buckets containing the same number of songs, and provided decreasing Gaussian rewards as a function of the distance between the interval chosen by the agent and the one containing the year the song was actually released. We initially selected n = 250000 songs at random from the training set.</p><p>The Statlog, Covertype, Adult, and Census datasets were tested in <ref type="bibr" target="#b11">Elmachtoub et al. (2017)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Two independent instances of Thompson Sampling with the true linear posterior. (b) Thompson Sampling with the true linear posterior (green), and the diagonalized version (red). (c) Linear posterior versus diagonal posterior fitted to the data collected by the former.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Visualizations of the posterior approximations in a linear example. (a) Posterior Approximations. (b) Case d = 15. (c) Case d = 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The impact on regret of different approximated posteriors. We show (green) the actual linear posterior, (orange) the diagonal posterior approximation and (blue) the precision approximation in 2a. In 2b and 2c we visualize the impact of the approximations on cumulative regret.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Linear data, d = 10 input size, and k = 6 actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Cumulative regret for Bayes By Backprop (Variational Inference, fixed noise σ = 0.75) applied to a linear model and an exact mean field solution, denoted PrecisionDiag, with a linear bandit (left) and with the Statlog bandit (right). The suffix of the BBB legend label indicates the number of training epochs in each training step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>We qualitatively compare plots of the sample distribution from various methods, similarly to Hernández-Lobato et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Cumulative regret incurred by the algorithms in Section 3 on the bandits described in Section A. Results are relative to the cumulative regret of the Uniform algorithm. We report the mean and standard error of the mean over 50 trials.</figDesc><table><row><cell></cell><cell>Mean Rank</cell><cell>Mushroom</cell><cell>Statlog</cell><cell>Covertype</cell><cell>Financial</cell><cell>Jester</cell></row><row><cell>AlphaDivergence (1)</cell><cell>47</cell><cell>54.29 ± 0.04</cell><cell>19.35 ± 1.72</cell><cell>39.42 ± 0.50</cell><cell>40.10 ± 0.69</cell><cell>72.99 ± 0.54</cell></row><row><cell>AlphaDivergence</cell><cell>46.6</cell><cell>54.17 ± 0.03</cell><cell>19.30 ± 0.84</cell><cell>44.31 ± 0.77</cell><cell>47.76 ± 0.89</cell><cell>71.86 ± 0.72</cell></row><row><cell>AlphaDivergence-SL</cell><cell>44.3</cell><cell>53.88 ± 0.03</cell><cell>21.12 ± 1.14</cell><cell>60.05 ± 0.02</cell><cell>70.19 ± 3.50</cell><cell>69.11 ± 0.75</cell></row><row><cell>BBB</cell><cell>39.8</cell><cell>3.57 ± 0.20</cell><cell>12.61 ± 1.53</cell><cell>58.19 ± 2.16</cell><cell>22.55 ± 1.27</cell><cell>71.43 ± 0.67</cell></row><row><cell>BBB-MR</cell><cell>37.5</cell><cell>10.93 ± 2.69</cell><cell>25.29 ± 0.00</cell><cell>60.92 ± 0.55</cell><cell>59.84 ± 2.89</cell><cell>65.01 ± 0.74</cell></row><row><cell>BBB-SL</cell><cell>34.4</cell><cell>4.08 ± 0.19</cell><cell>1.86 ± 0.29</cell><cell>38.50 ± 0.97</cell><cell>13.76 ± 0.60</cell><cell>74.70 ± 0.68</cell></row><row><cell>BootstrappedNN</cell><cell>22.4</cell><cell>5.60 ± 0.60</cell><cell>0.65 ± 0.02</cell><cell>26.23 ± 0.10</cell><cell>9.21 ± 0.44</cell><cell>73.38 ± 0.62</cell></row><row><cell>BootstrappedNN-MR</cell><cell>22.6</cell><cell>2.15 ± 0.13</cell><cell>1.19 ± 0.15</cell><cell>31.27 ± 0.08</cell><cell>7.72 ± 0.28</cell><cell>63.26 ± 0.58</cell></row><row><cell>BootstrappedNN-SL</cell><cell>22.9</cell><cell>3.93 ± 0.17</cell><cell>0.54 ± 0.01</cell><cell>25.53 ± 0.05</cell><cell>10.88 ± 0.61</cell><cell>70.64 ± 0.59</cell></row><row><cell>Dropout</cell><cell>30.5</cell><cell>5.57 ± 1.02</cell><cell>2.35 ± 0.59</cell><cell>30.65 ± 0.23</cell><cell>17.55 ± 0.67</cell><cell>66.24 ± 0.74</cell></row><row><cell>Dropout-MR</cell><cell>20.3</cell><cell>2.65 ± 0.08</cell><cell>1.30 ± 0.32</cell><cell>29.28 ± 0.12</cell><cell>10.16 ± 0.44</cell><cell>63.68 ± 0.60</cell></row><row><cell>Dropout-SL</cell><cell>26.4</cell><cell>4.39 ± 1.02</cell><cell>1.89 ± 0.47</cell><cell>26.39 ± 0.17</cell><cell>13.18 ± 0.86</cell><cell>66.90 ± 0.80</cell></row><row><cell>GP</cell><cell>31.75</cell><cell>11.49 ± 0.66</cell><cell>3.92 ± 0.74</cell><cell>46.25 ± 0.75</cell><cell>3.18 ± 0.08</cell><cell>74.95 ± 0.93</cell></row><row><cell>NeuralLinear</cell><cell>22</cell><cell>2.22 ± 0.08</cell><cell>0.91 ± 0.01</cell><cell>29.91 ± 0.17</cell><cell>11.44 ± 0.11</cell><cell>75.43 ± 0.41</cell></row><row><cell>NeuralLinear-MR</cell><cell>22.4</cell><cell>1.92 ± 0.10</cell><cell>1.30 ± 0.01</cell><cell>28.87 ± 0.14</cell><cell>13.47 ± 0.12</cell><cell>72.75 ± 0.50</cell></row><row><cell>NeuralLinear-SL</cell><cell>17.4</cell><cell>2.42 ± 0.09</cell><cell>0.52 ± 0.01</cell><cell>27.60 ± 0.10</cell><cell>9.98 ± 0.56</cell><cell>71.11 ± 0.47</cell></row><row><cell>RMS</cell><cell>28.7</cell><cell>6.68 ± 1.52</cell><cell>2.85 ± 0.73</cell><cell>27.74 ± 0.18</cell><cell>12.73 ± 0.73</cell><cell>69.93 ± 0.56</cell></row><row><cell>RMS-MR</cell><cell>29.8</cell><cell>4.32 ± 1.06</cell><cell>2.36 ± 0.44</cell><cell>32.46 ± 0.57</cell><cell>10.72 ± 0.51</cell><cell>68.43 ± 0.72</cell></row><row><cell>RMS-SL</cell><cell>30.7</cell><cell>3.29 ± 0.16</cell><cell>2.22 ± 0.87</cell><cell>28.25 ± 0.14</cell><cell>12.76 ± 0.63</cell><cell>71.50 ± 0.49</cell></row><row><cell>SGFS</cell><cell>30.8</cell><cell>5.99 ± 1.02</cell><cell>3.82 ± 0.45</cell><cell>36.57 ± 0.80</cell><cell>29.00 ± 0.53</cell><cell>68.02 ± 0.63</cell></row><row><cell>SGFS-MR</cell><cell>21.8</cell><cell>3.80 ± 0.46</cell><cell>1.44 ± 0.01</cell><cell>30.12 ± 0.05</cell><cell>12.49 ± 0.71</cell><cell>66.27 ± 0.72</cell></row><row><cell>SGFS-SL</cell><cell>30.9</cell><cell>2.79 ± 0.10</cell><cell>2.14 ± 0.13</cell><cell>35.27 ± 0.27</cell><cell>43.95 ± 1.12</cell><cell>73.90 ± 1.51</cell></row><row><cell>ConstSGD</cell><cell>36.9</cell><cell>26.37 ± 3.14</cell><cell>6.79 ± 1.42</cell><cell>22.47 ± 0.78</cell><cell>88.16 ± 2.80</cell><cell>70.09 ± 0.80</cell></row><row><cell>ConstSGD-MR</cell><cell>29.9</cell><cell>3.10 ± 0.08</cell><cell>7.24 ± 1.07</cell><cell>23.39 ± 0.66</cell><cell>46.47 ± 1.85</cell><cell>71.25 ± 0.61</cell></row><row><cell>ConstSGD-SL</cell><cell>35.1</cell><cell>41.94 ± 2.31</cell><cell>2.96 ± 0.79</cell><cell>21.61 ± 0.15</cell><cell>51.94 ± 3.78</cell><cell>70.24 ± 0.95</cell></row><row><cell>EpsGreedyRMS</cell><cell>23.6</cell><cell>4.97 ± 1.04</cell><cell>2.13 ± 0.34</cell><cell>27.42 ± 0.13</cell><cell>12.36 ± 0.47</cell><cell>69.65 ± 0.70</cell></row><row><cell>EpsGreedyRMS-SL</cell><cell>23.2</cell><cell>3.08 ± 0.15</cell><cell>1.09 ± 0.20</cell><cell>28.09 ± 0.09</cell><cell>7.93 ± 0.39</cell><cell>69.64 ± 0.61</cell></row><row><cell>EpsGreedyRMS-MR</cell><cell>24.4</cell><cell>2.44 ± 0.15</cell><cell>1.71 ± 0.44</cell><cell>30.03 ± 0.20</cell><cell>8.07 ± 0.45</cell><cell>66.18 ± 0.57</cell></row><row><cell>LinDiagPost</cell><cell>30.6</cell><cell>17.67 ± 0.18</cell><cell>51.29 ± 0.03</cell><cell>95.48 ± 0.02</cell><cell>9.59 ± 0.05</cell><cell>58.61 ± 0.49</cell></row><row><cell>LinDiagPost-MR</cell><cell>37.3</cell><cell>8.64 ± 2.33</cell><cell>31.51 ± 0.03</cell><cell>65.03 ± 0.03</cell><cell>20.57 ± 0.13</cell><cell>60.62 ± 0.49</cell></row><row><cell>LinDiagPost-SL</cell><cell>31.5</cell><cell>14.86 ± 2.12</cell><cell>15.60 ± 0.03</cell><cell>42.72 ± 0.03</cell><cell>2.04 ± 0.04</cell><cell>59.96 ± 0.67</cell></row><row><cell>LinDiagPrecPost</cell><cell>15.8</cell><cell>9.48 ± 1.59</cell><cell>7.53 ± 0.02</cell><cell>34.40 ± 0.02</cell><cell>4.58 ± 0.04</cell><cell>58.58 ± 0.60</cell></row><row><cell>LinDiagPrecPost-MR</cell><cell>26.1</cell><cell>16.21 ± 3.14</cell><cell>8.77 ± 0.07</cell><cell>35.69 ± 0.02</cell><cell>9.15 ± 0.10</cell><cell>59.08 ± 0.45</cell></row><row><cell>LinDiagPrecPost-SL</cell><cell>16.9</cell><cell>13.17 ± 1.73</cell><cell>6.80 ± 0.02</cell><cell>33.85 ± 0.07</cell><cell>1.82 ± 0.06</cell><cell>58.83 ± 0.45</cell></row><row><cell>LinGreedy</cell><cell>25.3</cell><cell>14.28 ± 1.99</cell><cell>11.32 ± 0.63</cell><cell>35.29 ± 0.11</cell><cell>2.18 ± 0.14</cell><cell>59.69 ± 0.60</cell></row><row><cell>LinGreedy ( = 0.01)</cell><cell>18.1</cell><cell>3.38 ± 0.18</cell><cell>10.42 ± 0.39</cell><cell>34.59 ± 0.08</cell><cell>2.94 ± 0.12</cell><cell>59.95 ± 0.58</cell></row><row><cell>LinGreedy ( = 0.05)</cell><cell>19.6</cell><cell>5.89 ± 0.06</cell><cell>12.75 ± 0.16</cell><cell>37.00 ± 0.03</cell><cell>6.57 ± 0.11</cell><cell>61.62 ± 0.43</cell></row><row><cell>LinPost</cell><cell>16.4</cell><cell>6.12 ± 0.67</cell><cell>7.64 ± 0.02</cell><cell>34.40 ± 0.02</cell><cell>7.26 ± 0.05</cell><cell>59.14 ± 0.50</cell></row><row><cell>LinPost-MR</cell><cell>28.3</cell><cell>7.93 ± 2.00</cell><cell>10.31 ± 0.03</cell><cell>38.64 ± 0.02</cell><cell>15.61 ± 0.10</cell><cell>59.17 ± 0.56</cell></row><row><cell>LinPost-SL</cell><cell>18.9</cell><cell>14.34 ± 1.84</cell><cell>6.82 ± 0.02</cell><cell>33.61 ± 0.03</cell><cell>2.50 ± 0.03</cell><cell>60.02 ± 0.57</cell></row><row><cell>LinFullDiagPost</cell><cell>33.1</cell><cell>86.80 ± 0.13</cell><cell>28.29 ± 0.02</cell><cell>73.82 ± 0.03</cell><cell>6.96 ± 0.06</cell><cell>63.22 ± 0.61</cell></row><row><cell>LinFullDiagPost-MR</cell><cell>28.2</cell><cell>2.39 ± 0.08</cell><cell>14.24 ± 0.02</cell><cell>37.59 ± 0.02</cell><cell>10.25 ± 0.11</cell><cell>62.87 ± 0.42</cell></row><row><cell>LinFullDiagPost-SL</cell><cell>27.9</cell><cell>2.24 ± 0.10</cell><cell>12.04 ± 0.04</cell><cell>37.08 ± 0.06</cell><cell>10.92 ± 0.45</cell><cell>62.56 ± 0.51</cell></row><row><cell>LinFullDiagPrecPost</cell><cell>14.3</cell><cell>3.47 ± 0.36</cell><cell>7.34 ± 0.03</cell><cell>34.04 ± 0.02</cell><cell>4.04 ± 0.05</cell><cell>60.63 ± 0.44</cell></row><row><cell>LinFullDiagPrecPost-MR</cell><cell>15.6</cell><cell>2.90 ± 0.34</cell><cell>7.88 ± 0.03</cell><cell>34.24 ± 0.03</cell><cell>7.74 ± 0.06</cell><cell>60.65 ± 0.50</cell></row><row><cell>LinFullDiagPrecPost-SL</cell><cell>17.7</cell><cell>2.65 ± 0.14</cell><cell>6.84 ± 0.02</cell><cell>33.97 ± 0.06</cell><cell>4.99 ± 0.18</cell><cell>60.99 ± 0.55</cell></row><row><cell>LinFullPost</cell><cell>13.9</cell><cell>2.37 ± 0.25</cell><cell>7.34 ± 0.02</cell><cell>34.00 ± 0.02</cell><cell>5.66 ± 0.04</cell><cell>61.87 ± 0.44</cell></row><row><cell>LinFullPost-MR</cell><cell>16.8</cell><cell>1.82 ± 0.15</cell><cell>7.35 ± 0.02</cell><cell>34.27 ± 0.02</cell><cell>7.85 ± 0.07</cell><cell>60.76 ± 0.46</cell></row><row><cell>LinFullPost-SL</cell><cell>18.1</cell><cell>2.62 ± 0.27</cell><cell>6.90 ± 0.02</cell><cell>33.91 ± 0.02</cell><cell>5.32 ± 0.07</cell><cell>60.89 ± 0.47</cell></row><row><cell>ParamNoise</cell><cell>27.4</cell><cell>2.77 ± 0.15</cell><cell>1.47 ± 0.17</cell><cell>26.81 ± 0.10</cell><cell>19.04 ± 0.78</cell><cell>68.92 ± 0.53</cell></row><row><cell>ParamNoise-MR</cell><cell>23</cell><cell>2.31 ± 0.11</cell><cell>1.76 ± 0.18</cell><cell>28.20 ± 0.11</cell><cell>20.25 ± 0.41</cell><cell>70.25 ± 0.64</cell></row><row><cell>ParamNoise-SL</cell><cell>20.9</cell><cell>2.49 ± 0.09</cell><cell>1.73 ± 0.24</cell><cell>25.63 ± 0.09</cell><cell>10.62 ± 0.64</cell><cell>66.75 ± 0.54</cell></row><row><cell>Uniform</cell><cell>51</cell><cell>100.00 ± 0.15</cell><cell>100.00 ± 0.03</cell><cell>100.00 ± 0.01</cell><cell>100.00 ± 1.48</cell><cell>100.00 ± 1.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Detailed description of the algorithms in the experiments. Unless otherwise stated, algorithms use t s = 20 (mini-batches per training period)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Detailed description of the algorithms in the linear experiments. Unless otherwise stated, algorithms use t s = 100 (mini-batches per training period)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available in Python and Tensorflow at https://sites.google.com/site/deepbayesianbandits/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For reference, the standard strategy for Deep Q-Networks on Atari is to make one model update after every 4 actions performed<ref type="bibr" target="#b31">(Mnih et al., 2015;</ref><ref type="bibr" target="#b34">Osband et al., 2016;</ref><ref type="bibr" target="#b37">Plappert et al., 2017;</ref><ref type="bibr" target="#b12">Fortunato et al., 2017)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We are extremely grateful to Dan Moldovan, Sven Schmit, Matt Hoffman, Matt Johnson, Ramon Iglesias, and Rif Saurous for their valuable feedback and comments. We also thank the anonymous reviewers, whose suggestions truly helped improve the current work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Published as a conference paper at ICLR 2018 <ref type="table">Table 6</ref>: Cumulative regret incurred by models using algorithms in Section 3 on the bandits described in Section A. Values reported are the mean over 50 independent trials with standard error of the mean. Normalized with respect to the performance of Uniform. Published as a conference paper at ICLR 2018 <ref type="table">Table 7</ref>: Simple regret incurred by models using algorithms in Section 3 on the bandits described in Section A. Simple regret was approximated by averaging the regret over the final 500 steps. Values reported are the mean over 50 independent trials with standard error of the mean. Normalized with respect to the performance of Uniform. Published as a conference paper at ICLR 2018 <ref type="table">Table 8</ref>: Elapsed time for algorithms in Section 3 on the bandits described in Section A. Values reported are the mean over 50 independent trials with standard error of the mean. Normalized with respect to the elapsed time required by RMS (which uses t s = 100 and t f = 20). Published as a conference paper at ICLR 2018 <ref type="table">Table 9</ref>: Cumulative regret incurred on the Wheel Bandit problem with increasing values of δ. Values reported are the mean over 50 independent trials with standard error of the mean. Normalized with respect to the performance of Uniform. δ = 0.5 δ = 0.7 δ = 0.9 δ = 0.95 δ = 0.99</p><p>AlphaDivergence <ref type="formula">(</ref> Published as a conference paper at ICLR 2018 <ref type="table">Table 10</ref>: Simple regret incurred on the Wheel Bandit problem with increasing values of δ. Simple regret was approximated by averaging the regret over the final 500 steps. Values reported are the mean over 50 independent trials with standard error of the mean. Normalized with respect to the performance of Uniform. δ = 0.5 δ = 0.7 δ = 0.9 δ = 0.95 δ = 0.99</p><p>AlphaDivergence <ref type="formula">(1</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of thompson sampling for the multi-armed bandit problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipra</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian posterior sampling via stochastic gradient fisher scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sungjin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The million song dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thierry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-task gaussian process prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pure exploration in multi-armed bandits problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sébastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Stoltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Algorithmic learning theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="23" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical evaluation of Thompson sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The jackknife, the bootstrap and other resampling plans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A practical method for solving contextual bandit problems using decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">N</forename><surname>Elmachtoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcnellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sechan</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Petrik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04687</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gheshlaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Remi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.10295</idno>
		<title level="m">Noisy networks for exploration</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Expectation propagation as a way of life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jylänki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chopin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4869</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eigentaste: A constant time collaborative filtering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theresa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Perkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="151" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Solving twoarmed Bernoulli bandit problems using a Bayesian learning automaton</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olechristoffer</forename><surname>Granmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Computing and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="234" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable variational gaussian process classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Black-box alpha divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yingzhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational learning theory</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02989</idno>
		<title level="m">Variational gaussian dropout is not bayesian</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tommi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saul</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference On Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Preconditioned stochastic gradient langevin dynamics for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Changyou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A variational analysis of stochastic gradient algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GPflow: A Gaussian process library using TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Wilk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Boukouvalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>León-Villagrá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ep</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Expectation propagation for approximate bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Seventeenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="362" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A family of algorithms for approximate Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Helen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dharshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gaussian processes for classification: Mean-field algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2655" to="2684" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped dqn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4026" to="4034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stochastic gradient riemannian langevin dynamics on the probability simplex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fusion, propagation, and structuring in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="288" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prafulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szymon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tamim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01905</idno>
		<title level="m">Parameter space noise for exploration</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Active learning for accurate estimation of linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lazaric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to optimize via information-directed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1583" to="1591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Time-sensitive bandit learning and satisficing thompson sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1704.09028</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mushroom records drawn from the audubon society field guide to north american mushrooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schlimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GH Lincoff (Pres)</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sparse gaussian processes using pseudo-inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Weiss, Y., Schölkopf, P. B., and Platt, J. C.</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scalable bayesian optimization using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mostofa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mr</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Variational learning of inducing variables in sparse gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Michael</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>BBB BayesByBackprop with noise σ = 0.1. (ts = 100. first 100 times linear decay from ts = 10000</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">BayesByBackprop with noise σ = 1.3, and prior σp = 1.48. (ts = 50</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bbb-Mr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>first 100 times linear decay from ts = 10000</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">BayesByBackprop with noise σ = 0.03, and prior σp = 2.86. (ts = 100</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bbb-Sl</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>first 100 times linear decay from ts = 10000</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">BootstrappedNN Bootstrapped with q = 10 models, and p = 1.0. Based on RMS3 net</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Bootstrapped with q = 2 models, p = 0.95, ts = 50, t f = 50</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bootstrappednn-Mr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Based on RMS2 net</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Bootstrapped with q = 3 models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Bootstrappednn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sl</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>p = 0.92, ts = 20, t f = 20. Based on RMS3 net</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Dropout with probability p = 0.8, ts = 50, t f = 50</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dropout-Mr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Based on RMS2 net</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Ridge prior λ = 23.0. Based on RMS2 net. Trained for ts = 50, t f = 20. NeuralLinear-SL Noise prior a 0 = 38, b 0 = 1. Ridge prior λ = 1.5. Based on RMS2 net. Trained for ts = 20, t f = 20. RMS1 Greedy NN approach, fixed learning rate (γ = 0.01). RMS2 Learning rate decays, and it is reset every training period. RMS3 Learning rate decays, and it is not reset at all. It starts at γ = 1. RMS Based on RMS3 net. Learning decay rate is 0.55, initial learning rate is 1.0. Trained for ts = 100</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Dropout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dropout with probability p = 0.95, ts = 100, t f = 5. Based on RMS3 net. GP For computational reasons, it only uses the first 1000 data points. NeuralLinear Noise prior a 0 = 3, b 0 = 3. Ridge prior λ = 0.25. Based on RMS2 net. Trained for ts = 20, t f = 20</title>
		<imprint>
			<biblScope unit="volume">500</biblScope>
		</imprint>
	</monogr>
	<note>RMS-SL Based on RMS3 net. Learning decay rate is 0.4, initial learning rate is 1.1. Trained for ts = 100, t f = 20. learning rate γ = 0.014, EMA decay = 0.9, noise σ = 0.75. SGFS-MR Burning = 100, learning rate γ = 0.19, EMA decay = 0.23, noise σ = 0.33. SGFS-SL Burning = 2000, learning rate γ = 0.15, EMA decay = 0.58, noise σ = 0.34. ConstSGD Burning = 500, EMA decay = 0.9, noise σ = 0.5</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">EMA decay = 0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constsgd-Mr</forename><surname>Burning</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>noise σ = 0.44. Trained for ts = 20, t f = 50</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">EMA decay = 0.82, noise σ = 1.05. Trained for ts = 20</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constsgd-Sl</forename><surname>Burning</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">500</biblScope>
		</imprint>
	</monogr>
	<note>t f = 10. EpsGreedyRMS Initial = 0.01, multiplied by 0.999 after every context. Based on RMS3 net</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">EpsGreedyRMS-SL Initial = 0.23, multiplied by 0.95 after every context. Based on RMS2 net. Trained for ts = 50, t f = 10</title>
		<idno>EpsGreedyRMS-MR Initial = 0.046</idno>
	</analytic>
	<monogr>
		<title level="m">Based on RMS3 net. Trained for ts = 50, t f = 20</title>
		<imprint/>
	</monogr>
	<note>LinPost Ridge prior λ = 0.25. Assumed noise level σ 2 = 0.25. LinPost-MR Ridge prior λ = 11.12. Assumed noise level σ 2 = 2.0. LinPost-SL Ridge prior λ = 37.58. Assumed noise level σ 2 = 0.037</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Assumed noise level σ 2 = 0.68. LinDiagPrecPost-SL Σ −1 in Eq. 1 is diagonalized. Ridge prior λ = 13.49. Assumed noise level σ 2 = 0.01. LinGreedy Takes action with highest predicted reward for Ridge regression, λ = 0.25. Noise level σ 2 = 0.25. LinGreedy (eps = 0.01) linGreedy that selects action uniformly at random with prob p = 0.01. LinGreedy (eps = 0.05) linGreedy that selects action uniformly at random with prob p = 0.05</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindiagpost Σ In Eq</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
	<note>Assumed noise level σ 2 = 2.49. LinDiagPost-SL Σ in Eq. 1 is diagonalized. Ridge prior λ = 40.0. Assumed noise level σ 2 = 0.011. LinDiagPrecPost Σ −1 in Eq. 1 is diagonalized. Ridge prior λ = 0.25. Assumed noise level σ 2 = 0.25. LinDiagPrecPost-MR Σ −1 in Eq. 1 is diagonalized. Ridge prior λ = 37. LinFullPost Noise prior a 0 = 6, b 0 = 6. Ridge prior λ = 0.25. LinFullPost-MR Noise prior a 0 = 30.0, b 0 = 35.0. Ridge prior λ = 20.0. LinFullPost-SL Noise prior a 0 = 35.0, b 0 = 5.0. Ridge prior λ = 20.0</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">LinFullDiagPost-SL Σ in Eq. 1 is diagonalized. Noise prior a 0 = 39.94, b 0 = 0.03. Ridge prior λ = 39.74. LinFullDiagPrecPost Σ in Eq. 1 is diagonalized. Noise prior a 0 = 6, b 0 = 6. Ridge prior λ = 0.25. LinFullDiagPrecPost-MR Σ in Eq. 1 is diagonalized. Noise prior a 0 = 0.23, b 0 = 21.23. Ridge prior λ = 2.21. LinFullDiagPrecPost-SL Σ in Eq. 1 is diagonalized</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linfulldiagpost Σ In Eq</surname></persName>
		</author>
		<idno>35.89</idno>
	</analytic>
	<monogr>
		<title level="m">LinFullDiagPost-MR Σ in Eq. 1 is diagonalized. Noise prior a 0 = 22</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
	<note>ParamNoise Layer normalization. Initial noise σ = 0.01, and level = 0.01. Based on RMS2 net. ParamNoise-MR Layer normalization. Initial noise σ = 2.6, and level = 1.5. Based on RMS3 net, ts = 20, t f = 20. ParamNoise-SL Layer normalization. Initial noise σ = 1.8, and level = 2.0. Based on RMS2 net, ts = 20, t f = 50. Uniform Takes each action at random with equal probability</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
