<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Convolution for Real-time Keyword Spotting on Mobile Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Choi</surname></persName>
							<email>seungwoo.choi@hpcnt.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Hyperconnect, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
							<email>seokjun.seo@hpcnt.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Hyperconnect, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomjun</forename><surname>Shin</surname></persName>
							<email>beomjun.shin@hpcnt.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Hyperconnect, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Byun</surname></persName>
							<email>hyeongmin.byun@hpcnt.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Hyperconnect, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kersner</surname></persName>
							<email>martin.kersner@hpcnt.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Hyperconnect, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
							<email>beomsu.kim@hpcnt.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Hyperconnect, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoung</forename><surname>Kim</surname></persName>
							<email>dongyoung.kim@hpcnt.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Hyperconnect, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoo</forename><surname>Ha</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Hyperconnect, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Convolution for Real-time Keyword Spotting on Mobile Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: keyword spotting</term>
					<term>real-time</term>
					<term>convolutional neu- ral network</term>
					<term>temporal convolution</term>
					<term>mobile device</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Keyword spotting (KWS) plays a critical role in enabling speech-based user interactions on smart devices. Recent developments in the field of deep learning have led to wide adoption of convolutional neural networks (CNNs) in KWS systems due to their exceptional accuracy and robustness. The main challenge faced by KWS systems is the trade-off between high accuracy and low latency. Unfortunately, there has been little quantitative analysis of the actual latency of KWS models on mobile devices. This is especially concerning since conventional convolution-based KWS approaches are known to require a large number of operations to attain an adequate level of performance.</p><p>In this paper, we propose a temporal convolution for real-time KWS on mobile devices. Unlike most of the 2D convolution-based KWS approaches that require a deep architecture to fully capture both low-and high-frequency domains, we exploit temporal convolutions with a compact ResNet architecture. In Google Speech Command Dataset, we achieve more than 385x speedup on Google Pixel 1 and surpass the accuracy compared to the state-of-the-art model. In addition, we release the implementation of the proposed and the baseline models including an end-to-end pipeline for training models and evaluating them on mobile devices.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Keyword spotting (KWS) aims to detect pre-defined keywords in a stream of audio signals. It is widely used for hands-free control of mobile applications. Since its use is commonly concentrated on recognizing wake-up words (e.g., "Hey Siri" <ref type="bibr" target="#b0">[1]</ref>, "Alexa" <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, and "Okay Google" <ref type="bibr" target="#b3">[4]</ref>) or distinguishing common commands (e.g., "yes" or "no") on mobile devices, the response of KWS should be both immediate and accurate. However, it is challenging to implement fast and accurate KWS models that meet the real-time constraint on mobile devices with restricted hardware resources. Recently, with the success of deep learning in a variety of cognitive tasks, neural network based approaches have become popular for KWS <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Especially, KWS studies based on convolutional neural networks (CNNs) show remarkable accuracy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Most of CNN-based KWS approaches receive features, such as mel-frequency cepstral coefficients (MFCC), as a 2D input of a convolutional network. Even though such CNN-based KWS approaches offer reliable accuracy, they demand considerable computations to meet a performance requirement. In addition, inference time on mobile devices has not been analyzed quantitatively, but instead, indirect metrics have been used as a proxy to the latency. Zhang et al. <ref type="bibr" target="#b6">[7]</ref> presented the total number of multiplications and additions performed by the whole network. Tang and Lin <ref type="bibr" target="#b7">[8]</ref> reported the number of multiplications of their network as a surrogate for inference speed. Unfortunately, it has been pointed out that the number of operations such as additions and multiplications, is only an indirect alternative for the direct metric such as latency <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Neglecting the memory access costs and different platforms being equipped with varying degrees of optimized operations are potential sources for the discrepancy. Thus, we focus on the measurement of actual latency on mobile devices.</p><p>In this paper, we propose a temporal convolutional neural network for real-time KWS on mobile devices, denoted as TC-ResNet. We apply temporal convolution, i.e., 1D convolution along the temporal dimension, and treat MFCC as input channels. The proposed model utilizes advantages of temporal convolution to enhance the accuracy and reduce the latency of mobile models for KWS. Our contributions are as follows:</p><p>• We propose TC-ResNet which is a fast and accurate convolutional neural network for real-time KWS on mobile devices. According to our experiments on Google Pixel 1, the proposed model shows 385x speedup and a 0.3%p increase in accuracy compared to the state-ofthe-art CNN-based KWS model on Google Speech Commands Dataset <ref type="bibr" target="#b13">[14]</ref>.</p><p>• We release our models 1 for KWS and implementations of the state-of-the-art CNN-based KWS models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> together with the complete benchmark tool to evaluate the models on mobile devices.</p><p>• We empirically demonstrate that temporal convolution is indeed responsible for reduced computation and increased performance in terms of accuracy compared to 2D convolutions in KWS on mobile devices. <ref type="figure" target="#fig_0">Figure 1</ref> is a simplified example illustrating the difference between 2D convolution and temporal convolution for KWS approaches utilizing MFCC as input data. Assuming that stride is one and zero padding is applied to match the input and the output resolution, given input X ∈ R w×h×c and weight W ∈ R kw ×k h ×c×c , 2D convolution outputs Y ∈ R w×h×c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Temporal Convolution for KWS</head><p>MFCC is widely used for transforming raw audio into a timefrequency representation, I ∈ R t×f , where t represents the time axis (x-axis in <ref type="figure" target="#fig_0">Figure 1a</ref>) and f denotes the feature axis extracted from frequency domain (y-axis in <ref type="figure" target="#fig_0">Figure 1a</ref>). Most of the previous studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> use input tensor X ∈ R w×h×c where w = t, h = f (or vice versa), and c = 1 (X 2d ∈ R t×f ×1 in <ref type="figure" target="#fig_0">Figure 1b</ref>). CNNs are known to perform a successive transformation of low-level features into higher level concepts. However, since modern CNNs commonly utilize small kernels, it is difficult to capture informative features from both low and high frequencies with a relatively shallow network (colored box in <ref type="figure" target="#fig_0">Figure 1b</ref> only covers a limited range of frequencies). Assuming that one naively stacks n convolutional layers of 3 × 3 weights with a stride of one, the receptive field of the network only grows up to 2n + 1. We can mitigate this problem by increasing the stride or adopting pooling, attention, and recurrent units. However, many models still require a large number of operations, even if we apply these methods, and has a hard time running real-time on mobile devices.</p><p>In order to implement a fast and accurate model for realtime KWS, we reshape the input from X 2d in <ref type="figure" target="#fig_0">Figure 1b</ref> to X 1d in <ref type="figure" target="#fig_0">Figure 1c</ref>. Our main idea is to treat per-frame MFCC as a time series data, rather than an intensity or grayscale image, which is a more natural way to interpret audio. We consider I as one-dimensional sequential data whose features at each time frame are denoted as f . In other words, rather than transforming I to X 2d ∈ R t×f ×1 , we set h = 1 and c = f , which results in X 1d ∈ R t×1×f , and feed it as an input to temporal convolution ( <ref type="figure" target="#fig_0">Figure 1c</ref>). The advantages of the proposed method are as follows:</p><p>Large receptive field of audio features. In the proposed method, all lower-level features always participate in forming the higher-level features in the next layer. Thus, it takes advantage of informative features in lower layers (colored box in Figure 1c covers a whole range of frequencies), thereby avoiding stacking many layers to form higher-level features. This enables us to achieve better performance even with a small number of layers.</p><p>Small footprint and low computational complexity. Applying the proposed method, a two-dimensional feature map shrinks in size if we keep the number of parameters the same as illustrated in <ref type="figure" target="#fig_0">Figure 1b</ref> and 1c. Assuming that both conventional 2D convolution, W 2d ∈ R 3×3×1×c , and proposed temporal convolution, W 1d ∈ R 3×1×f ×c , have the same number of parameters (i.e., c = 3×c f ), the proposed temporal convolution requires a smaller number of computations compared to the 2D convolution ( 2 is smaller than 1 in <ref type="figure" target="#fig_0">Figure 1</ref>). In addition, the output feature map (i.e., the input feature map of the next layer) of the temporal convolution, Y 1d ∈ R t×1×c , is smaller than that of a 2D convolution, Y 2d ∈ R t×f ×c . The decrease in feature map size leads to a dramatic reduction of the computational burden and footprint in the following layers, which is key to implementing fast KWS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">TC-ResNet Architecture</head><p>We adopt ResNet <ref type="bibr" target="#b14">[15]</ref>, one of the most widely used CNN architectures, but utilize m × 1 kernels (m = 3 for the first layer and m = 9 for the other layers) rather than 3 × 3 kernels ( <ref type="figure">Figure 2</ref>). None of the convolution layers and fully connected layers have biases, and each batch normalization layer <ref type="bibr" target="#b15">[16]</ref> has trainable parameters for scaling and shifting. The identity shortcuts can be directly used when the input and the output have matching dimensions <ref type="figure">(Figure 2a</ref>), otherwise, we use an extra conv-BN-ReLU to match the dimensions <ref type="figure">(Figure 2b</ref>). Tang and Lin <ref type="bibr" target="#b7">[8]</ref> also adopted the residual network, but they did not employ a temporal convolution and used a conventional 3 × 3 kernel. In addition, they replaced strided convolutions with dilated convolutions of stride one. Instead, we employ temporal convolutions to increase the effective receptive field and follow the original ResNet implementation for other layers by adopting strided convolutions and excluding dilated convolutions. We select TC-ResNet8 <ref type="figure">(Figure 2c</ref>), which has three residual blocks and {16, 24, 32, 48} channels for each layer including the first convolution layer, as our base model. TC-ResNet14 <ref type="figure">(Figure 2d</ref>) expands the network by incorporating twice as much residual blocks compared to TC-ResNet8.</p><formula xml:id="formula_0">time ( ) feature ( ) MFCC ∈ ℝ × (a) 1 Input feature map ∈ ℝ × ×1 ∈ ℝ 3×3×1× Output feature map ∈ ℝ × × ① MACs = 3 × 3 × 1 × × × = 5,644,800 (b) Input feature map ∈ ℝ ×1× Output feature map ∈ ℝ ×1× ′ ② MACs = 3 × 1 × × × 1 × ′ = 141,</formula><p>We introduce width multiplier <ref type="bibr" target="#b16">[17]</ref> (k in <ref type="figure">Figure 2c</ref> and Figure 2d) to increase (or decrease) the number of channels at each layer, thereby achieving flexibility in selecting the right capacity model for given constraints. For example, in TC-ResNet8, a width multiplier of 1.5 expands the model to have {24, 36, 48, 72} number of channels respectively. We denote such a model by appending a multiplier suffix such as TC-ResNet8-1.5. TC-ResNet14-1.5 is created in the same manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setup</head><p>Dataset. We evaluated the proposed models and baselines <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref> using Google Speech Commands Dataset <ref type="bibr" target="#b13">[14]</ref>. The dataset contains 64,727 one-second-long utterance files which are recorded and labeled with one of 30 target categories. Following Google's implementation <ref type="bibr" target="#b13">[14]</ref>, we distinguish 12 classes: "yes", "no", "up", "down", "left", "right", "on", "off", "stop", "go", silence, and unknown. Using SHA-1 hashed name of the audio files, we split the dataset into training, validation, and test sets, with 80% training, 10% validation, and 10% test, respectively.</p><p>Data augmentation and preprocessing. We followed Google's preprocessing procedures which apply random shift and noise injection to training data. First, in order to generate background noise, we randomly sample and crop background noises provided in the dataset, and multiply it with a random coefficient sampled from uniform distribution, U (0, 0.1). The audio file is decoded to a float tensor and shifted by s seconds with zero padding, where s is sampled from U (−0.1, 0.1). Then, it is blended with the background noise. The raw audio is decomposed into a sequence of frames following the settings of the previous study <ref type="bibr" target="#b7">[8]</ref> where the window length is 30 ms and the stride is 10 ms for feature extraction. We use 40 MFCC features for each frame and stack them over time-axis. Training. We trained and evaluated the models using Ten-sorFlow <ref type="bibr" target="#b17">[18]</ref>. We use a weight decay of 0.001 and dropout with a probability of 0.5 to alleviate overfitting. Stochastic gradient descent is used with a momentum of 0.9 on a mini-batch of 100 samples. Models are trained from scratch for 30k iterations. Learning rate starts at 0.1 and is divided by 10 at every 10k iterations. We employ early stopping <ref type="bibr" target="#b18">[19]</ref> with the validation split.</p><p>Evaluation. We use accuracy as the main metric to evaluate how well the model performs. We trained each model 15 times and report its average performance. Receiver operating characteristic (ROC) curves, of which the x-axis is the false alarm rate and the y-axis is the false reject rate, are plotted to compare different models. To extend the ROC curve to multiclasses, we perform micro-averaging over multiple classes per experiment, then vertically average them over the experiments for the final plot.</p><p>We report the number of operations and parameters which faithfully reflect the real-world environment for mobile deployment. Unlike previous works which only reported the numbers for part of the computation such as the number of multiply operations <ref type="bibr" target="#b7">[8]</ref> or the number of multiplications and additions only in the matrix-multiplication operations <ref type="bibr" target="#b6">[7]</ref>, we include FLOPs <ref type="bibr" target="#b19">[20]</ref>, computed by TensorFlow profiling tool <ref type="bibr" target="#b20">[21]</ref>, and the number of all parameters instead of only trainable parameters reported by previous studies <ref type="bibr" target="#b7">[8]</ref>.</p><p>Inference speed can be estimated by FLOPs but it is well known that FLOPs are not always proportional to speed. Therefore, we also measure inference time on a mobile device using the TensorFlow Lite Android benchmark tool <ref type="bibr" target="#b21">[22]</ref>. We mea-sured inference time on a Google Pixel 1 and forced the model to be executed on a single little core in order to emulate the always-on nature of KWS. The benchmark program measures the inference time 50 times for each model and reports the average. Note that the inference time is measured from the first layer of models that receives MFCC as input to focus on the performance of the model itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Baseline Implementations</head><p>We carefully selected baselines and verified advantages of the proposed models in terms of accuracy, the number of parameters, FLOPs, and inference time on mobile devices. Below are the baseline models:</p><p>• CNN-1 and CNN-2 <ref type="bibr" target="#b5">[6]</ref>. We followed the implementations of <ref type="bibr" target="#b6">[7]</ref> where window size is 40 ms and the stride is 20 ms using 40 MFCC features. CNN-1 and CNN-2 represent cnn-trad-fpool3 and cnn-one-fstride4 in <ref type="bibr" target="#b5">[6]</ref>, respectively.</p><p>• DS-CNN-S, DS-CNN-M, and DS-CNN-L <ref type="bibr" target="#b6">[7]</ref>. DS-CNN utilizes depthwise convolutions. It aims to achieve the best accuracy when memory and computation resources are constrained. We followed the implementation of <ref type="bibr" target="#b6">[7]</ref> which utilizes 40 ms window size with 20 ms stride and only uses 10 MFCCs to reduce the number of operations. DS-CNN-S, DS-CNN-M, and DS-CNN-L represent small-, medium-, and large-size model, respectively.</p><p>• Res8, Res8-Narrow, Res15, and Res15-Narrow <ref type="bibr" target="#b7">[8]</ref>.</p><p>Res-variants employ a residual architecture for keyword spotting. The number following Res (e.g., 8 and 15) denotes the number of layers and the -Narrow suffix represents that the number of channels is reduced. Res15 has shown the best accuracy with Google Speech Commands Dataset among the KWS studies which are based on CNNs. The window size is 30 ms, the stride is 10 ms, and MFCC feature size is 40.</p><p>We release our end-to-end pipeline codebase for training, evaluating, and benchmarking the baseline models and together with the proposed models. It consists of TensorFlow implementation of models, scripts to convert the models into the TensorFlow Lite models that can run on mobile devices, and the pre-built TensorFlow Lite Android benchmark tool.   baseline, Res15. Compared to a slimmer Res baseline, Res8-Narrow, proposed TC-ResNet8 achieves 43x speedup while improving 6%p accuracy. Note that our wider and deeper models (e.g., TC-ResNet8-1.5, TC-ResNet14, and TC-ResNet14-1.5) achieve better accuracy at the expense of inference speed. We also plot the ROC curves of models which depict the best accuracy among their variants: CNN-1, DS-CNN-L, Res15, and TC-ResNet14-1.5. As presented in <ref type="figure" target="#fig_3">Figure 3</ref>, TC-ResNet14-1.5 is less likely to miss target keywords compared to other baselines assuming that the number of incorrectly detected keywords is the same. The small area under the curve (AUC) means that the model would miss fewer target keywords on average for various false alarm rates. TC-ResNet14-1.5 shows the smallest AUC, which is critical for good user experience with KWS system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Google Speech Command Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Impact of Temporal Convolution</head><p>We demonstrate that the proposed method could effectively improve both accuracy and inference speed compared to the baseline models which treat the feature map as a 2D image. We further explore the impact of the temporal convolution by com-  paring variants of TC-ResNet8, named 2D-ResNet8 and 2D-ResNet8-Pool, which adopt a similar network architecture and the number of parameters but utilize 2D convolutions. We designed 2D-ResNet8, whose architecture is identical to TC-ResNet8 except for the use of 3 × 3 2D convolutions. 2D-ResNet8 (in <ref type="table" target="#tab_2">Table 2</ref>) shows comparable accuracy, but is 9.2x slower compared to TC-ResNet8 (in <ref type="table" target="#tab_0">Table 1</ref>). TC-ResNet8-1.5 is able to surpass 2D-ResNet8 while using less computational resources.</p><p>We also demonstrate the use of temporal convolution is superior to other methods of reducing the number of operations in CNNs such as applying a pooling layer. In order to reduce the number of operations while minimizing the accuracy loss, CNN-1, Res8, and Res8-Narrow adopt average pooling at an early stage, specifically, right after the first convolution layer. We inserted an average pooling layer, where both the window size and the stride are set to 4, after the first convolution layer of 2D-ResNet8, and named it 2D-ResNet8-Pool. 2D-ResNet8-Pool improves inference time with the same number of parameters, however, it loses 1.2%p accuracy and is still 3.2x slower compared to TC-ResNet8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Works</head><p>Recently, there has been a wide adoption of CNNs in KWS. Sainath et al. <ref type="bibr" target="#b5">[6]</ref> proposed small-footprint CNN models for KWS. Zhang et al. <ref type="bibr" target="#b6">[7]</ref> searched and evaluated proper neural network architectures within memory and computation constraints. Tang and Lin <ref type="bibr" target="#b7">[8]</ref> exploited residual architecture and dilated convolutions to achieve further improvement in accuracy while preserving compact models. In previous studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, it has been common to use 2D convolutions for inputs with time-frequency representations. However, there has been an increase in the use of 1D convolutions in acoustics and speech domain <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>. Unlike previous studies <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref> our work applies 1D convolution along the temporal axis of time-frequency representations instead of convolving along the frequency axis or processing raw audio signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this investigation, we aimed to implement fast and accurate models for real-time KWS on mobile devices. We measured inference speed on the mobile device, Google Pixel 1, and provided quantitative analysis of conventional convolutionbased KWS models and our models utilizing temporal convolutions. Our proposed model achieved 385x speedup while improving 0.3%p accuracy compared to the state-of-the-art model. Through ablation study, we demonstrated that temporal convolution is indeed responsible for the dramatic speedup while improving the accuracy of the model. Further studies analyzing the efficacy of temporal convolutions for a diverse set of network architectures would be worthwhile.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A simplified example illustrating the difference between 2D convolution and temporal convolution. (a) MFCC. (b) 2D convolution for conventional CNN-based KWS approaches. (c) Proposed temporal convolution. Note that both the parameters of a conventional 2D convolution and that of the temporal convolution have the same size in this example by setting t = 98, f = 40, c = 160, and c = 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>16 Figure 2 :</head><label>162</label><figDesc>The building block (denoted Block) of TC-ResNet when (a) stride = 1 and (b) stride = 2. (c) Architecture for TC-ResNet8 and (d) TC-ResNet14. Each of them utilizes ResNet8 and ResNet14 as the backbone-CNN, respectively. BN and FC denote batch normalization and fully connected layer. Note that 's', 'c', and 'k' indicates stride, channel size, and width multiplier, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>False</head><label></label><figDesc>reject rate (false negative) CNN-1 (AUC: 5.22e-03) DS-CNN-L (AUC: 1.68e-03) Res15 (AUC: 1.13e-03) TC-ResNet14-1.5 (AUC: 9.02e-04)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>ROC curves for selected models with corresponding values of AUC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>shows the experimental results. Utilizing advantages of</cell></row><row><cell>temporal convolutions, we improve the inference time measured</cell></row><row><cell>on mobile device dramatically while achieving better accuracy</cell></row><row><cell>compared to the baseline KWS models. TC-ResNet8 achieves</cell></row><row><cell>29x speedup while improving 5.4%p in accuracy compared to</cell></row><row><cell>CNN-1, and improves 11.5%p in accuracy while maintaining</cell></row><row><cell>a comparable latency to CNN-2. Since DS-CNN is designed</cell></row><row><cell>for the resource-constrained environment, it shows better accu-</cell></row><row><cell>racy compared to the naive CNN models without using large</cell></row><row><cell>number of computations. However, TC-ResNet8 achieves 1.5x</cell></row><row><cell>/ 4.7x / 15.3x speedup, and improves 1.7%p / 1.2%p / 0.7%p</cell></row><row><cell>accuracy compared to DS-CNN-S / DS-CNN-M / DS-CNN-L,</cell></row><row><cell>respectively. In addition, the proposed models show better accu-</cell></row><row><cell>racy and speed compared to Res which shows the best accuracy</cell></row><row><cell>among baselines. TC-ResNet8 achieves 385x speedup while im-</cell></row><row><cell>proving 0.3%p accuracy compared to deep and complex Res</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of TC-ResNet variants, 2D-ResNet8 and 2D-ResNet8-Pool, which utilize 2D convolutions while retaining the architecture and the number of parameters of TC-ResNet8.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code can be found at the following link: https:// github.com/hyperconnect/TC-ResNet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient voice trigger detection for low resource hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sigtia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bridle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the Annual Conference of the International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Compressed time delay neural network for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodehorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vitaladevuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the Annual Conference of the International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model compression applied to small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vitaladevuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the Annual Conference of the International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Small-footprint keyword spotting using deep neural network and connectionist temporal classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03665</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the Annual Conference of the International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Hello edge: Keyword spotting on microcontrollers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07128</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A neural attention model for speech command recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L D S</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bernkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08929</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Ö</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the Annual Conference of the International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mnas-Net: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ShuffleNet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">August) Launching the speech commands dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<ptr target="https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Internal Conference on Machine Learning (ICML)</title>
		<meeting>the Internal Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Early stopping-but when?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="55" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fast spectrogram inversion using multi-head convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06719</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tensorflow</forename><surname>Profiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advisor</forename></persName>
		</author>
		<ptr target="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/README.md" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">TFLite Model Benchmark Tool</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Available</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/tensorflow/tree/r1.13/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rare sound event detection using 1d convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Detection and Classification of Acoustic Scenes and Events</title>
		<meeting>the Detection and Classification of Acoustic Scenes and Events</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks for music classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
