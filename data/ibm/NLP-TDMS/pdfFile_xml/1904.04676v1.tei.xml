<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Block Neural Autoregressive Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
							<email>nicola.decao@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
							<email>w.aziz@uva.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
							<email>ititov@inf.ed.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Block Neural Autoregressive Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Normalising flows (NFs) map two density functions via a differentiable bijection whose Jacobian determinant can be computed efficiently. Recently, as an alternative to handcrafted bijections, <ref type="bibr" target="#b11">Huang et al. (2018)</ref> proposed neural autoregressive flow (NAF) which is a universal approximator for density functions. Their flow is a neural network (NN) whose parameters are predicted by another NN. The latter grows quadratically with the size of the former and thus an efficient technique for parametrization is needed. We propose block neural autoregressive flow (B-NAF), a much more compact universal approximator of density functions, where we model a bijection directly using a single feedforward network. Invertibility is ensured by carefully designing each affine transformation with block matrices that make the flow autoregressive and (strictly) monotone. We compare B-NAF to NAF and other established flows on density estimation and approximate inference for latent variable models. Our proposed flow is competitive across datasets while using orders of magnitude fewer parameters.</p><p>Contribution We propose block neural autoregressive flows (B-NAFs) 1 , which are AFs based on a novel transformer network which transforms conditionals directly, i.e. without the need for a conditioner network. To do that we exploit the fact that invertibility only requires 1 https://github.com/nicola-decao/BNAF arXiv:1904.04676v1 [stat.ML]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Normalizing flows (NFs) map two probability density functions via an invertible transformation with tractable Jacobian <ref type="bibr" target="#b30">(Tabak et al., 2010)</ref>. They have been employed in contexts where we need to model a complex density while maintaining efficient sampling and/or density assessments. In density estimation, for example, NFs are used to map observations from a complex <ref type="bibr">(and unknown)</ref> data distribution to samples from a simple base distri-bution <ref type="bibr" target="#b28">(Rippel and Adams, 2013)</ref>. In variational inference, NFs map from a simple fixed random source (e.g. a standard Gaussian) to a complex posterior approximation while allowing for reparametrized gradient estimates and density assessments <ref type="bibr" target="#b27">(Rezende and Mohamed, 2015)</ref>.</p><p>Much of the research in NFs focuses on designing expressive transformations while satisfying practical constraints. In particular, autoregressive flows (AFs) decompose a joint distribution over y ∈ R d into a product of d univariate conditionals. A transformation y = f (x), that realizes such a decomposition, has a lower triangular Jacobian with the determinant (necessary for application of the change of variables theorem for densities) computable in O(d)-time. <ref type="bibr" target="#b16">Kingma et al. (2016)</ref> proposed inverse autoregressive flows (IAFs), an AF based on transforming each conditional by a composition of a finite number of trivially invertible affine transformations.</p><p>Recently, <ref type="bibr" target="#b11">Huang et al. (2018)</ref> introduced neural autoregressive flows (NAFs). They replace IAF's transformation by a learned bijection realized as a strictly monotonic neural network. Notably, they prove that their method is a universal approximator of real and continuous distributions. Though, whereas parametrizing affine transformations in an IAF requires predicting d pairs of scalars per step of the flow, parametrizing a NAF requires predicting all the parameters of a feedforward transformer network. The conditioner network which parametrizes the transformer grows quadratically with the width of the transformer network, thus efficient parametrization techniques are necessary. A NAF is an instance of a hyper-network <ref type="bibr" target="#b9">(Ha et al., 2017)</ref>. ∂y i /∂x i &gt; 0, and therefore, careful design of a feedforward network can ensure that the transformation is both autoregressive (with unconstrained manipulation of x &lt;i ) and strictly monotone (with positive ∂y i /∂x i ). We do so by organizing the weight matrices of dense layers in block matrices that independently transform subsets of the variables and constrain these blocks to guarantee that ∂y i /∂x j = 0 for j &gt; i and that ∂y i /∂x i &gt; 0. Our B-NAFs are much more compact than NAFs while remaining universal approximators of density functions. We evaluate them both on density estimation and variational inference showing performance on par with stateof-the-art NFs, including NAFs, IAFs, and Sylvester flows (van den <ref type="bibr" target="#b31">Berg et al., 2018)</ref>, while using orders of magnitude fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In this section, we provide an introduction to normalizing flows and their applications ( § 2.1). Then, we motivate autoregressive flows in § 2.2 and present the necessary background for our contributions ( § 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NORMALIZING FLOW</head><p>A (finite) normalizing flow is a bijective function f : X → Y between two continuous random variables X ∈ X ⊆ R d and Y ∈ Y ⊆ R d <ref type="bibr" target="#b30">(Tabak et al., 2010)</ref>. The change of variables theorem expresses a relation between the probability density functions p Y (y) and p X (x):</p><formula xml:id="formula_0">p Y (y) = p X (x) det J f (x) −1 ,<label>(1)</label></formula><p>where y = f (x), and det J f (x) is the absolute value of the determinant of the Jacobian of f evaluated at x. The Jacobian matrix is defined as</p><formula xml:id="formula_1">J f (x) ij = ∂f (x) i /∂x j .</formula><p>The determinant quantifies how f locally expands or contracts regions of X . Note that a composition of invertible functions remain invertible, thus a composition of NFs is itself a normalizing flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Density estimation</head><p>In parametric density estimation, NFs map draws from complex distributions to draws from simple ones allowing assessments of a complex likelihood. Effectively, observations x ∼ p data are modeled as draws from an NF p X|θ whose parameters θ are chosen to minimize the Kullback-Leibler divergence KL(p data p X|θ ) of the model p X|θ from p data :</p><formula xml:id="formula_2">H(p data ) − E pdata log p Y (f (x)) det J f θ (x) ,<label>(2)</label></formula><p>where H indicates the entropy. Minimizing such KL is equivalent to maximizing the log-likelihood of observations (see Appendix A for details of such derivation).</p><p>Variational inference In variational inference for deep generative models, NFs map draws from a simple density q X , e.g. N (0, I), to draws from a complex (multimodal) density q Y |θ (y). The parameters θ of the flow are estimated to minimize the KL divergence</p><formula xml:id="formula_3">KL(q Y |θ p Y ) of the true posterior p Y from q Y |θ , E q X (x) log q X (x) det J f θ (x) −1 p Y (f θ (x)) ,<label>(3)</label></formula><p>and note that this enables backpropagation through Monte Carlo (MC) estimates of the KL.</p><p>Tractability of NFs When optimizing normalizing flows with stochastic gradient descent, we have to evaluate a base density and compute the gradient with respect to its inputs. This poses no challenge as we have the flexibility to choose a simple density (e.g. uniform or Gaussian). In addition, for every x (i.e. an observation in parametric density estimation (Equation 2) or a base sample in variational inference (Equation 3)), the term | det J f θ (x) | has to be evaluated and differentiated with respect to the parameters θ. Note that f θ (x) is required to be invertible, as expressive as possible, and ideally fast to compute. In general, it is non-trivial to construct invertible transformations with efficiently com-</p><formula xml:id="formula_4">putable | det J f θ (x) |. Computing the determinant of a generic Jacobian J f θ (x) ∈ R d×d runs in O(d 3 )-time.</formula><p>Our work and current research on NFs aim at constructing parametrized flows which meet efficiency requirements while maximizing the expressiveness of the densities they can represent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AUTOREGRESSIVE FLOWS</head><p>We can construct f (x) such that its Jacobian is lower triangular, and thus has determinant</p><formula xml:id="formula_5">d i=1 ∂f (x) i /∂x i , which is computed in time O(d).</formula><p>Flows based on autoregressive transformations meet precisely this requirement <ref type="bibr" target="#b23">Oliva et al., 2018;</ref><ref type="bibr" target="#b11">Huang et al., 2018)</ref>. For a multivariate random variable X = X 1 , . . . , X d with d &gt; 1, we can use the chain rule to express the joint probability of x as product of d univariate conditional densities:</p><formula xml:id="formula_6">p X (x) = p X1 (x 1 ) d i=2 p Xi|X&lt;i (x i |x &lt;i ) .<label>(4)</label></formula><p>When we then apply a normalizing flow to each univariate density, we have an autoregressive flow. Specifically, we can use a set of functions f (i) that can be decomposed via conditioners c (i) , and invertible transformers t (i) :</p><formula xml:id="formula_7">y i = f (i) θ (x ≤i ) = t (i) θ (x i , c (i) θ (x &lt;i )) ,<label>(5)</label></formula><p>where each transformer t (i) must be an invertible function with respect to x i , and each c (i) is an unrestricted function. The resulting flow has a lower triangular Jacobian since each y i depends only on x ≤i . The flow is invertible when the Jacobian is constructed to have nonzero diagonal entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">NEURAL AUTOREGRESSIVE FLOW</head><p>The invertibility of the flow as a whole depends on each t (i) being an invertible function of x i . For example, <ref type="bibr" target="#b3">Dinh et al. (2014)</ref> and <ref type="bibr" target="#b16">Kingma et al. (2016)</ref> model each t <ref type="bibr">(i)</ref> as an affine transformation whose parameters are predicted by c (i) . As argued by <ref type="bibr" target="#b11">Huang et al. (2018)</ref>, these transformations were constructed to be trivially invertible, but their simplicity leads to a cap on expressiveness of f , thus requiring complex conditioners and a composition of multiple flows. They propose instead to learn a complex bijection using a neural network monotonic in x i -this only requires constraining t (i) to having nonnegative weights and using strictly increasing activation functions. <ref type="figure">Figure 1a</ref> outlines a NAF. Each conditioner c (i) is an unrestricted function of x &lt;i . To parametrize a monotonically increasing transformer network t (i) , the outputs of each conditioner c (i) are mapped to the positive real coordinate space by application of an appropriate activation (e.g. exp). The result is a flexible transformation with lower triangular Jacobian whose diagonal elements are positive. 2</p><p>For efficient computation of all pseudo-parameters, as <ref type="bibr" target="#b11">Huang et al. (2018)</ref> call the conditioners' outputs, they use a masked autoregressive network <ref type="bibr" target="#b6">(Germain et al., 2015)</ref>. The Jacobian of a NAF is computed using the chain rule on f θ through all its hidden layers {h ( ) } l =1 :</p><formula xml:id="formula_8">J f θ (x) = ∇ h (l) y ( ∇ h (l−1) h (l) . . . ∇ x h (1) . (6)</formula><p>Since f θ is autoregressive, J f θ (x) is lower triangular and only the diagonal needs to be computed, i.e. ∂y i /∂x i for each i. Thus, this operation requires only computing the derivatives of each t (i) , reducing the time complexity.</p><p>Because the universal approximation theorem for densities holds for NAFs <ref type="bibr" target="#b11">(Huang et al., 2018)</ref>, increasing the expressiveness of a NAF is only a matter of employing larger transformer networks. However, the conditioner grows quadratically with the size of the transformer network and a combination of restricting the size of the transformer and a technique similar to conditional weight normalization <ref type="bibr" target="#b18">(Krueger et al., 2017)</ref> is necessary to reduce the number of parameters. In §3 we propose to 2 Note that the expressiveness of a NAF comes at the cost of analytic invertibility, i.e. though each t (i) is bijective, thus invertible in principle, inverting the network itself is non-trivial.</p><p>(a) NAF: each c (i) is a neural network that predicts pseudoparameters for t <ref type="bibr">(i)</ref> , which in turns processes xi.</p><p>(b) Our B-NAF: we do not use conditioner networks, instead we learn the flow network directly. Some weights are strictly positive (solid lines), others have no constraints (dashed lines). <ref type="figure">Figure 1</ref>: Main differences between NAF <ref type="bibr" target="#b11">(Huang et al., 2018)</ref> and our B-NAF. Both networks are autoregressive and invertible since y i is processed with a function t (i) which is monotonically increasing with respect to x i and there is an arbitrary dependence on x &lt;i . parametrize the transformer network directly, i.e. without a conditioner network, by exploiting the fact that the monotonicity constraint only requires ∂y i /∂x i &gt; 0, and therefore, careful design of a single feed-forward network can directly realize a transformation that is both autoregressive (with unconstrained manipulation of x &lt;i ) and strictly monotone (with positive ∂y i /∂x i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BLOCK NEURAL AUTOREGRESSIVE FLOW</head><p>In the spirit of NAFs, we model each f</p><formula xml:id="formula_9">(i)</formula><p>θ (x ≤i ) as a neural network with parameters θ, but differently from NAFs, we do not predict θ using a conditioner network, and instead, we learn θ directly. In dense layers of f (i) θ , we employ affine transformations with strictly positive weights to process x i . This ensures strict monotonicity and thus invertibility of each f (i) θ with respect to x i . However, we do not impose this constraint on affine transformations of x &lt;i . Additionally, we need to always use invertible activation functions to ensure that the whole network is bijective (e.g., tanh or LeakyReLU). Each f (i) θ is then a univariate flow implemented as an arbitrarily wide and deep neural network which can approximate any invertible transformation. Much like other AFs, we can efficiently compute all f (i) θ in parallel by employing a single masked autoregressive network <ref type="bibr" target="#b6">(Germain et al., 2015)</ref>. In the next section, we show how to construct each affine transformation using block matrices. From now on, we will refer to our novel family of flows as block neural autoregressive flows (B-NAFs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">AFFINE TRANSFORMATIONS WITH BLOCK MATRICES</head><p>For each affine transformation of x, we parametrize the bias term freely and we construct the weight matrix W ∈ R ad×bd as a lower triangular block matrix for some</p><formula xml:id="formula_10">a, b ≥ 1. We use d × (d + 1)/2 blocks B ij ∈ R a×b for i ∈ {1, .</formula><p>., d} and 1 ≤ j ≤ i. We let B ij (with i &gt; j) be freely parametrized and constrain diagonal blocks to be strictly positive applying an element-wise function g : R → R &gt;0 to each of them. Thus:</p><formula xml:id="formula_11">W =      g(B 11 ) 0 . . . 0 B 21 g(B 22 ) . . . 0 . . . . . . . . . . . . B d1 B d2 . . . g(B dd )      ,<label>(7)</label></formula><p>where we chose g(·) = exp(·). Since the flow has to preserve the input dimensionality, the first and the last affine transformations in the network must have b = 1 and a = 1, respectively. Inside the network, the size of a and b can grow arbitrarily.</p><p>The intuition behind the construction of W is that every row of blocks B i1 , .., B ii is a set of affine transformations (projections) that are processing x ≤i . In particular, blocks in the upper triangular part of W are set to zero to make the flow autoregressive. Since the blocks B ii are mapped to R &gt;0 through g, each transformation in such set is strictly monotonic for x i and unconstrained on x &lt;i .</p><p>B-NAF with masked networks In practice, a more convenient parameterization of W consists of using a full matrixŴ ∈ R ad×bd which is then transformed applying two masking operations. One mask M d ∈ {0, 1} ad×bd selects only elements in the diagonal blocks, and a second one M o selects only off-diagonal and lower diagonal blocks. Thus, for each layer we get</p><formula xml:id="formula_12">W ( ) = g Ŵ ( ) M ( ) d +Ŵ ( ) M ( ) o ,<label>(8)</label></formula><p>where is the element-wise product. <ref type="figure">Figure 1b</ref> shows an outline of our block neural autoregressive flow.</p><p>Since each weight matrix W ( ) has some strictly positive and some zero entries, we need to take care of a proper initialization which should take that into account. Indeed, weights are usually initialized to have a zero centred normally distributed output with variance dependent on the output dimensionality <ref type="bibr" target="#b7">(Glorot and Bengio, 2010)</ref>. Instead of carefully designing a new initialization technique to take care of this, we choose to initialize all blocks with a simple distribution and to apply weight normalization  to better cope the effect of such initialization. See Appendix C for more details.</p><p>When constructing a stacked flow though a composition of n B-NAF transformations, we add gated residual connections for improving stability such that the composi-</p><formula xml:id="formula_13">tion isf n • · · · •f 2 •f 1 wheref i (x) = αf i (x) + (1 − α)x</formula><p>and α ∈ (0, 1) is a trainable scalar parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AUTOREGRESSIVENESS AND INVERTIBILITY</head><p>In this section, we show that our flow f θ :</p><formula xml:id="formula_14">R d → R d meets the following requirements: i) its Jacobian J f θ (x)</formula><p>is lower triangular (needed for efficiency in computing its determinant), and ii) the diagonal entries of such Jacobian are positive (to ensure that f θ is a bijection). Proposition 1. The final Jacobian J f θ (x) of such transformation is lower triangular. Proof sketch. When applying the chain rule (Equation 6), the Jacobian of each affine transformation is W (Equation (8), a lower triangular block matrix), whereas the Jacobian of each element-wise activation function is a diagonal matrix. A matrix multiplication between a lower triangular block matrix and a diagonal matrix yields a lower triangular block matrix, and a multiplication between two lower triangular block matrices results in a lower triangular block matrix. Therefore, after multiplying all matrices in the chain, the overall Jacobian is lower triangular. Proposition 2. When using strictly increasing activation functions (e.g., tanh or LeakyReLU), the diagonal entries of J f θ (x) are strictly positive. Proof sketch. When applying the chain rule <ref type="formula">(Equation 6</ref>), the Jacobian of each affine transformation has strictly positive values in its diagonal blocks where the Jacobian of each element-wise activation function is a diagonal matrix with strictly positive elements. When using matrix multiplication between two lower triangular block matrices (or one diagonal and one lower triangular block matrix) C = AB the resulting blocks on the diagonal of C are the result of a multiplication between only diagonal blocks of A, B. Indeed, such resulting blocks depend only on blocks of the same row and column partition.</p><p>Using the notation of Equation 7, the resulting diagonal blocks of C are B</p><formula xml:id="formula_15">(C) ii = g(B (A) ii )g(B (B)</formula><p>ii ). Therefore, they are always positive. Eventually, using the chain rule, the final Jacobian is a lower triangular matrix with strictly positive elements in its diagonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">EFFICIENT JACOBIAN COMPUTATION</head><p>Proposition 1 is particularly useful for an efficient computation of det J f θ (x) since we only need the product of its diagonal elements ∂y i /∂x i . Thus, we can avoid computing the other entries. Since the determinant is the result of a product of positive values, we also remove the absolute-value operation resulting in</p><formula xml:id="formula_16">log | det J f θ (x) | = d i=0 log J f θ (x) ii .<label>(9)</label></formula><p>Additionally, as per Proposition 2, when using matrix multiplication, elements in the diagonal blocks (or entries) depend only on diagonal blocks of the same row and column partition. Since all diagonal blocks/entries are positive, we compute them directly in the log-domain to have more numerically stable operations:</p><formula xml:id="formula_17">log J f θ (x) ii = log g(B ( ) ii ) log J σ ( ) (h ( −1) α ) · · · log g(B (1) ii ) ,<label>(10)</label></formula><p>where denotes the log-matrix multiplication, σ ( ) the strictly increasing non-linear activation function at layer , and α indicates the set of indices corresponding to diagonal elements that depend on x i . Notice that, since we chose g(·) = exp(·) we can remove all redundant operations log g(·). The log-matrix multiplication C = A B of two matrices A ∈ R m×n = logÂ and B ∈ R n×p = logB can be implemented with a stable log-sum-exp operation since</p><formula xml:id="formula_18">C ij = log n k=1 exp Â ik +B kj .<label>(11)</label></formula><p>A similar idea is also employed in NAF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">UNIVERSAL DENSITY APPROXIMATOR</head><p>In this section, we expose an intuitive proof sketch that our block neural autoregressive flow can approximate any real continuous probability density function (PDF).</p><p>Given a multivariate real and continuous random variable X = X 1 , . . . , X d , its joint distribution can be factorized into a set of univariate conditional distributions (as in Equation 4), using an arbitrary ordering of the variables, and we can define a set of univariate conditional cumulative distribution functions (CDFs)</p><formula xml:id="formula_19">Y i = F Xi|X&lt;i (x i |x &lt;i ) = P[X i ≤ x i |X &lt;i = x &lt;i ].</formula><p>According to <ref type="bibr" target="#b12">Hyvärinen and Pajunen (1999)</ref>, such decomposition exists and each individual Y i is independent as well as uniformly distributed in [0, 1]. Therefore, we can see F X as a particular normalizing flow that maps X ∈ R n to Y ∈ [0, 1] n where the distribution p Y is uniform in the hyper-cube [0, 1] n . Note that F X is an autoregressive function and its Jacobian has a positive diagonal since ∂y i /∂x i = p Xi|X&lt;i (x i |x &lt;i ).</p><p>If each univariate flow f (i) θ (see Equation <ref type="formula" target="#formula_7">5</ref>) can approximate any invertible univariate conditional CDF, then f θ can approximate any PDF <ref type="bibr" target="#b11">(Huang et al., 2018)</ref>. Note that in general, a CDF F Xi|X&lt;i is non-decreasing, thus not necessary invertible <ref type="bibr" target="#b25">(Park and Park, 2018)</ref>. Using B-NAF, each CDF is approximated with an arbitrarily large neural network and the output can be eventually mapped to (0, 1) with a sigmoidal function. Recalling that we only use positive weights for processing x i , a neural network with non-negative weights is an universal approximator of monotonic functions <ref type="bibr" target="#b2">(Daniels and Velikova, 2010)</ref>. We use strictly positive weights to approximate a strictly monotonic function for x i and we use arbitrary weights for x &lt;i (as there is no monotonicity constraint for them). Therefore, B-NAF can approximate any invertible CDF, and thus its corresponding PDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Current research on NFs focuses on constructing expressive parametrized invertible trasformations with tractable Jacobians. <ref type="bibr" target="#b27">Rezende and Mohamed (2015)</ref> were the first to suggest the use of parameterized flows in the context of variational inference proposing two parametric families: the planar and the radial flow. A drawback and bottleneck of such flows is that their power comes from stacking a large number of such transformations. More recently, van den Berg et al. (2018) generalized the use of planar flows showing improvements without increasing the number of transformations, and instead, by making each transformation more expressive.</p><p>In the context of density estimation, <ref type="bibr" target="#b6">Germain et al. (2015)</ref> proposed MADE, a masked feed-forward network that efficiently computes an autoregressive transformation. MADEs are important building blocks in AFs, such as the inverse autoregressive flows (IAFs) introduced by <ref type="bibr" target="#b16">Kingma et al. (2016)</ref>. IAFs are based on trivially invertible affine transformations of the preceding coordinates of the input vector. The parameters of each transformation (a location and a positive scale) are predicted in parallel with a MADE, and therefore IAFs have a lower triangular Jacobian whose determinant is fast to evaluate. IAFs extend the parametric families available for approximate posterior inference in variational inference. Neural autoregressive flow (NAF) by <ref type="bibr" target="#b11">Huang et al. (2018)</ref>  jective transformation to one that can approximate any monotonically increasing function. They have been used both for parametric density estimation and approximate inference. In §2.3 we explain NAFs in detail and contrast them with our proposed B-NAFs (also, see <ref type="figure">Figure 1</ref>). <ref type="formula" target="#formula_0">(2011)</ref> were among the first to employ neural networks for autoregressive density estimation (NADE), in particular, for high-dimensional binary data. Non-linear independent components estimation (NICE) explored the direction of learning a map from high-dimensional data to a latent space with a simpler factorized distribution <ref type="bibr" target="#b3">(Dinh et al., 2014)</ref>. Papamakarios et al. <ref type="formula" target="#formula_0">(2017)</ref> proposed masked autoregressive flows (MAFs) as a generalization of real non-volumepreserving flows (Real NVP) by <ref type="bibr" target="#b4">Dinh et al. (2017)</ref> showing improvements on density estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Larochelle and Murray</head><p>In this work, we are modelling a discrete normalizing flow since at each transformation a discrete step is made. Continuous normalizing flows (CNF) were proposed by <ref type="bibr" target="#b1">Chen et al. (2018)</ref> and modelled through a network that instead of predicting the output of the transformation predicts its derivative. The resulting transformation is computed using an ODE solver. <ref type="bibr" target="#b8">Grathwohl et al. (2019)</ref> further improved such formulation proposing free-form Jacobian of reversible dynamics (FFJORD).</p><p>Orthogonal work has been done for constructing powerful invertible function such as invertible 1×1 convolution (Glow) by <ref type="bibr" target="#b15">Kingma and Dhariwal (2018)</ref>, invertible d × d convolutions <ref type="bibr" target="#b10">(Hoogeboom et al., 2019)</ref>, and invertible residual networks <ref type="bibr" target="#b0">(Behrmann et al., 2018)</ref>. Additionally, <ref type="bibr" target="#b23">Oliva et al. (2018)</ref> investigated different possibilities for the conditioner of an autoregressive transformation (e.g., recurrent neural networks).  <ref type="table" target="#tab_0">Table 1</ref> of <ref type="bibr" target="#b27">(Rezende and Mohamed, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DENSITY ESTIMATION ON TOY 2D DATA</head><p>In this experiment, we use our B-NAF to perform density estimation on 2-dimensional data as this helps us visualizing the model capabilities to learn. We use the same toy data as <ref type="bibr" target="#b8">Grathwohl et al. (2019)</ref> comparing the results with Glow <ref type="bibr" target="#b15">(Kingma and Dhariwal, 2018)</ref>, as they do. Given samples from a dataset with empirical distribution p data , we parametrize a density p X|θ with a normalizing flow p X|θ (x) = p Y (f θ (x))| det J f θ (x) | using B-NAF with p Y a standard Normal distribution. We train for 20k iterations a single flow of B-NAF with 3 hidden layers of 100 units each using maximum likelihood estimation (i.e., maximizing E pdata [log p X|θ (x)], see Appendix A for more details and derivation of the objective). We used Adam optimizer <ref type="bibr" target="#b14">(Kingma and Ba, 2014)</ref> with an initial learning rate of α = 10 −1 (and decay of 0.5 with patience of 2k steps), default β 1 , β 2 , and a batch size of 200. We took figures of Glow from <ref type="bibr" target="#b8">(Grathwohl et al., 2019)</ref> who trained such models with 100 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The learned distributions of both Glow and our method can be seen in <ref type="figure" target="#fig_0">Figure 2</ref>. Glow is capable of learning a multi-modal distribution, but it has issues assigning the correct density in areas of low probability between disconnected regions. Our model is instead able to perfectly capture both multi-modality and discontinuities.  <ref type="bibr" target="#b22">(Martin et al., 2001)</ref>. Here d is the dimensionality of datapoints and N the size of the dataset. We report average (±std) across 3 independently trained models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DENSITY MATCHING ON TOY 2D DATA</head><p>In this experiment, we use B-NAF to perform density matching on 2-dimensional target energy functions to visualize the model capabilities of matching them. We use the same energy functions described by <ref type="bibr" target="#b27">Rezende and Mohamed (2015)</ref> comparing the results with them (using planar flows). For this task, we train a parameterized flow minimizing the KL divergence between the learned q Y |θ and the given target p Y . We used a single flow using a B-NAF with 2 hidden layers of 100 units each. We train by minimizing KL(q Y |θ p Y ) (see Appendix B for a detailed derivation) using Monte Carlo sampling. We optimized using Adam for 20k iterations with an initial learning rate of α = 10 −2 (and decay of 0.5 with patience of 2k steps), default β 1 , β 2 , and a batch size of 200. Planar flow figures were taken from <ref type="bibr" target="#b1">Chen et al. (2018)</ref>. Note that planar flows were trained for 500k iterations using RMSProp <ref type="bibr" target="#b31">(Tieleman and Hinton, 2012)</ref>.</p><p>Results <ref type="figure" target="#fig_1">Figure 3</ref> shows that our model perfectly matches all target distributions. Indeed, on functions 3 and 4 it looks like B-NAF can better learn the density in certain areas. The model capacity of planar normalizing flows is determined by their depth (K) and <ref type="bibr" target="#b27">Rezende and Mohamed (2015)</ref> had to stack 32 flows to match the energy function reasonably well. Deeper networks are harder to optimize, and our flow matches all the targets using a neural network with only 2 hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">REAL DATA DENSITY ESTIMATION</head><p>In this experiment, we use a B-NAF to perform density estimation on 5 real datasets. Similarly to Section 5.1, we train using MLE maximizing E pdata [log p X|θ (x)]. We compare our results against Real NVP <ref type="bibr" target="#b4">(Dinh et al., 2017)</ref>, Glow <ref type="bibr" target="#b15">(Kingma and Dhariwal, 2018)</ref>, MADE <ref type="bibr" target="#b6">(Germain et al., 2015)</ref>, MAF <ref type="bibr" target="#b24">(Papamakarios et al., 2017)</ref>, TAN <ref type="bibr" target="#b23">(Oliva et al., 2018)</ref>, NAF <ref type="bibr" target="#b11">(Huang et al., 2018)</ref>, and FFJORD <ref type="bibr" target="#b8">(Grathwohl et al., 2019)</ref>. For our B-NAF, we stacked 5 flows and we employed a small grid search on the number of layers and the size of hidden units per flow <ref type="figure" target="#fig_0">(L ∈ {1, 2} and H ∈ {10d, 20d, 40d}</ref>, respectively, where d is the input size of datapoints which is different for each dataset). When stacking B-NAF flows, the elements of each output vector are permuted so that a different set of elements is considered at each flow. This technique is not novel and it is also used by <ref type="bibr" target="#b4">Dinh et al. (2017)</ref>; <ref type="bibr" target="#b24">Papamakarios et al. (2017)</ref>; <ref type="bibr" target="#b16">Kingma et al. (2016)</ref>. We trained using Adam with Polyak averaging (with φ = 0.998) as in NAF <ref type="bibr" target="#b26">(Polyak and Juditsky, 1992)</ref>. We also applied an exponentially decaying learning rate schedule (from α = 10 −2 with rate λ = 0.5) based on no-improvement with patience of 20 epochs. We trained until convergence (but maximum 1k epochs), stopping after 100 epochs without improvement on validation set.</p><p>Datasets Following Papamakarios et al. <ref type="formula" target="#formula_0">(2017)</ref>, we perform unconditional density estimation on four datasets <ref type="bibr" target="#b5">(Dua and Karra Taniskidou, 2017)</ref> from UCI machine learning repository 3 as well as one dataset of patches of images <ref type="bibr" target="#b22">(Martin et al., 2001)</ref>: POWER containing electric power consumption in a household over a period of 4 years, GAS containing logs of 8 chemical sensors exposed to a mixture of gases, HEP-MASS, a dataset from a Monte Carlo simulation for high energy physics experiments, MINIBOONE that contains examples of electron neutrino and muon neutrino, and BSDS300 which is obtained by extracting random patches from the homonym datasets of natural images. Results <ref type="table" target="#tab_0">Table 1</ref> shows the results of this experiment reporting log-likelihood on test set. In all datasets, our B-NAF is better than Real NVP, Glow, MADE, and MAF and it performs comparable or better to NAF. B-NAF also outperforms FFJORD in all dataset except on BSDS300 where there is a marginal difference (&lt; 0.02%) between the two methods. On GAS and HEPMASS, B-NAF performs better than most of the other models and even better than NAF. In the other datasets, the gap in performance compared to NAF is marginal. We observed that in most datasets, the best performing model was the largest one in the grid search (L = 2 and H = 40d). It is possible that we do a too narrow hyper-parameter search compared to what other methods do. For instance, FFJORD results come from a wider grid search than ours. <ref type="bibr" target="#b8">Grathwohl et al. (2019)</ref>, <ref type="bibr" target="#b11">Huang et al. (2018)</ref>, and <ref type="bibr" target="#b23">Oliva et al. (2018)</ref> also varied the number of flows during tuning.</p><p>We compare NAF and our B-NAF in terms of the number of parameters employed and report the ratio between the two for each dataset in <ref type="table" target="#tab_2">Table 2</ref>. For datasets with low-dimensional datapoints (i.e, GAS and POWER) our model uses a comparable number of parameters to NAF. For high-dimensional datapoints the gap between the parameters used by NAF and B-NAF grows, with B-NAF much smaller, as we intended. For instance, on both HEPMASS and MINIBOONE, our models have marginal differences in performance with NAF while having respectively ∼ 18× and ∼ 40× fewer parameters than NAF. This evidence supports our argument that NAF models are over-parametrized and it is possible to achieve similar performance with an order of magnitude fewer parameters. Besides, when training models on GPUs, being memory efficiency allows to train more models in parallel on the same device. Additionally, in general, a normalizing flow can be a component of a larger architecture that might require more memory than the flow itself (as the models for experiments in the next Section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">VARIATIONAL AUTO-ENCODERS</head><p>An interesting application of our framework is modelling more flexible posterior distributions in a variational autoencoder (VAE) setting <ref type="bibr" target="#b17">(Kingma and Welling, 2013)</ref>. In this setting, we assume that an observation x (i.e., the data) is drawn from the marginal of a deep latent model, <ref type="figure">N (0, I)</ref> is unobserved. The goal is performing maximum likelihood estimation of the marginal. Since Z is not observed, maximizing the objective would require marginalization over the latent variables, which is generally intractable. Using variational inference <ref type="bibr" target="#b13">(Jordan et al., 1999)</ref>, we can maximize a lower bound on log-likelihood:</p><formula xml:id="formula_20">i.e. X ∼ p X|θ , where p X|θ (x) = p Z (z)p X|Z,θ (x|z)dz where Z ∼</formula><formula xml:id="formula_21">log p X|θ (x) ≥ E q Z|X,φ (z) log p XZ|θ (x, z) q Z|X,φ (z|x) ,<label>(12)</label></formula><p>where p X|Z,θ and q Z|X,φ are parametrized via neural networks with learnable parameters θ and φ <ref type="bibr" target="#b17">(Kingma and Welling, 2013)</ref>, in particular, q Z|X,φ is an approximation to the intractable posterior p Z|X,θ . This bound is called the evidence lower bound (ELBO), and maximizing the ELBO is equivalent to minimizing KL(q Z|X,φ p Z|X,θ ). The more expressive the approximating family is, the more likely we are to obtain a tight bound. Recent literature approaches tighter bounds by approximating the posterior with normalizing flows. Also note that NFs reparametrize q Z|X,φ (z|x) = q Y (f φ (z; x)) det J f φ (z;x) via a simpler fixed base distribution, e.g. a standard Gaussian, and therefore we can follow stochastic gradient estimates of the ELBO with respect to both sets of parameters. In this experiment, we use our flow for posterior approximation showing that B-NAF compares with recently proposed NFs for variational inference. We reproduce experiments by van den Berg et al. <ref type="formula" target="#formula_0">(2018)</ref> (Sylvester flows or SNF) while replacing their flow with ours. We keep the encoder and decoder networks exactly the same to fairly compare with all models trained with such procedure. We compare our B-NAF to their flows on the same 4 datasets as well as to a normal VAE <ref type="bibr" target="#b17">(Kingma and Welling, 2013)</ref>, planar flows <ref type="bibr" target="#b27">(Rezende and Mohamed, 2015)</ref>, and IAFs <ref type="bibr">(Kingma et al., 2016). 4</ref> In this experiment, the input dimensionality of the flow is fixed to d = 64. We employed a small grid search on the MNIST dataset on the number of flows K ∈ {4, 8}, and on thee size of hidden units per flow H ∈ {2d, 4d, 8d} while keeping the number of layers fixed at L = 1. The elements of each output vector are permuted after each  <ref type="bibr" target="#b19">(Lake et al., 2015)</ref> and Caltech 101 Silhouettes <ref type="bibr" target="#b21">(Marlin et al., 2010)</ref>. All those datasets consist of black and white images of different sizes.</p><p>Amortizing flow parameters When using NFs in an amortized inference setting, the parameters of each flow are not learned directly but predicted with another function from each datapoint <ref type="bibr" target="#b27">(Rezende and Mohamed, 2015)</ref>. In our case, we do not amortize all parameters of B-NAF since that would require very large predictors and we want to keep our flow memory efficient. Alternatively, every affine matrix W ∈ R n×m is shared among all datapoints. Then, for each affine transformation, we achieve a degree of amortization by predicting 3 vectors, the bias b ∈ R n and 2 vectors v 1 ∈ R n and v 2 ∈ R m that we multiply row-and column-wise respectively to W .</p><p>Results <ref type="table" target="#tab_3">Table 3</ref> shows the results of these experiments. From the grid search, it turned out that the best B-NAF model has K = 8 (flows) and H = 4d (hidden units).</p><p>Note that the best models reported by van den Berg et al. to construct Sylvester flows: orthogonal SNF and Houseolder SNF. For each datapoint, SNF has to predict from 50.7k to 76.8k values (depending on the parametrization) to fully amortize parameters of the flow, while we use only 7.7k (i.e., 6.64× to 10.0× fewer). Notably, recalling that these are not trainable parameters, we use 6.16× (orthogonal SNF) and 9.35× (Householder SNF) fewer trainable parameters as well. Besides, we also use 14.45× fewer parameters than IAF. This shows that IAF and SNF are over-parametrized too, and it is possible to achieve similar performance in the context of variational inference with an order of magnitude fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We present a new family of flexible normalizing flows, block neural autoregressive flows. B-NAFs are universal approximators of density functions and maintain an efficient parametrization. Our flow is based on directly parametrizing a transformation that guarantees autoregressiveness and monotonicity without the need for large conditioner networks and without compromising parallelism. Compared to the only other flow (to the best of our knowledge) which is also a universal approximator, our B-NAFs require orders of magnitudes fewer parameters. We validate B-NAFs on parametric density estimation on toy and real datasets, as well as, on approximate posterior inference for deep latent variable models, showing favorable performance across datasets and against various established flows. For future work, we are interested in at least two directions. One concerns gaining access to the inverse of the flow-note that, while B-NAFs and NAFs are invertible in principle, their inverses are not available in closed form. Another concerns deep generative models with large decoders (e.g. in natural language processing applications): since we achieve high flexibility at a low memory footprint our flows seem to be a good fit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>extend IAFs by generalizing the bi-Data GlowOurs Comparison between Glow and B-NAF on density estimation for 2D toy data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison between planar flow (PF) and B-NAF on four 2D energy functions from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Log-likelihood on the test set (higher is better) for 4 datasets<ref type="bibr" target="#b5">(Dua and Karra Taniskidou, 2017)</ref> from UCI machine learning and BSDS300</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ratios between the number of parameters used by NAF-DDSF (with 5 or 10 flows) and our B-NAF on all datasets for density estimation (d is the dimensionality of datapoints). In highly dimensional datasets B-NAF uses orders of magnitude fewer parameters than NAF.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">NAF (5) NAF (10)</cell></row><row><cell>POWER</cell><cell>(d = 6)</cell><cell>2.29</cell><cell>4.57</cell></row><row><cell>GAS</cell><cell>(d = 8)</cell><cell>1.30</cell><cell>2.60</cell></row><row><cell>HEPMASS</cell><cell>(d = 21)</cell><cell>17.94</cell><cell>35.88</cell></row><row><cell cols="2">MINIBOONE (d = 43)</cell><cell>43.97</cell><cell>87.91</cell></row><row><cell>BSDS300</cell><cell>(d = 63)</cell><cell>8.24</cell><cell>16.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Negative log-likelihood (NLL) and negative evidence lower bound (-ELBO) for static MNIST, Freyfaces, Omniglot and Caltech 101 Silhouettes datasets. For the Freyfaces dataset the results are reported in bits per dim. For the other datasets the results are reported in nats. For all datasets we report the mean and the standard deviations over 3 runs with different random initializations. 55±.06 82.14±.07 4.53±.02 4.40±.03 104.28±.39 97.25±.23 110.80±.46 99.62±.74 Planar 86.06±.31 81.91±.22 4.40±.06 4.31±.06 102.65±.42 96.04±.28 109.66±.42 98.53±.68 IAF 84.20±.17 80.79±.12 4.47±.05 4.38±.04 102.41±.04 96.08±.16 111.58±.38 99.92±.30 Sylvester 83.32±.06 80.22±.03 4.45±.04 4.35±.04 99.00±.04 93.77±.03 104.62±.29 93.82±.62 Ours 83.59±.15 80.71±.09 4.42±.05 4.33±.04 100.08±.07 94.83±.10 105.42±.49 94.91±.51 B-NAF flow (as we do in § 5.3). We keep the best hyperparameters of this search for the other datasets. We train using Adamax with α = 5 · 10 −4 . We point to Appendix A of van den Berg et al. (2018) for details on the network architectures for the encoder and decoder.</figDesc><table><row><cell>Model</cell><cell>MNIST -ELBO↓ NLL↓</cell><cell>Freyfaces -ELBO↓ NLL↓</cell><cell>Omniglot -ELBO↓ NLL↓</cell><cell>Caltech 101 -ELBO↓ NLL↓</cell></row><row><cell cols="3">VAE 86.Datasets Following van den Berg et al. (2018) we car-</cell><cell></cell><cell></cell></row><row><cell cols="3">ried our experiments on 4 datasets: statically binarized</cell><cell></cell><cell></cell></row><row><cell cols="3">MNIST (Larochelle and Murray, 2011), Freyfaces 5 , Om-</cell><cell></cell><cell></cell></row><row><cell>niglot</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>used 16 flows. Our model is quite flexible without being as deep as other models. Results show that B-NAF is better than normal VAE, planar flows, and IAFs in all four datasets. Although B-NAF performs slightly worse than Sylvester flows, van den Berg et al. (2018) applied a full amortization for the parameters of the flow, while we do not. They proposed two alternative parametrizations</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://archive.ics.uci.edu/ml</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Although also<ref type="bibr" target="#b11">Huang et al. (2018)</ref> proposed an experiment with VAEs for NAF, they used only one dataset (MNIST) employed a different encoder/decoder architecture than van den<ref type="bibr" target="#b31">Berg et al. (2018)</ref>. Therefore, results are not comparable.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.cs.nyu.edu/˜roweis/data/ frey_rawface.mat</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank George Papamakarios and Luca Falorsi for insightful discussions. This project is supported by SAP Innovation Center Network, ERC Starting Grant BroadSem (678254) and the Dutch Organization for Scientific Research (NWO) VIDI 639.022.518. Wilker Aziz is supported by the European Union's Horizon 2020 research and innovation programme under grant agreement No 825299 (Gourmet).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A OBJECTIVE FOR DENSITY ESTIMATION</head><p>When performing density estimation for a random variable X, we only have access to samples from the unknown target distribution X ∼ p (i.e., the unknown data distribution) but we do not have access to p directly <ref type="bibr" target="#b24">(Papamakarios et al., 2017)</ref>. Using Equation 1, we can use a normalizing flow to transform a complex parametric model p X|θ of the target distribution into a simpler distribution p Y (i.e., a uniform or a Normal distribution), which can be easily evaluated. In this case, we will learn the parameters θ of the model by minimizing KL(p p X|θ ):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C WEIGHT INITIALIZATION AND NORMALIZATION</head><p>Since the weight matrix W has some strictly positive and some zero entries, we need to take care of a proper initialization. Indeed, it is well known that principled parameters initialization benefits not only training but also the generalization of neural networks <ref type="bibr" target="#b7">(Glorot and Bengio, 2010)</ref>. For instance, Xavier initialization is commonly used and it takes into account the size of the input and output spaces in the affine transformations. However, since we have some zero entries, we cannot benefit from it. We choose instead to initialize all blocks with a simple distribution and to apply weight normalization  to better regulate the effect of such initialization. Weight normalization decomposes each row w ∈ R b·d of W in terms of the new parameters using w = exp(s) · v/ v where v has the same dimensionality of w and s is a scalar. We initialize v with a simple Normal distribution of zero mean and unit variance and s = log(u) with u ∼ U(0, 1). Such reparametrization disentangles the direction and magnitude of w and it is known to improve and speed up optimization.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00995</idno>
		<title level="m">Invertible residual networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monotone and partially monotone neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Velikova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="906" to="917" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Karra Taniskidou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Made: Masked autoencoder for distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="881" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<title level="m">FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Hypernetworks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11137</idno>
		<title level="m">Emerging convolutions for generative normalizing flows</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Neural autoregressive flows. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlinear independent component analysis: Existence and uniqueness results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pajunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="429" to="439" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10236" to="10245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Auto-encoding variational bayes. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04759</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Bayesian hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inductive principles for restricted boltzmann machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Eighth IEEE International Conference</title>
		<meeting>Eighth IEEE International Conference</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transformation autoregressive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3898" to="3907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fundamentals of Probability and Stochastic Processes with Applications to Communications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">High-dimensional probability estimation with deep density models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.5125</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Density estimation by dual ascent of the log-likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Tabak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="233" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop, coursera: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sylvester normalizing flows for variational inference. 34th Conference on Uncertainty in Artificial Intelligence (UAI18)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
