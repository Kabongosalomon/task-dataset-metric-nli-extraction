<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">END-TO-END AUDIOVISUAL SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-02-22">22 Feb 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
							<email>stavros.petridis04@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
							<email>themos.stafylakis@nottingham.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Nottingham</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feipeng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">University of Nottingham</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">END-TO-END AUDIOVISUAL SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-02-22">22 Feb 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Audiovisual Speech Recognition</term>
					<term>Resid- ual Networks</term>
					<term>End-to-End Training</term>
					<term>BGRUs</term>
					<term>Audiovisual Fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Several end-to-end deep learning approaches have been recently presented which extract either audio or visual features from the input images or audio signals and perform speech recognition. However, research on end-to-end audiovisual models is very limited. In this work, we present an end-toend audiovisual model based on residual networks and Bidirectional Gated Recurrent Units (BGRUs). To the best of our knowledge, this is the first audiovisual fusion model which simultaneously learns to extract features directly from the image pixels and audio waveforms and performs within-context word recognition on a large publicly available dataset (LRW). The model consists of two streams, one for each modality, which extract features directly from mouth regions and raw waveforms. The temporal dynamics in each stream/modality are modeled by a 2-layer BGRU and the fusion of multiple streams/modalities takes place via another 2-layer BGRU. A slight improvement in the classification rate over an end-toend audio-only and MFCC-based model is reported in clean audio conditions and low levels of noise. In presence of high levels of noise, the end-to-end audiovisual model significantly outperforms both audio-only models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Traditional audiovisual fusion systems consist of two stages, feature extraction from the image and audio signals and combination of the features for joint classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Recently, several deep learning approaches for audiovisual fusion have been presented which aim to replace the feature extraction stage with deep bottleneck architectures. Usually a transform, like principal component analysis (PCA), is first applied to the mouth region of interest (ROI) and spectrograms or concatenated Mel-Frequency Cepstral Coefficients <ref type="bibr">ACCEPTED TO ICASSP 2018</ref> (MFCCs) and a deep autoencoder is trained to extract bottleneck features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Then these features are fed to a classifier like a support vector machine or a Hidden Markov Model.</p><p>Few works have been presented very recently which follow an end-to-end approach for visual speech recognition. The main approaches followed can be divided into two groups. In the first one, fully connected layers are used to extract features and LSTM layers model the temporal dynamics of the sequence <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. In the second group, a 3D convolutional layer is used followed either by standard convolutional layers <ref type="bibr" target="#b11">[12]</ref> or residual networks (ResNet) <ref type="bibr" target="#b12">[13]</ref> combined with LSTMs or GRUs. End-to-end approaches have also been successfully used for speech emotion recognition using 1D CNNs and LSTMs <ref type="bibr" target="#b13">[14]</ref>.</p><p>However, work on end-to-end audiovisual speech recognition has been very limited. To the best of our knowledge, there are only two works which perform end-to-end training for audiovisual speech recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. In the former, an attention mechanism is applied to both the mouth ROIs and MFCCs and the model is trained end-to-end. However, the system does not use the raw audio signal or spectrogram but relies on MFCC features. In the latter, fully connected layers together with LSTMs are used in order to extract features directly from raw images and spectrograms and perform classification on the OuluVS database <ref type="bibr" target="#b16">[17]</ref>.</p><p>In this paper, we extend the work of <ref type="bibr" target="#b9">[10]</ref>, which mainly works for small databases, using ResNets as proposed in <ref type="bibr" target="#b12">[13]</ref>. To the best of our knowledge, this is the first endto-end model which performs audiovisual word recognition from raw mouth ROIs and waveforms on a large in-the-wild database. The proposed model consists of two streams, one per modality, which extract features directly from the raw images and waveforms, respectively. Each stream consists of a ResNet which extracts features from the raw inputs. This is followed by a 2-layer BGRU network which models the temporal dynamics in each stream. Finally, the information of the different streams/modalities is fused via another 2-layer BGRU which models the joint temporal dynamics. A similar architecture has been proposed by <ref type="bibr" target="#b17">[18]</ref> for audiovisual emo- tion recognition. The main differences of our work are the following: 1) we use a ResNet for the audio stream instead of a rather shallow 2-layer CNN, 2) we do not use a pretrained ResNet for the visual stream but we train a ResNet from scratch, 3) we use BGRUs in each stream which help modeling the temporal dynamics of each modality instead of using just one BLSM layer at the top and 4) we use a training procedure which allows for efficient end-to-end training of the entire network.</p><p>We perform classification of 500 words from the LRW database achieving state-of-the-art performance for audiovisual fusion. The proposed system results in an absolute increase of 0.3% in classification accuracy over the end-to-end audio-only model and an MFCC-based system. The end-toend audiovisual fusion model also significantly outperforms (up to 14.1% absolute improvement) the audio-only models under high levels of noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">LRW DATABASE</head><p>For the purposes of this study we use the Lip Reading in the Wild (LRW) database <ref type="bibr" target="#b18">[19]</ref> which is the largest publicly available lipreading dataset in the wild. The database consists of short segments (1.16 seconds) from BBC programs, mainly news and talk shows. It is a very challenging set since it contains more than 1000 speakers and large variation in head pose and illumination. The number of words, 500, is also much higher than existing lipreading databases used for word recognition, which typically contain 10 to 50 words <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Another characteristic of the database is the presence of several words which are visually similar. For example, there are words which are present in their singular and plural forms or simply different forms of the same word, e.g., America and American. We should also emphasise that words appear in the middle of an utterance and there may be co-articulation of the lips from preceding and subsequent words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">END-TO-END AUDIOVISUAL SPEECH RECOGNITION</head><p>The proposed deep learning system for audiovisual fusion is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. It consists of two streams which extract 3D Conv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax (500) Target</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-18</head><p>Image Sequence Audio Waveform features directly from the raw input images and the audio waveforms, respectively. Each stream consists of two parts: a residual network (ResNet) <ref type="bibr" target="#b21">[22]</ref> which learns to automatically extract features from the raw image and waveform, respectively and a 2-layer BGRU which models the temporal dynamics of the features in each stream. Finally, 2 BGRU layers on top of the two streams are used in order to fuse the information of the audio and visual streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Visual Stream</head><p>The visual stream is similar to <ref type="bibr" target="#b12">[13]</ref> and consists of a spatiotemporal convolution followed by a 34-layer ResNet and a 2-layer BGRU. A spatiotemporal convolutional layer is capable of capturing the short-term dynamics of the mouth region and is proven to be advantageous, even when recurrent networks are deployed for back-end <ref type="bibr" target="#b11">[12]</ref>. It consists of a convolutional layer with 64 3D kernels of 5 by 7 by 7 size (time/width/height), followed by batch normalization and rectified linear units. We use the 34-layer identity mapping version, which was proposed for ImageNet <ref type="bibr" target="#b22">[23]</ref>. The ResNet drops progressively the spatial dimensionality until its output becomes a single dimensional tensor per time step. We should emphasize that we did not make use of pretrained models, as they are optimized for completely different tasks (e.g. static colored images from ImageNet or CIFAR). Finally, the output of ResNet-34 is fed to a 2-layer BGRU which consists of 1024 cells in each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Audio Stream</head><p>The audio stream consists of an 18-layer ResNet followed by two BGRU layers. There is no need to use a spatiotemporal convolution front-end in this case as the audio waveform is an 1D signal. We use the standard architecture for the ResNet-18 with the main difference being that we use 1D instead of 2D kernels which are used for image data. A temporal kernel of 5ms with a stride of 0.25ms is used in the first convolutional layer in order to extract fine-scale spectral information. The output of the ResNet is divided into 29 frames/windows using average pooling in order to ensure the same frame rate as the video is used. These audio frames are then fed to the following ResNet layers which consist of the default kernels of size 3 by 1 so deeper layers extract long-term speech characteristics. The output of the ResNet-18 is fed to a 2-layer BGRU which consists of 1024 cells in each layer (using the same architecture as in <ref type="bibr" target="#b12">[13]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classification Layers</head><p>The BGRU outputs of each stream are concatenated and fed to another 2-layer BGRU in order to fuse the information from the audio and visual streams and jointly model their temporal dynamics. The output layer is a softmax layer which provides a label to each frame. The sequence is labeled based on the highest average probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Preprocessing</head><p>Video: The first step is the extraction of the mouth region of interest (ROI). Since the mouth ROIs are already centered, a fixed bounding box of 96 by 96 is used for all videos as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Finally, the frames are transformed to grayscale and are normalized with respect to the overall mean and variance. Audio: Each audio segment is z-normalised, i.e., has zero mean and standard deviation one to account for variations in different levels of loudness between the speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Protocol</head><p>The video segments are already partitioned into training, validation and test sets. There are between 800 and 1000 sequences for each word in the training set and 50 sequences in the validation and test sets, respectively. In total there are 488766, 25000, and 25000 examples in the training, validation and test sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training</head><p>Training is divided into 2 phases: first the audio/visual streams are trained independently and then the audiovisual network is trained end-to-end. During training data augmentation is performed on the video sequences of mouth ROIs. This is done by applying random cropping and horizontal flips with probability 50% to all frames of a given clip. Data augmentation is also applied to the audio sequences. During training babble noise at different levels (between -5 dB to 20 db) might be added to the original audio clip. The selection of one of the noise levels or the use of the clean audio is done using a uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Single Stream Training</head><p>Initialisation: First, each stream is trained independently. Directly training end-to-end each stream leads to suboptimal performance so we follow the same 3-step procedure as in <ref type="bibr" target="#b12">[13]</ref>. Initially, a temporal convolutional back-end is used instead of the 2-layer BGRU. The combination of ResNet and temporal convolution (together with a softmax output layer) is trained until there is no improvement in the classification rate on the validation set for more than 5 epochs. Then the temporal convolutional back-end is removed and the BGRU back-end is attached. The 2-layer BGRU (again with a sotfmax output layer) is trained for 5 epochs, keeping the weights of the 3D convolution front-end and the ResNet fixed. End-to-End Training: Once the ResNet and the 2-layer BGRU in each stream have been pretrained then they are put together and the entire stream is trained end-to-end (using a softmax output layer). The Adam training algorithm <ref type="bibr" target="#b23">[24]</ref> is used for end-to-end training with a mini-batch size of 36 sequences and an initial learning rate of 0.0003. Early stopping with a delay of 5 epochs was also used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Audiovisual Training</head><p>Initialisation: Once the single streams have been trained then they are used for initialising the corresponding streams in the multi-stream architecture. Then another 2-layer BGRU is added on top of all streams in order to fuse the single stream outputs. The top BGRU is first trained for 5 epochs (with a softmax output layer), keeping the weights of the audio and visual streams fixed. End-to-End Training: Finally, the entire audiovisual network is trained jointly using Adam with a mini-batch size of 18 sequences and an initial learning rate of 0.0001. Early stopping is also applied with a delay of 5 epochs. <ref type="table" target="#tab_0">Table 1</ref>. We report the performance of the end-to-end audio-only, visual-only and audiovisual models. For comparison purposes, since there are no previous  <ref type="bibr" target="#b14">[15]</ref> 76.2 V <ref type="bibr" target="#b18">[19]</ref> 61.1 A + V (End-to-End) 98.0 audio/audiovisual results on the LRW database we also compute the performance of a 2-layer BGRU network trained with MFCC features which are the standard features for acoustic speech recognition. We use 13 coefficients (and their deltas) using a 40ms window and a 10ms step. The network is trained in the same way as the BGRU networks in section 4.3 with the only difference that it was trained for longer using early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results are shown in</head><p>The end-to-end audio system results in a similar performance to MFCCs which is a significant result given that the input to the system is just the raw waveform. However, we should note that the effort required in order to train the endto-end system is significantly higher than the 2-layer BGRU used with MFCCs. The end-to-end audiovisual system leads to a small improvement over the audio-only models of 0.3%. This is expected since the contribution of the visual modality is usually marginal in clean audio conditions as reported in previous works as well <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>In order to investigate the robustness to audio noise of the audiovisual fusion approach we run experiments under varying noise levels. The audio signal for each sequence is corrupted by additive babble noise from the NOISEX database <ref type="bibr" target="#b24">[25]</ref> so as the SNR varies from -5 dB to 20 dB.</p><p>Results for the audio, visual and audiovisual models under noisy conditions are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The video-only classifier (blue solid line) is not affected by the addition of the audio noise and therefore its performance remains constant over all noise levels. On the other hand, as expected, the performance of the audio classifier (red dashed line) is significantly affected. Similarly, the performance of the MFCC classifier (purple solid line) is also significantly affected by noise. It is interesting to point out that although the MFCC and end-to-end audio models result in the same performance when audio is clean or under low levels of noise (10 to 20 dB), the end-to-end audio model results in much better performance under high levels of noise (-5 dB to 5 dB). It results in an absolute improvement of 0.9%, 3.5% and 7.5% over the MFCC classifier, at 5 dB, 0 dB and -5 dB, respectively. The audiovisual model (yellow dotted line) is more robust to audio noise than the audio-only models. It performs slightly better under low noise levels (10 dB to 20 dB) but it significantly outperforms both of them under high noise levels (-5 dB to 5 dB). In particular, it leads to an absolute improvement of 1.3%, 3.9% and 14.1% over the end-to-end audio-only model at 5 dB, 0 dB and -5 dB, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this work, we present an end-to-end visual audiovisual fusion system which jointly learns to extract features directly from the pixels and audio waveforms and performs classification using BGRUs. Results on the largest publicly available database for within-context word recognition in the wild show that the end-to-end audiovisual model slightly outperforms a standard MFCC-based system under clean conditions and low levels of noise. It also significantly outperforms the end-to-end and MFCC-based audio models in the presence of high levels of noise. A natural next step would be to extend the system in order to be able to recognise sentences instead of isolated words. Finally, it would also be interesting to investigate in future work an adaptive fusion mechanism which learns to weight each modality based on the noise levels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Example of mouth ROI extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the end-to-end audiovisual speech recognition system. Two streams are used for feature extraction directly from the raw images and audio waveforms. The temporal dynamics are modelled by BGRUs in each stream. The top two BGRUs fuse the information of the audio and visual streams and jointly model their temporal dynamics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Classification Rate (CR) as a function of the noise level. A: End-to-End audio model. V: End-to-End visual model, AV: End-to-End audiovisual model. MFCC: A 2-layer BGRU trained with MFCCs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Classification Rate (CR) of the Audio-only (A), Video-only (V) and audiovisual models (A + V) on the LRW database. *This is a similar end-to-end model which uses a different mouth ROI, computed based on tracked facial landmarks, in each video. In this work, we use a fixed mouth ROI for all videos.</figDesc><table><row><cell>Stream</cell><cell>CR</cell></row><row><cell>A (End-to-End)</cell><cell>97.7</cell></row><row><cell>A (MFCC)</cell><cell>97.7</cell></row><row><cell>V (End-to-End)</cell><cell>82.0</cell></row><row><cell>V [13]*</cell><cell>83.0</cell></row><row><cell>V</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>This work has been funded by the European Community Horizon 2020 under grant agreement no. 645094 (SEWA). Themos Stafylakis has been partly funded by the European Commission program Horizon 2020, under grant agreement no. 706668 (Talking Heads).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recent advances in the automatic recognition of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1306" to="1326" />
			<date type="published" when="2003-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Audio-visual speech modeling for continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prediction-based audiovisual fusion for classification of non-linguistic vocalisations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="58" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal multimodal learning in audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3574" to="3582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Integration of deep bottleneck features for audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ninomiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iribe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep multimodal learning for audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2130" to="2134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Audio-visual speech recognition using bimodal-trained bottleneck features for a person with severe hearing loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ariki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Omori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakazono</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="277" to="281" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep complementary bottleneck features for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="2304" to="2308" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end visual speech recognition with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2592" to="2596" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lipreading with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6115" to="6119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Lipnet: Sentence-level lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining residual networks with LSTMs for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3652" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brueckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5200" to="5204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-toend audiovisual fusion with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Auditory-Visual Speech Processing Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ouluvs2: A multi-view audiovisual database for nonrigid mouth motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Anina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik√§inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE FG</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end multimodal emotion recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Moving-talker, speaker-independent feature study, and baseline results using the CUAVE multimodal speech corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gurbuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tufekci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gowdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Appl. Signal Process</title>
		<imprint>
			<biblScope unit="volume">2002</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1189" to="1201" />
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Assessment for automatic speech recognition: Ii. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steeneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="251" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
