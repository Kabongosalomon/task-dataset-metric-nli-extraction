<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interpolation Consistency Training for Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
							<email>vikasverma.iitm@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Québec Artificial Intelligence Institute</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
							<email>kkawaguchi@fas.harvard.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Harvard University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
							<email>lambalex@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Québec Artificial Intelligence Institute</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
							<email>juho.kannala@aalto.fiyoshua.umontreal@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Aalto University</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Québec Artificial Intelligence Institute</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interpolation Consistency Training for Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-theart performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets. Our theoretical analysis shows that ICT corresponds to a certain type of data-adaptive regularization with unlabeled points which reduces overfitting to labeled points under high confidence values.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning achieves excellent performance in supervised learning tasks where labeled data is abundant <ref type="bibr" target="#b9">(LeCun et al., 2015)</ref>. However, labeling large amounts of data is often prohibitive due to time, financial, and expertise constraints. As machine learning permeates an increasing variety of domains, the number of applications where unlabeled data is voluminous and labels are scarce increases. For instance, recognizing documents in extinct languages, where a machine learning system has access to a few labels, produced by highly-skilled scholars <ref type="bibr" target="#b4">(Clanuwat et al., 2018)</ref>.</p><p>The goal of Semi-Supervised Learning (SSL) <ref type="bibr">(Chapelle et al., 2010)</ref> is to leverage large amounts of unlabeled data to improve the performance of supervised learning over small datasets. Often, SSL algorithms use unlabeled data to learn additional structure about the input distribution. For instance, the existence of cluster structures in the input distribution could hint the separation of samples into different labels. This is often called the cluster assumption: if two samples belong to the same cluster in the input distribution, then they are likely to belong to the same class. The cluster assumption is equivalent to the low-density separation assumption: the decision boundary should lie in the low-density regions. The equivalence is easy to infer: A decision boundary which lies in a high-density region, will cut a cluster into two different classes, requiring that samples from different classes lie in the same cluster; which is the violation of the cluster assumption. The low-density separation assumption has inspired many recent consistency-regularization semi-supervised learning techniques, including the Π-model <ref type="bibr" target="#b20">(Sajjadi et al., 2016;</ref><ref type="bibr" target="#b7">Laine &amp; Aila, 2016)</ref>, temporal ensembling <ref type="bibr" target="#b7">(Laine &amp; Aila, 2016)</ref>, VAT <ref type="bibr" target="#b13">(Miyato et al., 2018)</ref>, and the Mean-Teacher <ref type="bibr" target="#b22">(Tarvainen &amp; Valpola, 2017)</ref>. : Interpolation Consistency Training (ICT) applied to the "two moons" dataset, when three labels per class (large dots) and a large amount of unlabeled data (small dots) is available. When compared to supervised learning (red), ICT encourages a decision boundary traversing a low-density region that would better reflect the structure of the unlabeled data. Both methods employ a multilayer perceptron with three hidden ReLU layers of twenty neurons.</p><p>Consistency regularization methods for semi-supervised learning enforce the low-density separation assumption by encouraging invariant prediction f (u) = f (u + δ) for perturbations u + δ of unlabeled points u. Such consistency and small prediction error can be satisfied simultaneously if and only if the decision boundary traverses a low-density path.</p><p>Different consistency regularization techniques vary in how they choose the unlabeled data perturbations δ. One simple alternative is to use random perturbations δ. However, random perturbations are inefficient in high dimensions, as only a tiny proportion of input perturbations are capable of pushing the decision boundary into low-density regions. To alleviate this issue, Virtual Adversarial Training or VAT <ref type="bibr" target="#b13">(Miyato et al., 2018)</ref>, searches for small perturbations δ that maximize the change in the prediction of the model. This involves computing the gradient of the predictor with respect to its input, which can be expensive for large neural network models.</p><p>This additional computation makes VAT <ref type="bibr" target="#b13">(Miyato et al., 2018)</ref> and other related methods such as <ref type="bibr" target="#b18">(Park et al., 2018)</ref> less appealing in situations where unlabeled data is available in large quantities. Furthermore, recent research has shown that training with adversarial perturbations can hurt generalization performance <ref type="bibr" target="#b16">(Nakkiran, 2019;</ref><ref type="bibr" target="#b24">Tsipras et al., 2018)</ref>.</p><p>To overcome the above limitations, we propose the Interpolation Consistency Training (ICT), an efficient consistency regularization technique for state-of-the-art semi-supervised learning. In a nutshell, ICT regularizes semi-supervised learning by encouraging consistent predictions f (αu 1 + (1 − α)u 2 ) = αf (u 1 ) + (1 − α)f (u 2 ) at interpolations αu 1 + (1 − α)u 2 of unlabeled points u 1 and u 2 .</p><p>Our experimental results on the benchmark datasets CIFAR10 and SVHN and neural network architectures CNN-13 <ref type="bibr" target="#b7">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b13">Miyato et al., 2018;</ref><ref type="bibr" target="#b22">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b18">Park et al., 2018;</ref><ref type="bibr" target="#b12">Luo et al., 2018)</ref> and WRN28-2 <ref type="bibr" target="#b17">(Oliver et al., 2018)</ref> outperform (or are competitive with) the state-of-the-art methods. ICT is simpler and more computation efficient than several of the recent SSL algorithms, making it an appealing approach to SSL. <ref type="figure" target="#fig_2">Figure 1</ref> illustrates how ICT learns a decision boundary traversing a low density region in the "two moons" problem. Moreover, whereas theoretical understanding of a state-of-the-art method in deep learning is known to be challenging in general, we provide a novel theory of ICT to understand how and when ICT can succeed or fail to effectively utilize unlabeled points. To begin with, observe that the most useful samples on which the consistency regularization should be applied are the samples near the decision boundary. Adding a small perturbation δ to such low-margin unlabeled samples u j is likely to push u j + δ over the other side of the decision boundary. This would violate the low-density separation assumption, making u j + δ a good place to apply consistency regularization. These violations do not occur at high-margin unlabeled points that lie far away from the decision boundary.</p><formula xml:id="formula_0">(xi, yi) ∼ DL f θ (xi) Supervised loss (ŷi, yi) uj ∼ DUL f θ (uj) Supervised Loss + wt· Consistency Loss Mix λ (uj, u k ) f θ (um) Consistency loss (Mix λ (ŷj,ŷ k ),ŷm) u k ∼ DUL f θ (u k )</formula><p>Back to low-margin unlabeled points u j , how can we find a perturbation δ such that u j and u j + δ lie on opposite sides of the decision boundary? Although tempting, using random perturbations is an inefficient strategy, since the subset of directions approaching the decision boundary is a tiny fraction of the ambient space.</p><p>Instead, consider interpolations u j +δ = Mix λ (u j , u k ) towards a second randomly selected unlabeled examples u k . Then, the two unlabeled samples u j and u k can either:</p><p>1. lie in the same cluster, 2. lie in different clusters but belong to the same class, 3. lie on different clusters and belong to the different classes.</p><p>Assuming the cluster assumption, the probability of (1) decreases as the number of classes increases. The probability of (2) is low if we assume that the number of clusters for each class is balanced. Finally, the probability of (3) is the highest. Then, assuming that one of (u j , u k ) lies near the decision boundary (it is a good candidate for enforcing consistency), it is likely (because of the high probability of (3)) that the interpolation towards u k points towards a region of low density, followed by the cluster of the other class. Since this is a good direction to move the decision, the interpolation is a good perturbation for consistency-based regularization.</p><p>Our exposition has argued so far that interpolations between random unlabeled samples are likely to fall in low-density regions. Thus, such interpolations are good locations where consistency-based regularization could be applied. But how should we label those interpolations? Unlike random or adversarial perturbations of single unlabeled examples u j , our scheme involves two unlabeled examples (u j , u k ). Intuitively, we would like to push the decision boundary as far as possible from the class boundaries, as it is well known that decision boundaries with large margin generalize better <ref type="bibr" target="#b21">(Shawe-Taylor et al., 1996)</ref>. In the supervised learning setting, one method to achieve large-margin decision boundaries is mixup . In mixup, the decision boundary is pushed far away from the class boundaries by enforcing the prediction model to change linearly in between samples. This is done by training the model f θ to predict Mix λ (y, y ) at location Mix λ (x, x ), for random pairs of labeled samples <ref type="bibr">((x, y)</ref>, (x , y )). Here we extend mixup to the semi-supervised learning setting by training the model f θ to predict the "fake label" Mix λ (f θ (u j ), f θ (u k )) at location Mix λ (u j , u k ). In order to follow a more conservative consistent regularization, we encourage the model f θ to predict the fake label Mix λ (f θ (u j ), f θ (u k )) at location Mix λ (u j , u k ), where θ is a moving average of θ, also known as a mean-teacher <ref type="bibr" target="#b22">(Tarvainen &amp; Valpola, 2017)</ref>.</p><p>We are now ready to describe in detail the proposed Interpolation Consistency Training (ICT). Consider access to labeled samples (x i , y i ) ∼ D L , drawn from the joint distribution P XY (X, Y ). Also, consider access to unlabeled samples u j , u k ∼ D U L , drawn from the marginal distribution P X (X) = P XY (X,Y ) P Y |X (Y |X) . Our learning goal is to train a model f θ , able to predict Y from X. By using stochastic gradient descent, at each iteration t, update the parameters θ to minimize</p><formula xml:id="formula_1">L = L S + w(t) · L U S</formula><p>where L S is the usual cross-entropy supervised learning loss over labeled samples D L , and L U S is our new interpolation consistency regularization term. These two losses are computed on top of (labeled and unlabeled) minibatches, and the ramp function w(t) increases the importance of the consistency regularization term L U S after each iteration. To compute L U S , sample two minibatches of unlabeled points u j and u k , and compute their fake</p><formula xml:id="formula_2">labelsŷ j = f θ (u j ) andŷ k = f θ (u k ),</formula><p>where θ is an moving average of θ <ref type="bibr" target="#b22">(Tarvainen &amp; Valpola, 2017)</ref>. Second, compute the interpolation u m = Mix λ (u j , u k ), as well as the model prediction at that location,ŷ m = f θ (u m ). Third, update the parameters θ as to bring the predictionŷ m closer to the interpolation of the fake labels Mix λ (ŷ j ,ŷ k ). The discrepancy between the predictionŷ m and Mix λ (ŷ j ,ŷ k ) can be measured using any loss; in our experiments, we use the mean squared error. Following , on each update we sample a random λ from Beta(α, α).</p><p>In sum, the population version of our ICT term can be written as:</p><formula xml:id="formula_3">L U S = E uj ,u k ∼P X E λ∼Beta(α,α) (f θ (Mix λ (u j , u k )), Mix λ (f θ (u j ), f θ (u k )))<label>(1)</label></formula><p>ICT is summarized in <ref type="figure" target="#fig_1">Figure 2</ref> and Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We follow the common practice in semi-supervised learning literature <ref type="bibr" target="#b7">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b13">Miyato et al., 2018;</ref><ref type="bibr" target="#b22">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b18">Park et al., 2018;</ref><ref type="bibr" target="#b12">Luo et al., 2018)</ref> and conduct experiments using the CIFAR-10 and SVHN datasets, where only a fraction of the training data is labeled, and the remaining data is used as unlabeled data. We followed the standardized procedures laid out by <ref type="bibr" target="#b17">(Oliver et al., 2018)</ref> to ensure a fair comparison.</p><p>The CIFAR-10 dataset consists of 60000 color images each of size 32 × 32, split between 50K training and 10K test images. This dataset has ten classes, which include images of natural objects such as cars, horses, airplanes and deer. The SVHN dataset consists of 73257 training samples and 26032 test samples each of size 32 × 32. Each example is a close-up image of a house number (the ten classes are the digits from 0-9).</p><p>We adopt the standard data-augmentation and pre-processing scheme which has become standard practice in the semi-supervised learning literature <ref type="bibr" target="#b20">(Sajjadi et al., 2016;</ref><ref type="bibr" target="#b7">Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b22">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b13">Miyato et al., 2018;</ref><ref type="bibr" target="#b12">Luo et al., 2018;</ref><ref type="bibr" target="#b0">Athiwaratkun et al., 2019)</ref>. More specifically, for CIFAR-10, we first zero-pad each image with 2 pixels on each side. Then, the resulting image is randomly cropped to produce a new 32 × 32 image. Next, the image is horizontally flipped with probability 0.5, followed by per-channel standardization and ZCA preprocessing. For SVHN, we zero-pad each image with 2 pixels on each side and then randomly crop the resulting image to produce a new 32 × 32 image, followed by zero-mean and unit-variance image whitening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Models</head><p>We conduct our experiments using CNN-13 and Wide-Resnet-28-2 architectures. The CNN-13 architecture has been adopted as the standard benchmark architecture in recent state-of-the-art SSL </p><formula xml:id="formula_4">Require: Mix λ (a, b) = λa + (1 − λ)b. for t = 1, . . . , T do Sample {(x i , y i )} B i=1 ∼ D L (x, y) Sample labeled minibatch L S = CrossEntropy({(f θ (x i ), y i )} B i=1 ) Supervised loss (cross-entropy) Sample {u j } U j=1 , {u k } U k=1 ∼ D U L (x) Sample two unlabeled minibatchs {ŷ j } U j=1 = {f θ (u j )} U j=1 , {ŷ k } U k=1 = {f θ (u k )} U k=1</formula><p>Compute psuedo labels</p><formula xml:id="formula_5">Sample λ ∼ Q sample an interpolation coefficient (u m = Mix λ (u j , u k ),ŷ m = Mix λ (ŷ j ,ŷ k )) Compute interpolation L U S = ConsistencyLoss({(f θ (u m ),ŷ m )} U m=1 ) e.g., mean squared error L = L S + w(t) · L U S Total Loss g θ ← ∇ θ L Compute Gradients θ = αθ + (1 − α)θ</formula><p>Update moving average of parameters θ ← Step(θ, g θ ) e.g. SGD, Adam end for return θ methods <ref type="bibr" target="#b7">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b22">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b13">Miyato et al., 2018;</ref><ref type="bibr" target="#b18">Park et al., 2018;</ref><ref type="bibr" target="#b12">Luo et al., 2018)</ref>. We use its variant (i.e., without additive Gaussian noise in the input layer) as implemented in <ref type="bibr" target="#b0">(Athiwaratkun et al., 2019)</ref>. We also removed the Dropout noise to isolate the improvement achieved through our method. Other SSL methods in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> use the Dropout noise, which gives them more regularizing capabilities. Despite this, our method outperforms other methods in several experimental settings. <ref type="bibr" target="#b17">(Oliver et al., 2018)</ref> performed a systematic study using Wide-Resnet-28-2 <ref type="bibr" target="#b26">(Zagoruyko &amp; Komodakis, 2016)</ref>, a specific residual network architecture, with extensive hyperparameter search to compare the performance of various consistency-based semi-supervised algorithms. We evaluate ICT using this same setup as a mean towards a fair comparison to these algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation details</head><p>We used the SGD with nesterov momentum optimizer for all of our experiments. For the experiments in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>, we run the experiments for 400 epochs. For the experiments in <ref type="table" target="#tab_2">Table 3</ref>, we run experiments for 600 epochs. The initial learning rate was set to 0.1, which is then annealed using the cosine annealing technique proposed in <ref type="bibr" target="#b11">(Loshchilov &amp; Hutter, 2016)</ref> and used by <ref type="bibr" target="#b22">(Tarvainen &amp; Valpola, 2017)</ref>. The momentum parameter was set to 0.9. We used an L2 regularization coefficient 0.0001 and a batch-size of 100 in our experiments.</p><p>In each experiment, we report mean and standard deviation across three independently run trials.  <ref type="bibr" target="#b22">(Tarvainen &amp; Valpola, 2017)</ref> 21.55 ± 1.48 15.73 ± 0.31 12.31 ± 0.28 VAT <ref type="bibr" target="#b13">(Miyato et al., 2018)</ref> --11.36 ± NA VAT+Ent <ref type="bibr" target="#b13">(Miyato et al., 2018)</ref> --10.55 ± NA VAdD <ref type="bibr" target="#b18">(Park et al., 2018)</ref> --11.32 ± 0.11 SNTG <ref type="bibr" target="#b12">(Luo et al., 2018)</ref> 18.41 ± 0.52 13.64 ± 0.32 10.93 ± 0.14 MT+ Fast SWA <ref type="bibr" target="#b0">(Athiwaratkun et al., 2019)</ref> 15.58 ± NA 11.02 ± NA 9.05 ± NA ICT 15.48 ± 0.78 9.26 ± 0.09 7.29 ± 0.02 The consistency coefficient w(t) is ramped up from its initial value 0.0 to its maximum value at onefourth of the total number of epochs using the same sigmoid schedule of <ref type="bibr" target="#b22">(Tarvainen &amp; Valpola, 2017)</ref>. We used MSE loss for computing the consistency loss following <ref type="bibr" target="#b7">(Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b22">Tarvainen &amp; Valpola, 2017)</ref>. We set the decay coefficient for the mean-teacher to 0.999 following <ref type="bibr" target="#b22">(Tarvainen &amp; Valpola, 2017)</ref>.</p><p>We conduct hyperparameter search over the two hyperparameters introduced by our method: the maximum value of the consistency coefficient w(t) (we searched over the values in {1.0, 10.0, 20.0, 50.0, 100.0}) and the parameter α of distribution Beta(α, α) (we searched over the values in {0.1, 0.2, 0.5, 1.0}). We select the best hyperparameter using a validation set of 5000 and 1000 labeled samples for CIFAR-10 and SVHN respectively. This size of the validation set is the same as that used in the other methods compared in this work.</p><p>We note the in all our experiments with ICT, to get the supervised loss, we perform the interpolation of labeled sample pair and their corresponding labels (as in mixup ). To make sure, that the improvements from ICT are not only because of the supervised mixup loss, we provide the direct comparison of ICT against supervised mixup and Manifold Mixup training in the <ref type="table" target="#tab_0">Table 1  and Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>We provide the results for CIFAR10 and SVHN datasets using CNN-13 architecture in the <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref>, respectively.  <ref type="bibr" target="#b17">(Oliver et al., 2018)</ref>. We did not conduct any hyperparameter search and used the best hyperparameters found in the experiments of <ref type="table" target="#tab_0">Table 1</ref>  To justify the use of a SSL algorithm, one must compare its performance against the state-of-the-art supervised learning algorithm <ref type="bibr" target="#b17">(Oliver et al., 2018)</ref>. To this end, we compare our method against two state-of-the-art supervised learning algorithms <ref type="bibr" target="#b25">Verma et al., 2018)</ref>, denoted as Supervised(Mixup) and Supervised(Manifold Mixup), respectively in <ref type="table" target="#tab_0">Table 1</ref> and 2. ICT method passes this test with a wide margin, often resulting in a two-fold reduction in the test error in the case of CIFAR10 <ref type="table" target="#tab_0">(Table 1</ref>) and a four-fold reduction in the case of SVHN <ref type="table" target="#tab_1">(Table 2)</ref> Furthermore, in <ref type="table" target="#tab_0">Table 1</ref>, we see that ICT improves the test error of other strong SSL methods. For example, in the case of 4000 labeled samples, it improves the test error of best-reported method by ∼ 25%. The best values of the hyperparameter max-consistency coefficient for 1000, 2000 and 4000 labels experiments were found to be 10.0, 100.0 and 100.0 respectively and the best values of the hyperparameter α for 1000, 2000 and 4000 labels experiments were found to be 0.2, 1.0 and 1.0 respectively. In general, we observed that for less number of labeled data, lower values of max-consistency coefficient and α obtained better validation errors.</p><p>For SVHN, the test errors obtained by ICT are competitive with other state-of-the-art SSL methods ( <ref type="table" target="#tab_1">Table 2</ref>). The best values of the hyperparameters max-consistency coefficient and α were found to be 100 and 0.1 respectively, for all the ICT results reported in the <ref type="table" target="#tab_1">Table 2</ref>. <ref type="bibr" target="#b17">(Oliver et al., 2018)</ref> performed extensive hyperparameter search for various consistency regularization SSL algorithm using the WRN-28-2 and they report the best test errors found for each of these algorithms. For a fair comparison of ICT against these SSL algorithms, we conduct experiments on WRN-28-2 architecture. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. ICT achieves improvement over other methods both for the CIFAR10 and SVHN datasets.</p><p>We note that unlike other SSL methods of <ref type="table" target="#tab_0">Table 1</ref>, <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref>, we do not use Dropout regularizer in our implementation of CNN-13 and WRN-28-2. Using Dropout along with the ICT may further reduce the test error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>• Effect of not using the mean-teacher in ICT: We note that Π-model, VAT and VAdD methods in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> do not use a mean-teacher to make predictions on the unlabeled data. Although the mean-teacher <ref type="bibr" target="#b22">(Tarvainen &amp; Valpola, 2017)</ref>   <ref type="table" target="#tab_0">Table 1</ref>). This shows that even without a mean-teacher, ICT has major a advantage over methods such as VAT <ref type="bibr" target="#b13">(Miyato et al., 2018)</ref> and VAdD <ref type="bibr" target="#b18">(Park et al., 2018</ref>) that it does not require an additional gradient computation yet performs on the same level of the test error.</p><p>• Effect of not having the mixup supervised loss: In Section 3.3, we noted that to get the supervised loss, we perform the interpolation of labeled sample pair and their corresponding labels (mixup supervised loss as in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>In this section, we establish mathematical properties of ICT for binary classification with f θ (u) ∈ R.</p><p>We begin in Section 4.1 with additional notation, an introduction of real analytic functions, and a property of the Euclidian norm of the Kronecker product. Using the real analyticity and the property of the Kronecker product, we show in Section 4.2 that ICT regularizes higher-order derivatives. We conclude in Section 4.3 by showing how ICT can reduce overfitting and lead to better generalization behaviors than those without ICT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminaries</head><p>To understand the essential mechanisms of ICT, we consider the same setting as in the first ablation study in Section 3.5: i.e., this section focuses on the mean square loss for the unlabeled data without the mean teacher as</p><formula xml:id="formula_6">L U S := E u,u ∼P X E λ∼Beta(α,α) (f θ (Mix λ (u, u )), Mix λ (f θ (u), f θ (u ))),<label>(2)</label></formula><p>where u ∈ X ⊆ R d and (a, b) = (a − b) 2 . For example, if P (X) is an empirical measure on the finite unlabeled data points, we can write L U S by</p><formula xml:id="formula_7">L U S = 1 n 2 n i=1 n j=1 E λ∼Beta(α,α) (f θ (Mix λ (u i , u j )) − Mix λ (f θ (u i ), f θ (u j ))),<label>(3)</label></formula><p>because L U S := Eu,u ∼P X g(u, u ) = g(u, u )dP X (u)dP X (u ) = 1 n 2 n i=1 n j=1 g(u i , u j ) for any measurable function g, where the last equality used the empirical measure P X = 1 n n i=1 δ ui with the Dirac measures δ ui . We consider the function f θ in the form of</p><formula xml:id="formula_8">f θ (u) = σ(h θ (u)),<label>(4)</label></formula><p>where σ(a) = 1 1+e −a is the sigmoid function and h θ (u) represents the pre-activation output of the last layer of a deep neural network.</p><p>The theory developed in this section requires the function f θ to be real analytic. This is easily satisfied in practice without degrading practical performances. Since a composition of real analytic functions is real analytic, we only need to require that each operation in each layer satisfies the real analyticity. The convolution, affine map, skip connection, batch normalization and average pooling are all real analytic functions. Therefore, the composition of these operations preserve real analyticity. Furthermore, many common activation functions are real analytic. For example, sigmoid σ, hyperbolic tangents and softplus activations φ(z) = ln(1 + exp(κz))/κ are all real analytic functions, with any hyperparameter κ &gt; 0. Here, the softplus activation can approximate the ReLU activation for any desired accuracy as</p><formula xml:id="formula_9">φ(x) → relu(x) as κ → ∞,</formula><p>where relu represents the ReLU activation. Therefore, the function f θ is real analytic for a large class of deep neural networks.</p><p>We close this subsection by introducing additional notation and proving a theoretical property of the Kronecker product. LetŜ = (u i ) n i=1 and S = ((x i , y i )) m i=1 be an unlabeled dataset and a labeled dataset, respectively, whereŜ and S are independent of each other. DefineR m (F) to be the empirical Rademacher complexity of the set of functions F, whereas R m (F) is the Rademacher complexity of the set F. We adopt the standard convention that min a∈∅ Ψ(a) = ∞ for any function Ψ with the empty set ∅. We define the k-th order tensor</p><formula xml:id="formula_10">∂ k f θ (u) ∈ R d×d×···×d by ∂ k f θ (u) t1t2···t k = ∂ k ∂u t1 ∂u t2 · · · ∂u t k f θ (u).<label>(5)</label></formula><p>For example, ∂ 1 f θ (u) is the gradient of f θ evaluated at u, and ∂ 2 f θ (u) is the Hessian of f θ evaluated at u. For any k-th order tensor ∂ k f θ (b) ∈ R d×d×···×d , we define the vectorization of the tensor by vec</p><formula xml:id="formula_11">[∂ k f θ (b)] ∈ R d k .</formula><p>For an vector a ∈ R d , we define a ⊗k = a ⊗ a ⊗ · · · ⊗ a ∈ R d k where ⊗ represents the Kronecker product. The following lemma proves that the Euclidean norm of the Kronecker products a ⊗k is the Euclidean norm of the vector a to the k-th power. Lemma 1. Let d ∈ N + and a ∈ R d . Then, for any k ∈ N + ,</p><formula xml:id="formula_12">a ⊗k 2 = a k 2</formula><p>Proof. We prove this statement by induction over k ∈ N + . For the base case with k = 1, a ⊗1 2 = a 2 = a 1 2 , as desired. For the inductive step, we show the statement to hold for k + 1:</p><formula xml:id="formula_13">a ⊗k+1 2 2 = a ⊗k ⊗ a 2 2 =    (a ⊗k ) 1 a . . . (a ⊗k ) d k a    2 2 = d k i=1 ((a ⊗k ) i ) 2 a 2 2 = a 2 2 d k i=1 ((a ⊗k ) i ) 2 = a 2 2 a ⊗k 2 2 .</formula><p>Here, the inductive hypothesis of a ⊗k 2 = a k 2 implies that a ⊗k 2 2 = a 2k 2 , and thus a ⊗k+1 2 2 = a 2 2 a 2k 2 = a 2(k+1) 2</p><p>. This implies that a ⊗k+1 2 = a k+1 2 , which completes the inductive step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Understanding ICT as regularizer on higher-order derivatives</head><p>Using the real analyticity and the Kronecker product, we show how ICT can act as a regularizer on higher-order derivatives. More concretely, Theorem 1 states that for any K ∈ N + , the ICT loss can be written as</p><formula xml:id="formula_14">(f θ (Mix λ (u, u )), Mix λ (f θ (u), f θ (u ))) (6) = K k=2 (λ −λ k ) k! vec[∂ k f θ (u)] (u − u) ⊗k + O( u − u K 2 ) 2 ,</formula><p>where O( u − u K 2 ) → 0 as K → ∞ if we normalize the input so that u − u 2 &lt; 1. Theorem 1. Let u, u ∈ R d and f θ be real analytic, and define ∆ = u − u. Then, for any K ∈ N + , there exists a pair (ζ, ζ ) ∈ [0,λ] × [0, 1] such that</p><formula xml:id="formula_15">(f θ (Mix λ (u, u )), Mix λ (f θ (u), f θ (u ))) = K k=2 (λ −λ k ) k! vec[∂ k f θ (u)] ∆ ⊗k + E θ (u, u , λ) 2 ,<label>(7)</label></formula><formula xml:id="formula_16">whereλ = 1 − λ, E θ (u, u , λ) = O( ∆ K 2 ) and E θ (u, u , λ) = 1 K! ((1 − ζ ) K vec[∂ K+1 f θ (u + ζ ∆)] −λ(λ − ζ) K vec[∂ K+1 f θ (u + ζ∆)]) ∆ ⊗K .</formula><p>Proof. Let (u, u ) be an arbitrary pair of unlabeled data points. We define the function ϕ by ϕ(a) = f θ (u + a(u − u)).</p><p>Since f θ is real analytic and a → u + a(u − u) is real analytic, their composition ϕ is also real analytic. We first observe that</p><formula xml:id="formula_17">Mix λ (a, b) = λa + (1 − λ)b = a + (1 − λ)(b − a) = a +λ(b − a).<label>(8)</label></formula><p>Using equation <ref type="formula" target="#formula_17">(8)</ref>,</p><formula xml:id="formula_18">f θ (Mix λ (u, u )) = f θ (u +λ(u − u)) = ϕ(λ).</formula><p>Using Taylor's theorem with the Cauchy remainder, we have the following: for any K ∈ N + , there exists ζ ∈ [0,λ] such that</p><formula xml:id="formula_19">f θ (Mix λ (u, u )) = ϕ(λ) = ϕ(0) + K k=1λ k k! ϕ (k) (0) +λ (λ − ζ) K K! ϕ (K+1) (ζ),</formula><p>where ϕ (k) is the k-th order derivative of ϕ. Since ϕ(0) = f θ (u), this implies that</p><formula xml:id="formula_20">f θ (Mix λ (u, u )) = f θ (u) + K k=1λ k k! ϕ (k) (0) +λ (λ − ζ) K K! ϕ (K+1) (ζ).<label>(9)</label></formula><p>On the other hand, using equation <ref type="formula" target="#formula_17">(8)</ref>,</p><formula xml:id="formula_21">Mix λ (f θ (u), f θ (u )) = f θ (u) +λ(f θ (u ) − f θ (u)).<label>(10)</label></formula><p>Using Taylor's theorem with the Cauchy remainder, we have the following: for any K ∈ N + , there exists ζ ∈ [0, 1] such that</p><formula xml:id="formula_22">f θ (u ) = ϕ(1) = ϕ(0) + K k=1 1 k! ϕ (k) (0) + (1 − ζ ) K K! ϕ (K+1) (ζ ).</formula><p>Since ϕ(0) = f θ (u), plugging this formula of f θ (u ) into equation <ref type="formula" target="#formula_3">(10)</ref> yields</p><formula xml:id="formula_23">Mix λ (f θ (u), f θ (u )) = f θ (u) +λ K k=1 1 k! ϕ (k) (0) + (1 − ζ ) K K! ϕ (K+1) (ζ ) .<label>(11)</label></formula><p>Using equations <ref type="formula" target="#formula_20">(9)</ref> and <ref type="formula" target="#formula_3">(11)</ref>,</p><formula xml:id="formula_24">(f θ (Mix λ (u, u )), Mix λ (f θ (u), f θ (u ))) = K k=1λ k k! ϕ (k) (0) +λ (λ − ζ) K K! ϕ (K+1) (ζ) −λ K k=1 1 k! ϕ (k) (0) + (1 − ζ ) K K! ϕ (K+1) (ζ ) 2 = K k=1 (λ k −λ) k! ϕ (k) (0) + 1 K! (λ(λ − ζ) K ϕ (K+1) (ζ) − (1 − ζ ) K ϕ (K+1) (ζ )) 2</formula><p>Sinceλ k −λ = 0 when k = 1, this implies that</p><formula xml:id="formula_25">(f θ (Mix λ (u, u )), Mix λ (f θ (u), f θ (u ))) (12) = K k=2 (λ k −λ) k! ϕ (k) (0) + 1 K! (λ(λ − ζ) K ϕ (K+1) (ζ) − (1 − ζ ) K ϕ (K+1) (ζ )) 2 .</formula><p>We now derive the formula of ϕ (k) (a). By the chain rule, with ∆ = u − u and b = u + a∆, we have that</p><formula xml:id="formula_26">ϕ (1) (a) = ∂f θ (u + a∆) ∂a = d t1=1 ∂f θ (b) ∂b t1 ∂b t1 ∂a = d t1=1 ∂f θ (b) ∂b t1 ∆ t1 ϕ (2) (a) = ∂ ∂a ∂f θ (u + a∆) ∂a = d t1=1 ∂ ∂a ∂f θ (b) ∂b t1 ∆ t1 = d t1=1 d t2=1 ∂ 2 f θ (b) ∂b t1 ∂b t2 ∆ t2 ∆ t1</formula><p>Based on this process, we consider the following formula of ϕ (k) (a) as a candidate to be proven by induction over k ∈ N + :</p><formula xml:id="formula_27">d t1=1 d t2=1 · · · d t k =1 ∂ k f θ (b) ∂b t1 ∂b t2 · · · ∂b t k ∆ t1 ∆ t2 · · · ∆ t k .</formula><p>For the base case, we have already shown that ϕ (1) (a) = d t1=1 ∂f θ (b) ∂bt 1 ∆ t1 as desired. For the inductive step, by using the inductive hypothesis,</p><formula xml:id="formula_28">ϕ (k+1) (a) = ∂ ∂a d t1=1 d t2=1 · · · d t k =1 ∂ k f θ (b) ∂b t1 ∂b t2 · · · ∂b t k ∆ t1 ∆ t2 · · · ∆ t k = d t1=1 d t2=1 · · · d t k =1 d t k+1 =1 ∂ k+1 f θ (b) ∂b t1 ∂b t2 · · · ∂b t k ∂b t k+1 ∆ t1 ∆ t2 · · · ∆ t k ∆ t k+1</formula><p>as desired. Therefore, we have proven that for any k ∈ N + ,</p><formula xml:id="formula_29">ϕ (k) (a) = d t1=1 d t2=1 · · · d t k =1 ∂ k f θ (b) ∂b t1 ∂b t2 · · · ∂b t k ∆ t1 ∆ t2 · · · ∆ t k .<label>(13)</label></formula><p>Then, by using the vectorization of the tensor vec[∂ k f θ (b)] ∈ R d k , we can rewrite equation <ref type="formula" target="#formula_3">(13)</ref> as</p><formula xml:id="formula_30">ϕ (k) (a) = vec[∂ k f θ (u + a∆)] ∆ ⊗k ,<label>(14)</label></formula><p>where ∆ ⊗k = ∆ ⊗ ∆ ⊗ · · · ⊗ ∆ ∈ R d k and ∆ = u − u. By combining equations <ref type="formula" target="#formula_3">(12)</ref> and <ref type="formula" target="#formula_3">(14)</ref>,</p><formula xml:id="formula_31">(f θ (Mix λ (u, u )), Mix λ (f θ (u), f θ (u ))) = K k=2 (λ k −λ) k! ϕ (k) (0) + 1 K! (λ(λ − ζ) K ϕ (K+1) (ζ) − (1 − ζ ) K ϕ (K+1) (ζ )) 2 = K k=2 (λ −λ k ) k! vec[∂ k f θ (u)] ∆ ⊗k + E θ (u, u , λ) 2</formula><p>By using the Cauchy-Schwarz inequality,</p><formula xml:id="formula_32">|E θ (u, u , λ)| = 1 K! ((1 − ζ ) K vec[∂ K+1 f θ (u + ζ ∆)] −λ(λ − ζ) K vec[∂ K+1 f θ (u + ζ∆)]) ∆ ⊗K ≤ ∆ ⊗K 2 K! ((1 − ζ ) K vec[∂ K+1 f θ (u + ζ ∆)] −λ(λ − ζ) K vec[∂ K+1 f θ (u + ζ∆)]) 2 = ∆ K 2 K! ((1 − ζ ) K vec[∂ K+1 f θ (u + ζ ∆)] −λ(λ − ζ) K vec[∂ K+1 f θ (u + ζ∆)]) 2</formula><p>where the last line follows from Lemma 1.</p><p>Theorem 1 suggests that ICT acts as a regularizer on derivatives of all orders when the confidence of the prediction on unlabeled data is high. This is because increasing confidence in ICT tends to decrease the norm of the first-order derivatives due to the following observation. By the chain rule, the first-order derivatives can be written as</p><formula xml:id="formula_33">∂f θ (u) = ∂σ(h θ (u))∂h θ (u).<label>(15)</label></formula><p>Moreover, since ∂σ(a) = e −a (1+e −a ) 2 = e a (1+e a ) 2 and ∂σ(h θ (u)) ≥ 0, we have</p><formula xml:id="formula_34">∂f θ (u) = |∂σ(h θ (u))| ∂h θ (u) = ∂σ(h θ (u)) ∂h θ (u) = ∂σ(|h θ (u)|) ∂h θ (u) .<label>(16)</label></formula><p>Here, ∂σ(|h θ (u)|) is maximized when |h θ (u)| = 0 and exponentially decreases towards 0 as |h θ (u)| increases or equivalently as the confidence of the prediction increases. In other words, ∂σ(|h θ (u)|) is maximized when we have lowest confidence (i.e., f θ (u) = 1/2), and ∂σ(|h θ (u)|) exponentially decreases towards 0 as we increase the confidence (i.e., as f θ (u) moves towards 0 or 1 from 1/2). Therefore, as long as ∂h θ (u) is bounded (or increase slower than the exponential rate), increasing the confidence of the prediction can implicitly minimize the norm of the first-order derivative ∂f θ (u) . Here, the weight decay also tends to bound ∂h θ (u) since ∂h θ (u) contains the products of the weight matrices for the deep networks by the chain rule.</p><p>Theorem 1 along this observation suggests that ICT works well when the confidence on unlabeled data is high, because that is when ICT acts as a regularizer on derivatives of all orders. To confirm this theoretical prediction, we conducted numerical simulations with the "two moons" dataset by intentionally decreasing confidence values on the unlabeled data points. <ref type="figure" target="#fig_3">Figure 3</ref> shows the results of this experiment with the high confidence case and the low confidence case. Here, the confidence value is defined by 1</p><formula xml:id="formula_35">n n i=1 | 1 2 − f θ (u i )| for the unlabeled dataset (u i ) n i=1</formula><p>, which is intentionally reduced in <ref type="figure" target="#fig_3">Figure 3 (b)</ref>. We used the same settings as those in <ref type="figure" target="#fig_2">Figure 1</ref>, except that we did not use mixup for the supervised loss in order to understand the essential mechanism of ICT (whereas we used mixup for the supervised loss in <ref type="figure" target="#fig_2">Figure 1</ref>). As can be seen in <ref type="figure" target="#fig_3">Figure 3</ref>, the numerical results are consistent with our theoretical prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">On overfitting</head><p>In the previous subsection, we have shown that ICT acts as a regularizer on the derivatives of all orders at unlabeled data points. In this subsection, we provide a theoretical explanation regarding how regularizing the derivatives of all orders at unlabeled points help reducing overfitting at labeled points.</p><p>We first recall the important lemma, Lemma 2, that bounds a possible degree of overfitting at labeled points with the Rademacher complexity of a hypothesis space <ref type="bibr" target="#b1">(Bartlett &amp; Mendelson, 2002;</ref><ref type="bibr" target="#b15">Mohri et al., 2012)</ref>. Since our notion of a hypothesis space differs from standard ones, we include a proof for the sake of completeness. The proof is based on an argument in <ref type="bibr" target="#b1">(Bartlett &amp; Mendelson, 2002)</ref>. A key observation in Lemma 2 is that we can define a hypothesis space based on the unlabeled dataset S = (u i ) n i=1 , which is to be used later to relate the regularization at unlabeled points to the degree of overfitting at labeled points.</p><p>Lemma 2. Let FŜ be a set of maps x → f (x) that depends on an unlabeled datasetŜ. Let q → (q, y) be a C-uniformly bounded function for any q ∈ {f (x) : f ∈ FŜ, x ∈ X } and y ∈ Y. Then, for any δ &gt; 0, with probability at least 1−δ over an i.i.d. draw of m samples S = ((x i , y i )) m i=1 , the following holds: for all maps f ∈ FŜ,</p><formula xml:id="formula_36">E x,y [ (f (x), y)] ≤ 1 m m i=1 (f (x i ), y i ) + 2R m ( • FŜ) + C ln(1/δ) 2m .</formula><p>Proof. Define</p><formula xml:id="formula_37">ϕ(S) = sup f ∈FŜ E x,y [ (f (x), y)] − 1 m m i=1 (f (x i ), y i ).</formula><p>To apply McDiarmid's inequality to ϕ(S), we compute an upper bound on |ϕ(S) − ϕ(S )| where S = ((x i , y i )) m i=1 and S = ((x i , y i )) m i=1 are two labeled datasets differing by exactly one point of an arbitrary index i 0 ; i.e., S i = S i for all i = i 0 and S i0 = S i0 . Then,</p><formula xml:id="formula_38">ϕ(S ) − ϕ(S) ≤ sup f ∈FŜ (f (x i0 ), y i0 ) − (f(x i0 ), y i0 ) m ≤ C m ,</formula><p>where we used that fact thatŜ and S are independent. Similarly, ϕ(S) − ϕ(S ) ≤ C m . Notice that these steps fail if FŜ depends on S.</p><p>Thus, by McDiarmid's inequality, for any δ &gt; 0, with probability at least 1 − δ,</p><formula xml:id="formula_39">ϕ(S) ≤ E S [ϕ(S)] + C ln(1/δ) 2m .</formula><p>Moreover,</p><formula xml:id="formula_40">E S [ϕ(S)] = E S sup f ∈FŜ E S 1 m m i=1 (f(x i ), y i ) − 1 m m i=1 (f (x i ), y i ) ≤ E S,S sup f ∈FŜ 1 m m i=1 ( (f(x i ), y i ) − (f (x i ), y i ) ≤ E ξ,S,S sup f ∈FŜ 1 m m i=1 ξ i ( (f(x i ), y i ) − (f (x i ), y i )) ≤ 2E ξ,S sup f ∈FŜ 1 m m i=1 ξ i (f (x i ), y i )) = 2R m ( • FŜ)</formula><p>where the fist line follows the definitions of each term, the second line uses the Jensen's inequality and the convexity of the supremum function, and the third line follows that for each ξ i ∈ {−1, +1}, the distribution of each term ξ i ( (f(x i ), y i ) − (f (x i ), y i )) is the distribution of ( (f(x i ), y i ) − (f (x i ), y i )) sinceS andS are drawn i.i.d. with the same distribution. The forth line uses the subadditivity of the supremum function.</p><p>Whereas Lemma 2 is applicable for a general class of loss functions, we now recall a concrete version of Lemma 2 for binary classification. We can write the standard 0-1 loss (i.e., classification error) for binary classification by</p><formula xml:id="formula_41">01 (f (x), y) = 1{y = 1}1{f (x) ≤ 1/2} + 1{y = −1}1{f (x) ≥ 1/2} = 1{y(2f(x) − 1) ≤ 0}.<label>(17)</label></formula><p>For any y ∈ {−1, +1}, we define the margin loss ρ (f (x), y) =¯ ρ (y(2f (x) − 1)) as follows:</p><formula xml:id="formula_42">ρ (a) =    0 if a ≥ ρ 1 − a/ρ if 0 ≤ a ≤ ρ 1 if a ≤ 0.</formula><p>Note that for any ρ &gt; 0, the margin loss ρ (yf (x)) is an upper bound on the 0-1 loss: i.e., ρ (f (x), y) ≥ 01 (f (x), y). We can instantiate Lemma 2 for this concrete choice of the loss functions by using the arguments in <ref type="bibr" target="#b15">(Mohri et al., 2012)</ref>:</p><p>Lemma 3. Let FŜ be a set of maps x → f (x) that depends on an unlabeled datasetŜ. Fix ρ &gt; 0. Then, for any δ &gt; 0, with probability at least 1 − δ over an i.i.d. draw of m samples ((x i , y i )) m i=1 , each of the following holds: for all maps f ∈ FŜ,</p><formula xml:id="formula_43">E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 ρ (f (x i ), y i ) + 2ρ −1 R m (FŜ) + ln(1/δ) 2m . E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 ρ (f (x i ), y i ) + 2ρ −1R m (FŜ) + 3 ln(2/δ) 2m .</formula><p>Proof. By combining the Lemma 2 and the fact that ρ (f (x), y) ≥ 01 (f (x), y) and ρ (f (x), y) ≤ 1, we have that for any δ &gt; 0, with probability at least 1 − δ,</p><formula xml:id="formula_44">E x,y [ 01 (f (x), y)] ≤ E x,y [ ρ (f (x), y)] ≤ 1 m m i=1 ρ (f (x i ), y i ) + 2R m ( ρ • FŜ) + ln(1/δ) 2m .</formula><p>Using Lemma 5 in Appendix A and the fact that ρ (f (x), y) is 1/ρ-Lipschitz,</p><formula xml:id="formula_45">E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 ρ (f (x i ), y i ) + 2ρ −1 R m (FŜ) + ln(1/δ) 2m .</formula><p>This proves the first statement. For the second statement, we replace δ by δ/2, use Lemma 4 in Appendix A, and take a union bound, yielding that with probability at least 1 − δ/2 − δ/2 = 1 − δ,</p><formula xml:id="formula_46">E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 ρ (f (x i ), y i ) + 2ρ −1 R m (FŜ) + ln(2/δ) 2m ≤ 1 m m i=1 ρ (f (x i ), y i ) + 2ρ −1R m (FŜ) + 3 ln(2/δ) 2m</formula><p>This proves the second statement.</p><p>Given these lemmas, we are ready to investigate how the overfitting at labeled points can be mitigated by using the hypothesis space FŜ ,τ with the regularization on the derivatives of all orders at unlabeled points:</p><formula xml:id="formula_47">FŜ ,τ = {x → f (x) : ∀S ⊆Ŝ, ∀(u, k) ∈S × N + , vec[∂ k f (u)] 2 ≤ τS},</formula><p>where τ = {τS ∈ R :S ⊆Ŝ} and τS measures the norm of the derivatives of all orders at unlabeled points. For eachS ⊆Ŝ, we writeS = (uS 1 , . . . , uS |S| ). We define R S,S = max x∈S min u∈S x −u 2 , S S,Ŝ = {S ⊆Ŝ : R S,S &lt; 1}, and</p><formula xml:id="formula_48">I S,S t = i ∈ [m] : t = argmin t∈{1,...,|S|} x i − uS t 2 .</formula><p>The following theorem shows that regularizing τS -the norm of the derivatives of all orders at unlabeled points -can help reducing overfitting at labeled points: Theorem 2. Fix FŜ ,τ and ρ &gt; 0. Then, for any δ &gt; 0, with probability at least 1 − δ over an i.i.d. draw of m samples ((x i , y i )) m i=1 , each of the following holds: for all maps f ∈ FŜ ,τ ,</p><formula xml:id="formula_49">E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 ρ (f (x i ), y i ) + 2ρ −1 E S   min S∈S S,Ŝ 1 + τSR S,S 1 − R S,S |S| t=1 |I S,S t | m   + ln(1/δ) 2m . E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 ρ (f (x i ), y i ) + 2ρ −1 min S∈S S,Ŝ 1 + τSR S,S 1 − R S,S |S| t=1 |I S,S t | m + 3 ln(2/δ) 2m .</formula><p>Proof. LetS ⊆Ŝ be arbitrary such that R S,S &lt; 1. Using Lemma 1 and the Cauchy-Schwarz inequality,</p><formula xml:id="formula_50">K k=1 vec[∂ k f (u)] (x − u) ⊗k ≤ K k=1 vec[∂ k f (u)] 2 x − u k 2 .</formula><p>Thus, for any (f, u) ∈ FŜ ,τ ×S,</p><formula xml:id="formula_51">lim K→∞ K k=1 vec[∂ k f (u)] 2 x − u k 2 ≤ τS lim K→∞ K k=1 x − u k 2 . Since lim K→∞ K k=1 x − u k 2 is a geometric series, if x − u 2 ≤ R &lt; 1, then lim K→∞ K k=1 x − u k 2 ≤ 1 1 − R − 1.</formula><p>Therefore, combining above inequalities, for any</p><formula xml:id="formula_52">(f, u) ∈ FŜ ,τ ×S, if x − u 2 ≤ R &lt; 1, ∞ k=1 vec[∂ k f (u)] (x − u) ⊗k ≤ τS 1 − R − 1.<label>(18)</label></formula><p>Lett(x) = argmin t∈{1,...|S|} x − uS t 2 . Then,</p><formula xml:id="formula_53">R m (FŜ ,τ ) = E ξ sup f ∈FŜ ,τ 1 m m i=1 ξ i f (x i ) = E ξ sup f ∈FŜ ,τ 1 m m i=1 ξ i f (uŜ t(xi) + (x i − uŜ t(xi) )).</formula><p>Thus, equation <ref type="formula" target="#formula_3">(18)</ref> implies that</p><formula xml:id="formula_54">R m (FŜ ,τ ) = E ξ sup f ∈FŜ ,τ 1 m m i=1 ξ i f (uŜ t(xi) + (x − uŜ t(xi) )) = E ξ sup f ∈FŜ ,τ 1 m m i=1 ξ i f (uŜ t(xi) ) + ∞ k=1 vec[∂ k f (uŜ t(xi) )] (x i − uŜ t(xi) ) ⊗k</formula><p>where the series converges based on equation <ref type="formula" target="#formula_3">(18)</ref> </p><formula xml:id="formula_55">since min u∈S x − u 2 &lt; 1 for all x ∈ S x . Since I S,S t = {i ∈ [m] : t = argmin t∈{1,...,|S|} x i − uS t 2 } = {i ∈ [m] : t =t(x i )}, we can further rewritê R m (FŜ ,τ ) = E ξ sup f ∈FŜ ,τ 1 m m i=1 ξ i f (uŜ t(xi) ) + ∞ k=1 vec[∂ k f (uŜ t(xi) )] (x i − uŜ t(xi) ) ⊗k = E ξ sup f ∈FŜ ,τ 1 m |S| t=1 i∈I S,S t ξ i f (uŜ t(xi) ) + ∞ k=1 vec[∂ k f (uŜ t(xi) )] (x i − uŜ t(xi) ) ⊗k = E ξ sup f ∈FŜ ,τ 1 m |S| t=1 i∈I S,S t ξ i f (uS t ) + ∞ k=1 vec[∂ k f (uS t )] (x i − uS t ) ⊗k = 1 m E ξ sup f ∈FŜ ,τ    |S| t=1 i∈I S,S t ξ i f (uS t ) + |S| t=1 i∈I S,S t ξ i ∞ k=1 vec[∂ k f (uS t )] (x i − uS t ) ⊗k    ≤ 1 m E ξ    |S| t=1 sup f ∈FŜ ,τ |f (uS t )| i∈I S,S t ξ i + |S| t=1 ∞ k=1 sup f ∈FŜ ,τ vec[∂ k f (uS t )] 2 i∈I S,S t ξ i (x i − u t ) ⊗k 2    Combining inequalities (19)-(21) yields mR m (FŜ ,τ ) ≤ |S| t=1 |I S,S t | + τS |S| t=1 |I S,S t | ∞ k=1 R k S,S .</formula><p>Since R S,S &lt; 1, the geometric series converges as</p><formula xml:id="formula_56">mR m (FŜ ,τ ) ≤ |S| t=1 |I S,S t | + τSR S,S 1 − R S,S |S| t=1 |I S,S t | = 1 + τSR S,S 1 − R S,S |S| t=1 |I S,S t |. Therefore,R m (FŜ ,τ ) ≤ 1 + τSR S,S 1 − R S,S |S| t=1 |I S,S t | m .</formula><p>SinceS ⊆Ŝ was arbitrary such that R S,S &lt; 1, this inequality holds for anyS such that R S,S &lt; 1, yielding that for any S = ((x 1 , y 1 ), . . . , (x m , y m )),</p><formula xml:id="formula_57">R m (FŜ ,τ ) = E ξ sup f ∈FŜ ,τ 1 m m i=1 ξ i f (x i ) ≤ min S∈S S,Ŝ 1 + τSR S,S 1 − R S,S |S| t=1 |I S,S t | m .<label>(22)</label></formula><p>By combining Lemma 3 and inequality (22) and taking expectation over S, we obtain the statement of this theorem.</p><p>Since minS ∈S S,Ŝ g(S) ≤ g(S ) for anyS ∈ S S,Ŝ and any function g, Theorem 2 implies that with probability at least 1 − δ, for anyS ∈ S S,Ŝ ,</p><formula xml:id="formula_58">E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 ρ (f (x i ), y i ) + 2ρ −1 1 + τSR S,S 1 − R S,S |S| t=1 |I S,S t | m + 3 ln(2/δ) 2m .</formula><p>To further understand Theorem 2, let us consider a simple case where the input space is normalized such that the maximum distance between some unlabeled point u c ∈Ŝ and the labeled points S is bounded as max x∈S x − u c 2 &lt; 1/2. Then, by settingS = {u c }, Theorem 2 implies that with probability at least 1 − δ,</p><formula xml:id="formula_59">E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 ρ (f (x i ), y i ) + 2ρ −1 (1 + τS) √ m + 3 ln(2/δ) 2m ,<label>(23)</label></formula><p>since R S,S = 1 2 , |S| = 1, and |I S,S t | = m for this particular choice ofS = {u c }. In equation <ref type="formula" target="#formula_6">(23)</ref>, we can clearly see that the degree of possible overfitting at labeled points is in the order of 1+τS √ m , which can be reduced by regularizing τS -the norm of the derivatives of all orders at unlabeled points.</p><p>In equation (23) (and Theorem 2), there is a tradeoff between reducing τS and minimizing the classification loss 1 m m i=1 ρ (f (x i ), y i ). In an extreme case, if we minimize τS to zero, then we can minimize the order of the overfitting from 1+τS √ m to 1 √ m but the model f becomes a constant function which cannot minimize the classification loss 1 m m i=1 ρ (f (x i ), y i ) in typical practical applications. Whereas this tradeoff is natural in the regime of large τS, it is not desirable to require f to be a constant function in order to obtain a global minimum value of the degree of the overfitting in the regime of small τS. Accordingly, we now prove an additional theorem to avoid this tradeoff in the regime of small τS. More concretely, Theorem 3 shows that we can reduce the order of the overfitting from 1+τS √ m to 1 √ m without requiring τS = 0 in the regime where τS is smaller than some confidence value at unlabeled points as τS &lt; CS</p><formula xml:id="formula_60">(1−R S,S ) R S,S</formula><p>, where CS measures the confidence values at unlabeled points. Thus, Theorem 3 also shows the benefit of increasing confidence values at unlabeled points, which is consistent with our observation in <ref type="figure" target="#fig_3">Figure 3</ref>. . Then, for any δ &gt; 0, with probability at least 1 − δ over an i.i.d. draw of m samples ((x i , y i )) m i=1 , each of the following holds: for all maps f ∈ FŜ ,τ ,</p><formula xml:id="formula_61">E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 01 (f (x), y) + E S minS ∈S * S,Ŝ |S| t=1 |I S,S t | m + ln(2/δ) 2m . E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 01 (f (x), y) + minS ∈S * S,Ŝ |S| t=1 |I S,S t | m + 3 ln(1/δ) 2m .</formula><p>Proof. Without the loss of generality, let us write y ∈ {−1, +1} (if the original label y is in {0, 1}, then we can define a bijection to map {0, 1} to {−1, +1}).</p><p>Define</p><formula xml:id="formula_62">ς(f (x)) = +1 if f (x) − 1 2 &gt; 0 −1 if f (x) − 1 2 ≤ 0</formula><p>Then, we can write the 0-1 loss of the classification as</p><formula xml:id="formula_63">01 (f (x), y) = 1{ς(f(x)) = y} = 1 − yς(f (x)) 2<label>(24)</label></formula><p>By using Lemma 2 with the 0-1 loss, we have that for any δ &gt; 0, with probability at least 1 − δ, for all f ∈ FŜ ,τ ,</p><formula xml:id="formula_64">E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 01 (f (x), y) + 2R m ( 01 • FŜ ,τ ) + ln(2/δ) 2m .<label>(25)</label></formula><p>By Using Lemma 4 and taking a union bound, we have that with probability at least 1 − δ/2 − δ/2 = 1 − δ, for all f ∈ FŜ ,τ ,</p><formula xml:id="formula_65">E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 01 (f (x), y) + 2R m ( 01 • FŜ ,τ ) + 3 ln(1/δ) 2m .<label>(26)</label></formula><p>Using equation <ref type="formula" target="#formula_6">(24)</ref>,R</p><formula xml:id="formula_66">m ( 01 • FS) = E ξ sup f ∈FŜ ,τ 1 m m i=1 ξ i 01 (f (x i ), y i ) = E ξ sup f ∈FŜ ,τ 1 m m i=1 ξ i 1 − y i ς(f (x i )) 2 = E ξ sup f ∈FŜ ,τ 1 m m i=1 −ξ i y i ς(f (x i )) 2 = 1 2m E ξ sup f ∈FŜ ,τ m i=1 ξ i ς(f (x i )).</formula><p>LetS ⊆Ŝ be arbitrary such that R S,S &lt; 1 and τS &lt;</p><formula xml:id="formula_67">CS (1−R S,S ) R S,S</formula><p>. Lett(x) = argmin t∈{1,...|S|} x− uS t 2 . Then, for any f ∈ FŜ ,τ , the proof of Theorem 2 shows that we can write</p><formula xml:id="formula_68">f (x i ) = f (uŜ t(xi) ) + ∞ k=1 vec[∂ k f (uŜ t(xi) )] (x i − uŜ t(xi) ) ⊗k . Since I S,S t = {i ∈ [m] : t = argmin t∈{1,...,|S|} x i − uS t 2 } = {i ∈ [m] : t =t(x i )}, we can rewritê R m ( 01 • FS) = 1 2m E ξ sup f ∈FŜ ,τ m i=1 ξ i ς f (uŜ t(xi) ) + ∞ k=1 vec[∂ k f (uŜ t(xi) )] (x i − uŜ t(xi) ) ⊗k = 1 2m E ξ sup f ∈FŜ ,τ |S| t=1 i∈I S,S t ξ i ς f (uS t ) + ∞ k=1 vec[∂ k f (uS t )] (x i − uS t ) ⊗k Using Lemma 1, ∞ k=1 vec[∂ k f (uS t )] (x i − uS t ) ⊗k ≤ ∞ k=1 vec[∂ k f (uS t )] 2 (x i − uS t ) ⊗k 2 ≤ τS R S,S 1 − R S,S &lt; CS</formula><p>where the last line follows the condition onS that τS &lt;</p><formula xml:id="formula_69">CS (1−R S,S ) R S,S</formula><p>(since τS is defined independently of S, this condition does not necessarily hold for someS ⊆Ŝ. If this does not hold for allS ⊆Ŝ, then the statement holds vacuously with the convention that min a∈∅ Ψ(a) = ∞ for any function Ψ).</p><formula xml:id="formula_70">Since |f (uS t ) − 1/2| ≥ CS and | ∞ k=1 vec[∂ k f (uS t )] (x i − uS t ) ⊗k | &lt; CS, we have that R m ( 01 • FS) = 1 2m E ξ sup f ∈FŜ ,τ |S| t=1 i∈I S,S t ξ i ς f (uS t ) + ∞ k=1 vec[∂ k f (uS t )] (x i − uS t ) ⊗k = 1 2m E ξ sup f ∈FŜ ,τ |S| t=1 ς f (uS t ) i∈I S,S t ξ i ≤ 1 2m E ξ |S| t=1 sup f ∈FŜ ,τ ς f (uS t ) i∈I S,S t ξ i ≤ 1 2m |S| t=1 E ξ    i∈I S,S t ξ i    2</formula><p>By using Jensen's inequality for the concave function,</p><formula xml:id="formula_71">E ξ    i∈I S,S t ξ i    2 ≤ E ξ    i∈I S,S t ξ i   </formula><p>Taking expectation and combining with equations (25)-(26) yields that for any δ &gt; 0, with probability at least 1 − δ, each of the following holds for all f ∈ FŜ ,τ : Since minS ∈S * S,Ŝ g(S) ≤ g(S ) for anyS ∈ S * S,Ŝ and any function g, Theorem 3 implies that with probability at least 1 − δ, for anyS = {u c } ∈ S * S,Ŝ ,</p><formula xml:id="formula_72">E x,</formula><formula xml:id="formula_73">E x,y [ 01 (f (x), y)] ≤ 1 m m i=1 01 (f (x), y) + 1 √ m + 3 ln(1/δ) 2m ,<label>(27)</label></formula><p>as |S| = 1 and |I S,S t | = m for the singleton setS = {u c }. Therefore, if we increase the confidence at unlabeled points and regularize the norm of the derivatives of all orders at unlabeled points, we can reduce overfitting at labeled points: i.e., the classification error 1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>This work builds on two threads of research: consistency-regularization for semi-supervised learning and interpolation-based regularizers.</p><p>On the one hand, consistency-regularization semi-supervised learning methods <ref type="bibr" target="#b20">(Sajjadi et al., 2016;</ref><ref type="bibr" target="#b7">Laine &amp; Aila, 2016;</ref><ref type="bibr" target="#b22">Tarvainen &amp; Valpola, 2017;</ref><ref type="bibr" target="#b13">Miyato et al., 2018;</ref><ref type="bibr" target="#b12">Luo et al., 2018;</ref><ref type="bibr" target="#b0">Athiwaratkun et al., 2019)</ref> encourage that realistic perturbations u + δ of unlabeled samples u should not change the model predictions f θ (u). These methods are motivated by the low-density separation assumption <ref type="bibr">(Chapelle et al., 2010)</ref>, and as such push the decision boundary to lie in the low-density regions of the input space, achieving larger classification margins. ICT differs from these approaches in two aspects. First, ICT chooses perturbations in the direction of another randomly chosen unlabeled sample, avoiding expensive gradient computations. When interpolating between distant points, the regularization effect of ICT applies to larger regions of the input space.</p><p>On the other hand, interpolation-based regularizers <ref type="bibr" target="#b23">Tokozume et al., 2018;</ref><ref type="bibr" target="#b25">Verma et al., 2018)</ref> have been recently proposed for supervised learning, achieving state-of-theart performances across a variety of tasks and network architectures. While <ref type="bibr" target="#b23">Tokozume et al., 2018)</ref> was proposed to perform interpolations in the input space, <ref type="bibr" target="#b25">(Verma et al., 2018)</ref> proposed to perform interpolation also in the hidden space representations. Furthermore, in the unsupervised learning setting, <ref type="bibr" target="#b2">(Berthelot et al., 2019)</ref> proposes to measure the realism of latent space interpolations from an autoencoder to improve its training.</p><p>Other works have approached semi-supervised learning from the perspective of generative models. Some have approached this from a consistency point of view, such as <ref type="bibr" target="#b8">(Lecouat et al., 2018)</ref>, who proposed to encourage smooth changes to the predictions along the data manifold estimated by the generative model (trained on both labeled and unlabeled samples). Others have used the discriminator from a trained generative adversarial network <ref type="bibr" target="#b6">(Goodfellow et al., 2014)</ref> as a way of extracting features for a purely supervised model <ref type="bibr" target="#b19">(Radford et al., 2015)</ref>. Still, others have used trained inference models as a way of extracting features <ref type="bibr" target="#b5">(Dumoulin et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Machine learning is having a transformative impact on diverse areas, yet its application is often limited by the amount of available labeled data. Progress in semi-supervised learning techniques holds promise for those applications where labels are expensive to obtain. In this paper, we have proposed a simple but efficient semi-supervised learning algorithm, Interpolation Consistency Training (ICT), which has two advantages over previous approaches to semi-supervised learning. First, it uses almost no additional computation, as opposed to computing adversarial perturbations or training generative models. Second, it outperforms strong baselines on two benchmark datasets, even without an extensive hyperparameter tuning. Finally, we have shown how ICT can act as a regularizer on the derivatives of all orders and reduce overfitting when confidence values are high. Our theoretical results predicts a failure mode of ICT with low confidence values, which was confirmed in the experiments, providing a practical guidance to use ICT with high confidence values. As for the future work, extending ICT to interpolations not only at the input but at hidden representations <ref type="bibr" target="#b25">(Verma et al., 2018)</ref> could improve the performance even further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Interpolation Consistency Training (ICT) applied to the "two moons" dataset, when three labels per class (large dots) and a large amount of unlabeled data (small dots) is available. When compared to supervised learning (red), ICT encourages a decision boundary traversing a low-density region that would better reflect the structure of the unlabeled data. Both methods employ a multilayer perceptron with three hidden ReLU layers of twenty neurons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Interpolation Consistency Training (ICT) learns a student network f θ in a semi-supervised manner. To this end, ICT uses a mean-teacher f θ , where the teacher parameters θ are an exponential moving average of the student parameters θ. During training, the student parameters θ are updated to encourage consistent predictions f θ (Mix λ (uj, u k )) ≈ Mix λ (f θ (uj), f θ (u k )), and correct predictions for labeled examples xi.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>The Interpolation Consistency Training (ICT) Algorithm Require: f θ (x): neural network with trainable parameters θ Require: f θ (x) mean teacher with θ equal to moving average of θ Require: D L (x, y): collection of the labeled samples Require: D U L (x): collection of the unlabeled samples Require: α: rate of moving average Require: w(t): ramp function for increasing the importance of consistency regularization Require: T : total number of iterations Require: Q: random distribution on [0,1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) High confidence value = 0.4817 (b) Low confidence value = 0.2961 Numerical validation of the theoretical prediction that ICT performs well when the confidence value 1 n n i=1 | 1 2 − f θ (ui)| is high, because that is when ICT acts as a regularizer on directional derivatives of all orders. Each line in each subplot shows the decision boundary of the predictor f θ (i.e., {u : f θ (u) = 1 2 ) after each update of 1, 10, 100, and 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 3 .</head><label>3</label><figDesc>Fix FŜ ,τ . Define CS = inf f ∈FŜ ,τ min u∈S |f (u) − 1/2| and S * S,Ŝ = S ⊆Ŝ : R S,S &lt; 1, τS &lt; CS (1−R S,S ) R S,S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>y [ 01 (f (x), y)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>f (x), y) at labeled points approaches E x,y [ 01 (f (x), y)] in the rate of O( ln(1/δ)/m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Error rates (%) on CIFAR-10 using CNN-13 architecture. We ran three trials for ICT.</figDesc><table><row><cell>Model</cell><cell>1000 labeled 50000 unlabeled</cell><cell>2000 labeled 50000 unlabeled</cell><cell>4000 labeled 50000 unlabeled</cell></row><row><cell>Supervised</cell><cell>39.95 ± 0.75</cell><cell>31.16 ± 0.66</cell><cell>21.75 ± 0.46</cell></row><row><cell>Supervised (Mixup)</cell><cell>36.48 ± 0.15</cell><cell>26.24 ± 0.46</cell><cell>19.67 ± 0.16</cell></row><row><cell>Supervised (Manifold Mixup)</cell><cell>34.58 ± 0.37</cell><cell>25.12 ± 0.52</cell><cell>18.59 ± 0.18</cell></row><row><cell>Π model (Laine &amp; Aila, 2016)</cell><cell>31.65 ± 1.20</cell><cell>17.57 ± 0.44</cell><cell>12.36 ± 0.31</cell></row><row><cell>TempEns (Laine &amp; Aila, 2016)</cell><cell>23.31 ± 1.01</cell><cell>15.64 ± 0.39</cell><cell>12.16 ± 0.24</cell></row><row><cell>MT</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Model</cell><cell>250 labeled 73257 unlabeled</cell><cell>500 labeled 73257 unlabeled</cell><cell>1000 labeled 73257 unlabeled</cell></row><row><cell>Supervised</cell><cell>40.62 ± 0.95</cell><cell>22.93 ± 0.67</cell><cell>15.54 ± 0.61</cell></row><row><cell>Supervised (Mixup)</cell><cell>33.73 ± 1.79</cell><cell>21.08 ± 0.61</cell><cell>13.70 ± 0.47</cell></row><row><cell>Supervised ( Manifold Mixup)</cell><cell>31.75 ± 1.39</cell><cell>20.57 ± 0.63</cell><cell>13.07 ± 0.53</cell></row><row><cell>Π model (Laine &amp; Aila, 2016)</cell><cell>9.93 ± 1.15</cell><cell>6.65 ± 0.53</cell><cell>4.82 ± 0.17</cell></row><row><cell>TempEns (Laine &amp; Aila, 2016)</cell><cell>12.62 ± 2.91</cell><cell>5.12 ± 0.13</cell><cell>4.42 ± 0.16</cell></row><row><cell>MT (Tarvainen &amp; Valpola, 2017)</cell><cell>4.35 ± 0.50</cell><cell>4.18 ± 0.27</cell><cell>3.95 ± 0.19</cell></row><row><cell>VAT (Miyato et al., 2018)</cell><cell>-</cell><cell>-</cell><cell>5.42 ± NA</cell></row><row><cell>VAT+Ent (Miyato et al., 2018)</cell><cell>-</cell><cell>-</cell><cell>3.86 ± NA</cell></row><row><cell>VAdD (Park et al., 2018)</cell><cell>-</cell><cell>-</cell><cell>4.16 ± 0.08</cell></row><row><cell>SNTG (Luo et al., 2018)</cell><cell>4.29 ± 0.23</cell><cell>3.99 ± 0.24</cell><cell>3.86 ± 0.27</cell></row><row><cell>ICT</cell><cell>4.78 ± 0.68</cell><cell>4.23 ± 0.15</cell><cell>3.89 ± 0.04</cell></row></table><note>Error rates (%) on SVHN using CNN-13 architecture. We ran three trials for ICT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on CIFAR10 (4000 labels) and SVHN (1000 labels) (in test error %). All results use the same standardized architecture (WideResNet-28-2). Each experiment was run for three trials. † refers to the results reported in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and 2 for CIFAR10(4000 labels) and SVHN(1000 labels)</figDesc><table><row><cell></cell><cell>CIFAR10</cell><cell>SVHN</cell></row><row><cell>SSL Approach</cell><cell>4000 labeled</cell><cell>1000 labeled</cell></row><row><cell></cell><cell>50000 unlabeled</cell><cell>73257 unlabeled</cell></row><row><cell>Supervised  †</cell><cell>20.26 ± 0.38</cell><cell>12.83 ± 0.47</cell></row><row><cell>Mean-Teacher  †</cell><cell>15.87 ± 0.28</cell><cell>5.65 ± 0.47</cell></row><row><cell>VAT  †</cell><cell>13.86 ± 0.27</cell><cell>5.63 ± 0.20</cell></row><row><cell>VAT-EM  †</cell><cell>13.13 ± 0.39</cell><cell>5.35 ± 0.19</cell></row><row><cell>ICT</cell><cell>7.66 ± 0.17</cell><cell>3.53 ± 0.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>used in ICT does not incur any significant computation cost, one might argue that a more direct comparison with Π-model, VAT and VAdD methods requires not using a mean-teacher. To this end, we conduct an experiment on the CIFAR10 dataset, without the mean-teacher in ICT, i.e. the prediction on the unlabeled data comes from the network f θ (x) instead of the mean-teacher network f θ (x) in Equation 1. We obtain test errors of 19.56 ± 0.56%, 14.35 ± 0.15% and 11.19 ± 0.14% for 1000, 2000, 4000 labeled samples respectively (We did not conduct any hyperparameter search for these experiments and used the best hyperparameters found in the ICT experiments of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>). Will the performance of ICT be significantly reduced by not having the mixup supervised loss? We conducted experiments with ICT on both CIFAR10 and SVHN with the vanilla supervised loss. For CIFAR10, we obtained test errors of 14.86 ± 0.39, 9.02 ± 0.12 and 8.23 ± 0.22 for 1000, 2000 and 4000 labeled samples respectively. We did not conduct any hyperparameter search and used the best values of hyperparameters (max-consistency coefficient and α) found in the experiments of theTable 1. We observe that in the case of 1000 and 2000 labeled samples, there is no increase in the test error (w.r.t having the mixup supervised loss), whereas in the case of 4000 labels, the test error increases by approximately 1% . This suggests that, in the low labeled data regimes, not having the mixup supervised loss in the ICT does not incur any significant increase in the test error.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">= E ξ i∈I S,S t j∈I S,S t ξ i ξ j = i∈I S,S t j∈I S,S t E ξ [ξ i ξ j ]</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">= E ξ i∈I S,S t j∈I S,S t ξ i ξ j ((x i − uS t ) ⊗k ) (x j − uS t ) ⊗k ≤ i∈I S,S t j∈I S,S t E ξ [ξ i ξ j ]((x i − uS t ) ⊗k ) (x j − uS t ) ⊗k ≤ i∈I S,S t E ξ [ξ 2 i ] (x i − uS t ) ⊗k 2 2where the last line follows the fact that Rademacher variables ξ 1 , . . . , ξ m are independent. Since E ξ [ξ 2 i ] = 1, using Lemma 1,E ξ i∈I S,S t ξ i (x i − uS t ) ⊗k 2 ≤ i∈I S,S t (x i − uS t ) ⊗k 2 2 = i∈I S,S t ( x i − uS t 2 ) 2k ≤ R k S,S |I S,S t |(21)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">= i∈I S,S t j∈I S,S t E ξ [ξ i ξ j ] = i∈I S,S t E ξ [ξ 2 i ] = |I S,S t | Therefore,R m ( 01 • FS) ≤ 1 2m |S| t=1 |I S,S t |.SinceS ⊆Ŝ was arbitrary such that R S,S &lt; 1 and τS &lt;CS (1−R S,S ) R S,S , this inequality holds for anyS such that R S,S &lt; 1 and τS &lt; CS (1−R S,S ) R S,S , yieldinĝ R m ( 01 • FS) ≤ 1 2m min S∈S * S,Ŝ |S| t=1 |I S,S t |.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Vikas Verma was supported by Academy of Finland project 13312683. We would also like to acknowledge Compute Canada for providing computing resources used in this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Additional Lemmas</head><p>The following lemmas are used in the proof of our theorems. Their proofs directly follows from those from previous works <ref type="bibr" target="#b1">(Bartlett &amp; Mendelson, 2002;</ref><ref type="bibr" target="#b15">Mohri et al., 2012;</ref><ref type="bibr" target="#b14">Mohri &amp; Medina, 2014)</ref>.</p><p>Lemma 4. Let FŜ be a set of maps x → f (x) that depends on an unlabeled datasetŜ. Let q → (q, y) be a C-uniformly bounded function for any q ∈ {f (x) : f ∈ FŜ, x ∈ X } and y ∈ Y. Then, for any δ &gt; 0, with probability at least 1 − δ over an i.i.d. draw of m samples ((x i , y i )) m i=1 , the following holds:</p><p>Proof. Since changing one point in S changesR m (FŜ) by at most C/m, McDiarmid's inequality implies the statement of this lemma.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">There are many consistent explanations of unlabeled data: Why you should average</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkgKBhA5Y7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rademacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding and improving interpolation in autoencoders via an adversarial regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1fQSiCcYm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<publisher>The MIT Press</publisher>
			<biblScope unit="page">9780262514125</biblScope>
		</imprint>
	</monogr>
	<note>1st edition, 2010. ISBN 0262514125</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep learning for classical japanese literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01718</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarially learned inference. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno>abs/1610.02242</idno>
		<ptr target="http://arxiv.org/abs/1610.02242" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Manifold regularization with gans for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lecouat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04307</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Talagrand</surname></persName>
		</author>
		<title level="m">Banach Spaces: isoperimetry and processes</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">SGDR: stochastic gradient descent with restarts. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1608.03983</idno>
		<ptr target="http://arxiv.org/abs/1608.03983" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Smooth neighbors on teacher graphs for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="8896" to="8905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning theory and algorithms for revenue optimization in second price auctions with reserve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Medina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Foundations of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial robustness may be at odds with simplicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakkiran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00532</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Realistic Evaluation of Deep Semi-Supervised Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adversarial dropout for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-C</forename><surname>Moon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<idno>978-1-5108-3881-9</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=3157096.3157227" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A framework for structural risk minimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="DOI">10.1145/238061.238070</idno>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page" from="68" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Between-class learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<title level="m">Robustness may be at odds with accuracy. stat</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Manifold</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05236</idno>
		<title level="m">Better Representations by Interpolating Hidden States. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.30.87</idno>
		<ptr target="https://dx.doi.org/10.5244/C.30.87" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<editor>Richard C. Wilson, E. R. H. and Smith, W. A. P.</editor>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixup</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
